{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b7ca14-d87b-4b16-8ada-608833a5c2e2",
   "metadata": {},
   "source": [
    "### Normalización de palabras\n",
    "\n",
    "La **normalización de palabras** es el proceso de estandarizar palabras o tokens para que sigan un formato uniforme. Un ejemplo común es el **case folding**, que consiste en convertir todas las letras a minúsculas. Este enfoque permite tratar términos como \"Woodchuck\" y \"woodchuck\" como idénticos, lo cual es particularmente útil en tareas como la recuperación de información o el reconocimiento de voz.\n",
    "\n",
    "Sin embargo, en tareas como el análisis de sentimientos, la clasificación de textos, la extracción de información y la traducción automática, la distinción entre mayúsculas y minúsculas puede ser crucial. Por ejemplo, diferenciar entre \"US\" (Estados Unidos) y \"us\" (nosotros) puede ser más importante que la simplificación proporcionada por el **case folding**. Por ello, en estos contextos, suele preferirse conservar la distinción entre mayúsculas y minúsculas. En algunos casos, se generan tanto versiones diferenciadas como versiones completamente en minúsculas para modelos de lenguaje, dependiendo de la tarea específica.\n",
    "\n",
    "Los sistemas que emplean **BPE** u otros métodos de tokenización bottom-up pueden prescindir de una normalización adicional de las palabras. Sin embargo, en otros sistemas de **NLP**, puede ser beneficioso aplicar normalizaciones más avanzadas, como unificar diferentes formas de una palabra, por ejemplo, \"USA\" y \"US\" o \"uh-huh\" y \"uhhuh\". Aunque esta estandarización puede implicar la pérdida de cierta información ortográfica, puede resultar valiosa en algunos casos.\n",
    "\n",
    "Por ejemplo, en la recuperación o extracción de información relacionada con \"US.\", podríamos querer identificar datos relevantes tanto si el documento menciona \"US\" como \"USA\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008113a6-9bb2-41a6-bcf9-fb3dde3f33c7",
   "metadata": {},
   "source": [
    "### Lematización\n",
    "\n",
    "En procesamiento de lenguaje natural, a menudo es deseable que dos formas morfológicamente diferentes de una palabra se traten como equivalentes. Por ejemplo, en una búsqueda web, un usuario podría escribir \"woodchucks\", pero un sistema eficiente debería ser capaz de devolver páginas que mencionen \"woodchuck\" sin la \"s\". Esto es especialmente relevante en idiomas morfológicamente complejos como el polaco, donde una palabra puede cambiar de forma según su función en la oración. Por ejemplo, \"Warsaw\" toma diferentes terminaciones según el caso gramatical: **\"Warszawa\"** como sujeto, **\"w Warszawie\"** para \"en Varsovia\" o **\"do Warszawy\"** para \"hacia Varsovia\", entre otras variantes.\n",
    "\n",
    "La **lematización** es el proceso de reducir las palabras a su forma base o **lema**, independientemente de sus variaciones morfológicas. Por ejemplo, \"am\", \"are\" e \"is\" tienen como lema común \"be\", mientras que \"dinner\" y \"dinners\" comparten el lema \"dinner\". Lematizar correctamente ayuda a agrupar menciones de palabras en diferentes formas, como en el caso de \"Warsaw\" en polaco. La forma lematizada de una oración como **\"He is reading detective stories\"** sería **\"He be read detective story\"**, eliminando la flexión verbal y el plural.\n",
    "\n",
    "Los métodos más sofisticados para la lematización implican un análisis morfológico detallado. La **morfología** es el estudio de la estructura de las palabras y cómo estas se forman a partir de unidades más pequeñas con significado, llamadas **morfemas**. Existen dos tipos principales de morfemas: \n",
    "- **Stems**: el núcleo de la palabra, que aporta el significado principal.\n",
    "- **Affixes**: elementos adicionales que modifican el significado o la función de la palabra.\n",
    "\n",
    "Por ejemplo, la palabra **\"fox\"** consta de un solo morfema (\"fox\"), mientras que **\"cats\"** se compone de dos: el morfema raíz **\"cat\"** y el sufijo **\"-s\"** que indica plural. Un **analizador morfológico** (morphological parser) descompone palabras como \"cats\" en estos morfemas, facilitando tareas como la lematización y la comprensión del significado en NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafd45d-e3c3-4658-90f2-dc7fe7df9575",
   "metadata": {},
   "source": [
    "#### Stemming: The Porter Stemmer\n",
    "\n",
    "Los algoritmos de **lematización** pueden ser complejos y computacionalmente costosos. Por esta razón, en algunos casos se emplea un método más simple basado en la eliminación de afijos finales de las palabras, conocido como **stemming**. A diferencia de la lematización, el stemming no busca identificar la forma base de la palabra con precisión gramatical, sino simplemente recortar sufijos comunes. \n",
    "\n",
    "Un ejemplo clásico es el **Porter Stemmer**, que cuando se aplica al siguiente párrafo:\n",
    "\n",
    "```plaintext\n",
    "This was not the map we found in Billy Bones’s chest, but\n",
    "an accurate copy, complete in all things—names and heights\n",
    "and soundings—with the single exception of the red crosses\n",
    "and the written notes.\n",
    "```\n",
    "\n",
    "produce la siguiente salida:\n",
    "\n",
    "```plaintext\n",
    "Thi wa not the map we found in Billi Bone s chest but an\n",
    "accur copi complet in all thing name and height and sound\n",
    "with the singl except of the red cross and the written note\n",
    "```\n",
    "\n",
    "El algoritmo se basa en una serie de reglas de reescritura que se aplican en secuencia, donde la salida de cada paso se convierte en la entrada del siguiente.\n",
    "\n",
    "Algunas reglas de muestra (más en [https://tartarus.org/martin/PorterStemmer/](https://tartarus.org/martin/PorterStemmer/)):\n",
    "\n",
    "| **Regla**        | **Ejemplo**                        |\n",
    "|------------------|------------------------------------|\n",
    "| **ATIONAL → ATE**| (relational → relate)              |\n",
    "| **SSES → SS**    | (grasses → grass)                  |\n",
    "\n",
    "Los **stemmers** simples pueden ser útiles en situaciones donde es necesario agrupar diferentes variantes de una palabra bajo una forma común. Sin embargo, su uso en sistemas modernos ha disminuido, ya que pueden cometer errores tanto de **sobregeneralización** (por ejemplo, reduciendo *policy* a *police*) como de **subgeneralización** (no reduciendo *European* a *Europe*). Debido a estas limitaciones, en muchas aplicaciones de NLP se prefiere la lematización, que proporciona resultados más precisos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61906511-9d65-42cc-9ffb-097b01334704",
   "metadata": {},
   "source": [
    "#### Segmentación de oraciones\n",
    "\n",
    "La **segmentación de oraciones** es un paso fundamental en el procesamiento de texto. Las señales más útiles para dividir un texto en oraciones son los signos de puntuación, como los puntos, signos de interrogación y signos de exclamación. Los signos de interrogación y exclamación suelen ser indicadores claros de los límites de una oración. Sin embargo, los puntos (**\".\"**) pueden ser más ambiguos, ya que pueden marcar el final de una oración o formar parte de abreviaciones como *\"Mr.\"* o *\"Inc.\"*. Debido a esta ambigüedad, la segmentación de oraciones y la tokenización de palabras a menudo se abordan conjuntamente.\n",
    "\n",
    "En general, los métodos de segmentación de oraciones funcionan determinando, ya sea mediante reglas o aprendizaje automático, si un punto actúa como un marcador de fin de oración o si forma parte de una palabra. Un diccionario de abreviaciones puede ser útil para identificar si un punto pertenece a una abreviatura de uso común. Estos diccionarios pueden construirse manualmente o aprenderse mediante técnicas de machine learning, al igual que los modelos que realizan la segmentación de oraciones.\n",
    "\n",
    "Por ejemplo, en el **[Stanford CoreNLP toolkit](https://stanfordnlp.github.io/CoreNLP/)**, la segmentación de oraciones se basa en reglas y es una consecuencia determinista de la tokenización. Una oración termina cuando una puntuación de final de oración (*\".\", \"!\", \"?\"*) no está agrupada con otros caracteres dentro de un solo token (como en el caso de abreviaturas o números), y puede estar seguida opcionalmente por comillas finales o corchetes adicionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854804be",
   "metadata": {},
   "source": [
    "Normalización de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c338e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Folding\n",
    "def normalize_case(text):\n",
    "    \"\"\"Convierte todo el texto a minúsculas para la normalización.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"Woodchuck\"\n",
    "normalized_text = normalize_case(texto)\n",
    "print(normalized_text)  # Salida: \"woodchuck\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6204d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unify_terms(texto):\n",
    "    \"\"\"Reemplaza diferentes variantes de un término por una versión estándar.\"\"\"\n",
    "    term_mapping = {\n",
    "        \"USA\": \"US\",\n",
    "        \"uh-huh\": \"uhhuh\"\n",
    "    }\n",
    "    words = texto.split()\n",
    "    unified_words = [term_mapping.get(word, word) for word in words]\n",
    "    return ' '.join(unified_words)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"The USA has many dialects. Uh-huh, that's true.\"\n",
    "unified_text = unify_terms(texto)\n",
    "print(unified_text)  # Salida: \"The US has many dialects. uhhuh, that's true.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427734cf",
   "metadata": {},
   "source": [
    "Lematización usando nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def lemmatize_text(texto):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = texto.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"He is reading detective stories\"\n",
    "lemmatized_text = lemmatize_text(texto)\n",
    "print(lemmatized_text)  # Salida: \"He be read detective story\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3c847",
   "metadata": {},
   "source": [
    "Stemming usando el Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ae857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "\n",
    "def stem_text(texto):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = nltk.word_tokenize(texto)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"\"\"\n",
    "This was not the map we found in Billy Bones’s chest, but\n",
    "an accurate copy, complete in all things-names and heights\n",
    "and soundings-with the single exception of the red crosses\n",
    "and the written notes.\n",
    "\"\"\"\n",
    "stemmed_text = stem_text(texto)\n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88018131",
   "metadata": {},
   "source": [
    "Segmentación de oraciones usando nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def sentence_tokenize(texto):\n",
    "    return nltk.sent_tokenize(texto)\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"This is a sentence. This is another sentence! Is this the third one?\"\n",
    "sentences = sentence_tokenize(texto)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5899eed-ed22-4a9b-9101-c7e3aa8adeac",
   "metadata": {},
   "source": [
    "#### Normalización en modelos de lenguaje (LLM)\n",
    "\n",
    "Los modelos de lenguaje a gran escala (LLM) requieren, como parte de su preprocesamiento, una tokenización y normalización que asegure la coherencia y el aprovechamiento de patrones lingüísticos. Aunque los sistemas basados en **BPE (Byte-Pair Encoding)** o en otras técnicas de tokenización bottom-up pueden prescindir de una normalización adicional, en algunos contextos se opta por aplicar transformaciones adicionales que unifiquen la representación de los datos.\n",
    "\n",
    "##### **Tokenización y normalización en LLM**\n",
    "\n",
    "En modelos de lenguaje, la **tokenización** es el primer paso para transformar el texto en una secuencia de tokens que pueda ser procesada por la red neuronal. La tokenización puede implicar desde la simple división en palabras hasta la descomposición en subpalabras o caracteres. Durante este proceso, la normalización puede involucrar pasos como el case folding, la eliminación de caracteres especiales o la unificación de ciertas variantes.\n",
    "\n",
    "El uso de algoritmos de tokenización avanzados, como BPE, permite a los modelos manejar vocabularios extensos sin necesidad de tener una lista fija de palabras. Estos métodos identifican las subunidades más frecuentes y las utilizan para representar palabras completas. Sin embargo, cuando se emplean modelos preentrenados, es habitual que el texto de entrada ya haya pasado por un proceso de normalización estándar que asegure la consistencia en la representación de las palabras.\n",
    "\n",
    "##### **Otros métodos de normalización en LLM**\n",
    "\n",
    "Aunque la normalización por case folding y la unificación de términos son comunes, en entornos LLM también se aplican otros procesos como:\n",
    "\n",
    "- **Eliminación o transformación de caracteres especiales:**  \n",
    "  Algunos sistemas eliminan o convierten caracteres no alfabéticos para evitar que elementos como emoticonos, símbolos o puntuaciones excesivas alteren el análisis.\n",
    "  \n",
    "- **Normalización Unicode:**  \n",
    "  En textos multilingües, es crucial normalizar las representaciones Unicode para que las variantes gráficas de un mismo carácter sean interpretadas de forma idéntica.\n",
    "\n",
    "- **Segmentación en subpalabras:**  \n",
    "  La tokenización en subpalabras permite que incluso palabras desconocidas se representen a partir de combinaciones de unidades más pequeñas, lo que mejora la capacidad del modelo para generalizar.\n",
    "\n",
    "Estos procesos no solo optimizan la representación del lenguaje, sino que además permiten a los modelos de lenguaje captar mejor patrones y relaciones semánticas, lo cual es fundamental para tareas complejas como la traducción automática, la generación de texto o la respuesta a preguntas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5eb3b5-355f-4dd7-ba0a-4508e818219b",
   "metadata": {},
   "source": [
    "#### Otras formas de normalización y análisis morfológico\n",
    "\n",
    "Además de los métodos ya descritos, existen otras estrategias de normalización que se utilizan en el procesamiento de texto. Entre ellas destaca la unificación de variantes ortográficas o la transformación de diferentes formas de una misma palabra a una forma canónica. Este proceso, aunque puede implicar la pérdida de información específica (como la distinción entre \"US\" y \"USA\"), es útil para tareas en las que se desea agrupar información similar.\n",
    "\n",
    "El análisis morfológico estudia la forma en la que se construyen las palabras a partir de **morfemas**, que son las unidades mínimas con significado. Se distinguen dos tipos principales:\n",
    "\n",
    "- **Stems (raíces):** La parte central de la palabra que contiene el significado principal.  \n",
    "- **Affixes (afijos):** Elementos que se añaden a la raíz para modificar o matizar su significado (como sufijos o prefijos).\n",
    "\n",
    "Un ejemplo simple es la palabra \"cats\", que se compone del stem \"cat\" y el sufijo \"-s\". Un **morphological parser** descompone la palabra en estas partes, permitiendo identificar la raíz y aplicar procesos de normalización o lematización de manera más precisa.\n",
    "\n",
    "El análisis morfológico no se limita únicamente a la lematización y el stemming, sino que también puede incluir procesos que separan explícitamente los componentes de una palabra. Esto es especialmente útil en lenguajes con alta morfología. \n",
    "\n",
    "Por ejemplo, en el procesamiento de la palabra \"Jakonmax\", un sistema avanzado podría identificar dos componentes: la raíz \"Jakon\" y el afijo \"max\". De forma similar, en \"Raiokon2, se distinguirían la raíz \"Raio\" y el afijo \"kon\". La representación de estos componentes podría organizarse en un diccionario o estructura de datos que relacione cada palabra con su raíz y sus afijos, como se muestra a continuación:\n",
    "\n",
    "```plaintext\n",
    "{'Jakonmax': {'raíz': 'Jakon', 'afijo': 'max'}, 'Raiokon': {'raíz': 'Raio', 'afijo': 'kon'}}\n",
    "```\n",
    "Este ejemplo ilustra la dirección en la que se pueden extender los procesos de normalización para manejar de manera más detallada la estructura interna de las palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f79f0-b6fe-4541-b08e-5f6cb08324de",
   "metadata": {},
   "source": [
    "#### Integración de conceptos\n",
    "\n",
    "En la práctica, la combinación de normalización, lematización, stemming y segmentación de oraciones conforma la base del preprocesamiento de datos para una gran variedad de aplicaciones de NLP. Este conjunto de técnicas permite:\n",
    "\n",
    "- **Reducir la dimensionalidad del vocabulario:**  \n",
    "  Al normalizar y unificar palabras, se evita que variaciones menores influyan en el desempeño de los modelos de búsqueda o clasificación. Por ejemplo, convertir \"Woodchuck\" a \"woodchuck\" o unificar \"USA\" a \"US\" reduce la cantidad de tokens diferentes a procesar.\n",
    "\n",
    "- **Mejorar la precisión de la búsqueda y recuperación de información:**  \n",
    "  Cuando un sistema de recuperación de información enfrenta consultas, la normalización asegura que se recuperen documentos relevantes aunque existan variaciones en la escritura de los términos. La lematización permite agrupar formas verbales y nominales, incrementando la tasa de acierto en las respuestas.\n",
    "\n",
    "- **Optimización en modelos de lenguaje a gran escala (LLM):**  \n",
    "  En los LLM, la calidad del preprocesamiento es determinante para el aprendizaje de patrones. La tokenización, combinada con técnicas de normalización, garantiza que el modelo reciba información homogénea, lo que mejora la capacidad de generalización y reduce la complejidad en el entrenamiento.\n",
    "\n",
    "- **Manejo de idiomas morfológicamente complejos:**  \n",
    "  Idiomas como el polaco o el ruso presentan una gran cantidad de variaciones morfológicas. La lematización y el análisis morfológico permiten tratar estas variantes de forma coherente, facilitando la construcción de sistemas multilingües y el análisis semántico.\n",
    "\n",
    "- **Facilitación de análisis sintácticos y semánticos:**  \n",
    "  La segmentación de oraciones es un paso esencial para descomponer un texto en unidades manejables, lo que permite aplicar modelos de análisis sintáctico y semántico de manera más efectiva. Cada oración segmentada se puede analizar individualmente, lo que resulta en una mejor identificación de patrones y relaciones dentro del texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01359a-784c-4f24-a177-2c85110b7a50",
   "metadata": {},
   "source": [
    "#### Aplicaciones avanzadas\n",
    "\n",
    "Si bien las técnicas descritas previamente son ampliamente utilizadas, en entornos más avanzados se han desarrollado métodos que combinan estas estrategias con técnicas de aprendizaje automático. Por ejemplo, algunos sistemas modernos de NLP utilizan redes neuronales para aprender directamente la mejor forma de representar palabras a partir de grandes corpus de datos. Estas representaciones, conocidas como *embeddings*, pueden incorporar información sobre la sintaxis, semántica y contexto en el que aparece cada palabra, lo que a su vez mejora la calidad de tareas como la traducción automática o la generación de texto.\n",
    "\n",
    "##### **Normalización y embeddings**\n",
    "\n",
    "En la generación de *word embeddings* o *subword embeddings*, la normalización previa es fundamental para reducir el ruido y garantizar que el modelo no distinga entre variantes irrelevantes. La eliminación de diferencias de mayúsculas y minúsculas, la unificación de términos y la segmentación en subpalabras contribuyen a obtener representaciones vectoriales más robustas y generalizables.\n",
    "\n",
    "##### **Normalización en el contexto de LLM**\n",
    "\n",
    "En los modelos de lenguaje de gran escala, la normalización es parte integral del proceso de tokenización. Los algoritmos modernos de tokenización, como el BPE, realizan un análisis estadístico de las frecuencias de subpalabras y aprenden una representación que minimiza la cantidad de tokens necesarios para representar un texto. Aunque estos métodos pueden reducir la necesidad de normalizaciones \"manuales\", en muchos casos se aplican transformaciones adicionales (por ejemplo, convertir todo el texto a minúsculas) para asegurar la coherencia en el entrenamiento del modelo.\n",
    "\n",
    "##### **Consideraciones sobre errores en el preprocesamiento**\n",
    "\n",
    "A pesar de los beneficios, es importante tener en cuenta que las técnicas de normalización pueden introducir errores. Por ejemplo, el stemming puede llevar a sobregeneralizaciones, mientras que la lematización, aunque más precisa, puede requerir mayor poder computacional y depender de recursos externos que deben actualizarse periódicamente. La elección entre lematización y stemming dependerá del equilibrio deseado entre precisión y velocidad, así como del tipo de tarea que se esté abordando."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb29ebe",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "**Ejercicio 1: Normalización de palabras en español y Quechua**\n",
    "\n",
    "1. **Objetivo:** Implementar un programa en Python que convierta las palabras de un texto dado a minúsculas y unifique ciertas variantes de términos tanto en español como en quechua.\n",
    "   \n",
    "2. **Descripción:** \n",
    "   - Escribe una función que normalice el caso (convierta todo a minúsculas).\n",
    "   - Implementa una función que unifique palabras comunes con variantes ortográficas en español y quechua. Por ejemplo, \"Kusikuy\" (alegrarse) y \"Kusi\" (alegría) podrían normalizarse a \"kusikuy\".\n",
    "   - Procesa un texto que incluya palabras tanto en español como en quechua.\n",
    "\n",
    "3. **Texto de ejemplo:**\n",
    "   ```plaintext\n",
    "   Soy muy Kusikuy de aprender Quechua. Kusi es muy importante en mi vida.\n",
    "   ```\n",
    "\n",
    "4. **Resultado esperado:**\n",
    "   ```plaintext\n",
    "   soy muy kusikuy de aprender quechua. kusikuy es muy importante en mi vida.\n",
    "   ```\n",
    "\n",
    "**Ejercicio 2: Lematización en Shipibo-Konibo y Ashaninka**\n",
    "\n",
    "1. **Objetivo:** Crear un lematizador sencillo que identifique y reduzca diferentes formas morfológicas de palabras en Shipibo-Konibo y Ashaninka a su lema base.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Investiga algunas palabras comunes en Shipibo-Konibo y Ashaninka que tengan variaciones morfológicas.\n",
    "   - Escribe un script en Python que lematice estas palabras en sus formas básicas.\n",
    "   - Procesa un pequeño texto en Shipibo-Konibo y otro en Ashaninka.\n",
    "\n",
    "3. **Texto de ejemplo:**\n",
    "   - **Shipibo-Konibo:** `Metsa ikábo bakebo.`\n",
    "   - **Ashaninka:** `Jokiro anampitsi onkenero.`\n",
    "   \n",
    "4. **Resultado esperado:**\n",
    "   - **Shipibo-Konibo:** `Metsa iká bake.`\n",
    "   - **Ashaninka:** `Jokiro anampi onke.`\n",
    "   \n",
    "**Ejercicio 3: Stemming en Yine**\n",
    "\n",
    "1. **Objetivo:** Aplicar un algoritmo de stemming a un texto en Yine para reducir las palabras a sus raíces.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Crea una lista de palabras comunes en Yine con sus variantes.\n",
    "   - Implementa un algoritmo simple de stemming que elimine sufijos comunes en Yine.\n",
    "   - Aplica el algoritmo a un texto dado en Yine.\n",
    "\n",
    "3. **Texto de ejemplo:**\n",
    "   ```plaintext\n",
    "   Ichikire irako jintika iya.\n",
    "   ```\n",
    "\n",
    "4. **Resultado esperado:**\n",
    "   ```plaintext\n",
    "   Ichikir irak jintik iya.\n",
    "   ```\n",
    "\n",
    "**Ejercicio 4: Segmentación de oraciones multilingües**\n",
    "\n",
    "1. **Objetivo:** Implementar un segmentador de oraciones que funcione para textos en español, quechua, shipibo-konibo, ashaninka, y yine.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Escribe un programa en Python que pueda identificar y segmentar oraciones en los diferentes idiomas mencionados.\n",
    "   - Considera las diferentes formas en que se pueden estructurar las oraciones en estos idiomas y cómo los signos de puntuación pueden variar.\n",
    "\n",
    "3. **Texto de ejemplo:**\n",
    "   ```plaintext\n",
    "   Hablo español. Noqa rimani runasimita. Metsa iroake. Jokiro anampitsi.\n",
    "   ```\n",
    "\n",
    "4. **Resultado esperado:**\n",
    "   ```plaintext\n",
    "   ['Hablo español.', 'Noqa rimani runasimita.', 'Metsa iroake.', 'Jokiro anampitsi.']\n",
    "   ```\n",
    "\n",
    "**Ejercicio 5: Unificación de términos multilingües**\n",
    "\n",
    "1. **Objetivo:** Desarrollar un sistema de unificación de términos que trabaje con textos multilingües, particularmente en español, quechua, shipibo-konibo, ashaninka, y yine.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Escribe una función que tome un texto y reemplace palabras y expresiones que sean variantes de un mismo concepto, independientemente del idioma.\n",
    "   - Por ejemplo, unifica \"Sol\", \"Inti\" (quechua), y \"Shiro\" (shipibo-konibo) bajo un término común.\n",
    "\n",
    "3. **Texto de ejemplo:**\n",
    "   ```plaintext\n",
    "   El Inti brilla en el cielo. Shiro es el dios del sol. El Sol es poderoso.\n",
    "   ```\n",
    "\n",
    "4. **Resultado esperado:**\n",
    "   ```plaintext\n",
    "   El Sol brilla en el cielo. Sol es el dios del sol. El Sol es poderoso.\n",
    "   ```\n",
    "\n",
    "Claro, aquí tienes algunos ejercicios adicionales que complementan los anteriores y profundizan en el procesamiento de textos en español, quechua, shipibo-konibo, ashaninka, y yine.\n",
    "\n",
    "\n",
    "**Ejercicio 6: Traducción automática básica entre español y Quechua**\n",
    "\n",
    "1. **Objetivo:** Desarrollar un sistema básico de traducción automática entre español y quechua utilizando reglas simples.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Implementa un diccionario bilingüe básico que contenga palabras y frases comunes en español y quechua.\n",
    "   - Crea una función en Python que traduzca un texto en español a quechua utilizando este diccionario.\n",
    "   - Considera reglas simples de concordancia y orden de palabras.\n",
    "\n",
    "3. **Texto de ejemplo:**\n",
    "   ```plaintext\n",
    "   Mi nombre es Juan. Vivo en Cusco. Me gusta la comida.\n",
    "   ```\n",
    "\n",
    "4. **Resultado esperado:**\n",
    "   ```plaintext\n",
    "   Noqaq sutiyqa Juanmi. Cuscomanta kani. Mikhuyta munani.\n",
    "   ```\n",
    "\n",
    "**Ejercicio 7: Detección de lengua en textos multilingües**\n",
    "\n",
    "1. **Objetivo:** Implementar un algoritmo para detectar el idioma de una oración o palabra en un texto que contenga español, quechua, shipibo-konibo, ashaninka, y yine.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Crea un script que tome como entrada un texto y determine en qué idioma está cada palabra u oración.\n",
    "   - Puedes utilizar técnicas de frecuencia de palabras, características de ortografía, o un diccionario para la detección.\n",
    "\n",
    "3. **Texto de ejemplo:**\n",
    "   ```plaintext\n",
    "   Runasimipi rimayta yachani. Me gusta aprender nuevas lenguas.\n",
    "   ```\n",
    "\n",
    "4. **Resultado esperado:**\n",
    "   ```plaintext\n",
    "   [\"Runasimipi rimayta yachani.\" -> Quechua, \"Me gusta aprender nuevas lenguas.\" -> Español]\n",
    "   ```\n",
    "\n",
    "**Ejercicio 8: Generación de palabras derivadas en Ashaninka y Shipibo-Konibo**\n",
    "\n",
    "1. **Objetivo:** Crear un sistema que genere automáticamente formas derivadas de una palabra raíz en ashaninka y shipibo-konibo.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Investiga cómo se forman palabras derivadas en ashaninka y shipibo-konibo (por ejemplo, mediante la adición de sufijos).\n",
    "   - Implementa un script en Python que genere formas derivadas a partir de una raíz dada.\n",
    "\n",
    "3. **Raíces de ejemplo:**\n",
    "   - **Ashaninka:** `ankotsi` (caminar) -> Derivados esperados: `ankotsineri` (caminará), `ankotsiki` (camino).\n",
    "   - **Shipibo-Konibo:** `raoma` (comer) -> Derivados esperados: `raomake` (comiendo), `raomaoki` (comeré).\n",
    "\n",
    "**Ejercicio 9: Tokenización y análisis de frecuencia de palabras en Yine**\n",
    "\n",
    "1. **Objetivo:** Desarrollar un sistema de tokenización y análisis de frecuencia de palabras para textos en Yine.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Implementa un tokenizador que divida un texto en Yine en palabras individuales.\n",
    "   - Crea una función que cuente la frecuencia de cada palabra en el texto.\n",
    "   - Genera un listado de las palabras más frecuentes.\n",
    "\n",
    "3. **Texto de ejemplo:**\n",
    "   ```plaintext\n",
    "   Ichikire irako jintika iya. Ichikire pokanka maik. Jintika jintika ichikire.\n",
    "   ```\n",
    "\n",
    "4. **Resultado esperado:**\n",
    "   ```plaintext\n",
    "   {'ichikire': 3, 'jintika': 3, 'irako': 1, 'iya': 1, 'pokanka': 1, 'maik': 1}\n",
    "   ```\n",
    "\n",
    "**Ejercicio 10: Análisis de sentimientos en textos en español y Quechua**\n",
    "\n",
    "1. **Objetivo:** Implementar un análisis de sentimientos básico para textos en español y quechua.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Crea un diccionario de palabras con polaridad positiva y negativa tanto en español como en quechua.\n",
    "   - Escribe un script en Python que tome un texto en cualquiera de los dos idiomas y determine si el sentimiento es positivo, negativo o neutral.\n",
    "   - Considera solo palabras individuales para la polaridad.\n",
    "\n",
    "3. **Texto de ejemplo:**\n",
    "   ```plaintext\n",
    "   Amo la naturaleza y su belleza. Ñuqaq sonqoyqa kusiwan phutiy.\n",
    "   ```\n",
    "\n",
    "4. **Resultado esperado:**\n",
    "   ```plaintext\n",
    "   ['Amo la naturaleza y su belleza.' -> Positivo, 'Ñuqaq sonqoyqa kusiwan phutiy.' -> Positivo]\n",
    "   ```\n",
    "\n",
    "**Ejercicio 11: Generación de oraciones aleatorias en Ashaninka**\n",
    "\n",
    "1. **Objetivo:** Desarrollar un generador de oraciones aleatorias en ashaninka utilizando un conjunto de palabras predefinido.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Define conjuntos de sustantivos, verbos, y adjetivos en ashaninka.\n",
    "   - Implementa un programa que genere oraciones simples combinando aleatoriamente estos elementos.\n",
    "   - Asegúrate de que las oraciones sean gramaticalmente correctas dentro de las reglas de ashaninka.\n",
    "\n",
    "3. **Conjunto de palabras:**\n",
    "   - **Sustantivos:** `ona (persona)`, `mapori (jefe)`, `betiro (árbol)`\n",
    "   - **Verbos:** `onketi (ver)`, `pikoti (correr)`\n",
    "   - **Adjetivos:** `itsiri (grande)`, `inanti (rápido)`\n",
    "\n",
    "4. **Ejemplo de oración generada:**\n",
    "   ```plaintext\n",
    "   Mapori itsiri pikoti.\n",
    "   ```\n",
    "\n",
    "**Ejercicio 12: Análisis morfológico en textos en Shipibo-Konibo**\n",
    "\n",
    "1. **Objetivo:** Implementar un analizador morfológico para textos en shipibo-konibo que identifique las raíces y afijos.\n",
    "\n",
    "2. **Descripción:**\n",
    "   - Investiga la estructura morfológica de palabras en shipibo-konibo.\n",
    "   - Escribe un script en Python que descomponga una palabra en sus componentes morfológicos.\n",
    "   - Procesa un conjunto de palabras para analizar su estructura.\n",
    "\n",
    "3. **Palabras de ejemplo:**\n",
    "   ```plaintext\n",
    "   Jakonmax (bueno), Raiokon (caminando)\n",
    "   ```\n",
    "\n",
    "4. **Resultado esperado:**\n",
    "   ```plaintext\n",
    "   {'Jakonmax': {'raíz': 'Jakon', 'afijo': 'max'}, 'Raiokon': {'raíz': 'Raio', 'afijo': 'kon'}}\n",
    "   ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bc28d-b614-4f40-8b28-f7034ab38934",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
