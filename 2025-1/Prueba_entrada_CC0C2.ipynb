{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f61672b-cc7f-43ff-aca0-0f56fcadf861",
   "metadata": {},
   "source": [
    "### Prueba de entrada CC0C2\n",
    "\n",
    "**Formato de la evaluación**\n",
    "\n",
    "- **Entrega**: Se sugiere un informe escrito para cada sección (o un único documento con apartados), que incluya:\n",
    "  - Planteamiento teórico y razonamiento.\n",
    "  - Descripciones de la implementación propuesta (no se exige código como tal, pero sí explicación de pasos o seudocódigo).\n",
    "  - Resultados (numéricos, analíticos y/o cualitativos).\n",
    "  - Conclusiones y reflexiones.\n",
    "\n",
    "- **Criterios de evaluación**:\n",
    "  1. Claridad y coherencia en la explicación teórica.\n",
    "  2. Profundidad en el desarrollo y justificación de las decisiones de diseño.\n",
    "  3. Rigor en el análisis de resultados y en la comparación de métodos.\n",
    "  4. Capacidad de relacionar los contenidos con los temas de NLP que se verán más adelante.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc823e0f-e98d-48bd-997b-d8568fa72a87",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "##### **Ejercicio 1**\n",
    "\n",
    "1. **Teoría de perceptrón multicapa**  \n",
    "   - Explicar el funcionamiento de un perceptrón multicapa (MLP). Incluir la descripción de capas, neuronas, funciones de activación y la importancia de la función de pérdida.\n",
    "   - Describir en detalle el procedimiento matemático de la retropropagación de errores (backpropagation) y la actualización de pesos, mostrando cómo se aplica la regla de la cadena.\n",
    "\n",
    "2. **Diseño de una red neuronal para un problema clásico**  \n",
    "   - Proponer una arquitectura de red neuronal simple (una o dos capas ocultas) para resolver un problema de clasificación binaria o multiclase en un dataset pequeño (por ejemplo, un conjunto de datos sintético de 2D o 3D).\n",
    "   - Explicar cómo se definiría la función de costo y la métrica de evaluación (accuracy, F1, etc.).\n",
    "   - Incluir un razonamiento sobre la elección de hiperparámetros (tasa de aprendizaje, número de neuronas por capa, etc.).\n",
    "   - Discutir los posibles problemas de **desvanecimiento/explosión del gradiente** y las estrategias de mitigación (inicialización, normalización, clipping de gradientes, etc.).\n",
    "\n",
    "**Entrega esperada**:  \n",
    "- Sección teórica detallada.  \n",
    "- Diseño conceptual de la red y justificación de hiperparámetros.  \n",
    "- Resultados esperados (por ejemplo, curva de pérdida vs. épocas) y conclusiones sobre desempeño y dificultad de entrenamiento.\n",
    "\n",
    "##### **Ejercicio 2**\n",
    "\n",
    "1. **Comparación de optimizers**  \n",
    "   - Describir y comparar brevemente **SGD**, **Momentum**, **Adam** y **RMSProp**.  \n",
    "   - Analizar ventajas, desventajas y contextos de uso.\n",
    "\n",
    "2. **Regularización**  \n",
    "   - Explicar distintas técnicas de regularización (L1, L2, Dropout, Batch Normalization, etc.).  \n",
    "   - Proponer cómo incluir estas técnicas en la misma red neuronal de la sección previa o en otra red de ejemplo.\n",
    "\n",
    "3. **Experimentos conceptuales**  \n",
    "   - Plantear un experimento (no se requiere código) donde se modifiquen sistemáticamente hiperparámetros como tasa de aprendizaje o porcentaje de Dropout, y se analicen los efectos esperados en la convergencia y la capacidad de generalización.\n",
    "\n",
    "**Entrega esperada**:  \n",
    "- Análisis escrito de los optimizadores y la regularización.  \n",
    "- Ejemplo ilustrativo (numérico o basado en gráficas hipotéticas) que muestre la influencia de estos métodos.  \n",
    "- Conclusiones sobre buenas prácticas y lecciones aprendidas para el entrenamiento de redes profundas.\n",
    "\n",
    "##### **Ejercicio 3**\n",
    "\n",
    "1. **Conceptos fundamentales de RL**  \n",
    "   - Definir formalmente **entorno**, **estado**, **acción**, **recompensa** y el objetivo de maximizar el retorno.  \n",
    "   - Explicar la diferencia entre **Q-learning**, **métodos de política** y **métodos actor-crítico**.\n",
    "\n",
    "2. **Diseño de un problema de RL sencillo**  \n",
    "   - Proponer un entorno simple (por ejemplo, un gridworld o un laberinto 2D) y describir:  \n",
    "     - Estados posibles.  \n",
    "     - Acciones disponibles.  \n",
    "     - Función de recompensa.  \n",
    "   - Discutir cómo se calcularían las actualizaciones de la **Q-Table** o de la **política** (dependiendo del método elegido: Q-learning vs. Policy Gradient).  \n",
    "\n",
    "3. **Desafíos y extensiones**  \n",
    "   - Mencionar las principales dificultades del RL clásico (exploración vs. explotación, dimensionalidad, etc.).  \n",
    "   - Relacionarlo con el uso de redes neuronales (Deep Q-Networks, Policy Gradients) y cómo esto se conecta más adelante con **RLHF** para alinear grandes modelos de lenguaje.\n",
    "\n",
    "**Entrega esperada**:  \n",
    "- Explicación teórica de Q-learning o Policy Gradient (elegir uno).  \n",
    "- Descripción detallada de un entorno simple y de la dinámica de entrenamiento (paso a paso).  \n",
    "- Reflexión sobre la escalabilidad de RL y su relevancia en IA/NLP avanzada.\n",
    "\n",
    "##### **Ejercicio 4**\n",
    "\n",
    "1. **Modelo de lenguaje N-grama**  \n",
    "   - Explicar brevemente qué es un modelo n-gram y cómo se calcula la **probabilidad** de una secuencia.  \n",
    "   - Estimar o razonar sobre la **perplejidad** y por qué es relevante.\n",
    "\n",
    "2. **Métodos de evaluación**  \n",
    "   - Describir las métricas clásicas para evaluar la generación de texto: **perplejidad**, **BLEU** y, opcionalmente, **ROUGE**.  \n",
    "   - Analizar fortalezas y limitaciones de estas métricas.\n",
    "\n",
    "3. **Ejemplo de evaluación simplificada**  \n",
    "   - Proponer un mini-corpus (por ejemplo, un conjunto de oraciones cortas) y describir cómo se calcularían manualmente BLEU y ROUGE comparando oraciones generadas vs. oraciones de referencia.\n",
    "\n",
    "**Entrega esperada**:  \n",
    "- Resumen conciso de la teoría de n-gramas.  \n",
    "- Explicación matemática de perplejidad y su interpretación.  \n",
    "- Demostración conceptual (con código) del cálculo de BLEU y ROUGE con ejemplos pequeños.\n",
    "\n",
    "##### **Ejercicio 5**\n",
    "\n",
    "1. **Arquitectura Seq2Seq**  \n",
    "   - Describir los módulos de un sistema seq2seq con RNN (encoder-decoder).  \n",
    "   - Explicar la diferencia entre LSTM y GRU (mecanismos internos, puertas, etc.).\n",
    "\n",
    "2. **Tarea de traducción sencilla**  \n",
    "   - Plantear una tarea simple (por ejemplo, convertir números en letras o transformar fechas).  \n",
    "   - Explicar paso a paso cómo se entrenaría el modelo (selección de función de pérdida, backpropagation through time, etc.).  \n",
    "   - Incluir una discusión sobre el uso de **greedy** vs. **beam search** para la fase de decodificación.\n",
    "\n",
    "3. **Evaluación y ejemplos**  \n",
    "   - Proponer cómo se mediría el desempeño (puede ser exactitud literal o BLEU).  \n",
    "   - Incluir ejemplos de salidas generadas e interpretación de errores comunes (pérdida de contexto, confusiones al final de la secuencia, etc.).\n",
    "\n",
    "**Entrega esperada**:  \n",
    "- Esquema conceptual del modelo seq2seq.  \n",
    "- Descripción clara de la tarea de traducción artificial y su proceso de entrenamiento.  \n",
    "- Reflexión sobre los principales desafíos que surgen (p. ej. gradiente que se atenúa, necesidad de más datos, etc.).\n",
    "\n",
    "##### **Ejercicio 6**\n",
    "\n",
    "1. **Visión general de RLHF**  \n",
    "   - Explicar, a alto nivel, la idea de **aprendizaje por refuerzo con retroalimentación humana (RLHF)** y por qué es relevante en modelos de lenguaje de gran tamaño (LLMs).\n",
    "\n",
    "2. **Diseño de un entrenador de recompensa sencillo**  \n",
    "   - Esbozar cómo se define la función de recompensa a partir de preferencias humanas (ej. clasificar respuestas como buenas o malas).  \n",
    "   - Describir el rol de un \"reward model\" y cómo se acopla al modelo de lenguaje principal.\n",
    "\n",
    "3. **Limitaciones y retos éticos**  \n",
    "   - Mencionar los retos en la recopilación de datos humanos, sesgos y la posibilidad de \"gaming\" del sistema.\n",
    "\n",
    "**Entrega esperada**:  \n",
    "- Ensayo breve que conecte los fundamentos de RL con la idea de RLHF.  \n",
    "- Reflexión sobre la importancia de la alineación de modelos con valores o preferencias humanas.\n",
    "\n",
    "> **Entrega**: El enlace del repositorio donde se ha alojado un cuaderno de jupyter notebook organizado por secciones, con explicaciones claras y suficientes ejemplos o esquemas que demuestren el entendimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682cb98-3820-41d5-8538-9d472d24545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
