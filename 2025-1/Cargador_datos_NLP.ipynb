{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de un cargador de datos para NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ingeniero de IA trabajando en un proyecto de traducción de idiomas de vanguardia, tienes la tarea de cerrar la brecha de comunicación entre hablantes de diferentes lenguas. Traducir idiomas no es una tarea sencilla, especialmente considerando las complejidades, matices y contextos culturales que contienen. El éxito de este esfuerzo depende en gran medida de los datos: grandes corpus de frases bilingües que sirven como base para entrenar tus modelos.\n",
    "\n",
    "En PyTorch, el cargador de datos juega un papel indispensable en la gestión de esta gran cantidad de datos. Para tareas de procesamiento de lenguaje natural (NLP) como la tuya, los datos suelen tener longitudes variables debido a las diferencias estructurales entre los idiomas. \n",
    "El cargador de datos agrupa eficientemente estas secuencias de longitud variable, garantizando que los modelos se entrenen con ejemplos diversos en cada iteración. Esta agrupación es crucial para aprovechar la computación paralela en las GPU, acelerando así el proceso de entrenamiento.\n",
    "\n",
    "Además, el cargador de datos ayuda a mezclar aleatoriamente el conjunto de datos, lo que es esencial para evitar que los modelos memoricen la secuencia de datos de entrenamiento y fomentar una mejor generalización. Esto es particularmente importante en tareas de NLP, donde los datos pueden estar ordenados o agrupados por temas. La mezcla aleatoria asegura que el modelo siga siendo robusto y no desarrolle sesgos basados en el orden de los datos de entrada.\n",
    "\n",
    "Por último, en el mundo del NLP, los pasos de preprocesamiento como la tokenización, el padding y la conversión numérica son fundamentales. El cargador de datos en PyTorch proporciona funciones que permiten integrar estos pasos de preprocesamiento de manera fluida, garantizando que los datos textuales en bruto se transformen en un formato adecuado para los modelos de aprendizaje profundo.\n",
    "\n",
    "En este cuaderno, cubrirás todo el proceso de carga y procesamiento de datos de texto utilizando PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración\n",
    "#### Instalando librerías requeridas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando librerías requeridas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "import torchtext\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conjunto de datos**\n",
    "\n",
    "Un conjunto de datos en **PyTorch** es un objeto que representa una colección de muestras de datos. Cada muestra de datos típicamente consta de una o más características de entrada y sus correspondientes etiquetas objetivo. También puedes utilizar tu conjunto de datos para transformar tus datos según sea necesario.\n",
    "\n",
    "#### **Cargador de datos**\n",
    "\n",
    "Un cargador de datos en **PyTorch** es responsable de cargar y agrupar en lotes los datos de un conjunto de datos de manera eficiente. Abstrae el proceso de iterar sobre un conjunto de datos, mezclarlos y dividirlos en lotes para el entrenamiento. En aplicaciones de NLP, el cargador de datos se utiliza para procesar y transformar tus datos textuales, en lugar de simplemente utilizar el conjunto de datos.\n",
    "\n",
    "Los cargadores de datos tienen varios parámetros clave, incluyendo el conjunto de datos desde el cual se cargan, el tamaño del lote (que determina cuántas muestras se incluyen por lote), si se mezclan los datos (shuffle, para cada época), y más. Los cargadores de datos también proporcionan una interfaz de iterador, lo que facilita iterar sobre los lotes de datos durante el entrenamiento.\n",
    "\n",
    "#### '**¿Qué es un iterador?**'\n",
    "\n",
    "Un iterador es un objeto sobre el cual se puede iterar. Contiene elementos que se pueden recorrer y típicamente incluye dos métodos, `__iter__()` y `__next__()`. Cuando no hay más elementos para iterar, lanza una excepción **`StopIteration`**.\n",
    "\n",
    "Los iteradores se utilizan comúnmente para recorrer grandes conjuntos de datos sin cargar todos los elementos en memoria simultáneamente, haciendo el proceso más eficiente en cuanto a memoria. En PyTorch, no todos los conjuntos de datos son iteradores, pero todos los cargadores de datos lo son.\n",
    "\n",
    "En PyTorch, el cargador de datos procesa la información en lotes, cargando y procesando un lote a la vez en memoria de manera eficiente. El tamaño del lote, que se especifica al crear el cargador de datos, determina cuántas muestras se procesan juntas en cada lote. El propósito del cargador de datos es convertir los datos de entrada y las etiquetas en lotes de tensores con la misma forma para que los modelos de aprendizaje profundo puedan interpretarlos.\n",
    "\n",
    "Finalmente, un cargador de datos puede utilizarse para tareas como la tokenización, secuenciación, convertir tus muestras al mismo tamaño y transformar tus datos en tensores que tu modelo pueda entender.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conjunto de datos personalizado y cargador de datos en PyTorch**\n",
    "\n",
    "En este fragmento de código, verás cómo crear un conjunto de datos personalizado y usar la clase DataLoader en PyTorch. El conjunto de datos consta de una lista de oraciones aleatorias, y el objetivo es crear lotes de oraciones para su posterior procesamiento, como el entrenamiento de un modelo de red neuronal.\n",
    "\n",
    "Comenzarás definiendo un conjunto de datos personalizado llamado CustomDataset. Este conjunto de datos hereda de la clase `torch.utils.data.Dataset` y se inicializa con una lista de oraciones. El conjunto de datos comprende dos métodos esenciales:\n",
    "\n",
    "- __init__(self, sentences): Inicializa el conjunto de datos con una lista de oraciones.\n",
    "- __getitem__(self, idx): Recupera un elemento (en este caso, una oración) en un índice específico, idx.\n",
    "\n",
    "A continuación, se crea una instancia de tu conjunto de datos personalizado (custom_dataset) pasando la lista de oraciones. Además, puedes especificar un tamaño de lote (batch_size), que determina cuántas oraciones se agruparán en cada lote durante la carga de datos.\n",
    "\n",
    "Luego, crearás un DataLoader (dataloader) proporcionando tu conjunto de datos personalizado y el tamaño del lote a la clase `torch.utils.data.DataLoader`. Además, se establece `shuffle=True`, lo que indica que las oraciones se mezclarán aleatoriamente antes de dividirse en lotes. Esta mezcla es particularmente útil para entrenar modelos de aprendizaje profundo, ya que evita que el modelo aprenda patrones basados en el orden de los datos.\n",
    "\n",
    "Finalmente, iterarás a través del DataLoader para demostrar cómo se cargan los datos en lotes. En este código, verás que el tamaño del lote se establece en 2, lo que significa que cada lote contendrá dos oraciones. El DataLoader gestiona eficientemente la carga de datos en lotes, lo que lo hace adecuado para entrenar modelos de aprendizaje profundo.\n",
    "\n",
    "Durante la iteración, se imprimen las oraciones en cada lote para ilustrar cómo el DataLoader agrupa y presenta los datos. Este fragmento de código proporciona un ejemplo fundamental de cómo configurar un conjunto de datos personalizado y un cargador de datos en PyTorch, lo cual es una práctica común en los flujos de trabajo de aprendizaje profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "# Definir un conjunto de datos personalizado\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "# Crear una instancia de tu conjunto de datos personalizado\n",
    "custom_dataset = CustomDataset(sentences)\n",
    "\n",
    "# Definir tamaño del lote\n",
    "batch_size = 2\n",
    "\n",
    "# Crear un DataLoader\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterar a través del DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se muestra arriba, los datos se organizan en lotes de 2 oraciones cada uno. Es importante notar que los modelos de aprendizaje profundo solo pueden comprender datos numéricos, y las palabras carecen de significado para ellos. Por lo tanto, tu siguiente paso es convertir estas oraciones en tensores. Veamos cómo hacerlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de tensores para un conjunto de datos personalizado\n",
    "\n",
    "En este ejemplo de código, verás la creación de un conjunto de datos personalizado para tareas de procesamiento de lenguaje natural (NLP) utilizando PyTorch. El conjunto de datos consiste en una lista de oraciones, y tu objetivo es preprocesar estas oraciones, tokenizarlas y convertirlas en tensores de índices de tokens para su uso en modelos de NLP. Desglosaremos el código paso a paso.\n",
    "\n",
    "Las oraciones y la clase `CustomDataset` se utilizan de la misma manera que en el fragmento de código anterior. Los cambios realizados en la clase CustomDataset son los siguientes:\n",
    "\n",
    "- __init__: El constructor recibe una lista de oraciones, una función de tokenización y un vocabulario (vocab) como entrada.\n",
    "- __len__: Este método devuelve el número total de muestras en el conjunto de datos.\n",
    "- __getitem__: Este método es responsable de procesar una muestra individual. Tokeniza la oración usando el tokenizador proporcionado y luego convierte los tokens en índices de tensores utilizando el vocabulario.\n",
    "\n",
    "Puedes definir un tokenizador utilizando la función `get_tokenizer` con la opción `basic_english`. La tokenización es el proceso de dividir un texto en tokens o palabras individuales. A continuación, construyes un vocabulario a partir de las oraciones. Utilizas la función `build_vocab_from_iterator` para construir el vocabulario a partir de las oraciones tokenizadas.\n",
    "\n",
    "Puedes crear una instancia de tu conjunto de datos personalizado, pasando las oraciones, el tokenizador y el vocabulario. Finalmente, imprimes la longitud del conjunto de datos personalizado y algunos elementos de muestra del conjunto de datos para ilustrarlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "# Definir un conjunto de datos personalizado\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, vocab):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.sentences[idx])\n",
    "        # Convertir tokens a índices de tensor usando el vocabulario\n",
    "        tensor_indices = [self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(tensor_indices)\n",
    "\n",
    "# Tokenizador\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Construir el vocabulario\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, sentences))\n",
    "\n",
    "# Crear una instancia de tu conjunto de datos personalizado\n",
    "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
    "\n",
    "print(\"Custom Dataset Length:\", len(custom_dataset))\n",
    "print(\"Sample Items:\")\n",
    "for i in range(6):\n",
    "    sample_item = custom_dataset[i]\n",
    "    print(f\"Item {i + 1}: {sample_item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adelante, descomenta el siguiente código para aplicar el cargador de datos y observar los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Crear una instancia de tu conjunto de datos personalizado\n",
    "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
    "\n",
    "# Definir tamaño del lote\n",
    "batch_size = 2\n",
    "\n",
    "# Crear un cargador de datos\n",
    "#dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterar a través del cargador de datos\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrarás un error al intentar crear lotes para los tensores. Este error surge porque los lotes de tensores tienen longitudes desiguales. El cargador de datos está utilizando la función de colación (`collate_function`) por defecto, que requiere que los tensores tengan longitudes iguales. Puedes definir tu propia función de colación y pasarle los datos para establecer tus propias reglas. \n",
    "\n",
    "Típicamente, para resolver el problema de longitudes de tensor desiguales, se emplea el `padding` de datos. Esto se demostrará en la siguiente sección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de colación personalizada\n",
    "\n",
    "Una función de colación se emplea en el contexto de la carga de datos y el procesamiento por lotes en el aprendizaje automático, particularmente cuando se trata de datos de longitud variable, como secuencias (por ejemplo, texto, series temporales, secuencias de eventos). Su propósito principal es preparar y formatear muestras individuales de datos en lotes que puedan ser procesados eficientemente por los modelos de aprendizaje automático.\n",
    "\n",
    "Comenzarás definiendo una función de colación personalizada llamada `collate_fn`. Esta función juega un papel crucial al manejar secuencias de longitudes variables, como oraciones en NLP. Su propósito es rellenar (pad) las secuencias dentro de un lote para que tengan longitudes iguales, lo cual es un paso de preprocesamiento común en tareas de NLP.\n",
    "\n",
    "`pad_sequence`: Esta función forma parte de PyTorch y se utiliza para rellenar (pad) las secuencias en un lote, asegurando una longitud uniforme. Toma un lote de secuencias como entrada y las rellena para igualar la longitud de la secuencia más larga. El argumento `padding_value=0` especifica el valor a utilizar para el relleno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una función de colación personalizada\n",
    "def collate_fn(batch):\n",
    "    # Rellenar las secuencias dentro del lote para que tengan longitudes iguales\n",
    "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior, al rellenar las secuencias, se establece `batch_first=True`. Cuando `batch_first=True`, la salida tendrá la forma [tamaño_lote x longitud_secuencia], de lo contrario, tendrá la forma [longitud_secuencia x tamaño_lote]. Algunos modelos aceptan entradas con la forma [tamaño_lote x longitud_secuencia] mientras que otros necesitan que la entrada tenga la forma [longitud_secuencia x tamaño_lote]. Ten en cuenta que este parámetro se encarga de colocar la entrada en la forma deseada. \n",
    "\n",
    "Veamos cómo afecta realmente la forma de los lotes curados. Primero, creas un cargador de datos con una función de colación con `batch_first=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un cargador de datos con la función de colación personalizada con batch_first=True,\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Iterar a través del cargador de datos\n",
    "for batch in dataloader: \n",
    "    for row in batch:\n",
    "        for idx in row:\n",
    "            words = [vocab.get_itos()[idx] for idx in row]\n",
    "        print(words)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar el resultado, puedes ver que la primera dimensión es el lote. Por ejemplo, el primer lote es la primera oración: \"['if', 'you', 'want', 'to', 'know', 'what', 'a', 'man', \"'\", 's', 'like', ',', 'take', 'a', 'good', 'look', 'at', 'how', 'he', 'treats', 'his', 'inferiors', ',', 'not', 'his', 'equals', '.']\".\n",
    "\n",
    "Ahora, puedes probar con `batch_first=False`, que es el valor por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una función de colación personalizada\n",
    "def collate_fn_bfFALSE(batch):\n",
    "    # Rellenar las secuencias dentro del lote para que tengan longitudes iguales\n",
    "    padded_batch = pad_sequence(batch, padding_value=0)\n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, observa los datos curados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un cargador de datos con la función de colación personalizada (batch_first=False)\n",
    "dataloader_bfFALSE = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=collate_fn_bfFALSE)\n",
    "\n",
    "# Iterar a través del cargador de datos\n",
    "for seq in dataloader_bfFALSE:\n",
    "    for row in seq:\n",
    "        #print(row)\n",
    "        words = [vocab.get_itos()[idx] for idx in row]\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que la primera dimensión es ahora la secuencia en lugar del lote, lo que significa que las oraciones se dividirán de manera que cada fila incluya un token de cada secuencia. Por ejemplo, la primera fila, (['if', 'fame']), incluye los primeros tokens de todas las secuencias en ese lote. Debes tener en cuenta este estándar para evitar confusiones al trabajar con redes neuronales recurrentes (RNNs) y transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar a través del cargador de datos con batch_first = TRUE\n",
    "for batch in dataloader:    \n",
    "    print(batch)\n",
    "    print(\"Longitud de las secuencias en el lote:\", batch.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verás que cada lote tiene un tamaño fijo para todas las secuencias dentro del lote.\n",
    "\n",
    "También tienes la opción de utilizar la función de colación para tareas como la tokenización, la conversión de índices tokenizados y transformar el resultado en un tensor. Es importante tener en cuenta que el conjunto de datos original permanece sin modificaciones por estas transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir un conjunto de datos personalizado\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tienes el texto sin procesar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creas la nueva ```collate_fn```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Tokenizar cada muestra en el lote usando el tokenizador especificado\n",
    "    tensor_batch = []\n",
    "    for sample in batch:\n",
    "        tokens = tokenizer(sample)\n",
    "        # Convertir los tokens a índices del vocabulario y crear un tensor para cada muestra\n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "\n",
    "    # Rellenar las secuencias dentro del lote para que tengan longitudes iguales usando pad_sequence\n",
    "    # batch_first=True asegura que los tensores tengan la forma (tamaño_lote, longitud_secuencia_maxima)\n",
    "    padded_batch = pad_sequence(tensor_batch, batch_first=True)\n",
    "    \n",
    "    # Devolver el lote rellenado\n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un cargador de datos con la función de colación personalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un cargador de datos para el conjunto de datos personalizado\n",
    "dataloader = DataLoader(\n",
    "    dataset=custom_dataset,   # Conjunto de datos personalizado de PyTorch que contiene tus datos\n",
    "    batch_size=batch_size,     # Número de muestras en cada mini-lote\n",
    "    shuffle=True,              # Mezclar los datos al inicio de cada época\n",
    "    collate_fn=collate_fn      # Función de colación personalizada para procesar los lotes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verás que el resultado es un tensor con la misma forma para cada muestra en el lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    print(\"forma de la muestra\", len(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado, se han creado con éxito lotes de tensores con longitudes iguales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un cargador de datos con una función de colación que procese lotes de texto en francés (proporcionado a continuación). Ordena el conjunto de datos según la longitud de las secuencias. Luego, tokeniza, numera y rellena las secuencias. Ordenar las secuencias minimizará la cantidad de tokens `<PAD>` añadidos a las secuencias, lo que mejora el rendimiento del modelo. Prepara los datos en lotes de tamaño 4 e imprímelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Ceci est une phrase.\",\n",
    "    \"C'est un autre exemple de phrase.\",\n",
    "    \"Voici une troisième phrase.\",\n",
    "    \"Il fait beau aujourd'hui.\",\n",
    "    \"J'aime beaucoup la cuisine française.\",\n",
    "    \"Quel est ton plat préféré ?\",\n",
    "    \"Je t'adore.\",\n",
    "    \"Bon appétit !\",\n",
    "    \"Je suis en train d'apprendre le français.\",\n",
    "    \"Nous devons partir tôt demain matin.\",\n",
    "    \"Je suis heureux.\",\n",
    "    \"Le film était vraiment captivant !\",\n",
    "    \"Je suis là.\",\n",
    "    \"Je ne sais pas.\",\n",
    "    \"Je suis fatigué après une longue journée de travail.\",\n",
    "    \"Est-ce que tu as des projets pour le week-end ?\",\n",
    "    \"Je vais chez le médecin cet après-midi.\",\n",
    "    \"La musique adoucit les mœurs.\",\n",
    "    \"Je dois acheter du pain et du lait.\",\n",
    "    \"Il y a beaucoup de monde dans cette ville.\",\n",
    "    \"Merci beaucoup !\",\n",
    "    \"Au revoir !\",\n",
    "    \"Je suis ravi de vous rencontrer enfin !\",\n",
    "    \"Les vacances sont toujours trop courtes.\",\n",
    "    \"Je suis en retard.\",\n",
    "    \"Félicitations pour ton nouveau travail !\",\n",
    "    \"Je suis désolé, je ne peux pas venir à la réunion.\",\n",
    "    \"À quelle heure est le prochain train ?\",\n",
    "    \"Bonjour !\",\n",
    "    \"C'est génial !\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu codigo de respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Opcional] Cargador de datos para la tarea de traducción Alemán-Inglés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta sección prepara el terreno para la traducción automática Alemán-Inglés utilizando las bibliotecas torchtext y spaCy. Ajusta las URLs del conjunto de datos Multi30k, configura los tokenizadores para ambos idiomas y establece vocabularios con tokens especiales. Esta base es crucial para construir y entrenar modelos de traducción efectivos.\n",
    "\n",
    "- **Configuración del conjunto de datos y definición del idioma**\n",
    "  - Se modifican las URLs por defecto del conjunto de datos Multi30k para corregir enlaces rotos.\n",
    "  - Se definen los idiomas de origen (`de` para alemán) y destino (`en` para inglés).\n",
    "\n",
    "- **Configuración del tokenizador**\n",
    "  - Se configuran tokenizadores para ambos idiomas utilizando `spaCy`.\n",
    "\n",
    "- **Generación de tokens**\n",
    "  - Se crea una función auxiliar, `yield_tokens`, para generar tokens a partir del conjunto de datos.\n",
    "\n",
    "- **Símbolos especiales**\n",
    "  - Se definen los símbolos especiales (por ejemplo, `<unk>`, `<pad>`) y sus índices.\n",
    "\n",
    "- **Construcción del vocabulario**\n",
    "  - Se construyen vocabularios para ambos idiomas (origen y destino) a partir de los datos de **entrenamiento** del conjunto de datos Multi30k, convirtiendo tokens en índices únicos (números).\n",
    "\n",
    "- **Manejo de tokens por defecto**\n",
    "  - Se establece un índice por defecto (`UNK_IDX`) para los tokens que no se encuentren en el vocabulario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunto de datos de traducción\n",
    "En esta sección, obtendrás un conjunto de datos de traducción llamado Multi30k. Modificarás las URLs de entrenamiento y validación por defecto, y luego recuperarás e imprimirás el primer par de oraciones en alemán e inglés del conjunto de entrenamiento. Primero, sobrescribirás las URLs por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se modificarán las URLs del conjunto de datos ya que los enlaces al conjunto de datos original están rotos\n",
    "\n",
    "multi30k.URL[\"train\"] = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/validation.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define el idioma de origen como Alemán ('de') y el idioma de destino como Inglés ('en'). En Python, las variables globales son aquellas definidas fuera de una función, accesibles tanto dentro como fuera de las funciones. A menudo se escriben en mayúsculas como una convención para indicar que son constantes, de naturaleza global y para diferenciarlas de las variables normales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa el iterador de datos de entrenamiento para el conjunto de datos Multi30k con los idiomas de origen y destino especificados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un iterador para el conjunto de datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = iter(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes imprimir los primeros cinco pares de oraciones de origen y destino del conjunto de datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(5):\n",
    "    # Obtener el siguiente par de oraciones de origen y destino del conjunto de datos de entrenamiento\n",
    "    src, tgt = next(data_set)\n",
    "\n",
    "    # Imprimir las oraciones de origen (alemán) y destino (inglés)\n",
    "    print(f\"muestra {str(n+1)}\")\n",
    "    print(f\"Origen ({SRC_LANGUAGE}): {src}\\nDestino ({TGT_LANGUAGE}): {tgt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración del tokenizador\n",
    "\n",
    "El tokenizador, configurado usando spaCy, descompone el texto en unidades más pequeñas o tokens, facilitando un procesamiento lingüístico preciso y asegurando que las palabras y puntuaciones se segmenten adecuadamente para la tarea de traducción. Utilicemos los siguientes ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german, english = next(data_set)\n",
    "print(f\"Origen Alemán ({SRC_LANGUAGE}): {german}\\nDestino Inglés ({TGT_LANGUAGE}): { english }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importa la función utilitaria ```get_tokenizer``` de ```torchtext``` para obtener tokenizadores para el procesamiento del lenguaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa los tokenizadores para alemán e inglés utilizando el modelo 'de_core_news_sm' de spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar ambos tokenizadores\n",
    "token_transform = {}\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La línea ```token_transform['de'](german)``` tokenizará la cadena en alemán (o texto) utilizando el tokenizador previamente definido ```token_transform['de']``` para el idioma alemán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform['de'](german)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo mismo para el inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform['en'](english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Símbolos especiales\n",
    "En un contexto típico de NLP, los tokens `['<unk>', '<pad>', '<bos>', '<eos>']` tienen significados específicos:\n",
    "\n",
    "1. `<unk>`: Este token representa \"desconocido\" o \"fuera del vocabulario\". Se utiliza cuando una palabra en el texto de entrada no se encuentra en el vocabulario o cuando se trata de palabras raras o no vistas durante el entrenamiento. Cuando el modelo se encuentra con una palabra desconocida, la reemplaza con el token `<unk>`.\n",
    "\n",
    "2. `<pad>`: Este token representa el relleno. En secuencias de datos textuales, como oraciones o documentos, las secuencias pueden tener diferentes longitudes. Para crear lotes de datos con dimensiones uniformes, las secuencias más cortas a menudo se rellenan con este token `<pad>` para igualar la longitud de la secuencia más larga en el lote.\n",
    "\n",
    "3. `<bos>`: Este token representa el \"inicio de secuencia.\" Se utiliza para indicar el comienzo de una oración o secuencia de tokens. Ayuda al modelo a entender el inicio de una secuencia de texto.\n",
    "\n",
    "4. `<eos>`: Este token representa el \"fin de secuencia.\" Se utiliza para indicar el final de una oración o secuencia de tokens. Ayuda al modelo a reconocer el final de una secuencia de texto.\n",
    "\n",
    "Definir símbolos especiales e índices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir símbolos especiales e índices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Asegurarse de que los tokens estén en orden de sus índices para insertarlos correctamente en el vocabulario\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación de tokens a índices (Vocabulario)\n",
    "El código inicializa un diccionario vocab_transform y luego construye vocabularios para ambos idiomas, alemán (de) e inglés (en), a partir del conjunto de datos ```train_iter``` utilizando la función auxiliar ```yield_tokens```. Estos vocabularios se almacenan en el diccionario vocab_transform. Los vocabularios se construyen con ciertas restricciones, como una frecuencia mínima para los tokens y la inclusión de símbolos especiales al inicio.\n",
    "\n",
    "Inicializa un diccionario para almacenar los vocabularios de ambos idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de reserva para las transformaciones de vocabulario de 'en' y 'de'\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearás una función `yield_tokens` que procese un iterador de conjunto de datos dado (`data_iter`) y, para cada muestra, tokenice los datos para el idioma especificado (language). Utiliza un mapeo predefinido (`token_transform`) de idiomas a sus respectivos tokenizadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    # Definir un mapeo para asociar los idiomas de origen y destino con sus respectivas posiciones en las muestras de datos.\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    # Iterar sobre cada muestra de datos en el iterador del conjunto de datos proporcionado\n",
    "    for data_sample in data_iter:\n",
    "        # Tokenizar la muestra de datos correspondiente al idioma especificado y devolver los tokens resultantes.\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyes y almacenas los vocabularios de alemán e inglés únicamente a partir del conjunto de datos de **entrenamiento**. Puedes utilizar la función auxiliar ```yield_tokens``` para tokenizar los datos. Incluye tokens que aparezcan al menos una vez (`min_freq=1`) y añade símbolos especiales (como <pad>, <unk>, etc.) al inicio del vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Iterador de datos de entrenamiento\n",
    "    train_iterator = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Para disminuir la cantidad de tokens de relleno, ordenas los datos según la longitud de la fuente para agrupar secuencias de longitudes similares\n",
    "    sorted_dataset = sorted(train_iterator, key=lambda x: len(x[0].split()))\n",
    "    # Crear el objeto Vocab de torchtext\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(sorted_dataset, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establece ```UNK_IDX``` como el índice por defecto. Este índice se devuelve cuando el token no se encuentra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si no se establece, lanza un ``RuntimeError`` cuando el token consultado no se encuentra en el vocabulario.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomarás la cadena de texto en inglés/alemán, la tokenizarás en palabras o subpalabras, y luego convertirás estos tokens en sus índices correspondientes del vocabulario, resultando en una secuencia de enteros, `seq_en`, que se puede utilizar para un procesamiento posterior en un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_en = vocab_transform['en'](token_transform['en'](english))\n",
    "print(f\"Cadena de texto en inglés: {english}\\nSecuencia en inglés: {seq_en}\")\n",
    "\n",
    "seq_de = vocab_transform['de'](token_transform['de'](german))\n",
    "print(f\"Cadena de texto en alemán: {german}\\nSecuencia en alemán: {seq_de}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función ```tensor_transform_s``` añade un token de inicio de secuencia (BOS) al comienzo, invierte la secuencia para revertir el orden de los IDs de tokens y añade un token de fin de secuencia (EOS) al final de una secuencia dada de IDs de tokens, luego devuelve el resultado concatenado como un tensor de PyTorch; esto se utilizará como entrada para nuestro modelo.\n",
    "\n",
    "La función ```tensor_transform_t``` realiza operaciones similares, excepto la inversión. Es una buena práctica invertir el orden de la oración de origen para que el LSTM funcione mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función para añadir BOS/EOS, invertir la oración de origen y crear un tensor para los índices de la secuencia de entrada\n",
    "def tensor_transform_s(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.flip(torch.tensor(token_ids), dims=(0,)),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# función para añadir BOS/EOS y crear un tensor para los índices de la secuencia de entrada\n",
    "def tensor_transform_t(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_en = tensor_transform_s(seq_en)\n",
    "seq_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_de = tensor_transform_t(seq_de)\n",
    "seq_de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que has definido la función de transformación, creas una función ```sequential_transforms``` para agrupar todas las transformaciones en el orden correcto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función auxiliar para agrupar operaciones secuenciales\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# Transformaciones de texto para los idiomas ``src`` y ``tgt`` para convertir cadenas sin procesar en índices de tensores\n",
    "text_transform = {}\n",
    "\n",
    "text_transform[SRC_LANGUAGE] = sequential_transforms(token_transform[SRC_LANGUAGE],  # Tokenización\n",
    "                                            vocab_transform[SRC_LANGUAGE],         # Numerización\n",
    "                                            tensor_transform_s)                    # Añadir BOS/EOS y crear tensor\n",
    "\n",
    "text_transform[TGT_LANGUAGE] = sequential_transforms(token_transform[TGT_LANGUAGE],  # Tokenización\n",
    "                                            vocab_transform[TGT_LANGUAGE],         # Numerización\n",
    "                                            tensor_transform_t)                    # Añadir BOS/EOS y crear tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procesamiento de datos en lotes\n",
    "La función collate_fn se basa en las utilidades que estableciste anteriormente. Aplica la transformación de texto (text_transform) a un lote de datos sin procesar. Además, asegura longitudes de secuencia consistentes dentro del lote mediante el relleno. Esta transformación prepara los datos para ser utilizados como entrada en un modelo transformer diseñado para tareas de traducción de idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función para agrupar muestras de datos en tensores por lotes\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_sequences = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        #src_sequences = torch.tensor(src_sequences, dtype=torch.int64)\n",
    "        src_sequences = src_sequences.clone().detach().to(torch.int64)\n",
    "        tgt_sequences = text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\"))\n",
    "        #tgt_sequences = torch.tensor(tgt_sequences, dtype=torch.int64)\n",
    "        tgt_sequences = tgt_sequences.clone().detach().to(torch.int64)\n",
    "        src_batch.append(src_sequences)\n",
    "        tgt_batch.append(tgt_sequences)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    \n",
    "    return src_batch.to(device), tgt_batch.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estableces un iterador de datos de entrenamiento utilizando el conjunto de datos Multi30k y configuras un cargador de datos con un tamaño de lote de 4. Esto aprovecha la función collate_fn predefinida para curar y preparar eficientemente los lotes para entrenar tu modelo transformer. Tu objetivo principal es profundizar en las complejidades de los componentes del codificador y decodificador de la RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_iterator = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "sorted_train_iterator = sorted(train_iterator, key=lambda x: len(x[0].split()))\n",
    "train_dataloader = DataLoader(sorted_train_iterator, batch_size=BATCH_SIZE, collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "valid_iterator = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "sorted_valid_dataloader = sorted(valid_iterator, key=lambda x: len(x[0].split()))\n",
    "valid_dataloader = DataLoader(sorted_valid_dataloader, batch_size=BATCH_SIZE, collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "\n",
    "src, trg = next(iter(train_dataloader))\n",
    "src, trg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "prev_pub_hash": "8c22317e777527276102edc9a5fee16b619deefd7fe94e10f36fb454230c542e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
