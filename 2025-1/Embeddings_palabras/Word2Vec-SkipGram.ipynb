{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aznbkgf5JfOH"
   },
   "source": [
    "### Word Embeddings: Word2Vec -Skip Gram\n",
    "\n",
    "En este cuaderno de trabajo ilustramos la obtención de representaciones distribuidas de palabras (*word embeddings*) a partir de un corpus en español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANCq5t90UPy3"
   },
   "source": [
    "**Preparación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2734,
     "status": "ok",
     "timestamp": 1672082346491,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "XzYLHUu1OGYg",
    "outputId": "30160540-2903-4dbb-85fd-e570160bc5da"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Corrección del dtype para evitar la advertencia de futuro\n",
    "_np_qint8 = np.dtype([(\"qint8\", np.int8, (1,))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YmaP_b5LCCZ"
   },
   "source": [
    "Trabajaremos con el texto de \"Don Quijote de la Mancha\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 568,
     "status": "ok",
     "timestamp": 1672082358283,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "Iq1dSP9sOLI1",
    "outputId": "c8f58454-810a-429b-b59c-8273a91077cd"
   },
   "outputs": [],
   "source": [
    "texto = open('Quijote.txt', 'rb').read().decode(encoding='utf-8')\n",
    "texto = texto[708:-19255]  # Eliminamos el encabezado y pie del texto (en inglés, no es parte de la obra)\n",
    "print ('Tamaño del texto: {} caracteres'.format(len(texto)))\n",
    "print ()\n",
    "print (texto[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g04gH0oMLT6o"
   },
   "source": [
    "Utilizamos la librería [`tf.keras.preprocessing.text.Tokenizer`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) para extraer un vocabulario del texto, asignar un índice a cada palabra del vocabulario, y representar el texto como una secuencia de valores enteros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1328,
     "status": "ok",
     "timestamp": 1672082365204,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "bfWZHQDDNvFK",
    "outputId": "e7db2821-b544-4f5e-f99e-b1fc96e4d0a9"
   },
   "outputs": [],
   "source": [
    "# Independientemente del número de palabras, el tokenizer limitará el número de\n",
    "# índices asignados a sólo 2000 palabras diferentes.\n",
    "TAM_VOCAB = 1500\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=TAM_VOCAB,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\r\\n\\'¡¿«»',\n",
    "    oov_token='<OOV>'\n",
    ")\n",
    "tokenizer.fit_on_texts([texto])\n",
    "\n",
    "print ('El tokenizador encontró %d palabras en el texto' % len(tokenizer.word_index))\n",
    "print ('pero sólo considerará %d en el vocabulario.\\n' % TAM_VOCAB)\n",
    "\n",
    "\n",
    "# El Tokenizer convierte entre secuencias de enteros y palabras.\n",
    "# Las siguientes funciones nos facilitarán el trabajo\n",
    "def sequence_to_text(sequence):\n",
    "  return tokenizer.sequences_to_texts([sequence])[0]\n",
    "\n",
    "def idx_to_word(idx):\n",
    "  return sequence_to_text([idx])\n",
    "\n",
    "def text_to_sequence(text):\n",
    "  return tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "def word_to_idx(word):\n",
    "  return text_to_sequence([word])[0]\n",
    "\n",
    "\n",
    "# Convertimos el texto en una secuencia de números enteros\n",
    "sequence = text_to_sequence(texto)\n",
    "\n",
    "\n",
    "# Veamos las primeras palabras de la secuencia\n",
    "print('Inicio de la secuencia:\\n')\n",
    "print('Índice  Palabra')\n",
    "print('------  -------')\n",
    "for idx in sequence[:15]:\n",
    "  print('{:6d}  {}'.format(idx, idx_to_word(idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1672082370466,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "9QCBV8ASrdUZ",
    "outputId": "043a2de7-01c4-43e5-ad43-7c8dd01781df"
   },
   "outputs": [],
   "source": [
    "# Veamos las primeras palabras de la secuencia\n",
    "print('Inicio de la secuencia:\\n')\n",
    "print('Índice  Palabra')\n",
    "print('------  -------')\n",
    "for idx in range(10):\n",
    "  print('{:6d}  {}'.format(idx, idx_to_word(idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Usue7zWmO_TV"
   },
   "source": [
    "**Word2Vec: Skip Gram**\n",
    "\n",
    "Word2Vec es una familia de modelos para la creación de representaciones distribuidas de palabras. Se inspiran en la idea de que el significado de una palabra puede ser capturado a partir de su contexto.\n",
    "\n",
    "El embedding Skip Gram de una palabra es obteniendo al aprender a predecir las palabras que se encuentran en su contexto.\n",
    "\n",
    "Definamos y probemos primero una función que nos permita transformar una secuencia en un generador de pares *(x, y)* que relacionen una palabra con las que se encuentran a *skip_window* o menos posiciones de distancia. Para simplificar, y dado que lo usaremos en secuencias largas, omitiremos el problema de lidiar con las palabras de los extremos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1672082451873,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "cM9hTz1Vnnir",
    "outputId": "d35c85b6-43e7-495c-f8e2-9dc8260371d4"
   },
   "outputs": [],
   "source": [
    "# Función para armar los pares x, y\n",
    "def genera_muestras(sequence, skip_window):\n",
    "  window_size = 2 * skip_window + 1\n",
    "  for i in range(len(sequence) - window_size + 1):\n",
    "    window = sequence[i : i + window_size]\n",
    "    x = window[skip_window]\n",
    "    for j in range(window_size):\n",
    "      if j != skip_window:\n",
    "        yield [[x], [window[j]]]\n",
    "\n",
    "def prueba_generacion_muestras():\n",
    "  for sample in genera_muestras([0, 1, 2, 3, 4, 5, 6, 7], 2):\n",
    "    print (sample)\n",
    "\n",
    "prueba_generacion_muestras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1745,
     "status": "ok",
     "timestamp": 1672082538609,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "hTebnOFLhkA3",
    "outputId": "9816de67-d5ea-439b-f3bf-c85ea2ba79d3"
   },
   "outputs": [],
   "source": [
    "# Esta función encapsula generate_samples para poder enviarla como argumento a tf.data.Dataset.from_generator\n",
    "def generador():\n",
    "  return genera_muestras(sequence, skip_window=3)\n",
    "\n",
    "# tf.data.Dataset importa los valores como un tensor de dos elementos para cada ejemplo\n",
    "# Con esta función los convertiremos en un tuple (input, label), que es lo que esperará Keras\n",
    "def division_muestras(sample):\n",
    "  return sample[0], sample[1]\n",
    "\n",
    "TAM_BATCH = 128\n",
    "TAM_BUFFER = 20000\n",
    "\n",
    "# Creación del dataset usando TensorFlow 1.13.1 API\n",
    "dataset = tf.data.Dataset.from_generator(generador, output_types=tf.int32, output_shapes=(2,1))\n",
    "dataset = dataset.map(division_muestras)\n",
    "dataset = dataset.shuffle(TAM_BUFFER).batch(TAM_BATCH, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWbMSqIP0nu1"
   },
   "source": [
    "### Construcción del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1672082624836,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "TOGP7NVGB1MI",
    "outputId": "7db6ef76-14b5-41d2-f1c5-06224e96684f"
   },
   "outputs": [],
   "source": [
    "DIM_EMBEDDING = 128\n",
    "\n",
    "# Usamos el API Funcional de Keras\n",
    "\n",
    "entradas = tf.keras.Input(shape=(1,))\n",
    "entradas_embedded= tf.keras.layers.Embedding(TAM_VOCAB, DIM_EMBEDDING, input_length=1)(entradas)\n",
    "logits = tf.keras.layers.Dense(TAM_VOCAB)(entradas_embedded)\n",
    "modelo = tf.keras.Model(inputs=entradas, outputs=logits)\n",
    "\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mesSqrGKcGQS"
   },
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk9v-4jPcdnI"
   },
   "source": [
    "**Uso de callbacks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRb_wXA4W4IZ"
   },
   "source": [
    "Preparemos una función *Callback* que nos permita ir visualizando la calidad de los embeddings que obtendremos. Cada vez que se ejecute, imprimirá las 8 palabras más parecidas a 15 palabras aleatorias de entre las más frecuentes del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj7mNjSw95AL"
   },
   "outputs": [],
   "source": [
    "val_words = ['quijote', 'él', 'dijo', 'tres', 'duque', 'mal', 'eres', 'boca', 'mundo', 'quiero', 'padre', 'hombre', 'había']\n",
    "val_indices = [word_to_idx(x) for x in val_words]\n",
    "\n",
    "# Imprimir muestra de palabras con las que les son más similares\n",
    "def palabras_mas_similares(batch):\n",
    "  if batch % 500 != 0:\n",
    "    return\n",
    "\n",
    "  # Obtener los embeddings\n",
    "  embeddings = modelo.layers[1].get_weights()[0]  # (TAM_VOCAB, DIM_EMBEDDING)\n",
    "\n",
    "  # Normalizar los embeddings\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  val_embeddings = np.take(normalized_embeddings, val_indices, axis=0)\n",
    "\n",
    "  # Calcular la matriz de similaridad de coseno (producto punto de vectores normalizados)\n",
    "  similaridad = tf.matmul(val_embeddings, tf.transpose(normalized_embeddings))\n",
    "\n",
    "  # Buscamos e imprimimos las palabras más cercanas a las palabras aleatorias que elegimos\n",
    "  print()\n",
    "  for i, (val_word, val_idx) in enumerate(zip(val_words, val_indices)):\n",
    "    top_k = 8 # número de palabras más cercanas\n",
    "    nearest = tf.argsort(-similaridad[i, :])[1:top_k+1].numpy()\n",
    "    print('Más similares a %-10s: %s' % (val_word, ', '.join(sequence_to_text(nearest).split())))\n",
    "  print()\n",
    "\n",
    "visualization_callback = tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch,logs: palabras_mas_similares(batch))\n",
    "\n",
    "# Checkpoint CallBack\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-TidSlUciaY"
   },
   "source": [
    "**Asignamiento de un optimizador y una función de pérdida**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA-hwpVzcpOK"
   },
   "outputs": [],
   "source": [
    "def perdida(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "modelo.compile(optimizer='adam', loss=perdida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqdNDXYDcp0C"
   },
   "source": [
    "**Ejecutación del entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 532833,
     "status": "ok",
     "timestamp": 1672083286581,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "_H8cnzAe-A1s",
    "outputId": "44674862-cdff-4b6b-d018-46954e64b01d"
   },
   "outputs": [],
   "source": [
    "EPOCAS=1  # Aprox. 10 minutos por época en Colab\n",
    "history = modelo.fit(dataset, epochs=EPOCAS, callbacks=[checkpoint_callback, visualization_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJrS4tKMdROS"
   },
   "source": [
    "**Recuperar los embeddings aprendidos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1672083300797,
     "user": {
      "displayName": "CESAR JESUS LARA AVILA",
      "userId": "01059333317062820707"
     },
     "user_tz": 300
    },
    "id": "2CzVjY1Ce4JS",
    "outputId": "6e2aaea6-9204-4395-9a8e-f2c92972e351"
   },
   "outputs": [],
   "source": [
    "embeddings = modelo.layers[1].get_weights()[0]  # (TAM_VOCAB, DIM_EMBEDDING)\n",
    "\n",
    "print('Dimensiones de la matriz de embeddings : ', embeddings.shape)\n",
    "\n",
    "ejemplo = 'quijote'\n",
    "ejemplo_idx = word_to_idx(ejemplo)\n",
    "print(f'''\n",
    "Ejemplo\n",
    "-------\n",
    "Palabra   :  {ejemplo}\n",
    "Ìndice    :  {ejemplo_idx}\n",
    "Embedding :\n",
    "{embeddings[ejemplo_idx]}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gJ-nNbwdHAS"
   },
   "source": [
    "**Visualizar en el proyector de embeddings de TensorFlow**\n",
    "\n",
    "Para esta visualización necesitamos grabar y descargar los vectores de embeddings y la lista de palabras como archivos de texto separados por tabs (.tsv):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdeG_2I1s2n1"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for i in range(TAM_VOCAB):\n",
    "  out_m.write(idx_to_word(i) + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings[i]]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFEp1yRWxQqG"
   },
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Carga ambos archivos en el [Proyector de embeddings de TensorFlow](http://projector.tensorflow.org), usando la opción Load. Analiza los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyGBqf3y34H2"
   },
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo CBOW predice una palabra objetivo basándose en las palabras de contexto circundantes. Experimenta con el código siguiente de acuerdo a lo visto a clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "        self.W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "\n",
    "    def train(self, context, target, epochs=1000, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            h = np.mean(self.W1[context], axis=0)\n",
    "            u = np.dot(h, self.W2)\n",
    "            y_pred = softmax(u)\n",
    "\n",
    "            # Error\n",
    "            EI = np.array(y_pred)\n",
    "            EI[target] -= 1\n",
    "\n",
    "            # Backpropagacion\n",
    "            dW2 = np.outer(h, EI)\n",
    "            dW1 = np.dot(self.W2, EI).reshape(self.W1[context].shape)\n",
    "\n",
    "            self.W1[context] -= learning_rate * dW1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoca {epoch}, Perdida: {np.sum(-np.log(y_pred[target]))}')\n",
    "\n",
    "    def word_vector(self, word_idx):\n",
    "        return self.W1[word_idx]\n",
    "\n",
    "# Ejemplo\n",
    "vocab_size = 10  \n",
    "embedding_dim = 5\n",
    "modelo = CBOW(vocab_size, embedding_dim)\n",
    "contextos = [1, 2, 3, 4]  \n",
    "objetivo = 5  \n",
    "modelo.train(contextos, objetivo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo Skip-Gram funciona de manera opuesta al CBOW, intenta predecir las palabras de contexto a partir de la palabra objetivo. Experimenta con el código siguiente de acuerdo a lo visto a clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "        self.W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "\n",
    "    def train(self, target, contexts, epochs=1000, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            h = self.W1[target]\n",
    "            u = np.dot(h, self.W2)\n",
    "            y_pred = softmax(u)\n",
    "\n",
    "            EI = np.array(y_pred)\n",
    "            EI[contexts] -= 1 / len(contexts)\n",
    "\n",
    "            dW2 = np.outer(h, EI)\n",
    "            dW1 = np.dot(self.W2, EI).reshape(self.W1[target].shape)\n",
    "\n",
    "            self.W1[target] -= learning_rate * dW1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoca {epoch}, Perdida: {np.sum(-np.log(y_pred[contexts]))}')\n",
    "\n",
    "    def word_vector(self, word_idx):\n",
    "        return self.W1[word_idx]\n",
    "\n",
    "# Ejemplo\n",
    "vocab_size = 10\n",
    "embedding_dim = 5\n",
    "modelo = SkipGram(vocab_size, embedding_dim)\n",
    "objetivo = 5  \n",
    "contextos = [1, 2, 3, 4]  # indices of context words\n",
    "modelo.train(objetivo, contextos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes modificar el código de este cuaderno en TensorFlow 2.10.1  e implementar el modelo CBOW visto en clases y analizar la visualización de tus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1NGBb1hofQxQNYg0CjEKhi8c1Ha8fNowW",
     "timestamp": 1672083571844
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
