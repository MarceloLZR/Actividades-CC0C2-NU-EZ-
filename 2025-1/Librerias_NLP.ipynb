{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Explorando librería de IA Generativa__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Tabla de contenidos__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objetivos</a></li>\n",
    "    <li>\n",
    "        <a href=\"#What-is-generative-AI?\">¿Qué es la IA generativa?</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Real-world-impact-of-generative-AI\">Impacto real de la IA generativa</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#1.-Art-and-creativity\">Arte y creatividad</a></li>\n",
    "            <li><a href=\"#2.-Natural-language-processing-(NLP)\">Procesamiento del lenguaje natural (NLP)</a></li>\n",
    "            <li><a href=\"#3.-Computer-vision\">Visión por computadora</a></li>\n",
    "            <li><a href=\"#4.-Virtual-avatars\">Avatares virtuales</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Text-generation-before-transformers\">Generación de texto antes de los transformers</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#1.-N-gram-language-models\">Modelos de lenguaje n-grama</a></li>\n",
    "            <li><a href=\"#2.-Recurrent-neural-networks-(RNN)\">Redes neuronales recurrentes (RNN)</a></li>\n",
    "            <li><a href=\"#3.-Long-short-term-memory-(LSTM)-and-gated-recurrent-units-(GRUs)\">Memoria a largo plazo (LSTM) y unidades recurrentes con puertas (GRUs)</a></li>\n",
    "            <li><a href=\"#4.-Seq2seq-models-with-attention\">Modelos Seq2seq con atención</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Transformers\">Transformers</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Implementation:-Building-a-simple-chatbot-with-transformers\">Implementación: Construyendo un chatbot simple con transformers</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Step-1:-Installing-libraries\">Paso 1: Instalación de librerías</a></li>\n",
    "                    <li><a href=\"#Step-2:-Importing-the-required-tools-from-the-transformers-library\">\n",
    "                        Paso 2: Importando las herramientas requeridas de la librería transformers</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objetivos\n",
    "\n",
    "Al completar este cuaderno, podrás:\n",
    "\n",
    "- Comprender la IA generativa y su impacto en diversos ámbitos.\n",
    "- Familiarizarte con los distintos tipos de modelos en IA generativa.\n",
    "- Adquirir habilidades para construir e interactuar con un chatbot utilizando transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Qué es la IA generativa?\n",
    "Imagina presentar a una computadora una amplia variedad de pinturas. Después de analizarlas, intenta crear una pintura única por sí misma. Esta capacidad se denomina IA generativa. Esencialmente, la computadora se inspira en el contenido proporcionado y lo utiliza para crear algo nuevo.\n",
    "\n",
    "#### Impacto real de la IA generativa\n",
    "La IA generativa está transformando múltiples industrias. Sus aplicaciones abarcan:\n",
    "\n",
    "#### 1. Arte y creatividad\n",
    "- **Arte generativo:** Artistas que emplean algoritmos de IA generativa pueden crear obras impresionantes al aprender de obras maestras existentes y producir piezas únicas inspiradas en ellas. Estas obras generadas por IA han ganado reconocimiento en el mundo del arte.\n",
    "- **Composición musical:** Proyectos en el ámbito de la IA generativa se han utilizado para componer música. Aprenden de un vasto conjunto de composiciones y pueden generar piezas originales en diversos estilos, desde clásico hasta jazz, revolucionando la industria musical.\n",
    "\n",
    "#### 2. Procesamiento del lenguaje natural (NLP)\n",
    "- **Generación de contenido:** Herramientas como el transformer generativo preentrenado (GPT) han demostrado su capacidad para generar textos coherentes y contextualizados. Pueden ayudar a creadores de contenido generando artículos, historias o textos publicitarios, siendo herramientas muy útiles.\n",
    "- **Chatbots y asistentes virtuales:** La IA generativa impulsa muchos de los chatbots y asistentes virtuales actuales. Estos agentes conversacionales basados en IA comprenden y generan respuestas similares a las humanas, mejorando la experiencia del usuario.\n",
    "- **Escritura de código:** Los modelos de IA generativa también pueden producir fragmentos de código basados en descripciones o requerimientos, agilizando el desarrollo de software.\n",
    "\n",
    "#### 3. Visión por computadora\n",
    "- **Síntesis de imágenes:** Modelos como el de análisis de datos con modelo de lenguaje para generación y exploración, conocido frecuentemente como DALL-E, pueden generar imágenes a partir de descripciones textuales. Esta tecnología tiene aplicaciones en diseño gráfico, publicidad y creación de contenido visual para marketing.\n",
    "- **Detección de deepfakes:** Con el avance de las técnicas de IA generativa, también ha aumentado la generación de contenido deepfake. En consecuencia, la IA generativa ahora participa en el desarrollo de herramientas y técnicas para detectar y combatir la difusión de información errónea mediante videos manipulados.\n",
    "\n",
    "#### 4. Avatares virtuales\n",
    "- **Entretenimiento:** La IA generativa se utiliza para crear avatares virtuales para juegos y entretenimiento. Estos avatares imitan expresiones y emociones humanas, aumentando el compromiso en entornos virtuales.\n",
    "- **Marketing:** Los influencers virtuales, impulsados por la IA generativa, están en auge en el marketing digital. Las marcas utilizan estas personalidades virtuales para promocionar sus productos y servicios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructuras neuronales detrás de la IA generativa\n",
    "\n",
    "Antes de contar con los poderosos transformers, que actúan como lectores súper rápidos y comprenden muchas palabras al mismo tiempo— existían otros métodos para que las computadoras generaran texto. Estos métodos fueron los bloques constructores que permitieron las increíbles capacidades actuales.\n",
    "\n",
    "#### Modelos de lenguaje a gran escala (LLMs)\n",
    "Los modelos de lenguaje a gran escala son como cerebros supercargados. Son programas de computadora masivos con muchas \"neuronas\" que aprenden de enormes cantidades de texto. Estos modelos se entrenan para tareas como comprender y generar texto, y se utilizan en muchas aplicaciones. Sin embargo, tienen una limitación: no son muy buenos para entender el contexto global o el significado profundo de las palabras. Funcionan bien para predicciones simples, pero tienen dificultades con textos más complejos.\n",
    "\n",
    "### Generación de texto antes de los transformers\n",
    "\n",
    "#### 1. Modelos de lenguaje n-gramas\n",
    "Los modelos n-grama son como detectives del lenguaje. Predicen qué palabra viene a continuación en una oración basándose en las palabras previas. Por ejemplo, si dices \"El cielo es\", estos modelos adivinan que la siguiente palabra podría ser \"azul\".\n",
    "\n",
    "#### 2. Redes neuronales recurrentes (RNN)\n",
    "Las redes neuronales recurrentes (RNN) están especialmente diseñadas para manejar datos secuenciales, lo que las convierte en una herramienta poderosa para tareas como el modelado del lenguaje y el pronóstico de series temporales. La esencia de su diseño radica en mantener una \"memoria\" o \"estado oculto\" a lo largo de la secuencia mediante bucles. Esto permite a las RNN reconocer y capturar las dependencias temporales inherentes a los datos secuenciales.\n",
    "- **Estado oculto:** Conocido como la \"memoria\" de la red, es un almacenamiento dinámico de información sobre entradas previas. Con cada nueva entrada, este estado se actualiza considerando tanto la entrada nueva como su valor anterior.\n",
    "- **Dependencia temporal:** Los bucles en las RNN facilitan la transferencia de información a través de los distintos pasos de la secuencia.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0J87EN/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\" width=\"50%\" height=\"70%\"> \n",
    "\n",
    "<div style=\"text-align:center\"><a href=\"https://commons.wikimedia.org/wiki/File:%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\">Fuente de la imagen</a></div>\n",
    "\n",
    "Ilustración del funcionamiento de una RNN: Considera una secuencia simple, por ejemplo la oración: \"I love RNNs\". La RNN interpreta esta oración palabra por palabra. Comenzando con la palabra \"I\", la RNN la procesa, genera una salida y actualiza su estado oculto. Al pasar a la palabra \"love\", la RNN la procesa junto al estado oculto actualizado, que ya contiene información sobre \"I\". Este patrón de procesamiento y actualización continúa hasta la última palabra. Al final, el estado oculto idealmente resume la información de toda la oración.\n",
    "\n",
    "#### 3. Memoria a largo plazo (LSTM) y unidades recurrentes con puertas (GRUs)\n",
    "La memoria a largo plazo (LSTM) y las unidades recurrentes con puertas (GRUs) son variantes avanzadas de las RNN, diseñadas para superar las limitaciones de las RNN tradicionales y mejorar su capacidad para modelar datos secuenciales de manera efectiva. Procesan las secuencias elemento por elemento y mantienen un estado interno para recordar información pasada. Aunque fueron efectivas para muchas tareas, tuvieron dificultades con secuencias muy largas y dependencias a largo plazo.\n",
    "\n",
    "#### 4. Modelos Seq2seq con atención\n",
    "- Los modelos de secuencia a secuencia (seq2seq), a menudo construidos con RNN o LSTM, fueron diseñados para tareas como la traducción, donde una secuencia de entrada se transforma en una secuencia de salida.\n",
    "- Se introdujo el mecanismo de atención para permitir que el modelo se \"concentre\" en las partes relevantes de la secuencia de entrada al generar la salida, mejorando significativamente el rendimiento en tareas como la traducción automática.\n",
    "\n",
    "Aunque estos métodos representaron avances importantes en la generación de texto, la introducción de los transformers supuso un cambio de paradigma. Los transformers, con su mecanismo de autoatención, demostraron ser sumamente eficientes para capturar información contextual en secuencias largas, estableciendo nuevos estándares en diversas tareas de NLP.\n",
    "\n",
    "#### Transformers\n",
    "Propuesto en el artículo [Attention Is All You Need](), la arquitectura transformer reemplazó el procesamiento secuencial por procesamiento en paralelo. ¿El componente clave de su éxito? El mecanismo de atención, o más precisamente, la autoatención.\n",
    "\n",
    "Pasos clave incluyen:\n",
    "- **Tokenización:** El primer paso es descomponer una oración en tokens (palabras o subpalabras).\n",
    "- **Embedding:** Cada token se representa como un vector que captura su significado.\n",
    "- **Autoatención:** El modelo calcula puntuaciones que determinan la importancia de cada palabra en relación a las demás dentro de la secuencia. Estas puntuaciones se usan para ponderar los tokens y generar una nueva representación de la secuencia. Por ejemplo, en la oración \"Le dio un regalo porque ella le había ayudado\", comprender a quién se refiere \"ella\" requiere que el modelo preste atención a otras palabras. El transformer hace esto para cada palabra, considerando todo el contexto, lo que resulta muy eficaz para entender el significado.\n",
    "- **Redes neuronales feed-forward:** Después de la atención, cada posición se procesa de manera independiente a través de una red neuronal feed-forward.\n",
    "- **Secuencia de salida:** El modelo produce una secuencia de salida, que puede utilizarse para tareas como clasificación, traducción o generación de texto.\n",
    "- **Capas:** Los transformers son modelos profundos con múltiples capas de atención y redes feed-forward, lo que les permite aprender patrones complejos.\n",
    "\n",
    "La flexibilidad de esta arquitectura ha permitido utilizar los transformers más allá del NLP, aplicándolos también al procesamiento de imágenes y video. En NLP, modelos basados en transformers como BERT, GPT y sus variantes han establecido resultados de vanguardia en tareas que van desde la clasificación de textos hasta la traducción.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación: Construyendo un chatbot simple con transformers\n",
    "Ahora construirás un chatbot simple utilizando la biblioteca `transformers` de Hugging Face, una herramienta de NLP de código abierto con muchas funcionalidades útiles.\n",
    "#### Paso 1: Instalación de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar las bibliotecas necesarias\n",
    "!pip install -qq tensorflow\n",
    "!pip install -qq transformers\n",
    "!pip install sentencepiece\n",
    "!pip install torch\n",
    "!pip install torchtext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2: Importando las herramientas requeridas de la librería transformers\n",
    "En el siguiente script se inicializan variables utilizando dos clases fundamentales de la biblioteca transformers:\n",
    "- `modelo` es una instancia de la clase `AutoModelForSeq2SeqLM`, la cual te permite interactuar con el modelo de lenguaje elegido.\n",
    "- `tokenizer` es una instancia de la clase `AutoTokenizer`, que facilita el procesamiento de tu entrada al convertir el texto en \"tokens\", la forma que tiene el modelo de interpretar el lenguaje.\n",
    "Se ha elegido el modelo \"facebook/blenderbot-400M-distill\" para este ejemplo, ya que está disponible de forma gratuita bajo una licencia de código abierto y funciona a un ritmo relativamente rápido. Para explorar una variedad de modelos y sus capacidades, visita la página de [Modelos de Hugging Face](https://huggingface.co/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Seleccionando el modelo. Se utilizará \"facebook/blenderbot-400M-distill\" en este ejemplo.\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "# Cargar el modelo y el tokenizer\n",
    "modelo = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de la inicialización, configuremos la función de chat para permitir la interacción en tiempo real con el chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de chat\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # Obtener la entrada del usuario\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Condiciones de salida\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenizar la entrada y generar la respuesta\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = modelo.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Mostrar la respuesta del chatbot\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Iniciar el chat\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has interactuado exitosamente con tu chatbot. Al proporcionarle una solicitud, el chatbot utilizó el poder de la biblioteca transformers y el modelo subyacente para generar una respuesta. Esto ejemplifica la destreza de los modelos basados en transformers para comprender y generar textos similares a los humanos a partir de un contexto dado. A medida que continúes interactuando, notarás su capacidad para simular una amplia gama de temas y estilos conversacionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 3: Probando otro modelo de lenguaje y comparando la salida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes utilizar un modelo de lenguaje diferente, por ejemplo el modelo \"[flan-t5-base](https://huggingface.co/google/flan-t5-base)\" de Google, para crear un chatbot similar. Utiliza una función de chat parecida a la definida en el Paso 2 y compara las salidas de ambos modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Seleccionar el modelo de Google flan-t5-base\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vamos a chatear con otro bot\n",
    "def chat_with_another_bot():\n",
    "    while True:\n",
    "        # Obtener la entrada del usuario\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Condiciones de salida\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenizar la entrada y generar la respuesta\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = modelo.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Mostrar la respuesta del chatbot\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Iniciar el chat\n",
    "chat_with_another_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen muchos modelos de lenguaje disponibles en Hugging Face. En el siguiente ejercicio, compararás la salida para la misma entrada utilizando dos modelos diferentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crea un chatbot utilizando diferentes modelos de Hugging Face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un chatbot simple utilizando la biblioteca transformers de Hugging Face (https://huggingface.co/models). Ejecuta el código usando los siguientes modelos y compara la salida. Los modelos son \"[google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\" y \"[facebook/bart-base](https://huggingface.co/facebook/bart-base)\".\n",
    "(Nota: Dependiendo del modelo seleccionado, es posible que notes diferencias en la salida del chatbot. Múltiples factores, como el entrenamiento y ajuste fino del modelo, influyen en el resultado.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega el código para el ejercicio aquí:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para ver la solución</summary>\n",
    "\n",
    "```python\n",
    "import sentencepiece\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-small\"  # aquí se puede cambiar el nombre del modelo según lo desees.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "modelo = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "prev_pub_hash": "5c5bec06088ad96b1ecbe1871624b6f4fcc99062c9772bf4e6ad46b1d556c1b8"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
