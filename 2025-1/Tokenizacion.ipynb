{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementando la tokenización**\n",
    "\n",
    "Los tokenizadores son herramientas esenciales en el procesamiento del lenguaje natural que descomponen el texto en unidades más pequeñas llamadas tokens. Estos tokens pueden ser palabras, caracteres o subpalabras, haciendo que un texto complejo sea comprensible para las computadoras. Al dividir el texto en partes manejables, los tokenizadores permiten que las máquinas procesen y analicen el lenguaje humano, impulsando diversas aplicaciones relacionadas con el lenguaje como la traducción, el análisis de sentimientos y los chatbots. Esencialmente, los tokenizadores cierran la brecha entre el lenguaje humano y la comprensión de la máquina.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuración**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este cuaderno, utilizarás las siguientes librerías:\n",
    "\n",
    "* [**nltk**](https://www.nltk.org/) o Natural Language Toolkit, se empleará para tareas de gestión de datos. Ofrece herramientas y recursos integrales para procesar texto en lenguaje natural, lo que la hace una opción valiosa para tareas como el preprocesamiento y análisis de texto.\n",
    "\n",
    "* [**spaCy**](https://spacy.io/) es una librería de software de código abierto para el procesamiento avanzado del lenguaje natural en Python. spaCy es reconocido por su velocidad y precisión al procesar grandes volúmenes de datos textuales.\n",
    "\n",
    "* [**BertTokenizer**](https://huggingface.co/docs/transformers/main_classes/tokenizer#berttokenizer) forma parte de la librería Hugging Face Transformers, una librería popular para trabajar con modelos de lenguaje preentrenados de última generación. BertTokenizer está diseñado específicamente para tokenizar texto según las especificaciones del modelo BERT.\n",
    "\n",
    "* [**XLNetTokenizer**](https://huggingface.co/docs/transformers/main_classes/tokenizer#xlnettokenizer) es otro componente de la librería Hugging Face Transformers. Está adaptado para tokenizar texto de acuerdo con los requerimientos del modelo XLNet.\n",
    "\n",
    "* [**torchtext**](https://pytorch.org/text/stable/index.html) es parte del ecosistema de PyTorch, para manejar diversas tareas de procesamiento del lenguaje natural. Simplifica el proceso de trabajar con datos textuales y provee funcionalidades para el preprocesamiento de datos, tokenización, gestión de vocabulario y agrupación en lotes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalación de librerías requeridas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install transformers\n",
    "#!pip install sentencepiece\n",
    "#!pip install spacy\n",
    "#!pip install numpy==1.24\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download de_core_news_sm\n",
    "#!pip install numpy scikit-learn\n",
    "#!pip install torch==2.0.1\n",
    "#!pip install torchtext==0.15.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de librerías requeridas\n",
    "\n",
    "_Se recomienda importar todas las librerías requeridas en un solo lugar (aquí):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Qué es un tokenizador y por qué lo usamos?\n",
    "\n",
    "Los tokenizadores juegan un papel fundamental en el procesamiento del lenguaje natural, segmentando el texto en unidades más pequeñas conocidas como tokens. Estos tokens se transforman posteriormente en representaciones numéricas llamadas índices de tokens, que son utilizados directamente por los algoritmos de aprendizaje profundo.\n",
    "<center>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%201.png\" width=\"50%\" alt=\"Image Description\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipos de tokenizador\n",
    "\n",
    "La representación significativa puede variar dependiendo del modelo en uso. Diversos modelos emplean algoritmos de tokenización distintos, y se cubrirán ampliamente los siguientes enfoques. Transformar el texto en valores numéricos puede parecer sencillo al principio, pero abarca varias consideraciones que se deben tener en cuenta.\n",
    "<center>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%202.png\" width=\"50%\" alt=\"Image Description\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizador basado en palabras\n",
    "\n",
    "#### nltk\n",
    "\n",
    "Como su nombre indica, se trata de dividir el texto basándose en palabras. Existen diferentes reglas para los tokenizadores basados en palabras, como dividir por espacios o por puntuación. Cada opción asigna un ID específico a la palabra dividida. Aquí se utiliza el ```word_tokenize``` de nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"This is a sample sentence for word tokenization.\"\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerías generales como nltk y spaCy a menudo dividen palabras como \"don't\" y \"couldn't\", que son contracciones, en palabras individuales separadas. **No existe una regla universal, y cada librería tiene sus propias reglas de tokenización para tokenizadores basados en palabras**. Sin embargo, la pauta general es preservar el formato de entrada después de la tokenización para que coincida con la forma en que se entrenó el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto muestra word_tokenize de la librería nltk\n",
    "\n",
    "texto = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto muestra el uso del tokenizador de 'spaCy' con la función get_tokenizer de torchtext\n",
    "\n",
    "texto = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Creando una lista de tokens e imprimiéndola\n",
    "token_list = [token.text for token in doc]\n",
    "print(\"Tokens:\", token_list)\n",
    "\n",
    "# Mostrando detalles de cada token\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación de algunas líneas:\n",
    "- I PRON nsubj: \"I\" es un pronombre (PRON) y es el sujeto nominal (nsubj) de la oración.\n",
    "- help VERB ROOT: \"help\" es un verbo (VERB) y es la acción principal (ROOT) de la oración.\n",
    "- afraid ADJ acomp: \"afraid\" es un adjetivo (ADJ) y es un complemento adjetival (acomp) que aporta más información sobre un estado o cualidad relacionado con el verbo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con este algoritmo es que las palabras con significados similares serán asignadas con IDs diferentes, lo que resulta en que se traten como palabras completamente separadas con significados distintos. Por ejemplo, *Unicorns* es la forma plural de *Unicorn*, pero un tokenizador basado en palabras las tokenizaría como dos palabras separadas, lo que podría causar que el modelo no reconozca su relación semántica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"Unicorns are real. I saw a unicorn yesterday.\"\n",
    "token = word_tokenize(texto)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada palabra se divide en un token, lo que conduce a un aumento significativo en el vocabulario total del modelo. Cada token se asigna a un vector grande que contiene los significados de la palabra, resultando en parámetros de modelo grandes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los idiomas generalmente tienen una gran cantidad de palabras, por lo que los vocabularios basados en ellas siempre serán extensos. Sin embargo, el número de caracteres en un idioma siempre es menor en comparación con el número de palabras. A continuación, exploraremos los tokenizadores basados en caracteres.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizador basado en caracteres\n",
    "\n",
    "Como su nombre indica, la tokenización basada en caracteres implica dividir el texto en caracteres individuales. La ventaja de utilizar este enfoque es que los vocabularios resultantes son intrínsecamente pequeños. Además, dado que los idiomas tienen un conjunto limitado de caracteres, el número de tokens fuera del vocabulario también es limitado, reduciendo el desperdicio de tokens.\n",
    "\n",
    "Por ejemplo:\n",
    "Texto de entrada: `This is a sample sentence for tokenization.`\n",
    "\n",
    "Salida de tokenización basada en caracteres: `['T', 'h', 'i', 's', 'i', 's', 'a', 's', 'a', 'm', 'p', 'l', 'e', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', 'f', 'o', 'r', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.']`\n",
    "\n",
    "Sin embargo, es importante notar que la tokenización basada en caracteres tiene sus limitaciones. Los caracteres individuales pueden no transmitir la misma información que las palabras completas, y la longitud total de los tokens aumenta significativamente, lo que podría causar problemas con el tamaño del modelo y una pérdida de rendimiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has explorado las limitaciones de los métodos de tokenización basados en palabras y caracteres. Para aprovechar las ventajas de ambos enfoques, los transformers emplean la tokenización basada en subpalabras, que se discutirá a continuación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizador basado en subpalabras\n",
    "\n",
    "El tokenizador basado en subpalabras permite que las palabras de uso frecuente permanezcan sin dividir, mientras que descompone las palabras poco frecuentes en subpalabras significativas. Técnicas como [SentencePiece](https://github.com/google/sentencepiece) o [WordPiece](https://paperswithcode.com/method/wordpiece) se utilizan comúnmente para la tokenización de subpalabras. Estos métodos aprenden unidades de subpalabras a partir de un corpus de texto dado, identificando prefijos, sufijos y raíces comunes como tokens de subpalabras basados en su frecuencia de aparición. Este enfoque ofrece la ventaja de representar una gama más amplia de palabras y adaptarse a los patrones específicos del lenguaje dentro de un corpus de texto.\n",
    "\n",
    "En ambos ejemplos a continuación, las palabras se dividen en subpalabras, lo que ayuda a preservar la información semántica asociada con la palabra completa. Por ejemplo, \"Unhappiness\" se divide en \"un\" y \"happiness\", las cuales pueden aparecer como subpalabras independientes. Cuando combinamos estas subpalabras individuales, forman \"unhappiness\", que conserva su contexto significativo. Este enfoque ayuda a mantener la información general y el significado semántico de las palabras.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%203.png\" width=\"50%\" alt=\"Image Description\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPiece\n",
    "\n",
    "Inicialmente, WordPiece inicializa su vocabulario para incluir cada carácter presente en los datos de entrenamiento y aprende progresivamente un número especificado de reglas de fusión. WordPiece no selecciona el par de símbolos más frecuente, sino aquel que maximiza la probabilidad de los datos de entrenamiento al añadirse al vocabulario. En esencia, WordPiece evalúa lo que sacrifica al fusionar dos símbolos para asegurar que sea un esfuerzo que valga la pena.\n",
    "\n",
    "Ahora, el tokenizador WordPiece está implementado en BertTokenizer.\n",
    "Ten en cuenta que BertTokenizer trata las palabras compuestas como tokens separados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se muestra un desglose de la salida:\n",
    "- 'ibm': \"IBM\" se tokeniza como 'ibm'. BERT convierte los tokens a minúsculas, ya que no conserva la información de mayúsculas cuando se utiliza el modelo \"bert-base-uncased\".\n",
    "- 'taught', 'me', '.': Estos tokens son iguales a las palabras o puntuaciones originales, solo que en minúsculas (excepto la puntuación).\n",
    "- 'token', '##ization': \"Tokenization\" se divide en dos tokens. \"Token\" es una palabra completa, y \"##ization\" es una parte de la palabra original. El \"##\" indica que \"ization\" debe conectarse de nuevo a \"token\" al realizar la detokenización (transformar tokens de vuelta a palabras).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram y SentencePiece\n",
    "\n",
    "Unigram es un método para dividir palabras o texto en piezas más pequeñas. Lo logra comenzando con una lista amplia de posibilidades y reduciéndola gradualmente según la frecuencia con la que aparecen esas piezas en el texto. Este enfoque ayuda a una tokenización eficiente del texto.\n",
    "\n",
    "SentencePiece es una herramienta que toma el texto, lo divide en partes más pequeñas y manejables, asigna IDs a estos segmentos y se asegura de hacerlo de manera consistente. En consecuencia, si utilizas SentencePiece en el mismo texto de manera repetida, obtendrás consistentemente las mismas subpalabras e IDs.\n",
    "\n",
    "Unigram y SentencePiece trabajan juntos implementando el método de tokenización de subpalabras de unigrama dentro del marco de SentencePiece. SentencePiece maneja la segmentación de subpalabras y la asignación de IDs, mientras que los principios de Unigrama guían el proceso de reducción del vocabulario para crear una representación más eficiente de los datos textuales. Esta combinación es especialmente valiosa para diversas tareas de NLP en las que la tokenización de subpalabras puede mejorar el rendimiento de los modelos de lenguaje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es lo que sucede con cada token:\n",
    "- '▁IBM': El \"▁\" (a menudo referido como \"carácter de espacio\") antes de \"IBM\" indica que este token está precedido por un espacio en el texto original. \"IBM\" se mantiene tal cual porque es reconocido como un token completo por XLNet y preserva la capitalización, ya que se está utilizando el modelo \"xlnet-base-cased\".\n",
    "- '▁taught', '▁me', '▁token': De manera similar, estos tokens están prefijados con \"▁\" para indicar que son palabras nuevas precedidas por un espacio en el texto original, preservando la palabra completa y manteniendo la capitalización original.\n",
    "- 'ization': A diferencia de \"BertTokenizer\", \"XLNetTokenizer\" no utiliza \"##\" para indicar tokens de subpalabras. \"ization\" aparece como un token propio sin prefijo porque sigue directamente a la palabra anterior \"token\" sin un espacio en el texto original.\n",
    "- '.': El punto se tokeniza como un token separado ya que la puntuación se trata por separado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización con PyTorch \n",
    "\n",
    "En PyTorch, especialmente con la librería `torchtext`, el tokenizador descompone el texto de un conjunto de datos en palabras o subpalabras individuales, facilitando su conversión a un formato numérico. Después de la tokenización, el vocabulario asigna a estos tokens enteros únicos, permitiendo que sean utilizados en redes neuronales. Este proceso es vital porque los modelos de aprendizaje profundo operan con datos numéricos y no pueden procesar texto sin formato directamente. \n",
    "\n",
    "Así, la tokenización y la asignación del vocabulario sirven como puente entre el texto legible para humanos y los datos numéricos operables por la máquina. Considera el siguiente conjunto de datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente línea importa la función ```get_tokenizer``` desde el módulo ```torchtext.data.utils```. En la librería torchtext, la función ```get_tokenizer``` se utiliza para obtener un tokenizador por nombre. Proporciona soporte para una variedad de métodos de tokenización, incluyendo la división básica de cadenas, y devuelve varios tokenizadores según el argumento que se le pase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica el tokenizador al conjunto de datos. Nota: Si se selecciona ```basic_english```, retorna la función ```_basic_english_normalize()```, que normaliza la cadena primero y luego la divide por espacios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Índices de tokens\n",
    "Se representan las palabras como números, ya que los algoritmos de PLN pueden procesar y manipular números de manera más eficiente y rápida que el texto sin procesar. Se utiliza la función **```build_vocab_from_iterator```**, cuyo resultado se denomina típicamente 'índices de tokens' o simplemente 'índices'. Estos índices representan las representaciones numéricas de los tokens en el vocabulario.\n",
    "\n",
    "La función **```build_vocab_from_iterator```**, cuando se aplica a una lista de tokens, asigna un índice único a cada token basado en su posición en el vocabulario. Estos índices sirven como una forma de representar los tokens en un formato numérico que puede ser procesado fácilmente por modelos de aprendizaje automático.\n",
    "\n",
    "Por ejemplo, dado un vocabulario con tokens `[\"apple\", \"banana\", \"orange\"]`, los índices correspondientes podrían ser `[0, 1, 2]`, donde \"apple\" se representa con el índice 0, \"banana\" con el 1, y \"orange\" con el 2.\n",
    "\n",
    "**```dataset```** es un iterable. Por lo tanto, se utiliza una función generadora `yield_tokens` para aplicar el **```tokenizer```**. El propósito de la función generadora **```yield_tokens```** es producir textos tokenizados uno a la vez. En lugar de procesar todo el conjunto de datos y devolver todos los textos tokenizados de una vez, la función generadora procesa y produce cada texto tokenizado individualmente a medida que se solicita. \n",
    "\n",
    "El proceso de tokenización se realiza de forma perezosa, lo que significa que el siguiente texto tokenizado se genera solo cuando es necesario, ahorrando memoria y recursos computacionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    # Función generadora para producir tokens a partir del conjunto de datos\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un iterador llamado my_iterator utilizando la función generadora yield_tokens\n",
    "my_iterator = yield_tokens(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto crea un iterador llamado **```my_iterator```** usando la función generadora. Para comenzar la evaluación del generador y recuperar los valores, puedes iterar sobre **```my_iterator```** utilizando un bucle for o recuperar valores usando la función **```next()```**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el siguiente elemento del iterador my_iterator\n",
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyes un vocabulario a partir de los textos tokenizados generados por la función generadora **```yield_tokens```**, que procesa el conjunto de datos. La función **```build_vocab_from_iterator()```** construye el vocabulario, incluyendo un token especial `unk` para representar las palabras fuera del vocabulario. \n",
    "\n",
    "#### Fuera del vocabulario (OOV)\n",
    "Cuando los datos de texto se tokenizan, puede haber palabras que no están presentes en el vocabulario porque son raras o no vistas durante el proceso de construcción del vocabulario. Al encontrar dichas palabras fuera del vocabulario durante tareas reales de procesamiento del lenguaje, como la generación de texto o el modelado de lenguaje, el modelo puede usar el token `<unk>` para representarlas.\n",
    "\n",
    "Por ejemplo, si la palabra \"apple\" está presente en el vocabulario, pero \"pineapple\" no lo está, \"apple\" se usará normalmente en el texto, pero \"pineapple\" (al ser una palabra fuera del vocabulario) se reemplazaría por el token `<unk>`.\n",
    "\n",
    "Al incluir el token `<unk>` en el vocabulario, se proporciona una forma consistente de manejar palabras fuera del vocabulario en tu modelo de lenguaje u otras tareas de procesamiento del lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código demuestra cómo obtener una oración tokenizada de un iterador, convertir sus tokens en índices utilizando un vocabulario provisto, y luego imprimir tanto la oración original como sus índices correspondientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)  # Obtener la siguiente oración tokenizada\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]  # Obtener los índices de los tokens\n",
    "    return tokenized_sentence, token_indices\n",
    "\n",
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "\n",
    "print(\"Oracion tokenizadas:\", tokenized_sentence)\n",
    "print(\"Índices de tokens:\", token_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando las líneas de código proporcionadas anteriormente en un ejemplo sencillo, demuestra la tokenización y la construcción del vocabulario en PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineas = [\"IBM taught me tokenization\", \n",
    "         \"Special tokenizers are ready and they will blow your mind\", \n",
    "         \"just saying hi!\"]\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for linea in lineas:\n",
    "    tokenized_line = tokenizer_en(linea)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
    "\n",
    "print(\"Líneas después de agregar tokens especiales:\\n\", tokens)\n",
    "\n",
    "# Construir vocabulario sin unk_init\n",
    "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Vocabulario e IDs de tokens\n",
    "print(\"Vocabulary:\", vocab.get_itos())\n",
    "print(\"Token IDs for 'tokenization':\", vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desglosemos la salida:\n",
    "1. **Tokens especiales**:\n",
    "- Token: \"`<unk>`\", Índice: 0: `<unk>` significa \"desconocido\" y representa las palabras que no se vieron durante la construcción del vocabulario, usualmente durante la inferencia en texto nuevo.\n",
    "- Token: \"`<pad>`\", Índice: 1: `<pad>` es un token de \"relleno\" utilizado para hacer que las secuencias de palabras tengan la misma longitud al agruparlas en lotes.\n",
    "- Token: \"`<bos>`\", Índice: 2: `<bos>` es el acrónimo de \"inicio de secuencia\" y se usa para denotar el comienzo de una secuencia de texto.\n",
    "- Token: \"`<eos>`\", Índice: 3: `<eos>` es el acrónimo de \"fin de secuencia\" y se usa para denotar el final de una secuencia de texto.\n",
    "\n",
    "2. **Tokens de palabras**:\n",
    "El resto de los tokens son palabras o signos de puntuación extraídos de las oraciones proporcionadas, cada uno asignado a un índice único:\n",
    "- Token: \"IBM\", Índice: 5\n",
    "- Token: \"taught\", Índice: 16\n",
    "- Token: \"me\", Índice: 12\n",
    "    ... y así sucesivamente.\n",
    "    \n",
    "3. **Vocabulario**:\n",
    "Representa el número total de tokens en las oraciones sobre las cuales se construyó el vocabulario.\n",
    "    \n",
    "4. **IDs de tokens para 'tokenization'**:\n",
    "Representa los IDs de tokens asignados en el vocabulario donde un número representa su presencia en la oración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nueva_linea = \"I learned about embeddings and attention mechanisms.\"\n",
    "\n",
    "# Tokenizar la nueva línea\n",
    "tokenized_new_line = tokenizer_en(nueva_linea)\n",
    "tokenized_new_line = ['<bos>'] + tokenized_new_line + ['<eos>']\n",
    "\n",
    "# Rellenar la nueva línea para que coincida con la longitud máxima de las líneas anteriores\n",
    "new_line_padded = tokenized_new_line + ['<pad>'] * (max_length - len(tokenized_new_line))\n",
    "\n",
    "# Convertir tokens a IDs y manejar palabras desconocidas\n",
    "new_line_ids = [vocab[token] if token in vocab else vocab['<unk>'] for token in new_line_padded]\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(\"Id de tokens para nueva linea:\", new_line_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desglosemos la salida:\n",
    "\n",
    "1. **Tokens especiales**:\n",
    "- Token: \"`<unk>`\", Índice: 0: `<unk>` significa \"desconocido\" y representa las palabras que no se vieron durante la construcción del vocabulario, usualmente durante la inferencia en texto nuevo.\n",
    "- Token: \"`<pad>`\", Índice: 1: `<pad>` es un token de \"relleno\" utilizado para hacer que las secuencias de palabras tengan la misma longitud al agruparlas en lotes.\n",
    "- Token: \"`<bos>`\", Índice: 2: `<bos>` es el acrónimo de \"inicio de secuencia\" y se usa para denotar el comienzo de una secuencia de texto.\n",
    "- Token: \"`<eos>`\", Índice: 3: `<eos>` es el acrónimo de \"fin de secuencia\" y se usa para denotar el final de una secuencia de texto.\n",
    "\n",
    "2. El token **`and`** es reconocido en la oración y se le asigna **`token_id` - 7**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio: Tokenización comparativa de texto y análisis de rendimiento\n",
    "- Objetivo: Evaluar y comparar las capacidades de tokenización de cuatro librerías de PLN diferentes (`nltk`, `spaCy`, `BertTokenizer` y `XLNetTokenizer`) analizando la frecuencia de las palabras tokenizadas y midiendo el tiempo de procesamiento de cada herramienta usando `datetime`.\n",
    "- El texto para la tokenización es el siguiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"\n",
    "Going through the world of tokenization has been like walking through a huge maze made of words, symbols, and meanings. Each turn shows a bit more about the cool ways computers learn to understand our language. And while I'm still finding my way through it, the journey’s been enlightening and, honestly, a bunch of fun.\n",
    "Eager to see where this learning path takes me next!\"\n",
    "\"\"\"\n",
    "\n",
    "# Contar y mostrar tokens y su frecuencia\n",
    "#from collections import Counter\n",
    "#def show_frequencies(tokens, method_name):\n",
    "#    print(f\"{method_name} Token Frequencies: {dict(Counter(tokens))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "prev_pub_hash": "e2e4ac52377879aab7a3473f68f22b37f91497ed67c9b2465cec45462c28fdec"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
