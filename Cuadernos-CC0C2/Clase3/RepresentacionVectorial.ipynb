{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación de texto elemental\n",
    "\n",
    "En el procesamiento del lenguaje, los vectores `x` se derivan de datos textuales, para reflejar diversas propiedades lingüísticas del texto.  \n",
    "\n",
    "Yoav Goldberg.\n",
    "\n",
    "La representación de texto en del procesamiento del lenguaje natural se refiere a métodos y técnicas para convertir texto en una forma que las computadoras pueden procesar más eficientemente. El objetivo de estas representaciones es capturar la información semántica y sintáctica del texto, de manera que sea posible realizar tareas como la clasificación de texto, búsqueda de información.\n",
    "\n",
    "Estos enfoques se clasifican en cuatro categorías: \n",
    "\n",
    "- Enfoques básicos de vectorización \n",
    "- Representaciones distribuidas \n",
    "- Representación del lenguaje universal \n",
    "- Características hechas a mano "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de espacios vectoriales\n",
    "\n",
    "Los modelos de espacio vectorial son una familia de algoritmos utilizados en el procesamiento del lenguaje natural (NLP), recuperación de información, y otras áreas de la inteligencia artificial, para representar elementos textuales, como palabras, frases, oraciones o documentos, como vectores en un espacio multidimensional. La idea clave detrás de estos modelos es que las entidades semánticamente similares se representan por puntos cercanos en el espacio vectorial, lo que facilita la comparación y el análisis de las relaciones entre ellas.\n",
    "\n",
    "En el contexto de NLP y recuperación de información, los modelos de espacio vectorial permiten realizar tareas como la búsqueda de documentos similares, la clasificación de textos, y el análisis de sentimientos, entre otras, mediante técnicas matemáticas y estadísticas. \n",
    "\n",
    "Algunos conceptos y técnicas importantes relacionados con los modelos de espacio vectorial incluyen:\n",
    "\n",
    "- Representación de Texto: Convertir texto en vectores numéricos. Técnicas como la bolsa de palabras (BoW) y TF-IDF son ejemplos clásicos de cómo se puede realizar esta conversión, asignando a cada palabra o término una dimensión en el espacio vectorial y utilizando la frecuencia de las palabras para determinar los valores en esas dimensiones.\n",
    "\n",
    "- Embeddings de Palabras: Métodos más avanzados como Word2Vec, GloVe, y FastText aprenden representaciones vectoriales densas de palabras a partir de grandes corpus de texto. Estos embeddings capturan relaciones semánticas y sintácticas, de modo que palabras con significados similares se mapean a puntos cercanos en el espacio vectorial.\n",
    "\n",
    "- Similitud y Distancia: Una vez que se ha representado el texto como vectores, se pueden utilizar medidas de distancia o similitud, como la distancia euclidiana, la distancia de Manhattan, o la similitud coseno, para evaluar qué tan cercanos o similares son dos textos en el espacio vectorial. La similitud coseno, en particular, es ampliamente utilizada para medir la similitud de dirección (independientemente de la magnitud) entre dos vectores, lo que es útil para comparar textos de diferentes longitudes.\n",
    "\n",
    "- Modelos Basados en Transformers: Aunque no exclusivamente modelos de espacio vectorial en el sentido clásico, los modelos basados en transformers, como BERT y GPT, utilizan técnicas de representación vectorial para codificar información textual en vectores de características de alta dimensión. Estos vectores capturan contextos complejos y pueden ser utilizados para tareas de NLP avanzadas.\n",
    "\n",
    "La efectividad de los modelos de espacio vectorial radica en su capacidad para convertir texto, que es intrínsecamente no estructurado y variado, en una forma estructurada y numérica que puede ser eficientemente procesada por algoritmos computacionales, permitiendo descubrir patrones, realizar comparaciones y ejecutar análisis de manera escalable.\n",
    "\n",
    "Todos los esquemas de representación de texto que estudiaremos en esta clase caen dentro del alcance de los modelos de espacio vectorial. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enfoques básicos de vectorización \n",
    "\n",
    "Asigne cada palabra en el vocabulario `(V)` del corpus de texto a una ID única (valor entero), luego represente cada oración o documento en el corpus como un vector de dimensión V. ¿Cómo ponemos en práctica esta idea?\n",
    "\n",
    "Sea el siguiente corpus:\n",
    "\n",
    "```\n",
    "D1: Dog bites man. \n",
    "\n",
    "D2: Man bites dog. \n",
    "\n",
    "D3: Dog eats meat. \n",
    "\n",
    "D4: Man eats food\n",
    "```\n",
    "\n",
    "Cada documento de este corpus ahora se puede representar con un vector de tamaño seis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación one-hot \n",
    "\n",
    "El código que sigue implementa codificación one-hot. En proyectos del mundo real, utilizamos principalmente la implementación de codificación one-hot de scikit-learn, que está mucho más optimizada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentos = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "docs_procesados = [doc.lower().replace(\".\",\"\") for doc in documentos]\n",
    "docs_procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
     ]
    }
   ],
   "source": [
    "#construir el vocabulario\n",
    "vocab = {}\n",
    "conteo = 0\n",
    "for doc in docs_procesados:\n",
    "    for palabra in doc.split():\n",
    "        if palabra not in vocab:\n",
    "            conteo = conteo +1\n",
    "            vocab[palabra] = conteo\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos una representación  para cualquier cadena basada en este vocabulario. Si la palabra existe en el vocabulario, se devuelve su representación, si no, se devuelve una lista de ceros para esa palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtiene_vector_onehot(cadena):\n",
    "    onehot_codificado = []\n",
    "    for palabra in cadena.split():\n",
    "        temp = [0]*len(vocab)\n",
    "    ## Completar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación one-hot usando scikit -learn\n",
    "Codificamos nuestro corpus como una matriz numérica one-hot usando `OneHotEncoder` de scikit-learn.\n",
    "Demostraremos:\n",
    "\n",
    "- Codificación one-hot: en la codificación one-hot, a cada palabra `w` en el vocabulario del corpus se le asigna un ID entero único $w_{id}$ que está entre `1` y `|V|`, donde `V` es el conjunto de vocabulario del corpus. Luego, cada palabra se representa mediante un vector binario de dimensión `V` de `0` y `1`.\n",
    "\n",
    "- Codificación de etiquetas: en codificación de etiquetas, cada palabra `w` en nuestro corpus se convierte en un valor numérico entre `0` y `n-1` (donde `n` se refiere al número de palabras únicas en nuestro corpus).\n",
    "\n",
    "El enlace a la documentación oficial de ambos se puede encontrar [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) y [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = 'dog bites man'\n",
    "S2 = 'man bites dog'\n",
    "S3 = 'dog eats meat'\n",
    "S4 = 'man eats food'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
    "valores = data[0]+data[1]+data[2]+data[3]\n",
    "print(\"Los datos: \",valores)\n",
    "\n",
    "#Label Encoding\n",
    "\n",
    "#completa\n",
    "\n",
    "# One-Hot Encoding\n",
    "\n",
    "# Completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy en día, rara vez se utiliza el esquema de codificación one-hot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bolsa de palabras \n",
    "\n",
    "Bolsa de palabras (BoW) es una técnica clásica de representación de texto. La idea clave detrás de esto es la siguiente: representar el texto bajo consideración como una bolsa (colección) de palabras ignorando el orden y el contexto. La intuición básica detrás de esto es que supone que el texto que pertenece a una clase determinada en el conjunto de datos se caracteriza por un conjunto único de palabras. Si dos fragmentos de texto tienen casi las mismas palabras, entonces pertenecen al mismo grupo (clase). Así, analizando las palabras presentes en un texto, se puede identificar la clase (bolsa) a la que pertenece. \n",
    "\n",
    "De manera similar a la codificación one-hot, BoW asigna palabras a ID enteros únicos entre `1` y `|V|`. Luego, cada documento del corpus se convierte en un vector de `|V|` dimensiones donde en el i-ésimo componente del vector, $i = w_{id}$, es simplemente el número de veces que aparece la palabra `w` en el documento, es decir, simplemente calificamos cada palabra en `V` según el conteo de apariciones en el documento. \n",
    "\n",
    "Ahora, hagamos la tarea principal de encontrar la representación de una bolsa de palabras. Usaremos [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Lista de documentos\n",
    "print(\"El corpus: \", docs_procesados)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Construccion la representacion BOW para el corpus\n",
    "# Completa\n",
    "\n",
    "#Mapeo del vocabulario\n",
    "print(\"El vocabulario: \", count_vect.vocabulary_)\n",
    "\n",
    "#Ver la representacion BOW para los dos primeros documentos\n",
    "print(\"Representacion BoW para 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"Representacion BoW para 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "# Representación usando este vocabulario, para un nuevo texto.\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Representacion Bow para  'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el código anterior, representamos el texto teniendo en cuenta la frecuencia de las palabras. Sin embargo, a veces, no nos importa mucho la frecuencia, solo queremos saber si una palabra apareció en un texto o no. Es decir, cada documento se representa como un vector de `0` y `1`. Usaremos la opción `binary=True` en `CountVectorizer` para este propósito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW con vectores binarios\n",
    "# Completa\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Representacion Bow para 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto da como resultado una representación diferente para la misma oración. `CountVectorizer` admite n-gramas tanto de palabras como de caracteres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Enuncia las ventajas y desventajas que puedes encontrar en el método BoW descrito con anterioridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bolsa de N-Gramas \n",
    "\n",
    "Todos los esquemas de representación que hemos visto hasta ahora tratan las palabras como unidades independientes. No hay noción de frases ni de orden de palabras. El enfoque de la bolsa de n gramas (BoN) intenta remediar esto. Lo hace dividiendo el texto en fragmentos de `n` palabras (o tokens) contiguas. Esto puede ayudarnos a captar algo de contexto, algo que los enfoques anteriores no pudieron lograr. Cada fragmento se llama n-grama. \n",
    "\n",
    "El vocabulario del corpus, `V`, no es más que una colección de todos los n-gramas únicos en el corpus del texto. Luego, cada documento del corpus está representado por un vector de longitud `|V|`. Este vector simplemente contiene los recuentos de frecuencia de los n-gramas presentes en el documento y cero para los n-gramas que no están presentes. \n",
    "\n",
    "En el lenguaje del NLP, el esquema BoN también se denomina **selección de características de n-gramas**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ejemplo de vectorización de n-gramas \n",
    "# Completa\n",
    "\n",
    "#Construccion la representacion BOW para el corpus\n",
    "bow_rep = count_vect.fit_transform(docs_procesados)\n",
    "\n",
    "#Mapeo del vocabulario\n",
    "print(\"El vocabulario: \", count_vect.vocabulary_)\n",
    "\n",
    "#Ver la representacion BOW para los dos primeros documentos\n",
    "print(\"Representacion BoW para 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"Representacion BoW para 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "# Representación usando este vocabulario, para un nuevo texto.\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Representacion Bow para  'dog and dog are friends':\", temp.toarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ten en cuenta que la cantidad de características (y, por lo tanto, el tamaño del vector de características) aumentó mucho para los mismos datos, en comparación con otras representaciones basadas en una sola palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Enuncia las ventajas y desventajas que puedes encontrar en el método BoN descrito con anterioridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "TF-IDF, que significa \"Frecuencia de Término - Frecuencia Inversa de Documento\", es un método estadístico utilizado para evaluar la importancia de una palabra en un documento en relación con una colección de documentos o corpus. La importancia aumenta proporcionalmente al número de veces que una palabra aparece en el documento, pero se compensa con la frecuencia de la palabra en el corpus. \n",
    "\n",
    "El cálculo de TF-IDF se compone de dos partes:\n",
    "\n",
    "- Frecuencia de Término (TF): Mide cuántas veces aparece un término en un documento. La idea es que cuanto más frecuentemente aparece una palabra en un documento, más importante es para ese documento. Sin embargo, en la práctica, la TF a menudo se ajusta para no favorecer injustamente a los documentos largos. Esto se puede hacer dividiendo el número de apariciones de una palabra en un documento por el número total de palabras en ese documento.\n",
    "\n",
    "- Frecuencia Inversa de Documento (IDF): Mide la importancia del término en todo el corpus. La idea es que si un término aparece en muchos documentos, probablemente no sea un buen discriminador y debe recibir menos peso. La IDF se calcula tomando el logaritmo del número de documentos en el corpus dividido por el número de documentos que contienen el término. Esto significa que los términos raros reciben más peso, ya que su presencia en menos documentos indica una mayor especificidad o relevancia.\n",
    "\n",
    "El valor TF-IDF para un término en un documento específico se calcula multiplicando la TF de ese término en el documento por la IDF del término en todo el corpus:\n",
    "\n",
    "$$TF-IDF(t,d)=TF(t,d)\\times IDF(t)$$\n",
    "\n",
    "Donde  `t` es el término,  `d` es el documento, y el corpus es el conjunto total de documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código muestra cómo usar TF-IDF para representar texto: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Completa\n",
    "\n",
    "#IDF para todas las palabras en el vocabulario\n",
    "print(\"IDF para todas las palabras en el vocabulario\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#Todas las palabras en el vocabulario.\n",
    "print(\"Todas las palabras en el vocabulario\",tfidf.get_feature_names())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#Representacion TFIDF para todos los documentos en el corpus \n",
    "print(\"Representacion TFIDF para todos los documentos en el corpus \\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Representacion Tfidf para 'dog and man are friends':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Enuncia las ventajas y desventajas que puedes encontrar en el método TF-IDF descrito con anterioridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1. Dado un pequeño conjunto de documentos (frases), implementa una función en Python que convierta cada palabra única en un vector one-hot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['el gato come pescado', 'el perro come carne', 'el gato juega con el perro']\n",
    "\n",
    "# Implementa la función de one-hot encoding aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Escribe una función en Python que tome como entrada el mismo conjunto de documentos del ejercicio anterior y devuelva una representación de bolsa de palabras de cada documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Modifica la función de Bolsa de Palabras del ejercicio 2 para que ahora soporte n-gramas. Por simplicidad, considera bigramas (n=2) para este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Implementa una función en Python que calcule la matriz TF-IDF para el mismo conjunto de documentos. Puedes usar `TfidfVectorizer` de sklearn para simplificar la implementación, pero intenta entender qué está haciendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
