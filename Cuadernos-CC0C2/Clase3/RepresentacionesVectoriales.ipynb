{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación de texto elemental\n",
    "\n",
    "En el procesamiento del lenguaje natural, los vectores `x` se derivan de datos textuales para reflejar diversas propiedades lingüísticas del texto.  \n",
    "\n",
    "— Yoav Goldberg.\n",
    "\n",
    "La representación de texto en el procesamiento del lenguaje natural se refiere a los métodos y técnicas utilizados para convertir el texto en una forma que las computadoras puedan procesar de manera más eficiente. El objetivo de estas representaciones es capturar la información semántica y sintáctica del texto, de modo que sea posible realizar tareas como la clasificación de textos y la búsqueda de información.\n",
    "\n",
    "Estos enfoques se clasifican en cuatro categorías:\n",
    "\n",
    "- Enfoques básicos de vectorización\n",
    "- Representaciones distribuidas\n",
    "- Representación del lenguaje universal\n",
    "- Características hechas a mano\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de espacios vectoriales\n",
    "\n",
    "Los modelos de espacio vectorial son una familia de algoritmos utilizados en el procesamiento del lenguaje natural (NLP), la recuperación de información y otras áreas de la inteligencia artificial para representar elementos textuales, como palabras, frases, oraciones o documentos, como vectores en un espacio multidimensional. La idea clave detrás de estos modelos es que las entidades semánticamente similares se representan por puntos cercanos en el espacio vectorial, lo que facilita la comparación y el análisis de las relaciones entre ellas.\n",
    "\n",
    "En el contexto de NLP y la recuperación de información, los modelos de espacio vectorial permiten realizar tareas como la búsqueda de documentos similares, la clasificación de textos y el análisis de sentimientos, entre otras, mediante técnicas matemáticas y estadísticas.\n",
    "\n",
    "Algunos conceptos y técnicas importantes relacionados con los modelos de espacio vectorial incluyen:\n",
    "\n",
    "- **Representación de texto**: Convertir texto en vectores numéricos. Técnicas como la bolsa de palabras (BoW) y TF-IDF son ejemplos clásicos de cómo se puede realizar esta conversión, asignando a cada palabra o término una dimensión en el espacio vectorial y utilizando la frecuencia de las palabras para determinar los valores en esas dimensiones.\n",
    "\n",
    "- **Embeddings de palabras**: Métodos más avanzados como Word2Vec, GloVe y FastText aprenden representaciones vectoriales densas de palabras a partir de grandes corpus de texto. Estos embeddings capturan relaciones semánticas y sintácticas, de modo que palabras con significados similares se mapean a puntos cercanos en el espacio vectorial.\n",
    "\n",
    "- **Similitud y distancia**: Una vez que se ha representado el texto como vectores, se pueden utilizar medidas de distancia o similitud, como la distancia euclidiana, la distancia de Manhattan o la similitud coseno, para evaluar qué tan cercanos o similares son dos textos en el espacio vectorial. La similitud coseno, en particular, es ampliamente utilizada para medir la similitud de dirección (independientemente de la magnitud) entre dos vectores, lo que es útil para comparar textos de diferentes longitudes.\n",
    "\n",
    "- **Modelos basados en transformers**: Aunque no son exclusivamente modelos de espacio vectorial en el sentido clásico, los modelos basados en transformers, como BERT y GPT, utilizan técnicas de representación vectorial para codificar información textual en vectores de características de alta dimensión. Estos vectores capturan contextos complejos y pueden ser utilizados para tareas avanzadas de NLP.\n",
    "\n",
    "La efectividad de los modelos de espacio vectorial radica en su capacidad para convertir texto, que es intrínsecamente no estructurado y variado, en una forma estructurada y numérica que puede ser procesada eficientemente por algoritmos computacionales. Esto permite descubrir patrones, realizar comparaciones y ejecutar análisis de manera escalable.\n",
    "\n",
    "Todos los esquemas de representación de texto que estudiaremos en esta clase se enmarcan dentro del alcance de los modelos de espacio vectorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enfoques básicos de vectorización \n",
    "\n",
    "Asigne cada palabra en el vocabulario `(V)` del corpus de texto a una ID única (valor entero), luego represente cada oración o documento en el corpus como un vector de dimensión V. ¿Cómo ponemos en práctica esta idea?\n",
    "\n",
    "Sea el siguiente corpus:\n",
    "\n",
    "```\n",
    "D1: Dog bites man. \n",
    "\n",
    "D2: Man bites dog. \n",
    "\n",
    "D3: Dog eats meat. \n",
    "\n",
    "D4: Man eats food\n",
    "```\n",
    "\n",
    "Cada documento de este corpus ahora se puede representar con un vector de tamaño seis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación one-hot \n",
    "\n",
    "El código que sigue implementa codificación one-hot. En proyectos del mundo real, utilizamos principalmente la implementación de codificación one-hot de scikit-learn, que está mucho más optimizada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "docs_procesados = [doc.lower().replace(\".\",\"\") for doc in documentos]\n",
    "docs_procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construir el vocabulario\n",
    "vocab = {}\n",
    "conteo = 0\n",
    "for doc in docs_procesados:\n",
    "    for palabra in doc.split():\n",
    "        if palabra not in vocab:\n",
    "            conteo = conteo +1\n",
    "            vocab[palabra] = conteo\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos una representación  para cualquier cadena basada en este vocabulario. Si la palabra existe en el vocabulario, se devuelve su representación, si no, se devuelve una lista de ceros para esa palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtiene_vector_onehot(cadena):\n",
    "    onehot_codificado = []\n",
    "    for palabra in cadena.split():\n",
    "        temp = [0]*len(vocab)\n",
    "    ## Completar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación one-hot usando scikit -learn\n",
    "Codificamos nuestro corpus como una matriz numérica one-hot usando `OneHotEncoder` de scikit-learn.\n",
    "Demostraremos:\n",
    "\n",
    "- Codificación one-hot: en la codificación one-hot, a cada palabra `w` en el vocabulario del corpus se le asigna un ID entero único $w_{id}$ que está entre `1` y `|V|`, donde `V` es el conjunto de vocabulario del corpus. Luego, cada palabra se representa mediante un vector binario de dimensión `V` de `0` y `1`.\n",
    "\n",
    "- Codificación de etiquetas: en codificación de etiquetas, cada palabra `w` en nuestro corpus se convierte en un valor numérico entre `0` y `n-1` (donde `n` se refiere al número de palabras únicas en nuestro corpus).\n",
    "\n",
    "El enlace a la documentación oficial de ambos se puede encontrar [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) y [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = 'dog bites man'\n",
    "S2 = 'man bites dog'\n",
    "S3 = 'dog eats meat'\n",
    "S4 = 'man eats food'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
    "valores = data[0]+data[1]+data[2]+data[3]\n",
    "print(\"Los datos: \",valores)\n",
    "\n",
    "#Label Encoding\n",
    "\n",
    "#completa\n",
    "\n",
    "# One-Hot Encoding\n",
    "\n",
    "# Completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy en día, rara vez se utiliza el esquema de codificación one-hot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bolsa de Palabras\n",
    "\n",
    "La **Bolsa de Palabras** (BoW) es una técnica clásica de representación de texto. La idea clave detrás de esta técnica es representar el texto como una bolsa (o colección) de palabras, ignorando el orden y el contexto en el que aparecen. La intuición básica es que se asume que el texto perteneciente a una clase determinada dentro de un conjunto de datos está caracterizado por un conjunto único de palabras. Si dos fragmentos de texto contienen casi las mismas palabras, es probable que pertenezcan al mismo grupo (o clase). Así, al analizar las palabras presentes en un texto, es posible identificar la clase (o bolsa) a la que pertenece.\n",
    "\n",
    "De manera similar a la codificación **one-hot**, BoW asigna a cada palabra un ID entero único entre `1` y `|V|` (el tamaño del vocabulario). Luego, cada documento del corpus se convierte en un vector de `|V|` dimensiones, donde el componente `i`, correspondiente a la palabra con ID `w_{id}`, representa simplemente el número de veces que dicha palabra `w` aparece en el documento. Es decir, calificamos cada palabra en `V` según su conteo de apariciones en el documento.\n",
    "\n",
    "A continuación, realizaremos la tarea de encontrar la representación de una bolsa de palabras. Utilizaremos [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Lista de documentos\n",
    "print(\"El corpus: \", docs_procesados)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Construccion la representacion BOW para el corpus\n",
    "# Completa\n",
    "\n",
    "#Mapeo del vocabulario\n",
    "print(\"El vocabulario: \", count_vect.vocabulary_)\n",
    "\n",
    "#Ver la representacion BOW para los dos primeros documentos\n",
    "print(\"Representacion BoW para 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"Representacion BoW para 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "# Representación usando este vocabulario, para un nuevo texto.\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Representacion Bow para  'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el código anterior, representamos el texto teniendo en cuenta la frecuencia de las palabras. Sin embargo, a veces, no nos importa mucho la frecuencia, solo queremos saber si una palabra apareció en un texto o no. Es decir, cada documento se representa como un vector de `0` y `1`. Usaremos la opción `binary=True` en `CountVectorizer` para este propósito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW con vectores binarios\n",
    "# Completa\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Representacion Bow para 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto da como resultado una representación diferente para la misma oración. `CountVectorizer` admite n-gramas tanto de palabras como de caracteres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Enuncia las ventajas y desventajas que puedes encontrar en el método BoW descrito con anterioridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bolsa de N-Gramas\n",
    "\n",
    "Los esquemas de representación que hemos visto hasta ahora tratan las palabras como unidades independientes, sin tener en cuenta frases ni el orden de las palabras. El enfoque de la **Bolsa de N-Gramas** (BoN) intenta remediar esta limitación dividiendo el texto en fragmentos de `n` palabras (o tokens) contiguas. Esto nos ayuda a captar algo de contexto, lo que los enfoques anteriores no lograban. Cada uno de estos fragmentos se denomina n-grama.\n",
    "\n",
    "El vocabulario del corpus, `V`, es simplemente una colección de todos los n-gramas únicos presentes en el corpus de texto. Luego, cada documento del corpus se representa mediante un vector de longitud `|V|`, donde cada componente del vector contiene el recuento de frecuencia de los n-gramas presentes en el documento, y se asigna un valor de cero para los n-gramas que no aparecen.\n",
    "\n",
    "En el ámbito del procesamiento del lenguaje natural (NLP), este esquema también se conoce como **selección de características de n-gramas**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ejemplo de vectorización de n-gramas \n",
    "# Completa\n",
    "\n",
    "#Construccion la representacion BOW para el corpus\n",
    "bow_rep = count_vect.fit_transform(docs_procesados)\n",
    "\n",
    "#Mapeo del vocabulario\n",
    "print(\"El vocabulario: \", count_vect.vocabulary_)\n",
    "\n",
    "#Ver la representacion BOW para los dos primeros documentos\n",
    "print(\"Representacion BoW para 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"Representacion BoW para 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "# Representación usando este vocabulario, para un nuevo texto.\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Representacion Bow para  'dog and dog are friends':\", temp.toarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ten en cuenta que la cantidad de características (y, por lo tanto, el tamaño del vector de características) aumentó mucho para los mismos datos, en comparación con otras representaciones basadas en una sola palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Enuncia las ventajas y desventajas que puedes encontrar en el método BoN descrito con anterioridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "**TF-IDF**, que significa \"Frecuencia de Término - Frecuencia Inversa de Documento\", es un método estadístico utilizado para evaluar la importancia de una palabra en un documento, en relación con una colección de documentos o corpus. La importancia de una palabra aumenta proporcionalmente al número de veces que aparece en el documento, pero se compensa con la frecuencia de la palabra en el corpus.\n",
    "\n",
    "El cálculo de TF-IDF se compone de dos componentes principales:\n",
    "\n",
    "- **Frecuencia de término (TF)**: Mide cuántas veces aparece un término en un documento. La idea es que cuanto más frecuentemente aparece una palabra en un documento, más importante es para ese documento. Sin embargo, en la práctica, la TF a menudo se ajusta para no favorecer injustamente a los documentos más largos. Esto se puede hacer dividiendo el número de apariciones de una palabra en un documento por el número total de palabras en ese documento.\n",
    "\n",
    "- **Frecuencia inversa de documento (IDF)**: Mide la importancia del término en todo el corpus. La idea es que si un término aparece en muchos documentos, probablemente no sea un buen discriminador y debe recibir menos peso. La IDF se calcula tomando el logaritmo del número total de documentos en el corpus dividido por el número de documentos que contienen el término. Esto significa que los términos raros reciben más peso, ya que su presencia en menos documentos indica una mayor especificidad o relevancia.\n",
    "\n",
    "El valor TF-IDF para un término en un documento específico se calcula multiplicando la TF de ese término en el documento por la IDF del término en el corpus:\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(t,d) = TF(t,d) \\times IDF(t)\n",
    "$$\n",
    "\n",
    "Donde `t` es el término, `d` es el documento, y el corpus es el conjunto total de documentos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código muestra cómo usar TF-IDF para representar texto: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Completa\n",
    "\n",
    "#IDF para todas las palabras en el vocabulario\n",
    "print(\"IDF para todas las palabras en el vocabulario\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#Todas las palabras en el vocabulario.\n",
    "print(\"Todas las palabras en el vocabulario\",tfidf.get_feature_names())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#Representacion TFIDF para todos los documentos en el corpus \n",
    "print(\"Representacion TFIDF para todos los documentos en el corpus \\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Representacion Tfidf para 'dog and man are friends':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Enuncia las ventajas y desventajas que puedes encontrar en el método TF-IDF descrito con anterioridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1. Dado un pequeño conjunto de documentos (frases), implementa una función en Python que convierta cada palabra única en un vector one-hot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['el gato come pescado', 'el perro come carne', 'el gato juega con el perro']\n",
    "\n",
    "# Implementa la función de one-hot encoding aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Escribe una función en Python que tome como entrada el mismo conjunto de documentos del ejercicio anterior y devuelva una representación de bolsa de palabras de cada documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Modifica la función de Bolsa de Palabras del ejercicio 2 para que ahora soporte n-gramas. Por simplicidad, considera bigramas (n=2) para este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Implementa una función en Python que calcule la matriz TF-IDF para el mismo conjunto de documentos. Puedes usar `TfidfVectorizer` de sklearn para simplificar la implementación, pero intenta entender qué está haciendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Bag of subwords**\n",
    "\n",
    "El concepto de **Bag de subwords** es una extensión del enfoque **bolsa de palabras (BoW)**, en el cual, en lugar de considerar cada palabra como una unidad independiente, se dividen las palabras en subpalabras o fragmentos más pequeños, como prefijos, sufijos o secuencias intermedias de caracteres. Esto es especialmente útil en lenguajes con alta morfología, donde las variaciones morfológicas de una palabra pueden cambiar su significado o contexto, y en lenguajes con grandes vocabularios o palabras raras. \n",
    "\n",
    "En **Bag de subwords**, un documento se representa como un vector de frecuencias de subpalabras, en lugar de palabras completas. Este enfoque permite que los modelos manejen mejor palabras desconocidas (OOV) o variaciones morfológicas y capture relaciones entre palabras relacionadas morfológicamente.\n",
    "\n",
    "#### Ejemplo:\n",
    "Supongamos que estamos procesando un corpus en inglés con las palabras \"running\", \"runner\", \"runs\". Si utilizamos **bag de subwords**, podríamos descomponer estas palabras en fragmentos como:\n",
    "\n",
    "- \"run\", \"ning\", \"er\", \"s\".\n",
    "\n",
    "Así, el modelo puede reconocer que estas palabras están relacionadas, aunque no aparezcan de manera idéntica en el texto. En vez de trabajar únicamente con las palabras completas, se toma en cuenta la estructura interna de las palabras. Por ejemplo, la palabra \"running\" se podría descomponer en los subwords `[\"run\", \"ning\"]`.\n",
    "\n",
    "Esto es particularmente útil para modelos de procesamiento de lenguaje en lenguajes como el alemán o el finlandés, que tienen palabras compuestas largas y complejas, o para manejar vocabularios en crecimiento en aplicaciones como modelos de generación de lenguaje o traducción automática.\n",
    "\n",
    "Un caso práctico es el uso de **Byte-Pair Encoding (BPE)** o **Unigram Language Model** en herramientas como **SentencePiece**, que realizan esta segmentación en subpalabras. En modelos como **GPT** y **BERT**, las palabras se descomponen en subpalabras usando BPE para reducir el vocabulario y manejar palabras desconocidas.\n",
    "\n",
    "#### Ventajas:\n",
    "- Reduce el tamaño del vocabulario al considerar fragmentos comunes de palabras.\n",
    "- Mejora el manejo de palabras raras o fuera de vocabulario (OOV).\n",
    "- Captura relaciones morfológicas entre palabras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Ejemplo de corpus (normalmente, esto sería un conjunto de textos más grande)\n",
    "corpus = [\"running\", \"runner\", \"runs\", \"jumping\", \"jumper\", \"jumps\"]\n",
    "\n",
    "# Guardamos el corpus en un archivo temporal\n",
    "with open('corpus.txt', 'w') as f:\n",
    "    for word in corpus:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "# Entrenar un modelo BPE con SentencePiece\n",
    "spm.SentencePieceTrainer.train('--input=corpus.txt --model_prefix=mymodel --vocab_size=30 --model_type=bpe')\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "sp = spm.SentencePieceProcessor(model_file='mymodel.model')\n",
    "\n",
    "# Probar la tokenización de subpalabras en el corpus\n",
    "test_words = [\"running\", \"runner\", \"runs\"]\n",
    "\n",
    "for word in test_words:\n",
    "    print(f\"Word: {word}\")\n",
    "    subwords = sp.encode(word, out_type=str)\n",
    "    print(f\"Subwords: {subwords}\")\n",
    "    print()\n",
    "\n",
    "# Ejemplo con una palabra nueva\n",
    "new_word = \"jumped\"\n",
    "print(f\"Word: {new_word}\")\n",
    "subwords_new = sp.encode(new_word, out_type=str)\n",
    "print(f\"Subpalabras: {subwords_new}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Representaciones a nivel de oración o documento**\n",
    "Las **representaciones a nivel de oración o documento** buscan capturar el significado completo de una frase, párrafo o documento entero, en lugar de trabajar únicamente con palabras individuales. Estas representaciones intentan modelar el contexto global y las relaciones semánticas entre frases o secciones de un texto, lo que es fundamental para tareas como la clasificación de documentos, la detección de sentimientos y la recuperación de información.\n",
    "\n",
    "A diferencia de los enfoques como la bolsa de palabras, que ignoran el orden de las palabras, estas representaciones tienen en cuenta tanto el contenido de las palabras como su secuencia o estructura en el texto.\n",
    "\n",
    "#### Ejemplos:\n",
    "\n",
    "1. **Doc2Vec (Paragraph Vectors)**:\n",
    "**Doc2Vec** es una extensión del modelo Word2Vec que genera vectores representativos no solo para palabras, sino para documentos completos (incluyendo oraciones, párrafos, etc.). Doc2Vec aprende a representar un documento en un espacio vectorial donde los documentos con contenido similar están más cerca entre sí. Este enfoque es útil para tareas como la clasificación de documentos y la detección de similitud entre textos.\n",
    "\n",
    "**Ejemplo**: En un corpus de noticias, si tienes artículos sobre deportes y política, **Doc2Vec** generará representaciones vectoriales donde los documentos sobre deportes estarán cercanos entre sí en el espacio vectorial, y los artículos sobre política estarán en otro grupo.\n",
    "\n",
    "Para entrenar el modelo, Doc2Vec utiliza dos enfoques:\n",
    "   - **Distributed Memory (DM)**: Aprender representaciones vectoriales de documentos, manteniendo el contexto de las palabras presentes.\n",
    "   - **Distributed Bag of Words (DBOW)**: Similar a Skip-Gram en Word2Vec, aprende representaciones de documentos prediciendo palabras aleatorias del documento.\n",
    "\n",
    "Esto permite no solo representar palabras, sino también representar documentos enteros, capturando el contexto y las relaciones semánticas a nivel de documento.\n",
    "\n",
    "2. **Universal Sentence Encoder (USE)**:\n",
    "   El **Universal Sentence Encoder**, desarrollado por Google, es un modelo basado en redes neuronales profundas (y transformers en su versión más avanzada) que convierte oraciones o documentos en vectores de alta dimensión. Estos vectores pueden ser utilizados para una variedad de tareas como análisis semántico, búsqueda de similitud de oraciones o detección de temas.\n",
    "\n",
    "**Ejemplo**: Supongamos que tenemos dos oraciones: *\"The cat is on the mat\"* y *\"A feline is resting on a rug\"*. Aunque estas oraciones utilizan palabras diferentes, el **Universal Sentence Encoder** generará representaciones vectoriales que estarán cercanas en el espacio vectorial porque capturan el significado semántico similar entre ambas oraciones. Esto hace que USE sea adecuado para tareas como la búsqueda semántica o la detección de parafraseo, donde oraciones con significados similares deben ser reconocidas, aunque usen diferentes palabras.\n",
    "\n",
    "Otra ventaja del USE es su capacidad para manejar frases o documentos completos, representándolos de una manera que captura las relaciones de largo alcance en un texto. Esto es crucial para tareas como la clasificación de documentos, la traducción automática, y la búsqueda de información, donde la estructura completa del documento es relevante para comprender su significado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow tensorflow_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cargar el modelo preentrenado de Universal Sentence Encoder desde TensorFlow Hub\n",
    "model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Definir oraciones para generar representaciones\n",
    "sentences = [\n",
    "    \"The cat is on the mat\",\n",
    "    \"A feline is resting on a rug\",\n",
    "    \"The dog barked at the stranger\",\n",
    "    \"The mouse ran away from the cat\"\n",
    "]\n",
    "\n",
    "# Generar las representaciones vectoriales de las oraciones\n",
    "sentence_vectors = model(sentences)\n",
    "\n",
    "# Calcular la similitud entre la primera oración y las demás\n",
    "similarities = cosine_similarity([sentence_vectors[0]], sentence_vectors)\n",
    "\n",
    "# Mostrar los resultados de similitud\n",
    "for idx, similarity in enumerate(similarities[0]):\n",
    "    print(f\"Similaridad con la oracion {idx}: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "**Ejercicio 1: Entrenamiento y análisis con Doc2Vec**\n",
    "1. **Entrena un modelo Doc2Vec** con un corpus de documentos de noticias que cubra al menos tres áreas temáticas diferentes (por ejemplo, deportes, política, tecnología). \n",
    "2. **Evalúa el modelo** generando representaciones vectoriales para los documentos y realiza las siguientes tareas:\n",
    "   - Agrupa los documentos en función de su similitud, utilizando medidas de similitud como la **similitud coseno**.\n",
    "   - Visualiza los documentos en un espacio bidimensional usando técnicas de reducción de dimensionalidad como **t-SNE** o **PCA** para observar cómo se agrupan los documentos.\n",
    "3. **Pregunta reflexiva**: ¿Cómo afecta el tamaño del corpus y el número de dimensiones del vector al rendimiento del modelo y la calidad de las agrupaciones?\n",
    "\n",
    "**Ejercicio 2: Comparación semántica con Universal Sentence Encoder (USE)**\n",
    "1. Carga un conjunto de oraciones que describan eventos similares con diferentes palabras (por ejemplo, *\"The cat sat on the mat\"* y *\"A feline rested on a rug\"*).\n",
    "2. Genera las representaciones vectoriales de estas oraciones utilizando el **Universal Sentence Encoder (USE)**.\n",
    "3. **Mide la similitud semántica** entre las oraciones utilizando la similitud coseno y analiza los resultados.\n",
    "   - ¿Qué patrones observas en las similitudes de oraciones con diferentes estructuras pero significados similares?\n",
    "   - ¿Cómo responde el modelo USE a sinónimos y diferentes expresiones gramaticales?\n",
    "\n",
    "**Ejercicio 3: Detección de parafraseo**\n",
    "1. Recopila un conjunto de oraciones que sean parafraseos entre sí y un conjunto de oraciones no relacionadas.\n",
    "2. Utiliza **Universal Sentence Encoder (USE)** para generar vectores de representación para todas las oraciones.\n",
    "3. Calcula la similitud entre cada par de oraciones y clasifica si son parafraseos o no en función de un umbral de similitud.\n",
    "   - **Pregunta reflexiva**: ¿Qué umbral de similitud es el más adecuado para detectar parafraseos? ¿Cómo cambiarías este umbral según el dominio (por ejemplo, noticias versus redes sociales)?\n",
    "\n",
    "**Ejercicio 4: Clasificación de documentos con Doc2Vec**\n",
    "1. Entrena un modelo **Doc2Vec** con un corpus de reseñas de productos de diferentes categorías (por ejemplo, tecnología, ropa, libros).\n",
    "2. Genera las representaciones vectoriales de las reseñas y usa estos vectores como entradas para un modelo de **clasificación supervisada** (como un clasificador SVM o un perceptrón multicapa) para predecir la categoría de cada reseña.\n",
    "3. **Pregunta reflexiva**: ¿Qué características del modelo Doc2Vec (como el tamaño de ventana o la cantidad de dimensiones) impactan más en el rendimiento del clasificador? ¿Qué observaciones puedes hacer al respecto?\n",
    "\n",
    "**Ejercicio 5: Detección de tópicos con Doc2Vec**\n",
    "1. Usando un corpus extenso (por ejemplo, artículos científicos o publicaciones de blogs), entrena un modelo **Doc2Vec**.\n",
    "2. Agrupa los documentos utilizando técnicas no supervisadas como **k-means** o **DBSCAN** basadas en las representaciones vectoriales de los documentos.\n",
    "3. **Pregunta reflexiva**: ¿Qué tópicos emergen de los documentos agrupados? ¿Los grupos formados por los documentos reflejan de manera precisa las categorías esperadas o aparecen relaciones temáticas nuevas y sorprendentes?\n",
    "\n",
    "**Ejercicio 6: Búsqueda de documentos semánticamente similares**\n",
    "1. Recopila un corpus de documentos cortos (por ejemplo, entradas de blog, descripciones de productos, artículos cortos).\n",
    "2. Utiliza **Universal Sentence Encoder (USE)** para generar embeddings vectoriales de estos documentos.\n",
    "3. Implementa un sistema de búsqueda semántica: dado un documento de consulta, encuentra los documentos más similares en el corpus usando la similitud coseno.\n",
    "   - **Pregunta reflexiva**: ¿Cómo influye la longitud del documento de consulta en la calidad de los documentos recuperados? ¿El modelo es capaz de capturar adecuadamente la semántica de consultas largas versus cortas?\n",
    "\n",
    "**Ejercicio 7: Evaluación de modelos Doc2Vec vs USE**\n",
    "1. Usando un conjunto de datos con documentos y oraciones, genera representaciones utilizando tanto **Doc2Vec** como **USE**.\n",
    "2. Evalúa la similitud entre documentos o la clasificación de textos con ambos modelos y compara el rendimiento en términos de precisión, tiempo de ejecución, y similitud semántica capturada.\n",
    "   - **Pregunta reflexiva**: ¿En qué tareas sobresale cada modelo? ¿Qué ventajas tiene el modelo USE frente a Doc2Vec y viceversa? ¿Cuál es más adecuado para conjuntos de datos pequeños versus grandes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
