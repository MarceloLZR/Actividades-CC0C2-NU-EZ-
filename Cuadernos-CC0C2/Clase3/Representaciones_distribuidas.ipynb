{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representaciones distribuidas\n",
    "\n",
    "Las representaciones distribuidas en procesamiento del lenguaje natural (NLP) y aprendizaje automático son una forma de representar palabras como vectores numéricos en un espacio de alta dimensión, lo que permite reflejar similitudes semánticas y sintácticas entre palabras. A diferencia de métodos como one-hot encoding, donde cada palabra es un vector único, las representaciones distribuidas agrupan palabras con significados similares en vectores cercanos.\n",
    "\n",
    "Estos métodos han revolucionado el NLP, permitiendo avances en tareas como la traducción automática o el análisis de sentimientos. Se basan en la hipótesis distributiva, que dice que las palabras que aparecen en contextos similares tienen significados similares. Esta representación se obtiene de la coocurrencia de palabras en contextos, usando vectores de alta dimensión que pueden ser ineficientes, pero son compactados en representaciones más manejables llamadas embeddings.\n",
    "\n",
    "La semántica vectorial agrupa todas las técnicas que aprenden representaciones de palabras usando sus propiedades distributivas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características principales\n",
    "\n",
    "Veamos algunos características de estos métodos:\n",
    "\n",
    "- A diferencia de las representaciones locales, las distribuidas pueden capturar relaciones complejas entre palabras, como sinónimos, antónimos o términos que suelen aparecer en contextos similares.\n",
    "\n",
    "- Al representar palabras como vectores de tamaño fijo en un espacio continuo, se reduce la dimensionalidad del problema comparado con métodos de representación más simples pero de alta dimensionalidad, como el one-hot encoding.\n",
    "\n",
    "- Estos modelos pueden generalizar para entender palabras nuevas o raras a partir de sus componentes (por ejemplo, entender palabras compuestas a partir de los significados de sus partes).\n",
    "\n",
    "**Ejemplos y modelos**\n",
    "\n",
    "- Word2Vec: Probablemente el ejemplo más conocido de representaciones distribuidas. Word2Vec utiliza redes neuronales para aprender representaciones vectoriales de palabras a partir de grandes conjuntos de datos de texto. Ofrece dos arquitecturas principales: CBOW (Continuous Bag of Words) y Skip-gram, cada una diseñada para aprender representaciones que predigan palabras en función de sus contextos o viceversa.\n",
    "\n",
    "- GloVe (Global Vectors for Word Representation): Un modelo que aprende representaciones de palabras a partir de las estadísticas co-ocurrenciales de palabras en un corpus. La idea es que las relaciones semánticas entre palabras pueden ser capturadas observando qué tan frecuentemente aparecen juntas en un gran corpus.\n",
    "\n",
    "- Embeddings contextuales: Modelos más recientes como ELMo, BERT y GPT ofrecen una evolución de las representaciones distribuidas, generando vectores de palabras que varían según el contexto en el que aparecen, lo que permite capturar usos y significados múltiples de una misma palabra dependiendo de la oración en la que se encuentre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings de palabras\n",
    "\n",
    "Los embeddings de palabras son representaciones vectoriales densas y de baja dimensión de palabras, diseñadas para capturar el significado semántico, sintáctico y relaciones entre ellas. A diferencia de las representaciones de texto más antiguas, como el one-hot encoding, que son dispersas (la mayoría de los valores son cero) y de alta dimensión, los embeddings de palabras se representan en un espacio vectorial continuo donde palabras con significados similares están ubicadas cercanamente en el espacio vectorial.\n",
    "\n",
    "**Características de los embeddings de palabras**\n",
    "\n",
    "- Cada palabra se representa como un vector denso, lo que significa que cada dimensión tiene un valor real, a diferencia de los vectores dispersos de otras técnicas de representación.\n",
    "\n",
    "- Los embeddings generalmente tienen un tamaño de dimensión fijo y relativamente pequeño (por ejemplo, 100, 200, 300 dimensiones) independientemente del tamaño del vocabulario.\n",
    "\n",
    "- Estos vectores intentan capturar el contexto y el significado de una palabra, no solo su presencia o ausencia. Palabras que se usan en contextos similares tendrán embeddings similares.\n",
    "\n",
    "- Pueden ayudar a los modelos de aprendizaje automático a generalizar mejor a palabras no vistas durante el entrenamiento, dado que las palabras con significados similares se mapean a puntos cercanos en el espacio vectorial.\n",
    "\n",
    "\n",
    "En 2013, un trabajo fundamental de Mikolov [Efficient Estimationof Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) demostraron que su modelo de representación de palabras basado en una red neuronal conocido como `Word2vec`, basado en la `similitud distributiva`, puede capturar relaciones de analogía de palabras como: \n",
    "\n",
    "$$King - Man + Woman \\approx Queen$$\n",
    "\n",
    "Conceptualmente, Word2vec toma un gran corpus de texto como entrada y \"aprende\" a representar las palabras en un espacio vectorial común en función de los contextos en los que aparecen en el corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings de palabras pre-entrenadas\n",
    "\n",
    "El siguente es un ejemplo de cómo cargar embeddings de Word2vec previamente entrenadas y buscar las palabras más similares (clasificadas por similitud de coseno) a una palabra determinada. \n",
    "\n",
    "Tomemos un ejemplo de un modelo word2vec previamente entrenado y cómo podemos usarlo para buscar la mayoría de las palabras similares. Usaremos los embeddings de vectores de Google News. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
    "\n",
    "Se pueden encontrar algunos otros modelos de embeddings de palabras previamente entrenados y detalles sobre los medios para acceder a ellos a través de gensim en: https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "El código que sigue cubre los pasos clave. Aquí encontramos las palabras que semánticamente son más similares a la palabra “beautiful”; la última línea devuelve el vector de embeddings de la palabra \" beautiful \":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# URL de Google Drive\n",
    "url = 'https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
    "\n",
    "# Ruta donde se guardará el archivo comprimido descargado\n",
    "ruta_descarga = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# Ruta del archivo descomprimido\n",
    "ruta_extraccion = \"GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# Descargar el archivo usando gdown\n",
    "gdown.download(url, ruta_descarga, quiet=False)\n",
    "\n",
    "# Descomprimir el archivo   \n",
    "with gzip.open(ruta_descarga, 'rb') as f_in:\n",
    "    with open(ruta_extraccion, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(f\"Archivo descomprimido en {ruta_extraccion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import psutil \n",
    "procesos = psutil.Process(os.getpid())\n",
    "from psutil import virtual_memory\n",
    "memoria = virtual_memory()\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos algunos cálculos del uso de los datos descargados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "pretrainedpath = ruta_extraccion\n",
    "\n",
    "#Se carga el modelo W2V. \n",
    "pre = procesos.memory_info().rss\n",
    "print(\"Memoria usada en GB antes de cargar el modelo: %0.2f\"%float(pre/(10**9))) \n",
    "print('-'*10)\n",
    "\n",
    "tiempo_inicio = time.time() \n",
    "ttl = memoria.total \n",
    "\n",
    "w2v_modelo = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) \n",
    "print(\"%0.2f segundos para tomar\"%float(time.time() - tiempo_inicio)) \n",
    "print('-'*10)\n",
    "\n",
    "print('Finalizacion de cargar  Word2Vec')\n",
    "print('-'*10)\n",
    "\n",
    "post = procesos.memory_info().rss\n",
    "print(\"Memoria usada en GB despues de cargar el modelo: {:.2f}\".format(float(post/(10**9))))\n",
    "print('-'*10)\n",
    "print(\"Aumento porcentual en el uso de memoria: {:.2f}% \".format(float((post/pre)*100))) \n",
    "print('-'*10)\n",
    "\n",
    "print(\"Numero de palabras en el vocabulario: \",len(w2v_modelo.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinemos el modelo sabiendo cuáles son las palabras más similares para una palabra determinada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.most_similar(\"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo['beautiful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.most_similar(\"Toronto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_modelo['practicaNLP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasa si busco una palabra que no está en este vocabulario?:\n",
    "`w2v_modelo['practicalnlp']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos cosas a tener en cuenta al utilizar modelos previamente entrenados:\n",
    "\n",
    "* Los tokens/palabras siempre están en minúsculas. Si una palabra no está en el vocabulario, el modelo genera una excepción.\n",
    "* Por lo tanto, siempre es una buena idea encapsular esas declaraciones en bloques `try/except`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando nuestros embeddings \n",
    "\n",
    "Ahora nos centraremos en entrenar nuestras propias embeddings de palabras. Para ello, veremos dos variantes arquitectónicas propuestas en el enfoque original de Word2vec. Las dos variantes son: \n",
    "\n",
    "* Bolsa continua de palabras (CBOW) \n",
    "\n",
    "* Skip-Gram \n",
    "\n",
    "Para utilizar los algoritmos CBOW y SkipGram en la práctica, hay varias implementaciones disponibles que nos abstraen los detalles matemáticos. Una de las implementaciones más utilizadas es [gensim](https://github.com/piskvorky/gensim). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW\n",
    "\n",
    "El modelo Continuous Bag of Words (CBOW) es uno de los dos enfoques arquitectónicos propuestos por Mikolov  para aprender representaciones vectoriales de palabras, también conocidos como embeddings de palabras.\n",
    "\n",
    "Este modelo predice una palabra objetivo (la palabra central) a partir de un conjunto dado de palabras de contexto que la rodean en una frase o un párrafo. El \"contexto\" se refiere generalmente a las `n` palabras antes y después de la palabra objetivo en una ventana específica de tamaño `2n+1`, excluyendo la palabra objetivo.\n",
    "\n",
    "Por ejemplo, en la oración `el gato come pescado`, si queremos predecir la palabra `come` utilizando un contexto de tamaño 1, las palabras de contexto serían `[\"gato\", \"pescado\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al definir datos de entrenamiento, Genism word2vec requiere que se proporcione un formato de \"lista de listas\" para el entrenamiento donde cada documento esté contenido en una lista. Cada lista contiene listas de tokens de ese documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [['dog','bites','man'], [\"man\", \"bites\" ,\"dog\"],[\"dog\",\"eats\",\"meat\"],[\"man\", \"eats\",\"food\"]]\n",
    "\n",
    "#entrenando el modelo\n",
    "modelo_cbow = Word2Vec(corpus, min_count=1,sg=0) #usando la arquitectura CBOW para entrenamiento\n",
    "modelo_skipgram = Word2Vec(corpus, min_count=1,sg=1)#usando la arquitectura skipGram para entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelo_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En CBOW, la tarea principal es construir un modelo de lenguaje que prediga correctamente la palabra central dadas las palabras de contexto en las que aparece esa palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder al vocabulario\n",
    "palabras = list(modelo_cbow.wv.index_to_key)\n",
    "\n",
    "# Acceder al vector para una palabra específica correctamente\n",
    "vector_dog=modelo_cbow.wv.get_vector('dog')\n",
    "# Otra manera válida pero menos explícita es modelo_cbow.wv['dog']\n",
    "# Completa\n",
    "print(vector_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la similaridad\n",
    "print(\"La similaridad entre eats y bites es:\", modelo_cbow.wv.similarity('eats', 'bites'))\n",
    "print(\"La similaridas entre eats y man es:\", modelo_cbow.wv.similarity('eats', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cbow.wv.most_similar('meat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardando el modelo\n",
    "modelo_cbow.save('modelo_cbow.bin')\n",
    "\n",
    "# cargando el modelo\n",
    "nuevo_modelo_cbow = Word2Vec.load('modelo_cbow.bin')\n",
    "print(nuevo_modelo_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram\n",
    "\n",
    "Continuous Bag of Words (CBOW) y Skip-gram  son dos arquitecturas del modelo Word2Vec desarrolladas por Mikolov  para generar representaciones vectoriales densas de palabras, conocidas como embeddings. Estos embeddings capturan relaciones semánticas y sintácticas entre palabras basadas en su co-ocurrencia en grandes corpus de texto. \n",
    "\n",
    "Ambas arquitecturas utilizan una red neuronal poco profunda para aprender estas representaciones, pero difieren en la forma en que están estructuradas y en cómo aprenden de los datos.\n",
    "\n",
    "La arquitectura Skip-gram predice las palabras de contexto (palabras circundantes) dada una palabra objetivo. Por ejemplo, si consideramos la frase `El rápido zorro marrón`, y nuestra palabra objetivo es `rápido`, con un tamaño de ventana de contexto de 2, Skip-gram intentaría predecir `El`, `zorro`, `marrón` a partir de `rápido`. Esto significa que para cada palabra objetivo en el corpus, se generan muestras de entrenamiento al emparejarla con las palabras de contexto dentro de una ventana específica alrededor de ella.\n",
    "\n",
    "Recuerda la arquitectura CBOW, por otro lado, hace lo opuesto: predice la palabra objetivo a partir de las palabras de contexto. Utilizando el mismo ejemplo anterior, CBOW tomaría `El`, `zorro`, `marrón` como entrada para predecir `rápido`. \n",
    "\n",
    "En esencia, CBOW promedia las palabras de contexto (o las suma, dependiendo de la implementación) para predecir la palabra en el centro de la ventana de contexto.\n",
    "\n",
    "A pesar de la disponibilidad de varias implementaciones listas para usar, todavía tenemos que tomar decisiones sobre varios hiperparámetros (es decir, las variables que deben configurarse antes de comenzar el proceso de entrenamiento). Veamos dos ejemplos. \n",
    "\n",
    "\n",
    "- Dimensionalidad de los vectores de palabras: como su nombre lo indica, esto decide el espacio de las embeddings aprendidas. Si bien no existe un número ideal, es común construir vectores de palabras con dimensiones en el rango de 50 a 500 y evaluarlos en la tarea para la que los estamos usando para elegir la mejor opción. \n",
    "\n",
    "- Ventana contextual: Qué tan largo o corto es el contexto que buscamos para aprender la representación vectorial. \n",
    "\n",
    "También hay otras opciones que hacemos, como usar CBOW o SkipGram para aprender las embeddings. Estas elecciones son más un arte que una ciencia en este momento, y hay mucha investigación en curso sobre métodos para elegir los hiperparámetros correctos. \n",
    "\n",
    "Usando paquetes como gensim, es bastante sencillo desde el punto de vista del código implementar Word2vec. \n",
    "\n",
    "El siguiente código muestra cómo entrenar nuestro propio modelo Word2vec usando un corpus llamado `common_texts` que está disponible en gensim. Suponiendo que tiene el corpus para su dominio, siguiendo este fragmento de código obtendrá rápidamente sus propias embeddings: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelo_skipgram)\n",
    "palabras = list(modelo_skipgram.wv.index_to_key)\n",
    "print(palabras)\n",
    "\n",
    "vector_dog = modelo_skipgram.wv['dog']\n",
    "\n",
    "# Opción 2: Usar el método `.get_vector()`\n",
    "vector_dog = modelo_skipgram.wv.get_vector('dog')\n",
    "\n",
    "print(vector_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la similaridad\n",
    "print(\"Similaridad entre eats y  bites:\",modelo_skipgram.wv.similarity('eats', 'bites'))\n",
    "print(\"Similaridad entre eats y  man:\",modelo_skipgram.wv.similarity('eats', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "modelo_w =Word2Vec(common_texts, vector_size=10, window=5, min_count=1, workers=4)\n",
    "modelo_w.save(\"modelo_ws.w2v\")\n",
    "\n",
    "print(modelo_w.wv.most_similar(\"computer\", topn=4))\n",
    "print(modelo_w.wv.get_vector('computer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "1.Experimenta con otras palabras y guarda el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Entrena un modelo Word2Vec en modo CBOW con un corpus de texto de tu elección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Ejemplo de corpus: lista de frases\n",
    "corpus = [\n",
    "    \"Gensim es una biblioteca de modelado de temas de Python.\",\n",
    "    \"Gensim incluye implementaciones de Word2Vec, Doc2Vec, y otros modelos.\",\n",
    "    \"Los embeddings de palabras son útiles para tareas de procesamiento de lenguaje natural.\"\n",
    "]\n",
    "\n",
    "# Preprocesamiento simple y tokenización\n",
    "corpus_tokenizado = [simple_preprocess(doc) for doc in corpus]\n",
    "\n",
    "# Entrenar un modelo Word2Vec en modo CBOW (sg=0)\n",
    "modelo_cbow = Word2Vec(sentences=corpus_tokenizado, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "\n",
    "# Guardar el modelo\n",
    "modelo_cbow.save(\"modelo_cbow.word2vec\")\n",
    "\n",
    "# Imprimir las palabras más similares a 'gensim'\n",
    "print(modelo_cbow.wv.most_similar('gensim'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 . Entrena un modelo Word2Vec en modo Skip-gram con el mismo corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando el mismo corpus_tokenizado del ejercicio anterior\n",
    "\n",
    "# Entrenar un modelo Word2Vec en modo Skip-gram (sg=1)\n",
    "modelo_skipgram = Word2Vec(sentences=corpus_tokenizado, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Guardar el modelo\n",
    "modelo_skipgram.save(\"modelo_skipgram.word2vec\")\n",
    "\n",
    "# Imprimir las palabras más similares a 'word2vec'\n",
    "print(modelo_skipgram.wv.most_similar('word2vec'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 . Carga un modelo de embeddings preentrenado y utiliza para encontrar palabras similares. Debes descargar un conjunto de embeddings preentrenados como Google News vectors o cualquier otro de tu elección y proporcionar la ruta correcta al cargarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Cargar embeddings preentrenados (reemplazar 'path_to_embeddings' con la ruta real)\n",
    "# Asegúrate de tener el archivo .bin o el formato correcto del modelo que estás cargando\n",
    "modelo_preentrenado = KeyedVectors.load_word2vec_format('path_to_embeddings.bin', binary=True)\n",
    "\n",
    "# Imprimir las palabras más similares a 'king'\n",
    "print(modelo_preentrenado.most_similar('king'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Existe alguna forma de utilizar embeddings de palabras para obtener representaciones de características para unidades de texto más grandes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código muestra cómo obtener la representación vectorial de texto promediando vectores de palabras usando la biblioteca spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "%time \n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "doc1 = nlp(\"Canada is a large country\")\n",
    "#print(doc[0].vector) #vector para 'Canada', la primera palabra en el texto\n",
    "print(doc1.vector)# Vector promedio para toda la oracion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué sucede cuando doy una oración con palabras extrañas e intento obtener su vector de palabras en Spacy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = nlp('practicalnlp is a newword')\n",
    "#temp[0].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectores de documentos\n",
    "\n",
    "Doc2vec nos permite aprender directamente las representaciones de textos de longitud arbitraria (frases, oraciones, párrafos y documentos), teniendo en cuenta el contexto de las palabras del texto.\n",
    "\n",
    "Esto es similar a Word2vec en términos de su arquitectura general, excepto que, además de los vectores de palabras, también aprende un \"vector de párrafo\" que aprende una representación del texto completo (es decir, con palabras en contexto). Cuando se aprende con un corpus grande de muchos textos, los vectores de párrafo son únicos para un texto determinado (donde \"texto\" puede significar cualquier fragmento de texto de longitud arbitraria), mientras que los vectores de palabras se compartirán en todos los textos.  \n",
    "\n",
    "\n",
    "Hay dos arquitecturas del modelo Doc2Vec, que es una extensión de Word2Vec diseñada para generar representaciones vectoriales no solo para palabras sino también para piezas de texto más grandes como oraciones, párrafos y documentos. Estas representaciones vectoriales son útiles para muchas tareas de procesamiento del lenguaje natural, como la clasificación de textos y la búsqueda semántica. Aquí están las dos arquitecturas: \n",
    "\n",
    "**Memoria distribuida (DM)**: \n",
    "\n",
    "En el modelo DM de Doc2Vec, cada palabra y el párrafo (o documento) entero tienen su propio vector de aprendizaje único en una \"Paragraph Matrix\" y en una \"Word Matrix\", respectivamente. \n",
    "\n",
    "Durante el entrenamiento, el modelo intenta predecir la siguiente palabra en un contexto dada una ventana de palabras y el vector único del párrafo/documento. \n",
    "\n",
    "Los vectores de las palabras y del párrafo se pueden promediar o concatenar antes de enviarlos a una capa de clasificador, que intenta predecir la palabra siguiente. \n",
    "\n",
    "El objetivo es que al final del entrenamiento, el vector del párrafo capture la esencia del texto, lo que hace posible usar este vector para tareas de clasificación o comparación de similitud. \n",
    "\n",
    "**Bolsa de palabras distribuidas (DBOW)**: \n",
    "\n",
    "El modelo DBOW funciona de manera inversa al DM. Ignora el contexto de las palabras y, en su lugar, fuerza al modelo a predecir las palabras en un párrafo/documento dada solo la identificación del párrafo (es decir, su vector único). \n",
    "\n",
    "No hay una capa de promedio o concatenación; el modelo directamente predice las palabras a partir del vector del párrafo. \n",
    "\n",
    "Al igual que en el modelo DM, el vector del párrafo se entrena para representar el contenido completo del párrafo/documento. \n",
    "\n",
    "DBOW es eficaz para grandes conjuntos de datos donde la semántica puede ser capturada incluso sin el orden exacto de las palabras. \n",
    "\n",
    "Ambos métodos son útiles para aprender representaciones vectoriales que reflejan el significado de los párrafos o documentos, aunque capturan diferentes aspectos de los datos: DM toma en cuenta el orden de las palabras, mientras que DBOW se centra en la ocurrencia de las palabras. Estos vectores resultantes pueden ser utilizados en diversas tareas, tales como agrupación de documentos, clasificación y búsqueda por similitud semántica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que cada frase de los documentos corresponde a un documento independiente y iteramos sobre cada documento e iniciar una instancia de NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "docs_procesados = [doc.lower().replace(\".\",\"\") for doc in documentos]\n",
    "docs_procesados\n",
    "\n",
    "print(\"Documento despues del preprocesamiento:\",docs_procesados)\n",
    "\n",
    "for doc in docs_procesados:\n",
    "    doc_nlp = nlp(doc)\n",
    "    \n",
    "    print(\"-\"*30)\n",
    "    print(\"Vector promedio de '{}'\\n\".format(doc),doc_nlp.vector)\n",
    "    for token in doc_nlp:\n",
    "        print()\n",
    "        print(token.text,token.vector)# esto da el texto de cada palabra en el doc y sus valores respectivos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ejercicios\n",
    "\n",
    "Entrena modelos Doc2Vec utilizando ambas arquitecturas, DM y DBOW, y compara su desempeño en una tarea de similitud de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Preparación de datos: Tagging de cada documento en el corpus\n",
    "documentos = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "\n",
    "# DM\n",
    "modelo_dm = Doc2Vec(documents=documentos, vector_size=100, window=5, min_count=1, dm=1)\n",
    "modelo_dm.save(\"modelo_dm.doc2vec\")\n",
    "\n",
    "# DBOW\n",
    "modelo_dbow = Doc2Vec(documents=documentos, vector_size=100, window=5, min_count=1, dm=0)\n",
    "modelo_dbow.save(\"modelo_dbow.doc2vec\")\n",
    "\n",
    "# Escoge un documento y compara los documentos más similares desde ambos modelos\n",
    "doc_id = 0  # Asumiendo que quieres comprobar el primer documento del corpus\n",
    "print(\"DM Similar:\", modelo_dm.dv.most_similar([modelo_dm[doc_id]]))\n",
    "print(\"DBOW Similar:\", modelo_dbow.dv.most_similar([modelo_dbow[doc_id]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec usando gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = [\"Dog bites man.\", \n",
    "              \"Man bites dog.\", \n",
    "              \"Dog eats meat.\", \n",
    "              \"Man eats food.\"]\n",
    "documentos_etiquetados = [TaggedDocument(words=word_tokenize(word.lower()), tags=[str(i)]) for i, word in enumerate(documentos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_etiquetados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando el modelo dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_dbow = Doc2Vec(documentos_etiquetados, vector_size=20, min_count=1, epochs=2, dm=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelo_dbow.infer_vector(['man', 'food', 'eats']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_dbow.wv.most_similar(\"food\", topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_dbow.wv.n_similarity([\"man\"],[\"dog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajando con el modelo DM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_dm = Doc2Vec(documentos_etiquetados, min_count=1, vector_size=20, epochs=2, dm=1)\n",
    "modelo_dm.infer_vector([\"man\", \"eats\", \"food\"])\n",
    "modelo_dm.wv.most_similar(\"dog\", topn=5)\n",
    "modelo_dm.wv.n_similarity([\"man\"],[\"dog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasa cuando comparamos palabras que no estan el vocabulario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_dm.wv.n_similarity([\"covid\"],[\"man\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
