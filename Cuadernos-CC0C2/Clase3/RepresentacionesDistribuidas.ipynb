{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representaciones distribuidas\n",
    "\n",
    "Las representaciones distribuidas en el contexto del procesamiento del lenguaje natural (NLP) y el aprendizaje automático se refieren a una forma de representar palabras o entidades como vectores de características continuas en un espacio vectorial de alta dimensión. Esta metodología contrasta con las representaciones discretas o locales, como los métodos de bolsa de palabras o one-hot encoding, donde cada palabra se representa como un vector único en un espacio de dimensiones muy grandes, con un solo elemento igual a 1 (representando la presencia de la palabra) y el resto igual a 0.\n",
    "\n",
    "La idea clave detrás de las representaciones distribuidas es que las palabras se representan mediante patrones de características numéricas, de tal manera que las similitudes semánticas y sintácticas entre las palabras se reflejan en la cercanía de sus vectores correspondientes en el espacio vectorial. En otras palabras, palabras con significados o usos similares tendrán representaciones vectoriales similares.\n",
    "\n",
    "Las representaciones distribuidas han revolucionado la manera en que las máquinas entienden el lenguaje humano, permitiendo avances significativos en tareas de NLP como traducción automática, análisis de sentimientos, clasificación de texto, y muchas otras, gracias a su capacidad para capturar y utilizar la rica información semántica y sintáctica del lenguaje.\n",
    "Utilizan arquitecturas de redes neuronales para crear representaciones densas y de baja dimensión de palabras y textos. Pero antes de analizar estos métodos, debemos comprender algunos términos clave: \n",
    "\n",
    "  \n",
    "- Similitud distributiva: Ésta es la idea de que el significado de una palabra puede entenderse a partir del contexto en el que aparece. Esto también se conoce como connotación: el significado se define por el contexto. Esto se opone a la denotación: el significado literal de cualquier palabra. Por ejemplo: \" NLP rocks \". El significado literal de la palabra \"rocks\" es \"piedras\", pero por el contexto, se usa para referirse a algo bueno y de moda. \n",
    "\n",
    "- Hipótesis distributiva: En lingüística, esto plantea la hipótesis de que las palabras que ocurren en contextos similares tienen significados similares. Por ejemplo, las palabras inglesas \"dog\" y \"cat\" aparecen en contextos similares. Por tanto, según la hipótesis distributiva, debe haber una gran similitud entre los significados de estas dos palabras. Ahora, siguiendo con VSM, el significado de una palabra está representado por el vector. Por lo tanto, si dos palabras aparecen a menudo en un contexto similar, entonces sus vectores de representación correspondientes también deben estar cerca uno del otro. \n",
    "\n",
    "- Representación distributiva: Se refiere a esquemas de representación que se obtienen en base a la distribución de palabras del contexto en el que aparecen. Estos esquemas se basan en hipótesis distributivas. La propiedad distributiva se induce a partir del contexto (vecindad textual). Matemáticamente, los esquemas de representación distributiva utilizan vectores de alta dimensión para representar palabras. Estos vectores se obtienen a partir de una matriz de coocurrencia que captura la coocurrencia de palabra y contexto. La dimensión de esta matriz es igual al tamaño del vocabulario del corpus. Los cuatro esquemas que hemos visto hasta ahora (one-hot, bolsa de palabras, bolsa de n-gramas y TF-IDF) caen bajo el paraguas de la representación distributiva. \n",
    "\n",
    "- Representación distribuida: Este es un concepto relacionado. También se basa en la hipótesis distributiva. Como se analizó en el párrafo anterior, los vectores en representación distributiva son dispersos y de muy altas dimensiones. Esto los hace computacionalmente ineficientes y dificulta el aprendizaje. Para aliviar esto, los esquemas de representación distribuida comprimen significativamente la dimensionalidad. Esto da como resultado vectores que son compactos (es decir, de baja dimensión) y densos (es decir, casi sin ceros). El espacio vectorial resultante se conoce como representación distribuida. Todos los esquemas posteriores que analizaremos en esta clase son ejemplos de representación distribuida. \n",
    "\n",
    "- Embeddings: Para el conjunto de palabras en un corpus, el embedding es un mapeo entre el espacio vectorial proveniente de la representación distributiva y el espacio vectorial proveniente de la representación distribuida. \n",
    "\n",
    "- Semántica vectorial: Esto se refiere al conjunto de métodos de NLP que tienen como objetivo aprender las representaciones de palabras basadas en las propiedades distributivas de las palabras en un corpus grande. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características Principales\n",
    "\n",
    "Veamos algunos características de estos métodos:\n",
    "\n",
    "- A diferencia de las representaciones locales, las distribuidas pueden capturar relaciones complejas entre palabras, como sinónimos, antónimos o términos que suelen aparecer en contextos similares.\n",
    "\n",
    "- Al representar palabras como vectores de tamaño fijo en un espacio continuo, se reduce la dimensionalidad del problema comparado con métodos de representación más simples pero de alta dimensionalidad, como el one-hot encoding.\n",
    "\n",
    "- Estos modelos pueden generalizar para entender palabras nuevas o raras a partir de sus componentes (por ejemplo, entender palabras compuestas a partir de los significados de sus partes).\n",
    "\n",
    "**Ejemplos y modelos**\n",
    "\n",
    "- Word2Vec: Probablemente el ejemplo más conocido de representaciones distribuidas. Word2Vec utiliza redes neuronales para aprender representaciones vectoriales de palabras a partir de grandes conjuntos de datos de texto. Ofrece dos arquitecturas principales: CBOW (Continuous Bag of Words) y Skip-gram, cada una diseñada para aprender representaciones que predigan palabras en función de sus contextos o viceversa.\n",
    "\n",
    "- GloVe (Global Vectors for Word Representation): Un modelo que aprende representaciones de palabras a partir de las estadísticas co-ocurrenciales de palabras en un corpus. La idea es que las relaciones semánticas entre palabras pueden ser capturadas observando qué tan frecuentemente aparecen juntas en un gran corpus.\n",
    "\n",
    "- Embeddings Contextuales: Modelos más recientes como ELMo, BERT y GPT ofrecen una evolución de las representaciones distribuidas, generando vectores de palabras que varían según el contexto en el que aparecen, lo que permite capturar usos y significados múltiples de una misma palabra dependiendo de la oración en la que se encuentre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings de palabras\n",
    "\n",
    "Los embeddings de palabras son representaciones vectoriales densas y de baja dimensión de palabras, diseñadas para capturar el significado semántico, sintáctico y relaciones entre ellas. A diferencia de las representaciones de texto más antiguas, como el one-hot encoding, que son dispersas (la mayoría de los valores son cero) y de alta dimensión, los embeddings de palabras se representan en un espacio vectorial continuo donde palabras con significados similares están ubicadas cercanamente en el espacio vectorial.\n",
    "\n",
    "**Características de los embeddings de palabras**\n",
    "\n",
    "- Cada palabra se representa como un vector denso, lo que significa que cada dimensión tiene un valor real, a diferencia de los vectores dispersos de otras técnicas de representación.\n",
    "\n",
    "- Los embeddings generalmente tienen un tamaño de dimensión fijo y relativamente pequeño (por ejemplo, 100, 200, 300 dimensiones) independientemente del tamaño del vocabulario.\n",
    "\n",
    "- Estos vectores intentan capturar el contexto y el significado de una palabra, no solo su presencia o ausencia. Palabras que se usan en contextos similares tendrán embeddings similares.\n",
    "\n",
    "- Pueden ayudar a los modelos de aprendizaje automático a generalizar mejor a palabras no vistas durante el entrenamiento, dado que las palabras con significados similares se mapean a puntos cercanos en el espacio vectorial.\n",
    "\n",
    "\n",
    "En 2013, un trabajo fundamental de Mikolov [Efficient Estimationof Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) demostraron que su modelo de representación de palabras basado en una red neuronal conocido como `Word2vec`, basado en la `similitud distributiva`, puede capturar relaciones de analogía de palabras como: \n",
    "\n",
    "$$King - Man + Woman \\approx Queen$$\n",
    "\n",
    "Conceptualmente, Word2vec toma un gran corpus de texto como entrada y \"aprende\" a representar las palabras en un espacio vectorial común en función de los contextos en los que aparecen en el corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings de palabras pre-entrenadas\n",
    "\n",
    "El siguente es un ejemplo de cómo cargar embeddings de Word2vec previamente entrenadas y buscar las palabras más similares (clasificadas por similitud de coseno) a una palabra determinada. \n",
    "\n",
    "Tomemos un ejemplo de un modelo word2vec previamente entrenado y cómo podemos usarlo para buscar la mayoría de las palabras similares. Usaremos los embeddings de vectores de Google News. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
    "\n",
    "Se pueden encontrar algunos otros modelos de embeddings de palabras previamente entrenados y detalles sobre los medios para acceder a ellos a través de gensim en: https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "El código que sigue cubre los pasos clave. Aquí encontramos las palabras que semánticamente son más similares a la palabra “beautiful”; la última línea devuelve el vector de embeddings de la palabra \" beautiful \":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# URL de Google Drive\n",
    "url = 'https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
    "\n",
    "# Ruta donde se guardará el archivo comprimido descargado\n",
    "ruta_descarga = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# Ruta del archivo descomprimido\n",
    "ruta_extraccion = \"GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# Descargar el archivo usando gdown\n",
    "gdown.download(url, ruta_descarga, quiet=False)\n",
    "\n",
    "# Descomprimir el archivo\n",
    "with gzip.open(ruta_descarga, 'rb') as f_in:\n",
    "    with open(ruta_extraccion, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(f\"Archivo descomprimido en {ruta_extraccion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import psutil \n",
    "procesos = psutil.Process(os.getpid())\n",
    "from psutil import virtual_memory\n",
    "memoria = virtual_memory()\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos algunos cálculos del uso de los datos descargados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "pretrainedpath = ruta_extraccion\n",
    "\n",
    "#Se carga el modelo W2V. \n",
    "pre = procesos.memory_info().rss\n",
    "print(\"Memoria usada en GB antes de cargar el modelo: %0.2f\"%float(pre/(10**9))) \n",
    "print('-'*10)\n",
    "\n",
    "tiempo_inicio = time.time() \n",
    "ttl = memoria.total \n",
    "\n",
    "w2v_modelo = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) \n",
    "print(\"%0.2f segundos para tomar\"%float(time.time() - tiempo_inicio)) \n",
    "print('-'*10)\n",
    "\n",
    "print('Finalizacion de cargar  Word2Vec')\n",
    "print('-'*10)\n",
    "\n",
    "post = procesos.memory_info().rss\n",
    "print(\"Memoria usada en GB despues de cargar el modelo: {:.2f}\".format(float(post/(10**9))))\n",
    "print('-'*10)\n",
    "print(\"Aumento porcentual en el uso de memoria: {:.2f}% \".format(float((post/pre)*100))) \n",
    "print('-'*10)\n",
    "\n",
    "print(\"Numero de palabras en el vocabulario: \",len(w2v_modelo.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinemos el modelo sabiendo cuáles son las palabras más similares para una palabra determinada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasa si busco una palabra que no está en este vocabulario?:\n",
    "`w2v_modelo['practicalnlp']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos cosas a tener en cuenta al utilizar modelos previamente entrenados:\n",
    "\n",
    "* Los tokens/palabras siempre están en minúsculas. Si una palabra no está en el vocabulario, el modelo genera una excepción.\n",
    "* Por lo tanto, siempre es una buena idea encapsular esas declaraciones en bloques `try/except`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando nuestros embeddings \n",
    "\n",
    "Ahora nos centraremos en entrenar nuestras propias embeddings de palabras. Para ello, veremos dos variantes arquitectónicas propuestas en el enfoque original de Word2vec. Las dos variantes son: \n",
    "\n",
    "* Bolsa continua de palabras (CBOW) \n",
    "\n",
    "* Skip-Gram \n",
    "\n",
    "Para utilizar los algoritmos CBOW y SkipGram en la práctica, hay varias implementaciones disponibles que nos abstraen los detalles matemáticos. Una de las implementaciones más utilizadas es [gensim](https://github.com/piskvorky/gensim). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW\n",
    "\n",
    "El modelo Continuous Bag of Words (CBOW) es uno de los dos enfoques arquitectónicos propuestos por Mikolov  para aprender representaciones vectoriales de palabras, también conocidos como embeddings de palabras.\n",
    "\n",
    "Este modelo predice una palabra objetivo (la palabra central) a partir de un conjunto dado de palabras de contexto que la rodean en una frase o un párrafo. El \"contexto\" se refiere generalmente a las `n` palabras antes y después de la palabra objetivo en una ventana específica de tamaño `2n+1`, excluyendo la palabra objetivo.\n",
    "\n",
    "Por ejemplo, en la oración `el gato come pescado`, si queremos predecir la palabra `come` utilizando un contexto de tamaño 1, las palabras de contexto serían `[\"gato\", \"pescado\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al definir datos de entrenamiento, Genism word2vec requiere que se proporcione un formato de \"lista de listas\" para el entrenamiento donde cada documento esté contenido en una lista. Cada lista contiene listas de tokens de ese documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [['dog','bites','man'], [\"man\", \"bites\" ,\"dog\"],[\"dog\",\"eats\",\"meat\"],[\"man\", \"eats\",\"food\"]]\n",
    "\n",
    "#entrenando el modelo\n",
    "modelo_cbow = Word2Vec(corpus, min_count=1,sg=0) #usando la arquitectura CBOW para entrenamiento\n",
    "modelo_skipgram = Word2Vec(corpus, min_count=1,sg=1)#usando la arquitectura skipGram para entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En CBOW, la tarea principal es construir un modelo de lenguaje que prediga correctamente la palabra central dadas las palabras de contexto en las que aparece esa palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder al vocabulario\n",
    "# Completa\n",
    "\n",
    "# Acceder al vector para una palabra específica correctamente\n",
    "# Completa Método preferido\n",
    "# Otra manera válida pero menos explícita es modelo_cbow.wv['dog']\n",
    "# Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la similaridad\n",
    "# Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cbow.wv.most_similar('meat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardando el modelo\n",
    "modelo_cbow.save('modelo_cbow.bin')\n",
    "\n",
    "# cargando el modelo\n",
    "nuevo_modelo_cbow = Word2Vec.load('modelo_cbow.bin')\n",
    "print(nuevo_modelo_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram\n",
    "\n",
    "Continuous Bag of Words (CBOW) y Skip-gram  son dos arquitecturas del modelo Word2Vec desarrolladas por Mikolov  para generar representaciones vectoriales densas de palabras, conocidas como embeddings. Estos embeddings capturan relaciones semánticas y sintácticas entre palabras basadas en su co-ocurrencia en grandes corpus de texto. \n",
    "\n",
    "Ambas arquitecturas utilizan una red neuronal poco profunda para aprender estas representaciones, pero difieren en la forma en que están estructuradas y en cómo aprenden de los datos.\n",
    "\n",
    "La arquitectura Skip-gram predice las palabras de contexto (palabras circundantes) dada una palabra objetivo. Por ejemplo, si consideramos la frase `El rápido zorro marrón`, y nuestra palabra objetivo es `rápido`, con un tamaño de ventana de contexto de 2, Skip-gram intentaría predecir `El`, `zorro`, `marrón` a partir de `rápido`. Esto significa que para cada palabra objetivo en el corpus, se generan muestras de entrenamiento al emparejarla con las palabras de contexto dentro de una ventana específica alrededor de ella.\n",
    "\n",
    "Recuerda la arquitectura CBOW, por otro lado, hace lo opuesto: predice la palabra objetivo a partir de las palabras de contexto. Utilizando el mismo ejemplo anterior, CBOW tomaría `El`, `zorro`, `marrón` como entrada para predecir `rápido`. \n",
    "\n",
    "En esencia, CBOW promedia las palabras de contexto (o las suma, dependiendo de la implementación) para predecir la palabra en el centro de la ventana de contexto.\n",
    "\n",
    "A pesar de la disponibilidad de varias implementaciones listas para usar, todavía tenemos que tomar decisiones sobre varios hiperparámetros (es decir, las variables que deben configurarse antes de comenzar el proceso de entrenamiento). Veamos dos ejemplos. \n",
    "\n",
    "\n",
    "- Dimensionalidad de los vectores de palabras: como su nombre lo indica, esto decide el espacio de las embeddings aprendidas. Si bien no existe un número ideal, es común construir vectores de palabras con dimensiones en el rango de 50 a 500 y evaluarlos en la tarea para la que los estamos usando para elegir la mejor opción. \n",
    "\n",
    "- Ventana contextual: Qué tan largo o corto es el contexto que buscamos para aprender la representación vectorial. \n",
    "\n",
    "También hay otras opciones que hacemos, como usar CBOW o SkipGram para aprender las embeddings. Estas elecciones son más un arte que una ciencia en este momento, y hay mucha investigación en curso sobre métodos para elegir los hiperparámetros correctos. \n",
    "\n",
    "Usando paquetes como gensim, es bastante sencillo desde el punto de vista del código implementar Word2vec. \n",
    "\n",
    "El siguiente código muestra cómo entrenar nuestro propio modelo Word2vec usando un corpus llamado `common_texts` que está disponible en gensim. Suponiendo que tiene el corpus para su dominio, siguiendo este fragmento de código obtendrá rápidamente sus propias embeddings: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelo_skipgram)\n",
    "palabras = list(modelo_skipgram.wv.index_to_key)\n",
    "print(palabras)\n",
    "\n",
    "vector_dog = modelo_skipgram.wv['dog']\n",
    "\n",
    "# Opción 2: Usar el método `.get_vector()`\n",
    "vector_dog = modelo_skipgram.wv.get_vector('dog')\n",
    "\n",
    "print(vector_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la similaridad\n",
    "print(\"Similaridad entre eats y  bites:\",modelo_skipgram.wv.similarity('eats', 'bites'))\n",
    "print(\"Similaridad entre eats y  man:\",modelo_skipgram.wv.similarity('eats', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "1.Experimenta con otras palabras y guarda el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Entrena un modelo Word2Vec en modo CBOW con un corpus de texto de tu elección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Ejemplo de corpus: lista de frases\n",
    "corpus = [\n",
    "    \"Gensim es una biblioteca de modelado de temas de Python.\",\n",
    "    \"Gensim incluye implementaciones de Word2Vec, Doc2Vec, y otros modelos.\",\n",
    "    \"Los embeddings de palabras son útiles para tareas de procesamiento de lenguaje natural.\"\n",
    "]\n",
    "\n",
    "# Preprocesamiento simple y tokenización\n",
    "corpus_tokenizado = [simple_preprocess(doc) for doc in corpus]\n",
    "\n",
    "# Entrenar un modelo Word2Vec en modo CBOW (sg=0)\n",
    "modelo_cbow = Word2Vec(sentences=corpus_tokenizado, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "\n",
    "# Guardar el modelo\n",
    "modelo_cbow.save(\"modelo_cbow.word2vec\")\n",
    "\n",
    "# Imprimir las palabras más similares a 'gensim'\n",
    "print(modelo_cbow.wv.most_similar('gensim'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 . Entrena un modelo Word2Vec en modo Skip-gram con el mismo corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando el mismo corpus_tokenizado del ejercicio anterior\n",
    "\n",
    "# Entrenar un modelo Word2Vec en modo Skip-gram (sg=1)\n",
    "modelo_skipgram = Word2Vec(sentences=corpus_tokenizado, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Guardar el modelo\n",
    "modelo_skipgram.save(\"modelo_skipgram.word2vec\")\n",
    "\n",
    "# Imprimir las palabras más similares a 'word2vec'\n",
    "print(modelo_skipgram.wv.most_similar('word2vec'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 . Carga un modelo de embeddings preentrenado y utiliza para encontrar palabras similares. Debes descargar un conjunto de embeddings preentrenados como Google News vectors o cualquier otro de tu elección y proporcionar la ruta correcta al cargarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Cargar embeddings preentrenados (reemplazar 'path_to_embeddings' con la ruta real)\n",
    "# Asegúrate de tener el archivo .bin o el formato correcto del modelo que estás cargando\n",
    "modelo_preentrenado = KeyedVectors.load_word2vec_format('path_to_embeddings.bin', binary=True)\n",
    "\n",
    "# Imprimir las palabras más similares a 'king'\n",
    "print(modelo_preentrenado.most_similar('king'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
