{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2a1ca0-9f04-4e78-ba1f-5693faa315e6",
   "metadata": {},
   "source": [
    "## Respuestas del examen parcial de procesamiento de lenguaje natural\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e873b-ced5-4486-a579-e8f23eee1ce0",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "**Parte 1**\n",
    "\n",
    "Dadas tres oraciones “all models are wrong”, `a model is wrong` y `some models are useful`, y un vocabulario $\\{<s>, </s>,\\ a,\\ all,\\ are,\\ model,\\ models,\\ some,\\ useful,\\ wrong\\}$. En codigo responde las siguientes preguntas\n",
    "\n",
    "**(a)** Calcule las probabilidades de todos los bigramas sin suavizado.\n",
    "\n",
    "**(b)** Calcule las probabilidades de todos los bigramas y el bigrama no visto `a models` con suavizado add-one.\n",
    "\n",
    "**(c)** Calcule las probabilidades de todos los bigramas y el bigrama no visto `a models` con suavizado add-$k$. Pruebe $k = 0.05$ y $k= 0.15$.\n",
    "\n",
    "**(d)** Calcule las probabilidades de todos los bigramas y el bigrama no visto `a models` con back-off y stupid-backoff.\n",
    "\n",
    "**Parte 2**\n",
    "\n",
    "**(e)** El suavizado de Good-Turing reasigna la masa de probabilidad de los n-gramas ricos a los n-gramas pobres. Dado un corpus $D$, supongamos que tratamos todas las unigrama desconocidas como $\\langle \\text{UNK} \\rangle$, por lo tanto, el vocabulario es $\\{w : w \\in D\\} \\cup \\{\\langle \\text{UNK} \\rangle\\}$ y $N_0 = 1$. Calcula $r$, $N_r$, para todas las unigramas de la parte 1.\n",
    "\n",
    "**(f)** Para $r < 3$, calcula $c_r$ y las probabilidades de todas las unigramas.\n",
    "\n",
    "**(g)** Para el valor máximo de $r$, $N_{r+1} = 0$. En este caso, la probabilidad $P(w : \\#w = r)$ aún puede estimarse mediante MLE (Máxima Verosimilitud). Calcula la probabilidad de las unigramas de la parte 1 que aparecen con mayor frecuencia, es decir, $ r = 3$.\n",
    "\n",
    "**(h)** Muestra que la suma de las probabilidades de todas las unigramas dadas en **(f)** y **(g)** no es 1. Intenta normalizar las probabilidades.\n",
    "\n",
    "**(i)** En un corpus grande, $N_r$ puede ser cero para valores grandes de $r$. Esto puede ser problemático, ya que conduce a que los valores estimados de $P(w : \\#w = r)$ sean cero. Una forma de resolver este problema es usar una línea suavizada para ajustar aproximadamente la distribución de los valores conocidos. Supongamos que cambiamos la segunda oración de ejemplo de la parte 1 a `a model is wrong wrong wrong wrong` de modo que $N_4 = 1$, pero $N_5 = 0 $. Adivina un buen valor suavizado de $N_5$. Usa el valor aproximado de $N_5$ y el valor original de los otros recuentos de frecuencia para calcular las probabilidades de todas las unigramas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08671be7",
   "metadata": {},
   "source": [
    "#### **1. Definición del corpus y vocabulario**\n",
    "\n",
    "Tenemos tres oraciones en el corpus y un vocabulario específico. Notar que, aunque la oración contiene la palabra `is`, no estaba incluida inicialmente en el vocabulario proporcionado. Por coherencia y para evitar inconsistencias, **incluiremos `is` en el vocabulario**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 1. Definición del Corpus y Vocabulario\n",
    "sentences = [\n",
    "    \"all models are wrong\",\n",
    "    \"a model is wrong\",\n",
    "    \"some models are useful\"\n",
    "]\n",
    "\n",
    "vocab = {'<s>', '</s>', 'a', 'all', 'are', 'is', 'model', 'models', 'some', 'useful', 'wrong'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d9b65",
   "metadata": {},
   "source": [
    "Generaremos los bigramas a partir de las oraciones, añadiendo tokens especiales de inicio `<s>` y fin `</s>`. Además, contaremos las frecuencias de cada bigrama y unigram que aparece como primera palabra en cualquier bigrama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generación de Bigramos y Conteo de Frecuencias\n",
    "def generate_bigrams(sentences, vocab):\n",
    "    \"\"\"\n",
    "    Genera bigramas a partir de las oraciones del corpus, incluyendo <s> y </s>.\n",
    "    También cuenta las frecuencias de bigramas y unigramas.\n",
    "    \"\"\"\n",
    "    bigram_counts = defaultdict(int)\n",
    "    unigram_counts = defaultdict(int)  # Conteo de unigramas como primera palabra en bigramas\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenizamos la oración y añadimos <s> y </s>\n",
    "        tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "        \n",
    "        # Verificamos que todas las palabras estén en el vocabulario\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                print(f\"Advertencia: La palabra '{token}' no está en el vocabulario.\")\n",
    "        \n",
    "        # Extraemos los bigramas y actualizamos los conteos\n",
    "        for i in range(len(tokens)-1):\n",
    "            w1, w2 = tokens[i], tokens[i+1]\n",
    "            bigram_counts[(w1, w2)] += 1\n",
    "            unigram_counts[w1] += 1  # Conteo de unigramas como primera palabra en bigramas\n",
    "    \n",
    "    return bigram_counts, unigram_counts\n",
    "\n",
    "# Generamos los bigramas y unigramas\n",
    "bigram_counts, unigram_counts = generate_bigrams(sentences, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9876312",
   "metadata": {},
   "source": [
    "Es útil visualizar los conteos de bigramas y unigramas para entender la distribución de palabras en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4259b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los conteos de bigramas\n",
    "print(\"Conteos de bigramas:\")\n",
    "for bigram, count in bigram_counts.items():\n",
    "    print(f\"{bigram}: {count}\")\n",
    "\n",
    "# Mostramos los conteos de unigramas\n",
    "print(\"\\nConteos de unigramas (como primera palabra en bigramas):\")\n",
    "for unigram, count in unigram_counts.items():\n",
    "    print(f\"{unigram}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24239594",
   "metadata": {},
   "source": [
    "#### **Parte (a): Calcula las probabilidades de todos los bigramas sin suavizado**\n",
    "\n",
    "\n",
    "\n",
    "Para calcular las probabilidades de los bigramas sin aplicar ningún tipo de suavizado, seguimos estos pasos:\n",
    "\n",
    "1. **Preparación del corpus:**\n",
    "   - Añadimos tokens especiales de inicio `<s>` y fin `</s>` a cada oración para capturar adecuadamente las transiciones al inicio y al final de las oraciones.\n",
    "\n",
    "2. **Tokenización y extracción de bigramos:**\n",
    "   - Dividimos cada oración en tokens (palabras individuales) y extraemos los bigramas, que son pares de palabras consecutivas.\n",
    "\n",
    "3. **Conteo de bigramas y unigramas:**\n",
    "   - Contamos cuántas veces aparece cada bigrama en el corpus.\n",
    "   - Contamos cuántas veces aparece cada palabra como la primera palabra en cualquier bigrama.\n",
    "\n",
    "4. **Cálculo de probabilidades:**\n",
    "   - Para cada bigrama `(w1, w2)`, la probabilidad `P(w2 | w1)` se calcula como el número de veces que aparece el bigrama `(w1, w2)` dividido por el número de veces que aparece `w1` como primera palabra en cualquier bigrama.\n",
    "\n",
    "\n",
    "Utilizaremos los conteos generados anteriormente para calcular las probabilidades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b69791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_probabilities(bigram_counts, unigram_counts):\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de bigramas sin suavizado.\n",
    "    \"\"\"\n",
    "    bigram_probabilities = {}\n",
    "    for bigram, count in bigram_counts.items():\n",
    "        w1 = bigram[0]\n",
    "        prob = count / unigram_counts[w1]\n",
    "        bigram_probabilities[bigram] = prob\n",
    "    return bigram_probabilities\n",
    "\n",
    "# Calculamos las probabilidades de los bigramas sin suavizado\n",
    "bigram_probabilities = calculate_bigram_probabilities(bigram_counts, unigram_counts)\n",
    "\n",
    "# Mostramos las probabilidades\n",
    "print(\"Probabilidades de bigramas sin suavizado:\")\n",
    "for bigram, prob in sorted(bigram_probabilities.items()):\n",
    "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4f3a0",
   "metadata": {},
   "source": [
    "#### **Interpretación**\n",
    "\n",
    "- **Inicio de oración (`<s>`):**\n",
    "  - La probabilidad de que una oración comience con cualquiera de las palabras `all`, `a` o `some` es de aproximadamente 0.333 cada una, ya que cada una aparece una vez como la primera palabra después de `<s>` y hay tres inicios posibles.\n",
    "\n",
    "- **Bigramas con probabilidad 1.0:**\n",
    "  - Algunas transiciones de bigramas tienen una probabilidad de 1.0, lo que indica que cada vez que ocurre `w1`, siempre está seguido por `w2` específico en el corpus. Por ejemplo:\n",
    "    - `models` siempre es seguido por `are`.\n",
    "    - `all` siempre es seguido por `models`.\n",
    "    - `a` siempre es seguido por `model`.\n",
    "    - `model` siempre es seguido por `is`.\n",
    "    - `is` siempre es seguido por `wrong`.\n",
    "    - `wrong` y `useful` siempre terminan la oración, ya que siempre son seguidas por `</s>`.\n",
    "\n",
    "- **Bigramos con probabilidad 0.5:**\n",
    "  - La palabra `are` puede ser seguida por `wrong` o `useful`, cada una con una probabilidad de 0.5. Esto refleja que, en el corpus, `are` aparece dos veces y se sigue una vez por `wrong` y otra vez por `useful`.\n",
    "\n",
    "Este cálculo proporciona una estimación de las probabilidades de los bigramas basadas únicamente en las frecuencias observadas en el corpus, sin considerar bigramas no observados ni aplicar ningún tipo de suavizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a4832",
   "metadata": {},
   "source": [
    "#### **Parte (b): Calcula las probabilidades de todos los bigramas y el bigrama no visto `a models` con suavizado add-one**\n",
    "\n",
    "\n",
    "El suavizado *add-one* (también conocido como suavizado de Laplace) es una técnica sencilla para asignar una pequeña probabilidad a los bigramas que no aparecen en el corpus, evitando así probabilidades de cero. Este método añade uno al conteo de cada bigrama y ajusta las probabilidades en consecuencia.\n",
    "\n",
    "**Pasos a seguir:**\n",
    "\n",
    "1. **Añadir uno a cada posible bigrama:**\n",
    "   - Para cada par de palabras en el vocabulario (incluyendo `<s>` y `</s>`), incrementamos el conteo de bigramas observados en el corpus en uno.\n",
    "\n",
    "2. **Actualizar los conteos de unigramas:**\n",
    "   - Debido al suavizado *add-one*, cada unigram que aparece como la primera palabra en un bigrama tendrá su conteo incrementado por el número de palabras en el vocabulario, es decir, por $V$.\n",
    "\n",
    "3. **Calcular las probabilidades:**\n",
    "   - Para cada bigrama `(w1, w2)`, la probabilidad `P(w2 | w1)` se calcula como `(count(w1 w2) + 1) / (count(w1) + V)`, donde `V` es el tamaño del vocabulario.\n",
    "\n",
    "4. **Incluir el bigrama no visto `a models`:**\n",
    "   - Este bigrama no aparece en el corpus original y, gracias al suavizado *add-one*, se le asignará una probabilidad positiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e362c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one_smoothing(bigram_counts, unigram_counts, vocab):\n",
    "    \"\"\"\n",
    "    Aplica el suavizado add-one a los bigramas.\n",
    "    \"\"\"\n",
    "    V = len(vocab)  # Tamaño del vocabulario\n",
    "    bigram_probabilities_add1 = {}\n",
    "    \n",
    "    # Generamos todos los posibles bigramas\n",
    "    all_possible_bigrams = [(w1, w2) for w1 in vocab for w2 in vocab]\n",
    "    \n",
    "    for bigram in all_possible_bigrams:\n",
    "        w1, w2 = bigram\n",
    "        count = bigram_counts.get(bigram, 0)\n",
    "        prob = (count + 1) / (unigram_counts[w1] + V)\n",
    "        bigram_probabilities_add1[bigram] = prob\n",
    "    \n",
    "    return bigram_probabilities_add1\n",
    "\n",
    "# Aplicamos el suavizado add-one\n",
    "bigram_probabilities_add1 = add_one_smoothing(bigram_counts, unigram_counts, vocab)\n",
    "\n",
    "# Mostramos la probabilidad del bigrama no visto 'a models'\n",
    "print(\"\\nProbabilidades de bigramas con suavizado add-one:\")\n",
    "print(f\"P(models | a) = {bigram_probabilities_add1.get(('a', 'models'), 1/(unigram_counts['a'] + len(vocab))):.3f}\")\n",
    "\n",
    "# Opcional: Mostrar todas las probabilidades\n",
    "# for bigram, prob in sorted(bigram_probabilities_add1.items()):\n",
    "#     print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce24ead",
   "metadata": {},
   "source": [
    "#### **Interpretación**\n",
    "\n",
    "- **Bigrama no visto `a models`:**\n",
    "  - Antes del suavizado, el bigrama `a models` no existía en el corpus, lo que le asignaba una probabilidad de 0. Con el suavizado *add-one*, se le asigna una probabilidad positiva de aproximadamente 0.091, permitiendo que el modelo maneje correctamente bigramas no observados.\n",
    "\n",
    "- **Impacto general del suavizado Add-One:**\n",
    "  - Todas las probabilidades de los bigramas existentes se ajustan ligeramente hacia abajo debido a la distribución de la masa de probabilidad adicional a los bigramas no observados.\n",
    "  - Los bigramas no observados ahora tienen una probabilidad positiva, evitando así problemas en modelos que no puedan manejar probabilidades de cero.\n",
    "\n",
    "Este enfoque asegura que el modelo de lenguaje sea más robusto frente a datos no vistos y mejora su capacidad de generalización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda8d9e",
   "metadata": {},
   "source": [
    "#### **Parte (c): Calcula las probabilidades de todos los bigramas y el bigrama no visto `a models` con suavizado add-$k$. Pruebe $k = 0.05$ y $k= 0.15$**\n",
    "\n",
    "\n",
    "El suavizado *add-$k$* es una generalización del suavizado *add-one*, donde en lugar de añadir una unidad a cada conteo de bigrama, se añade un valor constante $k$. Esto permite un mayor control sobre la asignación de probabilidad a los bigramas no observados.\n",
    "\n",
    "**Pasos a seguir:**\n",
    "\n",
    "1. **Seleccionar los valores de $k$:**\n",
    "   - Se probarán dos valores: $k = 0.05$ y $k = 0.15$.\n",
    "\n",
    "2. **Añadir $k$ a cada posible bigrama:**\n",
    "   - Para cada par de palabras en el vocabulario (incluyendo `<s>` y `</s>`), incrementaremos el conteo de bigramas observados en el corpus en $k$.\n",
    "\n",
    "3. **Actualizar los conteos de unigramas:**\n",
    "   - Debido al suavizado *add-$k$, cada unigram que aparece como la primera palabra en un bigrama tendrá su conteo incrementado por $k \\times V$, donde $V$ es el tamaño del vocabulario.\n",
    "\n",
    "4. **Calcular las probabilidades:**\n",
    "   - Para cada bigrama `(w1, w2)`, la probabilidad `P(w2 | w1)` se calcula como `(count(w1 w2) + k) / (count(w1) + k \\times V)`.\n",
    "\n",
    "5. **Incluir el bigrama no visto `a models`:**\n",
    "   - Este bigrama no aparece en el corpus original y, gracias al suavizado *add-$k$, se le asignará una probabilidad positiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_k_smoothing(bigram_counts, unigram_counts, vocab, k):\n",
    "    \"\"\"\n",
    "    Aplica el suavizado add-k a los bigramas.\n",
    "    \"\"\"\n",
    "    V = len(vocab)  # Tamaño del vocabulario\n",
    "    bigram_probabilities_addk = {}\n",
    "    \n",
    "    # Generamos todos los posibles bigramas\n",
    "    all_possible_bigrams = [(w1, w2) for w1 in vocab for w2 in vocab]\n",
    "    \n",
    "    for bigram in all_possible_bigrams:\n",
    "        w1, w2 = bigram\n",
    "        count = bigram_counts.get(bigram, 0)\n",
    "        prob = (count + k) / (unigram_counts[w1] + k * V)\n",
    "        bigram_probabilities_addk[bigram] = prob\n",
    "    \n",
    "    return bigram_probabilities_addk\n",
    "\n",
    "# Valores de k a probar\n",
    "k_values = [0.05, 0.15]\n",
    "\n",
    "for k in k_values:\n",
    "    bigram_probabilities_addk = add_k_smoothing(bigram_counts, unigram_counts, vocab, k)\n",
    "    # Probabilidad del bigrama no visto 'a models'\n",
    "    prob_a_models = bigram_probabilities_addk.get(('a', 'models'), (0 + k) / (unigram_counts['a'] + k * len(vocab)))\n",
    "    print(f\"\\nProbabilidades de bigramas con suavizado add-{k}:\")\n",
    "    print(f\"P(models | a) = {prob_a_models:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4cce09",
   "metadata": {},
   "source": [
    "#### **Interpretación**\n",
    "\n",
    "- **Bigrama no visto `a models`:**\n",
    "  - Con $k = 0.05$, la probabilidad de `a models` es aproximadamente 0.050.\n",
    "  - Con $k = 0.15$, la probabilidad de `a models` es aproximadamente 0.150.\n",
    "\n",
    "- **Impacto de $k$ en las probabilidades:**\n",
    "  - Un valor más alto de $k$ asigna una mayor probabilidad a los bigramas no observados.\n",
    "  - Esto permite un equilibrio entre la observación de bigramas y la asignación de probabilidad a los no observados, ajustando la \"suavidad\" del modelo.\n",
    "\n",
    "Este enfoque flexible permite adaptar el modelo de lenguaje según las necesidades específicas, ajustando la influencia de los bigramas no observados mediante el parámetro $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f975e00",
   "metadata": {},
   "source": [
    "#### **Parte (d): Calcula las probabilidades de todos los bigramas y el bigrama no visto `a models` con back-off y stupid-backoff**\n",
    "\n",
    "\n",
    "**Back-off** es una técnica de suavizado que permite manejar n-gramas no observados reduciendo el orden del modelo (por ejemplo, pasando de bigramas a unigramas) cuando se encuentra un bigrama no visto. Por otro lado, **stupid back-off** es una simplificación de esta técnica que no calcula probabilidades condicionales, sino que simplemente asigna un peso multiplicativo cuando hace back-off.\n",
    "\n",
    "**1. Back-off:**\n",
    "\n",
    "- **Pasos:**\n",
    "  1. Intentar utilizar la probabilidad del bigrama observado.\n",
    "  2. Si el bigrama no se ha observado, hacer back-off al unigram, es decir, usar la probabilidad de la palabra objetivo independientemente de la palabra precedente.\n",
    "\n",
    "- **Fórmula:**\n",
    "  $$\n",
    "  P(w_2 | w_1) = \n",
    "    \\begin{cases} \n",
    "      \\frac{\\text{count}(w_1 w_2)}{\\text{count}(w_1)} & \\text{si } \\text{count}(w_1 w_2) > 0 \\\\\n",
    "      \\frac{\\text{count}(w_2)}{\\text{Total\\ de\\ unigrams}} & \\text{de lo contrario}\n",
    "    \\end{cases}\n",
    "  $$\n",
    "\n",
    "**2. Stupid Back-off:**\n",
    "\n",
    "- **Pasos:**\n",
    "  1. Intentar utilizar la probabilidad del bigrama observado.\n",
    "  2. Si el bigrama no se ha observado, asignar una probabilidad basada en el unigram de la palabra objetivo, multiplicada por un factor de descuento $\\alpha$.\n",
    "\n",
    "- **Fórmula:**\n",
    "  $$\n",
    "  P(w_2 | w_1) = \n",
    "    \\begin{cases} \n",
    "      \\frac{\\text{count}(w_1 w_2)}{\\text{count}(w_1)} & \\text{si } \\text{count}(w_1 w_2) > 0 \\\\\n",
    "      \\alpha \\times \\frac{\\text{count}(w_2)}{\\text{Total\\ de\\ unigrams}} & \\text{de lo contrario}\n",
    "    \\end{cases}\n",
    "  $$\n",
    "  \n",
    "  Donde $\\alpha$ es un factor de descuento, típicamente $\\alpha = 0.4$.\n",
    "\n",
    "\n",
    "\n",
    "Para implementar estas técnicas, es crucial tener las probabilidades de unigramas correctamente calculadas, incluyendo todas las ocurrencias en el corpus.\n",
    "\n",
    "\n",
    "Primero, contamos todas las ocurrencias de cada palabra en el corpus, incluyendo aquellas que aparecen como segunda palabra en bigramas, para calcular correctamente las probabilidades de unigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252220b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_probabilities_full_corrected(sentences, vocab):\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de unigramas contando todas las ocurrencias,\n",
    "    incluyendo las palabras que aparecen como segunda palabra en bigramas.\n",
    "    \"\"\"\n",
    "    unigram_counts_full = defaultdict(int)\n",
    "    total_unigrams_full = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                unigram_counts_full[token] += 1\n",
    "                total_unigrams_full += 1\n",
    "            else:\n",
    "                print(f\"Advertencia: La palabra '{token}' no está en el vocabulario.\")\n",
    "    \n",
    "    # Calculamos las probabilidades\n",
    "    unigram_probabilities_full = {}\n",
    "    for unigram, count in unigram_counts_full.items():\n",
    "        unigram_probabilities_full[unigram] = count / total_unigrams_full\n",
    "    \n",
    "    return unigram_counts_full, unigram_probabilities_full, total_unigrams_full\n",
    "\n",
    "# Calculamos las probabilidades de unigramas completo\n",
    "unigram_counts_full, unigram_probabilities_full, total_unigrams_full = calculate_unigram_probabilities_full_corrected(sentences, vocab)\n",
    "\n",
    "# Mostramos las probabilidades corregidas de unigramas\n",
    "print(\"\\nProbabilidades corregidas de unigramas (incluyendo todas las ocurrencias):\")\n",
    "for unigram, prob in unigram_probabilities_full.items():\n",
    "    print(f\"P({unigram}) = {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e18800f",
   "metadata": {},
   "source": [
    "Implementamos funciones para calcular las probabilidades de bigramas utilizando back-off y stupid back-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_prob_corrected(bigram, bigram_counts, unigram_probabilities_full, total_unigrams_full):\n",
    "    \"\"\"\n",
    "    Calcula la probabilidad de un bigrama utilizando back-off corregido.\n",
    "    Si el bigrama no se ha visto, utiliza la probabilidad del unigram.\n",
    "    \"\"\"\n",
    "    w1, w2 = bigram\n",
    "    if bigram in bigram_counts and bigram_counts[bigram] > 0:\n",
    "        # Bigrama observado: P(w2 | w1) = count(w1 w2) / count(w1)\n",
    "        return bigram_counts[bigram] / unigram_counts[w1]\n",
    "    else:\n",
    "        # Bigramo no observado: P(w2) = count(w2) / total_unigrams\n",
    "        return unigram_probabilities_full.get(w2, 0)\n",
    "\n",
    "def stupid_backoff_prob_corrected(bigram, bigram_counts, unigram_probabilities_full, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Calcula la probabilidad de un bigrama utilizando stupid-backoff corregido.\n",
    "    Si el bigrama no se ha visto, utiliza alpha * P(w2).\n",
    "    \"\"\"\n",
    "    w1, w2 = bigram\n",
    "    if bigram in bigram_counts and bigram_counts[bigram] > 0:\n",
    "        # Bigrama observado: P(w2 | w1) = count(w1 w2) / count(w1)\n",
    "        return bigram_counts[bigram] / unigram_counts[w1]\n",
    "    else:\n",
    "        # Bigrama no observado: P(w2 | w1) = alpha * P(w2)\n",
    "        return alpha * unigram_probabilities_full.get(w2, 0)\n",
    "\n",
    "# Definimos el bigrama no visto\n",
    "unseen_bigram = ('a', 'models')\n",
    "\n",
    "# Calculamos la probabilidad usando back-off corregido\n",
    "prob_a_models_backoff_corrected = backoff_prob_corrected(unseen_bigram, bigram_counts, unigram_probabilities_full, total_unigrams_full)\n",
    "\n",
    "# Calculamos la probabilidad usando stupid-backoff corregido con alpha=0.4\n",
    "prob_a_models_stupid_backoff_corrected = stupid_backoff_prob_corrected(unseen_bigram, bigram_counts, unigram_probabilities_full, alpha=0.4)\n",
    "\n",
    "print(\"\\nProbabilidades de bigramas con back-off y stupid-backoff corregidos:\")\n",
    "print(f\"P(models | a) usando back-off = {prob_a_models_backoff_corrected:.3f}\")\n",
    "print(f\"P(models | a) usando stupid-backoff = {prob_a_models_stupid_backoff_corrected:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c298928",
   "metadata": {},
   "source": [
    "Calculamos la probabilidad del bigrama `a models` utilizando ambas técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca364bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el bigrama no visto\n",
    "unseen_bigram = ('a', 'models')\n",
    "\n",
    "# Calculamos la probabilidad usando back-off\n",
    "prob_a_models_backoff = backoff_prob_corrected(unseen_bigram, bigram_counts, unigram_probabilities_full, total_unigrams_full)\n",
    "\n",
    "# Calculamos la probabilidad usando stupid-backoff con alpha=0.4\n",
    "prob_a_models_stupid_backoff = stupid_backoff_prob_corrected(unseen_bigram, bigram_counts, unigram_probabilities_full, alpha=0.4)\n",
    "\n",
    "print(\"\\nProbabilidades de bigramas con back-off y stupid-backoff:\")\n",
    "print(f\"P(models | a) usando back-off = {prob_a_models_backoff:.3f}\")\n",
    "print(f\"P(models | a) usando stupid-backoff = {prob_a_models_stupid_backoff:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea909db",
   "metadata": {},
   "source": [
    "Observación: La función `backoff_prob_corrected` debería retornar `P(w2)` correctamente cuando el bigrama no se ha visto. Verifiquemos que estamos utilizando `unigram_probabilities_full` correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784feaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_prob_corrected_full(bigram, bigram_counts, unigram_probabilities_full):\n",
    "    \"\"\"\n",
    "    Calcula la probabilidad de un bigrama utilizando back-off corregido.\n",
    "    Si el bigrama no se ha visto, utiliza la probabilidad del unigram.\n",
    "    \"\"\"\n",
    "    w1, w2 = bigram\n",
    "    if bigram in bigram_counts and bigram_counts[bigram] > 0:\n",
    "        return bigram_counts[bigram] / unigram_counts[w1]\n",
    "    else:\n",
    "        return unigram_probabilities_full.get(w2, 0)\n",
    "\n",
    "def stupid_backoff_prob_corrected_full(bigram, bigram_counts, unigram_probabilities_full, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Calcula la probabilidad de un bigrama utilizando stupid-backoff corregido.\n",
    "    Si el bigrama no se ha visto, utiliza alpha * P(w2).\n",
    "    \"\"\"\n",
    "    w1, w2 = bigram\n",
    "    if bigram in bigram_counts and bigram_counts[bigram] > 0:\n",
    "        return bigram_counts[bigram] / unigram_counts[w1]\n",
    "    else:\n",
    "        return alpha * unigram_probabilities_full.get(w2, 0)\n",
    "\n",
    "# Recalculamos las probabilidades usando las funciones corregidas\n",
    "prob_a_models_backoff_corrected_full = backoff_prob_corrected_full(unseen_bigram, bigram_counts, unigram_probabilities_full)\n",
    "prob_a_models_stupid_backoff_corrected_full = stupid_backoff_prob_corrected_full(unseen_bigram, bigram_counts, unigram_probabilities_full, alpha=0.4)\n",
    "\n",
    "print(\"\\nProbabilidades de bigramas con back-off y stupid-backoff corregidos:\")\n",
    "print(f\"P(models | a) usando back-off = {prob_a_models_backoff_corrected_full:.3f}\")\n",
    "print(f\"P(models | a) usando stupid-backoff = {prob_a_models_stupid_backoff_corrected_full:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee3f4a",
   "metadata": {},
   "source": [
    "Estas técnicas permiten manejar bigramas no observados de manera efectiva, evitando probabilidades de cero y mejorando la capacidad de generalización del modelo de lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80036b3",
   "metadata": {},
   "source": [
    "#### **Parte (e): Suavizado de Good-Turing para unigramas desconocidos**\n",
    "\n",
    "\n",
    " El suavizado de Good-Turing reasigna la masa de probabilidad de los n-gramas ricos a los n-gramas pobres. En este caso, trabajamos con unigramas.\n",
    "\n",
    "Dado un corpus $D$, se asume que todas las unigramas desconocidas se tratan como $\\langle \\text{UNK} \\rangle$. El vocabulario se extiende a $\\{w : w \\in D\\} \\cup \\{\\langle \\text{UNK} \\rangle\\}$. Además, se asume que $N_0 = 1$, donde $N_r$ es el número de unigramas que aparecen exactamente $r$ veces.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Calcular $r$ y $N_r$ para todas las unigramas de la parte 1.\n",
    "\n",
    "### **Pasos a seguir:**\n",
    "\n",
    " 1. **Identificar el recuento de frecuencia ($r$):**\n",
    "       - Determinar cuántas veces aparece cada unigram en el corpus.\n",
    "\n",
    " 2. **Calcular $N_r$:**\n",
    "       - Para cada valor de $r$, contar cuántos unigramas aparecen exactamente $r$ veces en el corpus.\n",
    "\n",
    "3. **Incluir el unigram desconocido ($\\langle \\text{UNK} \\rangle$):**\n",
    "       - Se considera que hay al menos un unigram no observado, asignando $N_0 = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb871669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Nr(unigram_counts_full, vocab):\n",
    "    \"\"\"\n",
    "    Calcula N_r para cada r en los unigramas.\n",
    "    \"\"\"\n",
    "    Nr = defaultdict(int)\n",
    "    for unigram in vocab:\n",
    "        count = unigram_counts_full.get(unigram, 0)\n",
    "        Nr[count] += 1\n",
    "    Nr[0] = 1  # Añadimos N_0 = 1 para <UNK>\n",
    "    return Nr\n",
    "\n",
    "# Calculamos N_r\n",
    "Nr = calculate_Nr(unigram_counts_full, vocab)\n",
    "\n",
    "# Mostramos r y N_r\n",
    "print(\"\\nValores de r y N_r para los unigramas:\")\n",
    "for r in sorted(Nr.keys()):\n",
    "    print(f\"r = {r}, N_r = {Nr[r]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06252f1",
   "metadata": {},
   "source": [
    "### **Interpretación**\n",
    "\n",
    "- **r = 0, N_r = 1:**\n",
    "   - Indica que hay un unigram no observado en el corpus, representado por $\\langle \\text{UNK} \\rangle$.\n",
    "\n",
    "- **r = 1, N_r = 6:**\n",
    "    - Hay 6 unigramas que aparecen exactamente una vez en el corpus. Estos son `all`, `a`, `model`, `is`, `some`, y `useful`.\n",
    "\n",
    "- **r = 2, N_r = 3:**\n",
    "   - Hay 3 unigramas que aparecen exactamente dos veces en el corpus. Estos son `<s>`, `models`, y `are`.\n",
    "\n",
    " Este análisis es fundamental para aplicar el suavizado de Good-Turing, ya que permite reasignar la masa de probabilidad de unigramas con alta frecuencia a aquellos con baja frecuencia o no observados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc6c24",
   "metadata": {},
   "source": [
    "#### **Parte (f): Para $r < 3$, calcule $c_r$ y las probabilidades de todas las unigramas**\n",
    "\n",
    "\n",
    "En el suavizado de Good-Turing, para unigramas con frecuencia $r < 3$, se utiliza una función de recuento ajustada para estimar las probabilidades. Aquí, necesitamos calcular $c_r$, que generalmente representa un conteo observado, y luego asignar probabilidades basadas en estos valores.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "- Calcular $c_r$ y las probabilidades de todas las unigramas para $r < 3$.\n",
    "\n",
    "####  **Pasos a seguir:**\n",
    "\n",
    "1. **Definir $c_r$ para $r < 3$:**\n",
    "   - En el contexto de Good-Turing, $c_r$ podría representar el recuento ajustado. Sin embargo, en la formulación clásica, Good-Turing utiliza $c_r^* = (r+1) \\cdot \\frac{N_{r+1}}{N_r}$.\n",
    "\n",
    "2. **Calcular probabilidades de unigramas:**\n",
    "   - Para cada unigram, asignar una probabilidad basada en $c_r^*$.\n",
    "   - Normalizar las probabilidades para que sumen a 1.\n",
    "\n",
    "**Nota:** Dado que el enunciado no proporciona una fórmula específica para $c_r$, asumiremos que se refiere a $c_r^*$ de Good-Turing.\n",
    "\n",
    "\n",
    "\n",
    "Calculamos $c_r^*$ para $r < 3$ y asignamos las probabilidades correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_turing_c_star(Nr, r):\n",
    "    \"\"\"\n",
    "    Calcula c_r^* = (r + 1) * (N_{r+1} / N_r)\n",
    "    \"\"\"\n",
    "    if Nr[r] > 0 and Nr[r + 1] > 0:\n",
    "        return (r + 1) * (Nr[r + 1] / Nr[r])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_probabilities_good_turing(Nr, unigram_counts_full, vocab, total_unigrams_full):\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de unigramas utilizando el suavizado de Good-Turing.\n",
    "    Para r < 3, utiliza c_r^*, y para r >= 3, utiliza la frecuencia observada.\n",
    "    Incluye <UNK>.\n",
    "    \"\"\"\n",
    "    probabilities = {}\n",
    "    for unigram in vocab:\n",
    "        r = unigram_counts_full.get(unigram, 0)\n",
    "        if r < 3:\n",
    "            c_star = good_turing_c_star(Nr, r)\n",
    "            probabilities[unigram] = c_star / total_unigrams_full\n",
    "        else:\n",
    "            probabilities[unigram] = (unigram_counts_full[unigram]) / total_unigrams_full\n",
    "    # Asignamos probabilidad a <UNK>\n",
    "    c_star_UNK = good_turing_c_star(Nr, 0)\n",
    "    probabilities['<UNK>'] = c_star_UNK / total_unigrams_full\n",
    "    return probabilities\n",
    "\n",
    "# Calculamos las probabilidades de Good-Turing para r < 3\n",
    "probabilities_good_turing = calculate_probabilities_good_turing(Nr, unigram_counts_full, vocab, total_unigrams_full)\n",
    "\n",
    "# Mostramos las probabilidades de unigramas\n",
    "print(\"\\nProbabilidades de unigramas con Good-Turing (r < 3):\")\n",
    "for unigram, prob in probabilities_good_turing.items():\n",
    "    print(f\"P({unigram}) = {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e642bf",
   "metadata": {},
   "source": [
    "#### **Parte (g): Para el valor máximo de $r$, $N_{r+1} = 0$. En este caso, la probabilidad $P(w : \\#w = r)$ aún puede estimarse mediante MLE (Máxima Verosimilitud). Calcula la probabilidad de las unigramas de la parte 1 que aparecen con mayor frecuencia, es decir, $ r = 3$**\n",
    "\n",
    "\n",
    "En el suavizado de Good-Turing, para el valor máximo de $r$ donde $N_{r+1} = 0$, no podemos calcular $c_r^*$ directamente. En este caso, se utiliza la estimación de máxima verosimilitud (MLE) para asignar probabilidades a las unigramas con la frecuencia máxima.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "- Calcular la probabilidad de las unigramas que aparecen con frecuencia $r = 3$ utilizando MLE.\n",
    "\n",
    "#### **Pasos a seguir:**\n",
    "\n",
    "1. **Identificar el valor máximo de $r$:**\n",
    "   - Determinar el valor de $r$ más alto en el corpus donde $N_{r+1} > 0$. Dado que en nuestra configuración, $N_3 = 0$.\n",
    "\n",
    "2. **Asignar probabilidad mediante MLE:**\n",
    "   - Para unigramas con $r = 2$ (en este caso, ningún unigram con $r=3$), asignar probabilidades basadas en su frecuencia observada.\n",
    "\n",
    "**Nota:** En nuestro corpus, el valor máximo de $r$ es 2, ya que ningún unigram aparece más de dos veces.\n",
    "\n",
    "\n",
    "Dado que en nuestro corpus ningún unigrama aparece más de dos veces, aplicaremos MLE para las unigramas con $r = 2$. Para que esto funcione vuelvo a escribir el codigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb82da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 1. Definición del corpus modificado y vocabulario\n",
    "sentences_mod = [\n",
    "    \"all models are wrong\",\n",
    "    \"a model is wrong wrong wrong wrong\",  # 'wrong' aparece 4 veces\n",
    "    \"some models are useful\"\n",
    "]\n",
    "\n",
    "vocab = {'<s>', '</s>', 'a', 'all', 'are', 'is', 'model', 'models', 'some', 'useful', 'wrong'}\n",
    "\n",
    "# 2. Generación de bigramas y conteo de frecuencias para el corpus modificado\n",
    "def generate_bigrams(sentences, vocab):\n",
    "    \"\"\"\n",
    "    Genera bigramas a partir de las oraciones del corpus, incluyendo <s> y </s>.\n",
    "    También cuenta las frecuencias de bigramas y unigramas.\n",
    "    \"\"\"\n",
    "    bigram_counts = defaultdict(int)\n",
    "    unigram_counts = defaultdict(int)  # Conteo de unigramas como primera palabra en bigramas\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenizamos la oración y añadimos <s> y </s>\n",
    "        tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "        \n",
    "        # Verificamos que todas las palabras estén en el vocabulario\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                print(f\"Advertencia: La palabra '{token}' no está en el vocabulario.\")\n",
    "        \n",
    "        # Extraemos los bigramas y actualizamos los conteos\n",
    "        for i in range(len(tokens)-1):\n",
    "            w1, w2 = tokens[i], tokens[i+1]\n",
    "            bigram_counts[(w1, w2)] += 1\n",
    "            unigram_counts[w1] += 1  # Conteo de unigramas como primera palabra en bigramas\n",
    "    \n",
    "    return bigram_counts, unigram_counts\n",
    "\n",
    "# Generamos los bigramas y unigramas para el corpus modificado\n",
    "bigram_counts_mod, unigram_counts_mod = generate_bigrams(sentences_mod, vocab)\n",
    "\n",
    "# Visualización de los Conteos\n",
    "print(\"Conteos de bigramas (corpus modificado):\")\n",
    "for bigram, count in bigram_counts_mod.items():\n",
    "    print(f\"{bigram}: {count}\")\n",
    "\n",
    "print(\"\\nConteos de unigramas (como primera palabra en bigramas, corpus modificado):\")\n",
    "for unigram, count in unigram_counts_mod.items():\n",
    "    print(f\"{unigram}: {count}\")\n",
    "\n",
    "# 3. Cálculo de Probabilidades de Unigramas Completo para el Corpus Modificado\n",
    "def calculate_unigram_probabilities_full_corrected(sentences, vocab):\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de unigramas contando todas las ocurrencias,\n",
    "    incluyendo las palabras que aparecen como segunda palabra en bigramas.\n",
    "    \"\"\"\n",
    "    unigram_counts_full = defaultdict(int)\n",
    "    total_unigrams_full = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                unigram_counts_full[token] += 1\n",
    "                total_unigrams_full += 1\n",
    "            else:\n",
    "                print(f\"Advertencia: La palabra '{token}' no está en el vocabulario.\")\n",
    "    \n",
    "    # Calculamos las probabilidades\n",
    "    unigram_probabilities_full = {}\n",
    "    for unigram, count in unigram_counts_full.items():\n",
    "        unigram_probabilities_full[unigram] = count / total_unigrams_full\n",
    "    \n",
    "    return unigram_counts_full, unigram_probabilities_full, total_unigrams_full\n",
    "\n",
    "# Calculamos las probabilidades de unigramas completo para el corpus modificado\n",
    "unigram_counts_full_mod, unigram_probabilities_full_mod, total_unigrams_full_mod = calculate_unigram_probabilities_full_corrected(sentences_mod, vocab)\n",
    "\n",
    "# Mostramos las probabilidades corregidas de unigramas\n",
    "print(\"\\nProbabilidades corregidas de unigramas (incluyendo todas las ocurrencias, corpus modificado):\")\n",
    "for unigram, prob in unigram_probabilities_full_mod.items():\n",
    "    print(f\"P({unigram}) = {prob:.3f}\")\n",
    "\n",
    "# 4. Cálculo de N_r para Good-Turing en el Corpus Modificado\n",
    "def calculate_Nr(unigram_counts_full, vocab):\n",
    "    \"\"\"\n",
    "    Calcula N_r para cada r en los unigramas.\n",
    "    \"\"\"\n",
    "    Nr = defaultdict(int)\n",
    "    for unigram in vocab:\n",
    "        count = unigram_counts_full.get(unigram, 0)\n",
    "        Nr[count] += 1\n",
    "    Nr[0] = 1  # Añadimos N_0 = 1 para <UNK>\n",
    "    return Nr\n",
    "\n",
    "# Calculamos N_r para el corpus modificado\n",
    "Nr_mod = calculate_Nr(unigram_counts_full_mod, vocab)\n",
    "\n",
    "# Mostramos r y N_r\n",
    "print(\"\\nValores de r y N_r para los unigramas (corpus modificado):\")\n",
    "for r in sorted(Nr_mod.keys()):\n",
    "    print(f\"r = {r}, N_r = {Nr_mod[r]}\")\n",
    "\n",
    "# 5. Implementación del Suavizado de Good-Turing con MLE para el Valor Máximo de r\n",
    "def calculate_c_r_star(Nr, r):\n",
    "    \"\"\"\n",
    "    Calcula c_r^* = (r + 1) * (N_{r+1} / N_r)\n",
    "    \"\"\"\n",
    "    if Nr[r] > 0 and Nr[r + 1] > 0:\n",
    "        return (r + 1) * (Nr[r + 1] / Nr[r])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_probabilities_good_turing_mle(Nr, unigram_counts_full, vocab, total_unigrams_full):\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de unigramas utilizando Good-Turing.\n",
    "    Para r < 3, utiliza c_r^*, para r = 3 utiliza MLE, y para r > 3 también utiliza MLE.\n",
    "    \"\"\"\n",
    "    probabilities = {}\n",
    "    for unigram in vocab:\n",
    "        r = unigram_counts_full.get(unigram, 0)\n",
    "        if r < 3:\n",
    "            c_star = calculate_c_r_star(Nr, r)\n",
    "            probabilities[unigram] = c_star / total_unigrams_full\n",
    "        elif r == 3:\n",
    "            # En nuestro caso, no hay unigramas con r = 3\n",
    "            probabilities[unigram] = (r) / total_unigrams_full\n",
    "        else:\n",
    "            # Para r > 3, utilizamos MLE\n",
    "            probabilities[unigram] = (unigram_counts_full[unigram]) / total_unigrams_full\n",
    "    # Asignamos probabilidad a <UNK>\n",
    "    c_star_UNK = calculate_c_r_star(Nr, 0)\n",
    "    probabilities['<UNK>'] = c_star_UNK / total_unigrams_full\n",
    "    return probabilities\n",
    "\n",
    "# Calculamos las probabilidades de Good-Turing con MLE para el corpus modificado\n",
    "probabilities_good_turing_mle = calculate_probabilities_good_turing_mle(Nr_mod, unigram_counts_mod, vocab, total_unigrams_full_mod)\n",
    "\n",
    "# Mostramos las probabilidades de unigramas\n",
    "print(\"\\nProbabilidades de unigramas con Good-Turing y MLE para r = 3:\")\n",
    "for unigram, prob in probabilities_good_turing_mle.items():\n",
    "    print(f\"P({unigram}) = {prob:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560f782",
   "metadata": {},
   "source": [
    "#### **Parte (h): Demuestra que la suma de las probabilidades de todas las unigramas dadas en (f) y (g) no es 1. Intenta normalizar las probabilidades**\n",
    "\n",
    "\n",
    "En la práctica, al aplicar técnicas de suavizado como Good-Turing, la suma de las probabilidades de todas las unigramas puede no ser exactamente 1 debido a reasignaciones y ajustes en las probabilidades. Es crucial verificar esto y, si es necesario, normalizar las probabilidades para asegurar que el modelo sea válido.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "- Mostrar que la suma de las probabilidades de todas las unigramas dadas en las partes anteriores no es exactamente 1.\n",
    "- Normalizar las probabilidades para que sumen 1.\n",
    "\n",
    "#### **Pasos a seguir:**\n",
    "\n",
    "1. **Calcular la suma de las probabilidades:**\n",
    "   - Sumar todas las probabilidades de las unigramas, incluyendo $\\langle \\text{UNK} \\rangle$.\n",
    "\n",
    "2. **Normalizar las probabilidades:**\n",
    "   - Dividir cada probabilidad por la suma total para ajustar las probabilidades de manera que sumen 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379ec14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 1. Definición del Corpus Modificado y Vocabulario\n",
    "sentences_mod = [\n",
    "    \"all models are wrong\",\n",
    "    \"a model is wrong wrong wrong wrong\",  # 'wrong' aparece 4 veces\n",
    "    \"some models are useful\"\n",
    "]\n",
    "\n",
    "vocab = {'<s>', '</s>', 'a', 'all', 'are', 'is', 'model', 'models', 'some', 'useful', 'wrong'}\n",
    "\n",
    "# 2. Generación de Bigramos y Conteo de Frecuencias para el Corpus Modificado\n",
    "def generate_bigrams(sentences, vocab):\n",
    "    \"\"\"\n",
    "    Genera bigramas a partir de las oraciones del corpus, incluyendo <s> y </s>.\n",
    "    También cuenta las frecuencias de bigramas y unigramas.\n",
    "    \"\"\"\n",
    "    bigram_counts = defaultdict(int)\n",
    "    unigram_counts = defaultdict(int)  # Conteo de unigramas como primera palabra en bigramas\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenizamos la oración y añadimos <s> y </s>\n",
    "        tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "        \n",
    "        # Verificamos que todas las palabras estén en el vocabulario\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                print(f\"Advertencia: La palabra '{token}' no está en el vocabulario.\")\n",
    "        \n",
    "        # Extraemos los bigramas y actualizamos los conteos\n",
    "        for i in range(len(tokens)-1):\n",
    "            w1, w2 = tokens[i], tokens[i+1]\n",
    "            bigram_counts[(w1, w2)] += 1\n",
    "            unigram_counts[w1] += 1  # Conteo de unigramas como primera palabra en bigramas\n",
    "    \n",
    "    return bigram_counts, unigram_counts\n",
    "\n",
    "# Generamos los bigramas y unigramas para el corpus modificado\n",
    "bigram_counts_mod, unigram_counts_mod = generate_bigrams(sentences_mod, vocab)\n",
    "\n",
    "# Visualización de los Conteos\n",
    "print(\"Conteos de bigramas (corpus modificado):\")\n",
    "for bigram, count in bigram_counts_mod.items():\n",
    "    print(f\"{bigram}: {count}\")\n",
    "\n",
    "print(\"\\nConteos de unigramas (como primera palabra en bigramas, corpus modificado):\")\n",
    "for unigram, count in unigram_counts_mod.items():\n",
    "    print(f\"{unigram}: {count}\")\n",
    "\n",
    "# 3. Cálculo de Probabilidades de Unigramas Completo para el Corpus Modificado\n",
    "def calculate_unigram_probabilities_full_corrected(sentences, vocab):\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de unigramas contando todas las ocurrencias,\n",
    "    incluyendo las palabras que aparecen como segunda palabra en bigramas.\n",
    "    \"\"\"\n",
    "    unigram_counts_full = defaultdict(int)\n",
    "    total_unigrams_full = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                unigram_counts_full[token] += 1\n",
    "                total_unigrams_full += 1\n",
    "            else:\n",
    "                print(f\"Advertencia: La palabra '{token}' no está en el vocabulario.\")\n",
    "    \n",
    "    # Calculamos las probabilidades\n",
    "    unigram_probabilities_full = {}\n",
    "    for unigram, count in unigram_counts_full.items():\n",
    "        unigram_probabilities_full[unigram] = count / total_unigrams_full\n",
    "    \n",
    "    return unigram_counts_full, unigram_probabilities_full, total_unigrams_full\n",
    "\n",
    "# Calculamos las probabilidades de unigramas completo para el corpus modificado\n",
    "unigram_counts_full_mod, unigram_probabilities_full_mod, total_unigrams_full_mod = calculate_unigram_probabilities_full_corrected(sentences_mod, vocab)\n",
    "\n",
    "# Mostramos las probabilidades corregidas de unigramas\n",
    "print(\"\\nProbabilidades corregidas de unigramas (incluyendo todas las ocurrencias, corpus modificado):\")\n",
    "for unigram, prob in unigram_probabilities_full_mod.items():\n",
    "    print(f\"P({unigram}) = {prob:.3f}\")\n",
    "\n",
    "# 4. Cálculo de N_r para Good-Turing en el Corpus Modificado\n",
    "def calculate_Nr(unigram_counts_full, vocab):\n",
    "    \"\"\"\n",
    "    Calcula N_r para cada r en los unigramas.\n",
    "    \"\"\"\n",
    "    Nr = defaultdict(int)\n",
    "    for unigram in vocab:\n",
    "        count = unigram_counts_full.get(unigram, 0)\n",
    "        Nr[count] += 1\n",
    "    Nr[0] = 1  # Añadimos N_0 = 1 para <UNK>\n",
    "    return Nr\n",
    "\n",
    "# Calculamos N_r para el corpus modificado\n",
    "Nr_mod = calculate_Nr(unigram_counts_full_mod, vocab)\n",
    "\n",
    "# Mostramos r y N_r\n",
    "print(\"\\nValores de r y N_r para los unigramas (corpus modificado):\")\n",
    "for r in sorted(Nr_mod.keys()):\n",
    "    print(f\"r = {r}, N_r = {Nr_mod[r]}\")\n",
    "\n",
    "# 5. Implementación del Suavizado de Good-Turing con MLE para el Valor Máximo de r\n",
    "def calculate_c_r_star(Nr, r):\n",
    "    \"\"\"\n",
    "    Calcula c_r^* = (r + 1) * (N_{r+1} / N_r)\n",
    "    \"\"\"\n",
    "    if Nr[r] > 0 and Nr[r + 1] > 0:\n",
    "        return (r + 1) * (Nr[r + 1] / Nr[r])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_probabilities_good_turing_mle(Nr, unigram_counts_full, vocab, total_unigrams_full):\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de unigramas utilizando Good-Turing.\n",
    "    Para r < 3, utiliza c_r^*, para r = 3 utiliza MLE, y para r > 3 también utiliza MLE.\n",
    "    \"\"\"\n",
    "    probabilities = {}\n",
    "    for unigram in vocab:\n",
    "        r = unigram_counts_full.get(unigram, 0)\n",
    "        if r < 3:\n",
    "            c_star = calculate_c_r_star(Nr, r)\n",
    "            probabilities[unigram] = c_star / total_unigrams_full\n",
    "        elif r == 3:\n",
    "            # En nuestro caso, no hay unigramas con r = 3\n",
    "            probabilities[unigram] = (r) / total_unigrams_full\n",
    "        else:\n",
    "            # Para r > 3, utilizamos MLE\n",
    "            probabilities[unigram] = (unigram_counts_full[unigram]) / total_unigrams_full\n",
    "    # Asignamos probabilidad a <UNK>\n",
    "    c_star_UNK = calculate_c_r_star(Nr, 0)\n",
    "    probabilities['<UNK>'] = c_star_UNK / total_unigrams_full\n",
    "    return probabilities\n",
    "\n",
    "# Calculamos las probabilidades de Good-Turing con MLE para el corpus modificado\n",
    "probabilities_good_turing_mle = calculate_probabilities_good_turing_mle(Nr_mod, unigram_counts_mod, vocab, total_unigrams_full_mod)\n",
    "\n",
    "# Mostramos las probabilidades de unigramas\n",
    "print(\"\\nProbabilidades de unigramas con Good-Turing y MLE para r = 3:\")\n",
    "for unigram, prob in probabilities_good_turing_mle.items():\n",
    "    print(f\"P({unigram}) = {prob:.3f}\")\n",
    "\n",
    "# 6. Demostración de que la Suma de las Probabilidades No es 1\n",
    "# Calculamos la suma de las probabilidades\n",
    "sum_prob_gt_mle = sum(probabilities_good_turing_mle.values())\n",
    "print(f\"\\nSuma de las probabilidades de unigramas con Good-Turing y MLE: {sum_prob_gt_mle:.3f}\")\n",
    "\n",
    "# 7. Normalización de las Probabilidades (si es necesario)\n",
    "def normalize_probabilities(probabilities):\n",
    "    \"\"\"\n",
    "    Normaliza las probabilidades para que sumen 1.\n",
    "    \"\"\"\n",
    "    total_prob = sum(probabilities.values())\n",
    "    if total_prob == 0:\n",
    "        raise ValueError(\"La suma de las probabilidades es 0. No se puede normalizar.\")\n",
    "    normalized_probabilities = {k: v / total_prob for k, v in probabilities.items()}\n",
    "    return normalized_probabilities\n",
    "\n",
    "# Verificamos si la suma es diferente de 1 y normalizamos si es necesario\n",
    "if abs(sum_prob_gt_mle - 1.0) > 1e-6:\n",
    "    probabilities_good_turing_mle_normalized = normalize_probabilities(probabilities_good_turing_mle)\n",
    "    print(\"\\nProbabilidades normalizadas de unigramas con Good-Turing y MLE:\")\n",
    "    for unigram, prob in probabilities_good_turing_mle_normalized.items():\n",
    "        print(f\"P({unigram}) = {prob:.3f}\")\n",
    "    # Verificamos la suma de las probabilidades normalizadas\n",
    "    sum_prob_normalized = sum(probabilities_good_turing_mle_normalized.values())\n",
    "    print(f\"\\nSuma de las probabilidades normalizadas: {sum_prob_normalized:.3f}\")\n",
    "else:\n",
    "    print(\"\\nLas probabilidades ya están correctamente normalizadas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb003423",
   "metadata": {},
   "source": [
    "Aunque en este ejemplo la suma de las probabilidades ya es 1.000, en la práctica, especialmente con conjuntos de datos más grandes o técnicas de suavizado más complejas, es posible que la suma no sea exactamente 1. Por ello, es una buena práctica siempre verificar y, si es necesario, normalizar las probabilidades.\n",
    "\n",
    "Además, recuerda que al asignar una probabilidad a `<UNK>`, estás reservando una parte de la masa de probabilidad para manejar palabras que no aparecen en el vocabulario durante el entrenamiento. Asegúrate de que este token esté correctamente integrado en tu modelo de lenguaje para manejar eficientemente las palabras desconocidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ccb7bd",
   "metadata": {},
   "source": [
    "#### **Parte (i): Suavizado de Good-Turing con line smoothing para manejar $N_r = 0$**\n",
    "\n",
    "\n",
    "En un corpus grande, es posible que $N_r = 0$ para valores grandes de $r$, lo que puede conducir a estimaciones de probabilidad de cero para ciertas palabras. Para resolver este problema, se puede utilizar un suavizado lineal para ajustar aproximadamente la distribución de los valores conocidos.\n",
    "\n",
    "**Escenario propuesto:**\n",
    "\n",
    "- Se modifica la segunda oración del corpus a `a model is wrong wrong wrong wrong`, de modo que $N_4 = 1$, pero $N_5 = 0$.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "- Asignar un valor suavizado a $N_5$ y calcular las probabilidades de todas las unigramas utilizando este valor.\n",
    "\n",
    "#### **Pasos a seguir:**\n",
    "\n",
    "1. **Modificar el corpus:**\n",
    "   - Cambiar la segunda oración a `a model is wrong wrong wrong wrong`.\n",
    "\n",
    "2. **Recalcular los conteos de unigramas y $N_r$:**\n",
    "   - Actualizar los conteos de unigramas para reflejar los cambios.\n",
    "\n",
    "3. **Asignar un valor suavizado a $N_5$:**\n",
    "   - Proponer un valor para $N_5$ utilizando una técnica de suavizado lineal.\n",
    "\n",
    "4. **Recalcular las probabilidades de unigramas:**\n",
    "   - Utilizar los nuevos $N_r$ para asignar probabilidades ajustadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ddc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 1. Definición del Corpus Modificado y Vocabulario\n",
    "sentences_mod = [\n",
    "    \"all models are wrong\",\n",
    "    \"a model is wrong wrong wrong wrong\",  # 'wrong' aparece 4 veces\n",
    "    \"some models are useful\"\n",
    "]\n",
    "\n",
    "vocab = {'<s>', '</s>', 'a', 'all', 'are', 'is', 'model', 'models', 'some', 'useful', 'wrong'}\n",
    "\n",
    "# 2. Generación de Bigramos y Conteo de Frecuencias para el Corpus Modificado\n",
    "def generate_bigrams(sentences, vocab):\n",
    "    \"\"\"\n",
    "    Genera bigramas a partir de las oraciones del corpus, incluyendo <s> y </s>.\n",
    "    También cuenta las frecuencias de bigramas y unigramas.\n",
    "    \"\"\"\n",
    "    bigram_counts = defaultdict(int)\n",
    "    unigram_counts = defaultdict(int)  # Conteo de unigramas como primera palabra en bigramas\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenizamos la oración y añadimos <s> y </s>\n",
    "        tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "        \n",
    "        # Verificamos que todas las palabras estén en el vocabulario\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                print(f\"Advertencia: La palabra '{token}' no está en el vocabulario.\")\n",
    "        \n",
    "        # Extraemos los bigramas y actualizamos los conteos\n",
    "        for i in range(len(tokens)-1):\n",
    "            w1, w2 = tokens[i], tokens[i+1]\n",
    "            bigram_counts[(w1, w2)] += 1\n",
    "            unigram_counts[w1] += 1  # Conteo de unigramas como primera palabra en bigramas\n",
    "    \n",
    "    return bigram_counts, unigram_counts\n",
    "\n",
    "# Generamos los bigramas y unigramas para el corpus modificado\n",
    "bigram_counts_mod, unigram_counts_mod = generate_bigrams(sentences_mod, vocab)\n",
    "\n",
    "# Visualización de los Conteos\n",
    "print(\"Conteos de bigramas (corpus modificado):\")\n",
    "for bigram, count in bigram_counts_mod.items():\n",
    "    print(f\"{bigram}: {count}\")\n",
    "\n",
    "print(\"\\nConteos de unigramas (como primera palabra en bigramas, corpus modificado):\")\n",
    "for unigram, count in unigram_counts_mod.items():\n",
    "    print(f\"{unigram}: {count}\")\n",
    "\n",
    "# 3. Cálculo de Probabilidades de Unigramas Completo para el Corpus Modificado\n",
    "def calculate_unigram_probabilities_full_corrected(sentences, vocab):\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de unigramas contando todas las ocurrencias,\n",
    "    incluyendo las palabras que aparecen como segunda palabra en bigramas.\n",
    "    \"\"\"\n",
    "    unigram_counts_full = defaultdict(int)\n",
    "    total_unigrams_full = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                unigram_counts_full[token] += 1\n",
    "                total_unigrams_full += 1\n",
    "            else:\n",
    "                print(f\"Advertencia: La palabra '{token}' no está en el vocabulario.\")\n",
    "    \n",
    "    # Calculamos las probabilidades\n",
    "    unigram_probabilities_full = {}\n",
    "    for unigram, count in unigram_counts_full.items():\n",
    "        unigram_probabilities_full[unigram] = count / total_unigrams_full\n",
    "    \n",
    "    return unigram_counts_full, unigram_probabilities_full, total_unigrams_full\n",
    "\n",
    "# Calculamos las probabilidades de unigramas completo para el corpus modificado\n",
    "unigram_counts_full_mod, unigram_probabilities_full_mod, total_unigrams_full_mod = calculate_unigram_probabilities_full_corrected(sentences_mod, vocab)\n",
    "\n",
    "# Mostramos las probabilidades corregidas de unigramas\n",
    "print(\"\\nProbabilidades corregidas de unigramas (incluyendo todas las ocurrencias, corpus modificado):\")\n",
    "for unigram, prob in unigram_probabilities_full_mod.items():\n",
    "    print(f\"P({unigram}) = {prob:.3f}\")\n",
    "\n",
    "# 4. Cálculo de N_r para Good-Turing en el Corpus Modificado\n",
    "def calculate_Nr(unigram_counts_full, vocab):\n",
    "    \"\"\"\n",
    "    Calcula N_r para cada r en los unigramas.\n",
    "    \"\"\"\n",
    "    Nr = defaultdict(int)\n",
    "    for unigram in vocab:\n",
    "        count = unigram_counts_full.get(unigram, 0)\n",
    "        Nr[count] += 1\n",
    "    Nr[0] = 1  # Añadimos N_0 = 1 para <UNK>\n",
    "    return Nr\n",
    "\n",
    "# Calculamos N_r para el corpus modificado\n",
    "Nr_mod = calculate_Nr(unigram_counts_full_mod, vocab)\n",
    "\n",
    "# Mostramos r y N_r\n",
    "print(\"\\nValores de r y N_r para los unigramas (corpus modificado):\")\n",
    "for r in sorted(Nr_mod.keys()):\n",
    "    print(f\"r = {r}, N_r = {Nr_mod[r]}\")\n",
    "\n",
    "# 5. Asignar un Valor Suavizado a N_5\n",
    "# Asignamos N_5 = lambda * N_4, donde lambda es un pequeño factor, e.g., 0.5\n",
    "lambda_factor = 0.5\n",
    "Nr_mod[5] = lambda_factor * Nr_mod[4]  # N_5 = 0.5 * N_4 = 0.5 * 1 = 0.5\n",
    "\n",
    "print(\"\\nDespués de asignar un valor suavizado a N_5:\")\n",
    "for r in sorted(Nr_mod.keys()):\n",
    "    print(f\"r = {r}, N_r = {Nr_mod[r]}\")\n",
    "\n",
    "# 6. Implementación del Suavizado de Good-Turing con Suavizado Lineal para Manejar N_5 = 0\n",
    "def calculate_c_r_star(Nr, r):\n",
    "    \"\"\"\n",
    "    Calcula c_r^* = (r + 1) * (N_{r+1} / N_r)\n",
    "    \"\"\"\n",
    "    if Nr[r] > 0 and Nr[r + 1] > 0:\n",
    "        return (r + 1) * (Nr[r + 1] / Nr[r])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_probabilities_good_turing_linear(Nr, unigram_counts_full, vocab, total_unigrams_full):\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de unigramas utilizando Good-Turing con suavizado lineal para manejar N_r = 0.\n",
    "    Para r < 3, utiliza c_r^*, para r >= 3 utiliza MLE.\n",
    "    \"\"\"\n",
    "    probabilities = {}\n",
    "    for unigram in vocab:\n",
    "        r = unigram_counts_full.get(unigram, 0)\n",
    "        if r < 3:\n",
    "            c_star = calculate_c_r_star(Nr, r)\n",
    "            probabilities[unigram] = c_star / total_unigrams_full\n",
    "        else:\n",
    "            # Para r >= 3, utilizamos MLE\n",
    "            probabilities[unigram] = unigram_counts_full[unigram] / total_unigrams_full\n",
    "    # Asignamos probabilidad a <UNK>\n",
    "    c_star_UNK = calculate_c_r_star(Nr, 0)\n",
    "    probabilities['<UNK>'] = c_star_UNK / total_unigrams_full\n",
    "    return probabilities\n",
    "\n",
    "# Calculamos las probabilidades de Good-Turing con suavizado lineal\n",
    "probabilities_good_turing_linear = calculate_probabilities_good_turing_linear(Nr_mod, unigram_counts_mod, vocab, total_unigrams_full_mod)\n",
    "\n",
    "# Mostramos las probabilidades de unigramas\n",
    "print(\"\\nProbabilidades de unigramas con Good-Turing y suavizado lineal para manejar N_5 = 0:\")\n",
    "for unigram, prob in probabilities_good_turing_linear.items():\n",
    "    print(f\"P({unigram}) = {prob:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674cec5",
   "metadata": {},
   "source": [
    "Hemos logrado demostrar cómo aplicar el suavizado de Good-Turing con suavizado lineal para manejar casos donde $N_r = 0$ específicamente para r=5. Además, se ha verificado que la suma de las probabilidades de todas las unigramas puede no ser exactamente 1 y se ha implementado una técnica para normalizar las probabilidades cuando es necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7b095-79b9-4cb5-a175-e8ae67962309",
   "metadata": {},
   "source": [
    "## Brown clustering\n",
    "\n",
    "El algoritmo de **Brown Clustering** es un método de agrupamiento jerárquico que agrupa palabras basándose en el contexto en el que aparecen, siguiendo la hipótesis distribucional: *las palabras que aparecen en contextos similares tienden a tener significados similares*. El objetivo es maximizar la probabilidad del corpus bajo un modelo de lenguaje bigrama basado en clases.\n",
    "\n",
    "### Modelo\n",
    "\n",
    "Sea $V$ el vocabulario de tamaño $|V|$ y $C$ el conjunto de clases o clusters. Cada palabra $w \\in V$ se asigna a una clase $c(w) \\in C$.\n",
    "\n",
    "El modelo de lenguaje basado en clases define la probabilidad de una secuencia de palabras $w_1, w_2, \\dots, w_n$ como:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots, w_n) = P(c(w_1)) \\left[ \\prod_{i=2}^{n} P(c(w_i) \\mid c(w_{i-1})) \\right] \\left[ \\prod_{i=1}^{n} P(w_i \\mid c(w_i)) \\right]\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $P(c(w_i))$ es la probabilidad inicial de la clase $c(w_i)$.\n",
    "- $P(c(w_i) \\mid c(w_{i-1}))$ es la probabilidad de transición entre clases.\n",
    "- $P(w_i \\mid c(w_i))$ es la probabilidad de la palabra $w_i$ dada su clase.\n",
    "\n",
    "### Función objetivo\n",
    "\n",
    "El objetivo es encontrar la asignación de clases que maximiza la verosimilitud del corpus. Equivalente a minimizar la función de costo negativa:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{w_{i-1}, w_i} \\text{freq}(w_{i-1}, w_i) \\log P(w_i \\mid w_{i-1})\n",
    "$$\n",
    "\n",
    "Sin embargo, dado que calcular esta función es computacionalmente costoso, se utiliza una aproximación mediante la maximización de la **Información mutua** entre las clases:\n",
    "\n",
    "$$\n",
    "I(C) = \\sum_{c_i, c_j \\in C} P(c_i, c_j) \\log \\frac{P(c_i, c_j)}{P(c_i) P(c_j)}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $P(c_i, c_j)$ es la probabilidad conjunta de las clases $c_i$ y $c_j$.\n",
    "- $P(c_i)$ es la probabilidad marginal de la clase $c_i$.\n",
    "\n",
    "### Algoritmo\n",
    "\n",
    "El algoritmo es aglomerativo y jerárquico:\n",
    "\n",
    "1. **Inicialización**: Cada palabra se asigna a su propio cluster.\n",
    "\n",
    "2. **Iteración**: En cada paso, se fusionan los dos clusters que resultan en la menor disminución (o mayor incremento) de la información mutua $I(C)$.\n",
    "\n",
    "3. **Actualización**: Se recalculan las probabilidades $P(c)$ y $P(c_i, c_j)$ para los clusters afectados.\n",
    "\n",
    "4. **Terminación**: El proceso continúa hasta que se alcanza el número deseado de clusters.\n",
    "\n",
    "### Cálculo de probabilidades\n",
    "\n",
    "Las probabilidades se estiman a partir del corpus:\n",
    "\n",
    "- $P(w) = \\frac{\\text{freq}(w)}{N}$\n",
    "- $P(c) = \\sum_{w \\in c} P(w)$\n",
    "- $P(c_i, c_j) = \\sum_{w_i \\in c_i} \\sum_{w_j \\in c_j} P(w_i, w_j)$\n",
    "\n",
    "Donde $\\text{freq}(w)$ es la frecuencia de la palabra $w$ y $N$ es el número total de palabras en el corpus.\n",
    "\n",
    "## Latent semantic analysis (LSA)\n",
    "\n",
    "**Latent semantic analysis** es una técnica que reduce la dimensionalidad de los datos para capturar las relaciones semánticas latentes entre términos y documentos. Utiliza la **descomposición en valores singulares** (SVD) para factorizar la matriz término-documento.\n",
    "\n",
    "### Construcción de la matriz término-documento\n",
    "\n",
    "Sea $X \\in \\mathbb{R}^{m \\times n}$ una matriz donde:\n",
    "\n",
    "- $m$ es el número de términos.\n",
    "- $n$ es el número de documentos.\n",
    "- $X_{ij}$ representa el peso (e.g., TF-IDF) del término $i$ en el documento $j$.\n",
    "\n",
    "### Descomposición en valores singulares (SVD)\n",
    "\n",
    "Se aplica SVD a la matriz $X$:\n",
    "\n",
    "$$\n",
    "X = U \\Sigma V^\\top\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $U \\in \\mathbb{R}^{m \\times r}$ contiene los vectores singulares izquierdos (términos).\n",
    "- $\\Sigma \\in \\mathbb{R}^{r \\times r}$ es una matriz diagonal con valores singulares $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r \\geq 0$.\n",
    "- $V \\in \\mathbb{R}^{n \\times r}$ contiene los vectores singulares derechos (documentos).\n",
    "- $r = \\text{rank}(X)$.\n",
    "\n",
    "### Reducción de dimensionalidad\n",
    "\n",
    "Se seleccionan los $k$ valores singulares más grandes para obtener una aproximación de rango $k$:\n",
    "\n",
    "$$\n",
    "X_k = U_k \\Sigma_k V_k^\\top\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $U_k \\in \\mathbb{R}^{m \\times k}$.\n",
    "- $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$.\n",
    "- $V_k \\in \\mathbb{R}^{n \\times k}$.\n",
    "\n",
    "### Interpretación\n",
    "\n",
    "- Los términos y documentos se representan en un espacio de características de dimensión reducida $\\mathbb{R}^k$.\n",
    "- Captura relaciones semánticas latentes al combinar términos correlacionados.\n",
    "\n",
    "### Medida de similaridad\n",
    "\n",
    "La similitud entre términos o documentos en LSA se calcula típicamente utilizando el producto punto entre sus representaciones en el espacio reducido:\n",
    "\n",
    "  - **Para términos**:\n",
    "\n",
    "    $$\n",
    "    \\text{sim}(i, j) = (\\mathbf{u}_i \\Sigma_k)(\\mathbf{u}_j \\Sigma_k)^\\top\n",
    "    $$\n",
    "\n",
    "    Donde $\\mathbf{u}_i$ es la fila $i$ de $U_k$.\n",
    "\n",
    "  - **Para documentos**:\n",
    "\n",
    "    $$\n",
    "    \\text{sim}(i, j) = (\\mathbf{v}_i \\Sigma_k)(\\mathbf{v}_j \\Sigma_k)^\\top\n",
    "    $$\n",
    "\n",
    "    Donde $\\mathbf{v}_i$ es la fila $i$ de $V_k$.\n",
    "\n",
    "  - **Entre términos y documentos**:\n",
    "\n",
    "    $$\n",
    "    \\text{sim}(i, j) = (\\mathbf{u}_i \\Sigma_k)(\\mathbf{v}_j \\Sigma_k)^\\top\n",
    "    $$\n",
    "\n",
    "## Word2vec\n",
    "\n",
    "**Word2vec** es un conjunto de modelos que generan representaciones vectoriales de palabras (embeddings) al entrenar redes neuronales para predecir contextos de palabras. Las dos arquitecturas principales son:\n",
    "\n",
    "- **Continuous Bag-of-Words (CBOW)**\n",
    "- **Skip-Gram**\n",
    "\n",
    "### Continuous Bag-of-Words (CBOW)\n",
    "\n",
    "#### Modelo\n",
    "\n",
    "CBOW predice la palabra objetivo $w_t$ basándose en su contexto. El contexto es el conjunto de palabras en una ventana de tamaño $c$ alrededor de $w_t$.\n",
    "\n",
    "La probabilidad condicional se define como:\n",
    "\n",
    "$$\n",
    "P(w_t \\mid \\text{contexto}) = \\frac{\\exp\\left( (\\mathbf{w}'_{w_t})^\\top \\mathbf{v}_{\\text{contexto}} \\right)}{\\sum_{i=1}^{V} \\exp\\left( (\\mathbf{w}'_i)^\\top \\mathbf{v}_{\\text{contexto}} \\right)}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\mathbf{v}_{\\text{contexto}} = \\frac{1}{2c} \\sum_{\\substack{-c \\leq j \\leq c \\\\ j \\ne 0}} \\mathbf{w}_{w_{t+j}}$\n",
    "- $\\mathbf{w}_{w_{t+j}}$ es el vector de entrada de la palabra en el contexto.\n",
    "- $\\mathbf{w}'_{w_t}$ es el vector de salida de la palabra objetivo.\n",
    "\n",
    "#### Función de costo\n",
    "\n",
    "El objetivo es minimizar la función de pérdida negativa logarítmica:\n",
    "\n",
    "$$\n",
    "J = - \\sum_{t=1}^{T} \\log P(w_t \\mid \\text{contexto})\n",
    "$$\n",
    "\n",
    "### Skip-Gram\n",
    "\n",
    "#### Modelo\n",
    "\n",
    "Skip-Gram predice las palabras de contexto basándose en la palabra objetivo $w_t$.\n",
    "\n",
    "La probabilidad condicional para cada palabra de contexto $w_{t+j}$ es:\n",
    "\n",
    "$$\n",
    "P(w_{t+j} \\mid w_t) = \\frac{\\exp\\left( (\\mathbf{w}'_{w_{t+j}})^\\top \\mathbf{w}_{w_t} \\right)}{\\sum_{i=1}^{V} \\exp\\left( (\\mathbf{w}'_i)^\\top \\mathbf{w}_{w_t} \\right)}\n",
    "$$\n",
    "\n",
    "#### Función de costo\n",
    "\n",
    "El objetivo es maximizar la probabilidad de las palabras de contexto:\n",
    "\n",
    "$$\n",
    "J = - \\sum_{t=1}^{T} \\sum_{\\substack{-c \\leq j \\leq c \\\\ j \\ne 0}} \\log P(w_{t+j} \\mid w_t)\n",
    "$$\n",
    "\n",
    "### Problemas computacionales y soluciones\n",
    "\n",
    "Calcular el softmax completo es costoso ($O(V)$). Dos técnicas comunes para abordar esto son:\n",
    "\n",
    "- **Hierarchical Softmax**\n",
    "- **Negative Sampling**\n",
    "\n",
    "#### Hierarchical softmax\n",
    "\n",
    "##### Modelo\n",
    "\n",
    "Reemplaza el softmax plano por una estructura jerárquica (árbol binario). Cada palabra está representada como una hoja en el árbol, y su probabilidad se calcula a lo largo del camino desde la raíz hasta la hoja.\n",
    "\n",
    "La probabilidad de la palabra $w$ es:\n",
    "\n",
    "$$\n",
    "P(w \\mid \\text{contexto}) = \\prod_{j=1}^{L_w} P(b_{n(w,j)} \\mid \\text{contexto})\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $n(w, j)$ es el nodo en el nivel $j$ en el camino a $w$.\n",
    "- $b_{n(w,j)}$ indica si se toma el hijo izquierdo ($1$) o derecho ($0$).\n",
    "- $P(b_{n(w,j)} \\mid \\text{contexto}) = \\sigma\\left( \\mathbf{w}_{n(w,j)}^\\top \\mathbf{v}_{\\text{contexto}} \\right)$\n",
    "\n",
    "##### Ventajas\n",
    "\n",
    "Reduce la complejidad computacional a $O(\\log V)$ por ejemplo.\n",
    "\n",
    "#### Negative sampling\n",
    "\n",
    "##### Modelo\n",
    "\n",
    "Simplifica el objetivo al considerar sólo un pequeño número de muestras negativas $K$ para cada par positivo.\n",
    "\n",
    "La función de pérdida para cada par es:\n",
    "\n",
    "$$\n",
    "J = - \\log \\sigma\\left( (\\mathbf{w}'_{w_O})^\\top \\mathbf{w}_{w_I} \\right) - \\sum_{k=1}^{K} \\log \\sigma\\left( - (\\mathbf{w}'_{w_k})^\\top \\mathbf{w}_{w_I} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$\n",
    "- $w_O$ es la palabra objetivo.\n",
    "- $w_I$ es la palabra de contexto.\n",
    "- $w_k$ son las palabras negativas sampleadas.\n",
    "\n",
    "##### Ventajas\n",
    "\n",
    "Reduce la complejidad a $O(K)$ por ejemplo, donde $K$ es pequeño.\n",
    "\n",
    "## GloVe (Global vectors for word representation)\n",
    "\n",
    "**GloVe** es un método que combina técnicas basadas en conteo global y métodos basados en ventanas locales para generar embeddings de palabras. Se basa en la matriz de co-ocurrencia global de palabras.\n",
    "\n",
    "### Matriz de co-ocurrencia\n",
    "\n",
    "Se construye una matriz $X \\in \\mathbb{R}^{V \\times V}$ donde cada entrada $X_{ij}$ representa el número de veces que la palabra $j$ aparece en el contexto de la palabra $i$.\n",
    "\n",
    "### Objetivo del modelo\n",
    "\n",
    "El objetivo es encontrar vectores $\\mathbf{w}_i$ y $\\tilde{\\mathbf{w}}_j$ tales que:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j = \\log X_{ij}\n",
    "$$\n",
    "\n",
    "Donde $b_i$ y $\\tilde{b}_j$ son términos de sesgo.\n",
    "\n",
    "### Función de costo\n",
    "\n",
    "Se define una función de costo ponderada de mínimos cuadrados:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i,j=1}^{V} f(X_{ij}) \\left( \\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n",
    "$$\n",
    "\n",
    "Donde la función de ponderación $f(X_{ij})$ es:\n",
    "\n",
    "$$\n",
    "f(X_{ij}) = \\begin{cases}\n",
    "\\left( \\frac{X_{ij}}{X_{\\text{max}}} \\right)^\\alpha & \\text{si } X_{ij} < X_{\\text{max}} \\\\\n",
    "1 & \\text{si } X_{ij} \\geq X_{\\text{max}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Con típicamente $X_{\\text{max}} = 100$ y $\\alpha = 0.75$.\n",
    "\n",
    "### Interpretación\n",
    "\n",
    "- El modelo busca capturar relaciones semánticas mediante la aproximación de las razones de probabilidades de co-ocurrencia.\n",
    "- La utilización de la logaritmo de $X_{ij}$ permite modelar relaciones lineales en el espacio vectorial.\n",
    "\n",
    "\n",
    "## Algoritmos de los métodos\n",
    "\n",
    "### Algoritmo de Brown clustering\n",
    "\n",
    "1. **Inicialización**: Asignar cada palabra a su propio cluster.\n",
    "\n",
    "2. **Cálculo de probabilidades**: Calcular $P(c)$ y $P(c_i, c_j)$.\n",
    "\n",
    "3. **Búsqueda de fusión óptima**: Para cada par de clusters $(c_i, c_j)$, calcular la disminución en la información mutua $\\Delta I(c_i, c_j)$ si se fusionan.\n",
    "\n",
    "4. **Fusión de clusters**: Fusionar el par $(c_i, c_j)$ que minimiza $\\Delta I$.\n",
    "\n",
    "5. **Actualización**: Actualizar las probabilidades y repetir desde el paso 3 hasta alcanzar el número deseado de clusters.\n",
    "\n",
    "### Algoritmo de LSA\n",
    "\n",
    "1. **Construcción de la matriz**: Crear la matriz término-documento $X$ con pesos apropiados.\n",
    "\n",
    "2. **Aplicación de SVD**: Descomponer $X$ en $U \\Sigma V^\\top$.\n",
    "\n",
    "3. **Selección de dimensiones**: Elegir el número $k$ de dimensiones significativas.\n",
    "\n",
    "4. **Proyección**: Obtener las representaciones reducidas $U_k \\Sigma_k$ y $V_k \\Sigma_k$ para términos y documentos.\n",
    "\n",
    "### Algoritmo de Word2vec (Skip-Gram con negative sampling)\n",
    "\n",
    "1. **Inicialización**: Asignar vectores aleatorios a las palabras.\n",
    "\n",
    "2. **Recorrido del corpus**: Para cada palabra $w_t$ en el corpus:\n",
    "\n",
    "   a. **Selección del contexto**: Identificar palabras de contexto $w_{t+j}$ dentro de la ventana.\n",
    "\n",
    "   b. **Actualización de parámetros**: Para cada palabra de contexto:\n",
    "\n",
    "      - Calcular la pérdida con negative sampling.\n",
    "      - Actualizar los vectores $\\mathbf{w}_{w_t}$ y $\\mathbf{w}'_{w_{t+j}}$.\n",
    "\n",
    "3. **Iteración**: Repetir el paso 2 para varias épocas hasta la convergencia.\n",
    "\n",
    "### Algoritmo de GloVe\n",
    "\n",
    "1. **Construcción de la matriz**: Calcular la matriz de co-ocurrencia $X$.\n",
    "\n",
    "2. **Inicialización**: Asignar vectores y sesgos aleatorios a las palabras.\n",
    "\n",
    "3. **Optimización**: Minimizar la función de costo $J$ utilizando un método de gradiente (e.g., AdaGrad):\n",
    "\n",
    "   a. **Cálculo del gradiente**: Para cada par $(i, j)$, calcular el gradiente de $J$ respecto a $\\mathbf{w}_i$, $\\tilde{\\mathbf{w}}_j$, $b_i$, $\\tilde{b}_j$.\n",
    "\n",
    "   b. **Actualización de parámetros**: Actualizar los vectores y sesgos.\n",
    "\n",
    "4. **Iteración**: Repetir hasta la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7433f7-9d51-44c9-b0d4-abdec9fa6033",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "*\n",
    "Implementa todas las técnicas incluyendo Brown Clustering, latent semantic analysis (LSA), Word2vec (CBOW y Skip-Gram con negative sampling). Además, analizar y optimizar la complejidad algorítmica de los métodos desarrollados.\n",
    "\n",
    "#### **Descripción del ejercicio**\n",
    "\n",
    "1. **Preparación del entorno de trabajo**\n",
    "   - **Lenguaje de programación:** Python 3.x\n",
    "   - **Requisitos:** Utilizar únicamente las librerías estándar de Python. No se permite el uso de librerías externas `gensim` o `spaCy`, etc.\n",
    "   - **Herramientas sugeridas:**\n",
    "     - **Estructuras de Datos:** Listas, diccionarios, conjuntos.\n",
    "     - **Manejo de archivos:** Lectura y escritura de archivos de texto.\n",
    "     - **Optimización:** Uso eficiente de estructuras de datos y algoritmos para manejar grandes volúmenes de datos.\n",
    "\n",
    "2. **Selección y preparación del corpus**\n",
    "   - **Corpus Sugerido:** [Corpus de Wikipedia en Español](https://dumps.wikimedia.org/eswiki/latest/eswiki-latest-pages-articles.xml.bz2) o cualquier otro corpus extenso y representativo en español.\n",
    "   - **Preprocesamiento: (2 puntos)**\n",
    "     - **Descarga y extracción:** Descargar el dump de Wikipedia y extraer el texto relevante.\n",
    "     - **Tokenización (0.5 puntos):** Implementar un tokenizer desde cero que divida el texto en palabras, manejando puntuación, mayúsculas/minúsculas y otros aspectos lingüísticos básicos.\n",
    "     - **Lematización/stemming (0.5 puntos):** Crear una función simple de stemming o lematización para reducir las palabras a su forma base. (Nota: Dado que no se usan librerías externas, la implementación será básica y posiblemente menos precisa).\n",
    "     - **Remoción de stopwords (0.5 puntos):** Definir una lista de palabras vacías (stopwords) en español y eliminar todas las ocurrencias de estas palabras en el corpus.\n",
    "     - **Filtrado de palabras raras (0.5 puntos):** Establecer un umbral de frecuencia (e.g., eliminar palabras que aparecen menos de 5 veces) para reducir el tamaño del vocabulario.\n",
    "\n",
    "3. **Implementación de técnicas**\n",
    "\n",
    "   ##### **A. Brown clustering (4 puntos)**\n",
    "   - **Descripción:** Implementar el algoritmo de Brown Clustering para agrupar palabras basándose en su contexto.\n",
    "   - **Pasos:**\n",
    "     1. **Inicialización (0.5 puntos):** Asignar cada palabra a su propio cluster utilizando estructuras de datos como diccionarios para mapear palabras a clusters.\n",
    "     2. **Cálculo de probabilidades (0.5 puntos):** Implementar funciones para estimar las probabilidades $P(c)$ y $P(c_i, c_j)$ a partir del corpus procesado.\n",
    "     3. **Búsqueda de fusión óptima (0.5 puntos):** Desarrollar una función que evalúe, para cada par de clusters, la disminución en la información mutua $\\Delta I(c_i, c_j)$ si se fusionan.\n",
    "     4. **Fusión de clusters (0.5 puntos):** Implementar la lógica para unir el par de clusters que minimiza $\\Delta I$.\n",
    "     5. **Repetición:** Continuar el proceso hasta alcanzar el número deseado de clusters (e.g., 100 clusters).\n",
    "   - **Mejoras en complejidad :**\n",
    "     - **Estructuras de datos eficientes (1 punto):** Utilizar estructuras como árboles binarios o matrices de adyacencia para manejar pares de clusters y sus probabilidades.\n",
    "     - **Técnicas de poda (1 punto):** Implementar métodos para limitar el número de fusiones candidatas en cada iteración, reduciendo así el tiempo de cómputo.\n",
    "\n",
    "   ##### **B. Latent semantic analysis (LSA) (3 puntos)**\n",
    "   - **Descripción:** Aplicar LSA para reducir la dimensionalidad y capturar relaciones semánticas.\n",
    "   - **Pasos:**\n",
    "     1. **Construcción de la matriz término-socumento:** Crear una matriz $X$ donde las filas representen términos y las columnas representen documentos, asignando pesos como TF-IDF.\n",
    "     2. **Implementación de SVD (1 punto):** Desarrollar una función desde cero para realizar la descomposición en valores singulares (SVD) de la matriz $X$. Dado que SVD es complejo de implementar, considerar simplificaciones o métodos iterativos básicos.\n",
    "     3. **Reducción de Dimensionalidad (0.5 puntos):** Seleccionar los $k$ valores singulares más grandes para obtener $X_k = U_k \\Sigma_k V_k^\\top$.\n",
    "     4. **Proyección (0.5 puntos):** Representar términos y documentos en el espacio reducido utilizando las matrices resultantes.\n",
    "   - **Mejoras en complejidad:**\n",
    "     - **Métodos de SVD eficientes (opcional):** Implementar algoritmos iterativos como el método de potencia para calcular los valores y vectores singulares más significativos.\n",
    "     - **Manejo de matrices dispersas (1 punto):** Optimizar el almacenamiento y operaciones sobre matrices dispersas para reducir el uso de memoria y tiempo de procesamiento.\n",
    "\n",
    "   ##### **C. Word2vec (2 puntos)**\n",
    "   - **Descripción:** Entrenar modelos Word2vec utilizando las arquitecturas CBOW y Skip-Gram con Negative Sampling.\n",
    "   - **Pasos:**\n",
    "     1. **Preprocesamiento:** Realizar los mismos pasos de preprocesamiento que en la preparación del corpus.\n",
    "     2. **Implementación CBOW (1 punto):**\n",
    "        - **Ventana de contexto:** Definir una ventana de tamaño $c$ alrededor de la palabra objetivo.\n",
    "        - **Función de pérdida:** Implementar la función de pérdida negativa logarítmica para predecir la palabra objetivo basada en el contexto.\n",
    "        - **Optimización:** Desarrollar un algoritmo de descenso de gradiente básico para actualizar los vectores de palabras.\n",
    "     3. **Implementación skip-Gram con negative sampling (1 punto):**\n",
    "        - **Predicción de contexto:** Para cada palabra objetivo, predecir palabras de contexto dentro de la ventana.\n",
    "        - **Negative sampling:** Implementar una función para muestrear palabras negativas de manera eficiente.\n",
    "        - **Función de pérdida:** Implementar la función de pérdida que considera tanto los pares positivos como los negativos.\n",
    "   - **Mejoras en complejidad:**\n",
    "     - **Optimización Paralela (opcional):** Simular paralelización utilizando múltiples hilos o procesos para acelerar el entrenamiento.\n",
    "     - **Tablas de muestreo eficientes (opcional):** Implementar estructuras de datos como tablas de alias para realizar el muestreo negativo de manera más rápida.\n",
    "\n",
    "   ##### **D. GloVe (Global vectors for word representation) (2 puntos)**\n",
    "   - **Descripción:** Entrenar el modelo GloVe para obtener embeddings de palabras basados en co-ocurrencias globales.\n",
    "   - **Pasos:**\n",
    "     1. **Construcción de la Matriz de co-ocurrencia $X$:** Implementar una función que recorra el corpus y cuente las co-ocurrencias de palabras dentro de una ventana determinada.\n",
    "     2. **Inicialización de vectores y sesgos:** Asignar vectores y términos de sesgo iniciales a cada palabra de manera aleatoria.\n",
    "     3. **Optimización de la función de costo (1 punto):** Desarrollar una función para minimizar $J = \\sum_{i,j=1}^{V} f(X_{ij}) (\\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij})^2$ utilizando un método de gradiente básico como el descenso de gradiente estocástico.\n",
    "     4. **Iteración hasta convergencia:** Repetir el proceso de optimización hasta que la función de costo converja o se alcance un número máximo de iteraciones.\n",
    "   - **Mejoras en complejidad:**\n",
    "     - **Paralelización de actualizaciones (opcional):** Implementar mecanismos para actualizar múltiples vectores simultáneamente sin conflictos.\n",
    "     - **Optimización de funciones de ponderación (1 punto):** Simplificar o precomputar partes de la función de ponderación $f(X_{ij})$ para acelerar el cálculo durante la optimización.\n",
    "\n",
    "4. **Evaluación y comparación ( 3 puntos)**\n",
    "   - **Tareas de evaluación:**\n",
    "     - **Analogías semánticas (0.5 puntos):** Evaluar la capacidad de los embeddings para resolver analogías simples (e.g., \"rey\" es a \"reina\" como \"hombre\" es a \"mujer\").\n",
    "     - **Clasificación de textos (0.5 puntos):** Utilizar las representaciones obtenidas para clasificar documentos en categorías predefinidas mediante implementaciones básicas de algoritmos de clasificación (e.g., k-NN, Naive Bayes).\n",
    "     - **Clustering de palabras (0.5 puntos):** Analizar la coherencia de los clusters obtenidos por Brown Clustering comparados con los embeddings de Word2vec y GloVe implementados.\n",
    "   - **Métricas:**\n",
    "     - **Precisión y Recall (0.5 puntos):** Implementar métricas básicas para evaluar la exactitud de las tareas de clasificación y analogías.\n",
    "     - **Coeficiente de Silhouette (0.5 puntos):** Desarrollar una función para calcular el coeficiente de Silhouette y evaluar la calidad del clustering.\n",
    "     - **Tiempo de ejecución y uso de memoria (0.5 puntos):** Medir y comparar los tiempos de ejecución y el uso de memoria de las implementaciones, especialmente después de aplicar mejoras en la complejidad algorítmica.\n",
    "\n",
    "\n",
    "Debes tener en cuenta que cuando implementes tus algoritmos las mejoras se deben dar de la siguiente forma:\n",
    "\n",
    "   - **Brown Clustering:**\n",
    "     - **Complejidad inicial:** Aproximadamente $O(N \\cdot |V|^2)$, donde $N$ es el número de bigramas y $|V|$ el tamaño del vocabulario.\n",
    "     - **Mejoras implementadas:** Uso de estructuras de datos eficientes para reducir la complejidad a $O(N \\cdot |V| \\log |V|)$ mediante técnicas de poda y estructuras jerárquicas.\n",
    "   - **LSA:**\n",
    "     - **Complejidad inicial:** $O(mn \\min(m, n))$ para una implementación básica de SVD.\n",
    "     - **Mejoras implementadas:** Implementación de métodos iterativos para Truncated SVD, reduciendo la complejidad a $O(mnk)$, donde $k$ es la dimensión reducida.\n",
    "   - **Word2vec y GloVe:**\n",
    "     - **Complejidad inicial:** $O(T \\cdot c \\cdot V)$, donde $T$ es el tamaño del corpus y $c$ la ventana de contexto.\n",
    "     - **Mejoras implementadas:** Implementación de negative sampling para reducir la complejidad a $O(T \\cdot c \\cdot K)$, donde $K$ es el número de muestras negativas, y optimizaciones en la actualización de vectores para disminuir tiempos de cómputo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07a8c1-0ae9-4aa7-b97c-831af8d558ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import heapq\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict, Set\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokeniza el texto en palabras, eliminando puntuación y convirtiendo a minúsculas.\n",
    "    Maneja caracteres especiales comunes en español.\n",
    "    \"\"\"\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Reemplazar caracteres no alfanuméricos por espacios\n",
    "    text = re.sub(r'[^a-záéíóúüñ0-9]+', ' ', text)\n",
    "    # Dividir el texto en tokens\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def simple_stemmer(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Aplica reglas básicas de stemming en español.\n",
    "    Implementación más robusta con reglas adicionales.\n",
    "    \"\"\"\n",
    "    suffixes = [\n",
    "        'amiento', 'amientos', 'ación', 'aciones', 'ador', 'adores', 'adora', 'adoras',\n",
    "        'ablemente', 'abilidades', 'idades', 'idad', 'able', 'ables', 'ible', 'ibles',\n",
    "        'ación', 'aciones', 'ción', 'ciones', 'anza', 'anzas', 'mente',\n",
    "        'ando', 'iendo', 'ado', 'ada', 'idos', 'idas', 'ar', 'er', 'ir', 'es',\n",
    "        'os', 'as', 'a', 'o', 'e', 'í', 'ó'\n",
    "    ]\n",
    "    for suffix in sorted(suffixes, key=lambda x: -len(x)):\n",
    "        if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "def stem_tokens(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Aplica stemming a una lista de tokens.\n",
    "    \"\"\"\n",
    "    return [simple_stemmer(token) for token in tokens]\n",
    "\n",
    "def load_stopwords(file_path: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Carga una lista de stopwords desde un archivo de texto.\n",
    "    Cada línea del archivo debe contener una stopword.\n",
    "    \"\"\"\n",
    "    stopwords = set()\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                word = line.strip().lower()\n",
    "                if word:\n",
    "                    stopwords.add(word)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Archivo de stopwords no encontrado en {file_path}. Asegúrate de crearlo.\")\n",
    "    return stopwords\n",
    "\n",
    "def remove_stopwords(tokens: List[str], stopwords: Set[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Elimina las stopwords de una lista de tokens.\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "def filter_rare_words(tokens: List[str], threshold: int = 2) -> Tuple[List[str], Counter]:\n",
    "    \"\"\"\n",
    "    Filtra palabras que aparecen menos de `threshold` veces.\n",
    "    Retorna la lista de tokens filtrados y el conteo de palabras.\n",
    "    \"\"\"\n",
    "    freq = Counter(tokens)\n",
    "    return [token for token in tokens if freq[token] >= threshold], freq\n",
    "\n",
    "def filter_rare_words_dynamic(tokens: List[str], percentile: float = 0.1) -> Tuple[List[str], Counter]:\n",
    "    \"\"\"\n",
    "    Filtra palabras basándose en un percentil de frecuencia.\n",
    "    Las palabras con frecuencia por debajo del percentil especificado son eliminadas.\n",
    "    \"\"\"\n",
    "    freq = Counter(tokens)\n",
    "    frequencies = list(freq.values())\n",
    "    frequencies.sort()\n",
    "    cutoff_index = int(len(frequencies) * percentile)\n",
    "    if cutoff_index < 1:\n",
    "        cutoff_index = 1\n",
    "    cutoff = frequencies[cutoff_index]\n",
    "    return [token for token in tokens if freq[token] >= cutoff], freq\n",
    "\n",
    "def generate_bigrams(tokens: List[str]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Genera bigramas a partir de una lista de tokens.\n",
    "    \"\"\"\n",
    "    return list(zip(tokens[:-1], tokens[1:]))\n",
    "\n",
    "def initialize_clusters(vocab: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Inicializa cada palabra en su propio cluster.\n",
    "    Retorna un diccionario mapeando palabras a identificadores de clusters.\n",
    "    \"\"\"\n",
    "    clusters = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return clusters\n",
    "\n",
    "def calculate_probabilities(\n",
    "    clusters: Dict[str, int],\n",
    "    word_counts: Counter,\n",
    "    bigram_counts: Counter,\n",
    "    total_words: int,\n",
    "    total_bigrams: int\n",
    ") -> Tuple[Dict[int, float], Dict[Tuple[int, int], float]]:\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades P(c) y P(c_i, c_j).\n",
    "    \"\"\"\n",
    "    P_c = defaultdict(float)\n",
    "    for word, cluster in clusters.items():\n",
    "        P_c[cluster] += word_counts[word] / total_words\n",
    "\n",
    "    P_ci_cj = defaultdict(float)\n",
    "    for (w_i, w_j), freq in bigram_counts.items():\n",
    "        c_i = clusters[w_i]\n",
    "        c_j = clusters[w_j]\n",
    "        P_ci_cj[(c_i, c_j)] += freq / total_bigrams\n",
    "\n",
    "    return dict(P_c), dict(P_ci_cj)\n",
    "\n",
    "def compute_mutual_information(\n",
    "    P_c: Dict[int, float],\n",
    "    P_ci_cj: Dict[Tuple[int, int], float]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calcula la información mutua I(C).\n",
    "    \"\"\"\n",
    "    I = 0.0\n",
    "    for (c_i, c_j), P_val in P_ci_cj.items():\n",
    "        if P_val > 0 and P_c[c_i] > 0 and P_c[c_j] > 0:\n",
    "            I += P_val * math.log(P_val / (P_c[c_i] * P_c[c_j]), 2)\n",
    "    return I\n",
    "\n",
    "def find_best_merge(\n",
    "    clusters: Dict[str, int],\n",
    "    P_c: Dict[int, float],\n",
    "    P_ci_cj: Dict[Tuple[int, int], float],\n",
    "    word_counts: Counter,\n",
    "    bigram_counts: Counter,\n",
    "    total_bigrams: int\n",
    ") -> Tuple[Tuple[int, int], float]:\n",
    "    \"\"\"\n",
    "    Encuentra el mejor par de clusters para fusionar que maximiza la información mutua.\n",
    "    Utiliza una cola de prioridad para seleccionar el mejor par de manera eficiente.\n",
    "    \"\"\"\n",
    "    # Usamos una heap como cola de prioridad\n",
    "    heap = []\n",
    "    # Pre-calculamos la información mutua actual\n",
    "    current_I = compute_mutual_information(P_c, P_ci_cj)\n",
    "    \n",
    "    unique_clusters = list(P_c.keys())\n",
    "    n = len(unique_clusters)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            c_i = unique_clusters[i]\n",
    "            c_j = unique_clusters[j]\n",
    "            \n",
    "            # Calcular el nuevo P(c) para la fusión\n",
    "            P_c_new = P_c[c_i] + P_c[c_j]\n",
    "            \n",
    "            # Calcular la nueva P(ci, cj) después de la fusión\n",
    "            P_ci_cj_new = defaultdict(float)\n",
    "            for (a, b), P_val in P_ci_cj.items():\n",
    "                # Actualizar los clusters después de la fusión\n",
    "                a_new = c_i if a == c_i or a == c_j else a\n",
    "                b_new = c_j if b == c_i or b == c_j else b\n",
    "                key = tuple(sorted((a_new, b_new)))\n",
    "                P_ci_cj_new[key] += P_val\n",
    "            \n",
    "            # Calcular la nueva información mutua\n",
    "            I_new = compute_mutual_information(P_c, P_ci_cj_new)\n",
    "            \n",
    "            # Calcular el delta de información mutua\n",
    "            delta_I = I_new - current_I\n",
    "            \n",
    "            # Usamos -delta_I porque heapq es una min-heap y queremos una max-heap\n",
    "            heapq.heappush(heap, (-delta_I, (c_i, c_j)))\n",
    "    \n",
    "    if not heap:\n",
    "        return None, 0.0\n",
    "    \n",
    "    # Obtener la fusión con el mayor delta_I\n",
    "    best_delta_I, best_pair = heapq.heappop(heap)\n",
    "    return best_pair, -best_delta_I\n",
    "\n",
    "def save_clusters(clusters: Dict[str, int], file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda los clusters en un archivo JSON.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(clusters, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_clusters(file_path: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Carga los clusters desde un archivo JSON.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        clusters = json.load(f)\n",
    "    # Convertir las claves de los clusters a enteros si es necesario\n",
    "    return {word: int(cluster) for word, cluster in clusters.items()}\n",
    "\n",
    "def print_progress(current: int, total: int, bar_length: int = 50) -> None:\n",
    "    \"\"\"\n",
    "    Imprime una barra de progreso en la consola.\n",
    "    \"\"\"\n",
    "    fraction = current / total\n",
    "    arrow = '=' * int(fraction * bar_length)\n",
    "    padding = ' ' * (bar_length - len(arrow))\n",
    "    ending = '\\n' if current == total else '\\r'\n",
    "    print(f'Progreso: [{arrow}{padding}] {int(fraction * 100)}%', end=ending)\n",
    "\n",
    "def evaluate_clusters(\n",
    "    clusters: Dict[str, int],\n",
    "    word_counts: Counter,\n",
    "    bigram_counts: Counter\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evalúa la calidad de los clusters calculando la entropía de las clases.\n",
    "    Menor entropía indica mayor coherencia dentro de las clases.\n",
    "    \"\"\"\n",
    "    cluster_sizes = defaultdict(int)\n",
    "    for word, cluster in clusters.items():\n",
    "        cluster_sizes[cluster] += word_counts[word]\n",
    "    \n",
    "    total_words = sum(cluster_sizes.values())\n",
    "    entropy = 0.0\n",
    "    for cluster, size in cluster_sizes.items():\n",
    "        p = size / total_words\n",
    "        if p > 0:\n",
    "            entropy -= p * math.log(p, 2)\n",
    "    return entropy\n",
    "\n",
    "def brown_clustering_main(\n",
    "    corpus_text: str,\n",
    "    stopwords_file: str,\n",
    "    desired_clusters: int = 100,\n",
    "    rare_word_threshold: int = 2,\n",
    "    save_clusters_path: str = 'clusters.json',\n",
    "    load_clusters_path: str = None\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Ejecuta el algoritmo de Brown Clustering con mejoras avanzadas.\n",
    "    \n",
    "    Parámetros:\n",
    "    - corpus_text: Texto del corpus a procesar.\n",
    "    - stopwords_file: Ruta al archivo de stopwords.\n",
    "    - desired_clusters: Número de clusters deseados.\n",
    "    - rare_word_threshold: Umbral de frecuencia para filtrar palabras raras.\n",
    "    - save_clusters_path: Ruta para guardar los clusters resultantes.\n",
    "    - load_clusters_path: Ruta para cargar clusters existentes (opcional).\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario de palabras mapeadas a sus clusters.\n",
    "    \"\"\"\n",
    "    # Tokenizar el texto\n",
    "    tokens = tokenize(corpus_text)\n",
    "    print(f\"Total de tokens después de tokenización: {len(tokens)}\")\n",
    "    \n",
    "    # Aplicar stemming\n",
    "    stemmed_tokens = stem_tokens(tokens)\n",
    "    print(f\"Total de tokens después de stemming: {len(stemmed_tokens)}\")\n",
    "    \n",
    "    # Cargar stopwords y eliminar\n",
    "    stopwords = load_stopwords(stopwords_file)\n",
    "    if not stopwords:\n",
    "        print(\"No se cargaron stopwords. Asegúrate de que el archivo exista y tenga contenido.\")\n",
    "    filtered_tokens = remove_stopwords(stemmed_tokens, stopwords)\n",
    "    print(f\"Total de tokens después de eliminar stopwords: {len(filtered_tokens)}\")\n",
    "    \n",
    "    # Filtrar palabras raras\n",
    "    filtered_tokens, word_freq = filter_rare_words(filtered_tokens, threshold=rare_word_threshold)\n",
    "    print(f\"Total de tokens después de filtrar palabras raras (umbral={rare_word_threshold}): {len(filtered_tokens)}\")\n",
    "    print(f\"Vocabulario tamaño: {len(word_freq)}\")\n",
    "    \n",
    "    # Generar bigrams\n",
    "    bigrams = generate_bigrams(filtered_tokens)\n",
    "    print(f\"Total de bigrams generados: {len(bigrams)}\")\n",
    "    \n",
    "    # Contar frecuencias\n",
    "    word_counts, bigram_counts, total_words, total_bigrams = count_frequencies(filtered_tokens, bigrams)\n",
    "    print(f\"Total de palabras contadas: {total_words}\")\n",
    "    print(f\"Total de bigrams contados: {total_bigrams}\")\n",
    "    \n",
    "    # Inicializar clusters\n",
    "    vocab = list(word_freq.keys())\n",
    "    clusters = initialize_clusters(vocab)\n",
    "    print(f\"Clusters inicializados: {len(clusters)} clusters.\")\n",
    "    \n",
    "    # Si se proporciona un archivo de clusters existentes, cargarlo\n",
    "    if load_clusters_path:\n",
    "        clusters = load_clusters(load_clusters_path)\n",
    "        print(f\"Clusters cargados desde {load_clusters_path}. Total de clusters: {len(set(clusters.values()))}\")\n",
    "    \n",
    "    # Ejecutar Brown Clustering\n",
    "    current_iteration = 0\n",
    "    max_iterations = len(set(clusters.values())) - desired_clusters\n",
    "    print(f\"Iniciando Brown Clustering: {len(set(clusters.values()))} clusters iniciales, objetivo {desired_clusters} clusters.\")\n",
    "    while len(set(clusters.values())) > desired_clusters and current_iteration < max_iterations:\n",
    "        P_c, P_ci_cj = calculate_probabilities(clusters, word_counts, bigram_counts, total_words, total_bigrams)\n",
    "        best_pair, best_delta_I = find_best_merge(clusters, P_c, P_ci_cj, word_counts, bigram_counts, total_bigrams)\n",
    "        if best_pair is None:\n",
    "            print(\"No se encontraron más fusiones posibles.\")\n",
    "            break\n",
    "        c_i, c_j = best_pair\n",
    "        new_cluster_id = max(P_c.keys()) + 1\n",
    "        clusters = merge_clusters(clusters, c_i, c_j, new_cluster_id)\n",
    "        current_iteration += 1\n",
    "        # Mostrar progreso\n",
    "        print_progress(current_iteration, max_iterations)\n",
    "    \n",
    "    # Guardar clusters resultantes\n",
    "    save_clusters(clusters, save_clusters_path)\n",
    "    print(f\"\\nClusters guardados en {save_clusters_path}\")\n",
    "    \n",
    "    # Evaluar clusters\n",
    "    entropy = evaluate_clusters(clusters, word_counts, bigram_counts)\n",
    "    print(f\"Entropía de los clusters: {entropy:.4f}\")\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def count_frequencies(tokens: List[str], bigrams: List[Tuple[str, str]]) -> Tuple[Counter, Counter, int, int]:\n",
    "    \"\"\"\n",
    "    Cuenta las frecuencias de las palabras y las bigramas.\n",
    "    \"\"\"\n",
    "    word_counts = Counter(tokens)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    total_words = sum(word_counts.values())\n",
    "    total_bigrams = sum(bigram_counts.values())\n",
    "    return word_counts, bigram_counts, total_words, total_bigrams\n",
    "\n",
    "def merge_clusters(clusters: Dict[str, int], c_i: int, c_j: int, new_cluster_id: int) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Fusiona dos clusters en un nuevo cluster.\n",
    "    \"\"\"\n",
    "    for word, cluster in clusters.items():\n",
    "        if cluster == c_i or cluster == c_j:\n",
    "            clusters[word] = new_cluster_id\n",
    "    return clusters\n",
    "\n",
    "# Ejemplo de Uso Completo\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejemplo de corpus de texto\n",
    "    corpus_text = \"\"\"\n",
    "    El rápido zorro marrón salta sobre el perro perezoso. \n",
    "    La vida es bella y llena de oportunidades. \n",
    "    Python es un lenguaje de programación poderoso y versátil. \n",
    "    El aprendizaje automático es una rama de la inteligencia artificial. \n",
    "    Aprender a programar en Python facilita el desarrollo de aplicaciones complejas.\n",
    "    La inteligencia artificial y el aprendizaje automático están revolucionando la tecnología.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear un archivo de stopwords si no existe\n",
    "    stopwords_file = 'stopwords_es.txt'\n",
    "    try:\n",
    "        with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "            pass  # El archivo existe\n",
    "    except FileNotFoundError:\n",
    "        # Crear el archivo con algunas stopwords básicas\n",
    "        with open(stopwords_file, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join([\n",
    "                'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'y', 'de', 'en',\n",
    "                'es', 'con', 'por', 'para', 'a', 'e', 'o', 'u', 'del', 'al', 'si',\n",
    "                'que', 'se', 's'\n",
    "            ]))\n",
    "        print(f\"Archivo de stopwords creado en {stopwords_file}\")\n",
    "    \n",
    "    # Ejecutar Brown Clustering\n",
    "    clusters_finales = brown_clustering_main(\n",
    "        corpus_text=corpus_text,\n",
    "        stopwords_file=stopwords_file,\n",
    "        desired_clusters=5,\n",
    "        rare_word_threshold=1,\n",
    "        save_clusters_path='clusters_finales.json'\n",
    "    )\n",
    "    \n",
    "    # Mostrar los clusters finales\n",
    "    print(\"\\nClusters finales:\")\n",
    "    for word, cluster in clusters_finales.items():\n",
    "        print(f\"Palabra: {word}, Cluster: {cluster}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6c136",
   "metadata": {},
   "source": [
    "Esta es una implementación funcional del algoritmo de Brown Clustering. Es adecuado para pequeños conjuntos de datos y lo que pide el problema. Para aplicaciones más serias y corpora extensos, se recomienda implementar optimizaciones adicionales y posiblemente utilizar librerías especializadas para mejorar el rendimiento y la precisión.\n",
    "\n",
    "Combinemos primero con LSA y Brown Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ba6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import copy\n",
    "\n",
    "# 1. Preprocesamiento del corpus\n",
    "\n",
    "def leer_corpus(ruta_archivo):\n",
    "    \"\"\"\n",
    "    Lee el corpus desde un archivo de texto.\n",
    "    Supone que cada línea es un documento.\n",
    "    \"\"\"\n",
    "    documentos = []\n",
    "    try:\n",
    "        with open(ruta_archivo, 'r', encoding='utf-8') as archivo:\n",
    "            for linea in archivo:\n",
    "                if linea.strip():  # Ignorar líneas vacías\n",
    "                    documentos.append(linea.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo '{ruta_archivo}' no se encontró.\")\n",
    "    return documentos\n",
    "\n",
    "def tokenizer(texto):\n",
    "    \"\"\"\n",
    "    Tokeniza el texto en palabras, eliminando puntuación y convirtiendo a minúsculas.\n",
    "    \"\"\"\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r'[^a-záéíóúñü]+', ' ', texto)\n",
    "    tokens = texto.split()\n",
    "    return tokens\n",
    "\n",
    "def stemming(palabra):\n",
    "    \"\"\"\n",
    "    Aplica una versión simplificada de stemming para palabras en español.\n",
    "    \"\"\"\n",
    "    sufijos = ['ción', 'sión', 'mente', 'ando', 'iendo', 'ado', 'ido', 'ar', 'er', 'ir',\n",
    "               'es', 'os', 'as', 'e', 'a', 'o']\n",
    "    for sufijo in sufijos:\n",
    "        if palabra.endswith(sufijo) and len(palabra) > len(sufijo) + 2:\n",
    "            return palabra[:-len(sufijo)]\n",
    "    return palabra\n",
    "\n",
    "def aplicar_stemming(tokens):\n",
    "    \"\"\"\n",
    "    Aplica stemming a una lista de tokens.\n",
    "    \"\"\"\n",
    "    return [stemming(token) for token in tokens]\n",
    "\n",
    "# Lista de stopwords en español (parcial)\n",
    "STOPWORDS = {\n",
    "    'de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las',\n",
    "    'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como',\n",
    "    'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta',\n",
    "    'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta',\n",
    "    'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos',\n",
    "    'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos',\n",
    "    'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro',\n",
    "    'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes',\n",
    "    'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas',\n",
    "    'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus',\n",
    "    'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía',\n",
    "    'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya',\n",
    "    'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras',\n",
    "    'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy',\n",
    "    'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés',\n",
    "    'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará',\n",
    "    'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías',\n",
    "    'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas',\n",
    "    'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste',\n",
    "    'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera',\n",
    "    'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran',\n",
    "    'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis',\n",
    "    'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas',\n",
    "    'estad'\n",
    "}\n",
    "\n",
    "def remover_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remueve las stopwords de una lista de tokens.\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token not in STOPWORDS]\n",
    "\n",
    "def filtrar_palabras_raras(tokens, umbral=5):\n",
    "    \"\"\"\n",
    "    Filtra las palabras que aparecen menos de 'umbral' veces.\n",
    "    \"\"\"\n",
    "    contador = Counter(tokens)\n",
    "    vocabulario = {palabra for palabra, freq in contador.items() if freq >= umbral}\n",
    "    tokens_filtrados = [token for token in tokens if token in vocabulario]\n",
    "    return tokens_filtrados, vocabulario\n",
    "\n",
    "def preprocesar_documentos(documentos, umbral=5):\n",
    "    \"\"\"\n",
    "    Preprocesa cada documento: tokenización, stemming, remoción de stopwords y filtrado.\n",
    "    \"\"\"\n",
    "    documentos_tokens = []\n",
    "    contador_total = Counter()\n",
    "    for doc in documentos:\n",
    "        tokens = tokenizer(doc)\n",
    "        tokens = aplicar_stemming(tokens)\n",
    "        tokens = remover_stopwords(tokens)\n",
    "        documentos_tokens.append(tokens)\n",
    "        contador_total.update(tokens)\n",
    "    # Filtrar palabras raras\n",
    "    vocabulario = {palabra for palabra, freq in contador_total.items() if freq >= umbral}\n",
    "    if not vocabulario:\n",
    "        print(\"Advertencia: El vocabulario está vacío después del filtrado. Considera reducir el umbral.\")\n",
    "    documentos_tokens_filtrados = [\n",
    "        [token for token in tokens if token in vocabulario]\n",
    "        for tokens in documentos_tokens\n",
    "    ]\n",
    "    return documentos_tokens_filtrados, vocabulario\n",
    "\n",
    "\n",
    "# 2. Brown clustering (simplificado)\n",
    "\n",
    "def inicializar_clusters(vocabulario):\n",
    "    \"\"\"\n",
    "    Inicializa cada palabra en su propio clúster.\n",
    "    \"\"\"\n",
    "    clusters = {palabra: idx for idx, palabra in enumerate(vocabulario)}\n",
    "    return clusters\n",
    "\n",
    "def calcular_probabilidades(tokens, clusters):\n",
    "    \"\"\"\n",
    "    Calcula P(c) y P(c_i, c_j) a partir de los tokens y la asignación de clusters.\n",
    "    \"\"\"\n",
    "    contador_clusters = Counter()\n",
    "    contador_clusters_parejas = Counter()\n",
    "\n",
    "    for i in range(len(tokens)-1):\n",
    "        palabra = tokens[i]\n",
    "        siguiente_palabra = tokens[i+1]\n",
    "        cluster_actual = clusters.get(palabra, None)\n",
    "        cluster_siguiente = clusters.get(siguiente_palabra, None)\n",
    "        if cluster_actual is not None and cluster_siguiente is not None:\n",
    "            contador_clusters[cluster_actual] += 1\n",
    "            contador_clusters_parejas[(cluster_actual, cluster_siguiente)] += 1\n",
    "\n",
    "    total_clusters = sum(contador_clusters.values())\n",
    "    if total_clusters == 0:\n",
    "        return {}, {}\n",
    "    P_c = {c: count / total_clusters for c, count in contador_clusters.items()}\n",
    "\n",
    "    total_parejas = sum(contador_clusters_parejas.values())\n",
    "    if total_parejas == 0:\n",
    "        return P_c, {}\n",
    "    P_ci_cj = {pair: count / total_parejas for pair, count in contador_clusters_parejas.items()}\n",
    "\n",
    "    return P_c, P_ci_cj\n",
    "\n",
    "def informacion_mutua(P_c, P_ci_cj):\n",
    "    \"\"\"\n",
    "    Calcula la información mutua I(c_i, c_j).\n",
    "    \"\"\"\n",
    "    I = 0.0\n",
    "    for (c_i, c_j), P in P_ci_cj.items():\n",
    "        if P > 0 and c_i in P_c and c_j in P_c:\n",
    "            I += P * math.log(P / (P_c[c_i] * P_c[c_j]) + 1e-10)\n",
    "    return I\n",
    "\n",
    "def buscar_fusion_optima(P_c, P_ci_cj, clusters, vocabulario, tokens):\n",
    "    \"\"\"\n",
    "    Encuentra el par de clústeres cuya fusión minimiza la disminución de la información mutua.\n",
    "    \"\"\"\n",
    "    min_delta_I = float('inf')\n",
    "    par_optimo = None\n",
    "    clusters_unicos = set(clusters.values())\n",
    "\n",
    "    for c1 in clusters_unicos:\n",
    "        for c2 in clusters_unicos:\n",
    "            if c1 >= c2:\n",
    "                continue\n",
    "            # Simular fusión\n",
    "            clusters_simulados = clusters.copy()\n",
    "            nuevo_c = max(clusters_unicos) + 1\n",
    "            for palabra in vocabulario:\n",
    "                if clusters_simulados.get(palabra, None) == c1 or clusters_simulados.get(palabra, None) == c2:\n",
    "                    clusters_simulados[palabra] = nuevo_c\n",
    "            # Calcular nuevas probabilidades\n",
    "            P_c_sim, P_ci_cj_sim = calcular_probabilidades(tokens, clusters_simulados)\n",
    "            # Calcular diferencia en información mutua\n",
    "            I_original = informacion_mutua(P_c, P_ci_cj)\n",
    "            I_sim = informacion_mutua(P_c_sim, P_ci_cj_sim)\n",
    "            delta_I = I_original - I_sim\n",
    "            if delta_I < min_delta_I:\n",
    "                min_delta_I = delta_I\n",
    "                par_optimo = (c1, c2)\n",
    "    return par_optimo, min_delta_I\n",
    "\n",
    "def fusionar_clusters(clusters, par):\n",
    "    \"\"\"\n",
    "    Fusiona dos clústeres dados en 'par' y actualiza la asignación de clusters.\n",
    "    \"\"\"\n",
    "    if par is None:\n",
    "        return clusters\n",
    "    c1, c2 = par\n",
    "    nuevo_c = max(clusters.values()) + 1\n",
    "    for palabra, c in clusters.items():\n",
    "        if c == c1 or c == c2:\n",
    "            clusters[palabra] = nuevo_c\n",
    "    return clusters\n",
    "\n",
    "def brown_clustering(documentos_tokens, vocabulario, num_clusters=100):\n",
    "    \"\"\"\n",
    "    Implementa una versión simplificada de Brown Clustering.\n",
    "    \"\"\"\n",
    "    clusters = inicializar_clusters(vocabulario)\n",
    "    iteracion = 0\n",
    "    tokens_flat = [token for doc in documentos_tokens for token in doc]\n",
    "    while len(set(clusters.values())) > num_clusters:\n",
    "        iteracion += 1\n",
    "        print(f\"Iteración {iteracion}: Número de clústeres = {len(set(clusters.values()))}\")\n",
    "        P_c, P_ci_cj = calcular_probabilidades(tokens_flat, clusters)\n",
    "        if not P_ci_cj:\n",
    "            print(\"No hay pares de clústeres para fusionar.\")\n",
    "            break\n",
    "        par_optimo, delta_I = buscar_fusion_optima(P_c, P_ci_cj, clusters, vocabulario, tokens_flat)\n",
    "        if par_optimo is None:\n",
    "            print(\"No se pudo encontrar un par óptimo para fusionar.\")\n",
    "            break\n",
    "        clusters = fusionar_clusters(clusters, par_optimo)\n",
    "        print(f\"Fusión de clústeres {par_optimo} en uno nuevo. Delta I = {delta_I:.6f}\")\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# 3. Latent Semantic Analysis (LSA)\n",
    "\n",
    "\n",
    "def calcular_tf(documentos_tokens):\n",
    "    \"\"\"\n",
    "    Calcula el término frecuencia (TF) para cada documento.\n",
    "    \"\"\"\n",
    "    tf = []\n",
    "    for tokens in documentos_tokens:\n",
    "        contador = Counter(tokens)\n",
    "        tf_doc = {term: count for term, count in contador.items()}\n",
    "        tf.append(tf_doc)\n",
    "    return tf\n",
    "\n",
    "def calcular_idf(documentos_tokens, vocabulario):\n",
    "    \"\"\"\n",
    "    Calcula el inverso de la frecuencia de documentos (IDF) para cada término.\n",
    "    \"\"\"\n",
    "    N = len(documentos_tokens)\n",
    "    df = Counter()\n",
    "    for tokens in documentos_tokens:\n",
    "        tokens_unicos = set(tokens)\n",
    "        for term in tokens_unicos:\n",
    "            df[term] += 1\n",
    "    idf = {}\n",
    "    for term in vocabulario:\n",
    "        df_term = df.get(term, 0)\n",
    "        idf[term] = math.log((N + 1) / (df_term + 1)) + 1  # Evitar división por cero\n",
    "    return idf\n",
    "\n",
    "def construir_matriz_tf_idf(documentos_tokens, vocabulario):\n",
    "    \"\"\"\n",
    "    Construye la matriz TF-IDF.\n",
    "    Retorna una matriz dispersa representada como una lista de diccionarios por término.\n",
    "    \"\"\"\n",
    "    tf = calcular_tf(documentos_tokens)\n",
    "    idf = calcular_idf(documentos_tokens, vocabulario)\n",
    "    term_to_index = {term: idx for idx, term in enumerate(sorted(vocabulario))}\n",
    "    doc_to_index = {idx: idx for idx in range(len(documentos_tokens))}\n",
    "    m = len(vocabulario)\n",
    "    n = len(documentos_tokens)\n",
    "    if m == 0 or n == 0:\n",
    "        print(\"Advertencia: La matriz TF-IDF no puede ser construida debido a dimensiones inválidas.\")\n",
    "        return [], term_to_index, doc_to_index\n",
    "    # Inicializar matriz con diccionarios\n",
    "    X = [dict() for _ in range(m)]\n",
    "    for j, tf_doc in enumerate(tf):\n",
    "        for term, freq in tf_doc.items():\n",
    "            if term in term_to_index:\n",
    "                i = term_to_index[term]\n",
    "                X[i][j] = freq * idf[term]\n",
    "    return X, term_to_index, doc_to_index\n",
    "\n",
    "def matriz_discreta_transpuesta(X):\n",
    "    \"\"\"\n",
    "    Calcula la transpuesta de una matriz dispersa.\n",
    "    \"\"\"\n",
    "    if not X:\n",
    "        return []\n",
    "    num_cols = max((max(fila.keys()) if fila else -1) for fila in X) + 1\n",
    "    Xt = [dict() for _ in range(num_cols)]\n",
    "    for i, fila in enumerate(X):\n",
    "        for j, valor in fila.items():\n",
    "            Xt[j][i] = valor\n",
    "    return Xt\n",
    "\n",
    "def multiplicar_matriz_discreta_vector(X, v):\n",
    "    \"\"\"\n",
    "    Multiplica una matriz dispersa X con un vector v.\n",
    "    X: lista de diccionarios (filas)\n",
    "    v: lista\n",
    "    \"\"\"\n",
    "    resultado = []\n",
    "    for fila in X:\n",
    "        suma = 0.0\n",
    "        for j, valor in fila.items():\n",
    "            if j < len(v):\n",
    "                suma += valor * v[j]\n",
    "        resultado.append(suma)\n",
    "    return resultado\n",
    "\n",
    "def vector_norm(v):\n",
    "    \"\"\"\n",
    "    Calcula la norma Euclidiana de un vector.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([x**2 for x in v]))\n",
    "\n",
    "def multiplicar_vector_matriz(Xt, v):\n",
    "    \"\"\"\n",
    "    Multiplica una matriz transpuesta dispersa Xt con un vector v.\n",
    "    Xt: lista de diccionarios (columnas de la original)\n",
    "    v: lista\n",
    "    \"\"\"\n",
    "    resultado = [0.0 for _ in range(len(Xt))]\n",
    "    for j, columna in enumerate(Xt):\n",
    "        for i, valor in columna.items():\n",
    "            if i < len(v):\n",
    "                resultado[j] += valor * v[i]\n",
    "    return resultado\n",
    "\n",
    "def obtener_principal_autovector(X, num_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Calcula el principal autovector y autovalor de la matriz X^T X utilizando el método de potencia.\n",
    "    X: lista de diccionarios (filas)\n",
    "    \"\"\"\n",
    "    Xt = matriz_discreta_transpuesta(X)\n",
    "    if not Xt:\n",
    "        print(\"Advertencia: La matriz transpuesta está vacía.\")\n",
    "        return None, None\n",
    "    n = len(Xt)\n",
    "    if n == 0:\n",
    "        print(\"Advertencia: El número de documentos es cero.\")\n",
    "        return None, None\n",
    "    v = [random.random() for _ in range(n)]\n",
    "    # Normalizar\n",
    "    norm = vector_norm(v)\n",
    "    if norm == 0:\n",
    "        print(\"Advertencia: Vector inicial tiene norma cero.\")\n",
    "        return None, None\n",
    "    v = [x / norm for x in v]\n",
    "    for iteracion in range(num_iter):\n",
    "        # v_new = X^T * (X * v)\n",
    "        Xv = multiplicar_matriz_discreta_vector(X, v)\n",
    "        v_new = multiplicar_vector_matriz(Xt, Xv)\n",
    "        norm = vector_norm(v_new)\n",
    "        if norm == 0:\n",
    "            print(\"Advertencia: Vector resultante tiene norma cero durante la iteración.\")\n",
    "            return None, None\n",
    "        v_new = [x / norm for x in v_new]\n",
    "        # Verificar convergencia\n",
    "        diff = sum([abs(x - y) for x, y in zip(v, v_new)])\n",
    "        if diff < tol:\n",
    "            print(f\"Convergencia alcanzada en iteración {iteracion+1}\")\n",
    "            break\n",
    "        v = v_new\n",
    "    # Calcular autovalor\n",
    "    Xv = multiplicar_matriz_discreta_vector(X, v)\n",
    "    autovalor = sum([x * y for x, y in zip(v, Xv)])\n",
    "    return autovalor, v\n",
    "\n",
    "def reducir_dimensionalidad(U, Sigma, V, k):\n",
    "    \"\"\"\n",
    "    Reduce la dimensionalidad a 'k' dimensiones.\n",
    "    \"\"\"\n",
    "    U_k = U[:k]\n",
    "    Sigma_k = Sigma[:k]\n",
    "    V_k = V[:k]\n",
    "    return U_k, Sigma_k, V_k\n",
    "\n",
    "def proyectar(U_k, Sigma_k, V_k, k):\n",
    "    \"\"\"\n",
    "    Proyecta la matriz original en el espacio reducido.\n",
    "    \"\"\"\n",
    "    # En una implementación completa, se calcularían U_k, Sigma_k y V_k\n",
    "    # Aquí, como simplificación, se considera solo el primer componente\n",
    "    X_k = []\n",
    "    for i in range(len(U_k)):\n",
    "        fila = []\n",
    "        for j in range(len(V_k)):\n",
    "            fila.append(U_k[i] * Sigma_k * V_k[j])\n",
    "        X_k.append(fila)\n",
    "    return X_k\n",
    "\n",
    "\n",
    "# 4. Ejecución completa\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Ruta al archivo del corpus\n",
    "    ruta_corpus = 'corpus.txt'  # Reemplazar con la ruta real\n",
    "    \n",
    "    # Para propósitos de prueba, si el archivo no existe, usar un corpus de ejemplo\n",
    "    import os\n",
    "    if not os.path.isfile(ruta_corpus):\n",
    "        print(f\"El archivo '{ruta_corpus}' no existe. Usando un corpus de ejemplo.\")\n",
    "        documentos = [\n",
    "            \"Este es el primer documento.\",\n",
    "            \"Este documento es el segundo documento.\",\n",
    "            \"Y este es el tercer documento.\",\n",
    "            \"¿Está este documento funcionando correctamente?\",\n",
    "            \"El documento de prueba es este.\"\n",
    "        ]\n",
    "    else:\n",
    "        print(\"Leyendo el corpus...\")\n",
    "        documentos = leer_corpus(ruta_corpus)\n",
    "        print(f\"Total de documentos leídos: {len(documentos)}\")\n",
    "    \n",
    "    if not documentos:\n",
    "        print(\"Error: El corpus está vacío. Por favor, proporciona un corpus válido.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Preprocesando documentos...\")\n",
    "    documentos_tokens, vocabulario = preprocesar_documentos(documentos, umbral=2)  # Reducido el umbral para prueba\n",
    "    print(f\"Vocabulario después de filtrado: {len(vocabulario)} términos\")\n",
    "    if not vocabulario:\n",
    "        print(\"Error: El vocabulario está vacío después del preprocesamiento. Ajusta el umbral o revisa el corpus.\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    # Brown Clustering\n",
    "\n",
    "    print(\"\\nIniciando Brown Clustering...\")\n",
    "    clusters_finales = brown_clustering(documentos_tokens, vocabulario, num_clusters=5)  # Reducido el número de clústeres para prueba\n",
    "    print(f\"Brown Clustering completado. Total de clústeres: {len(set(clusters_finales.values()))}\")\n",
    "\n",
    "    # Latent Semantic Analysis (LSA)\n",
    "\n",
    "    print(\"\\nConstruyendo la matriz TF-IDF...\")\n",
    "    X, term_to_index, doc_to_index = construir_matriz_tf_idf(documentos_tokens, vocabulario)\n",
    "    if not X:\n",
    "        print(\"Error: La matriz TF-IDF no se pudo construir.\")\n",
    "        return\n",
    "    print(f\"Matriz TF-IDF construida con dimensiones: {len(X)} términos x {len(documentos_tokens)} documentos\")\n",
    "    \n",
    "    print(\"\\nCalculando el principal autovector y autovalor usando el método de potencia...\")\n",
    "    autovalor, autovector = obtener_principal_autovector(X)\n",
    "    if autovalor is None or autovector is None:\n",
    "        print(\"Error: No se pudo calcular el principal autovector y autovalor.\")\n",
    "        return\n",
    "    print(f\"Principal Autovalor: {autovalor}\")\n",
    "    print(f\"Principal Autovector (primeros 10 elementos): {autovector[:10]}\")\n",
    "    \n",
    "    # Reducción de dimensionalidad a k=1 (simplificación)\n",
    "    k = 1\n",
    "    print(f\"\\nReduciendo la dimensionalidad a k={k}...\")\n",
    "    # En esta implementación simplificada, solo se ha calculado un componente\n",
    "    # Por lo tanto, U_k y V_k corresponden al autovector y autovalor\n",
    "    U_k = autovector  # Representa los documentos en el espacio reducido\n",
    "    Sigma_k = autovalor\n",
    "    V_k = autovector  # Representa los términos en el espacio reducido\n",
    "    \n",
    "    print(\"\\nProyectando términos y documentos en el espacio reducido...\")\n",
    "    # Dado que es k=1, la proyección es una dimensión\n",
    "    # Para una implementación completa, se necesitarían más componentes\n",
    "    # Aquí, se muestra una simplificación\n",
    "    # Por ejemplo, representar cada documento por su peso en el principal componente\n",
    "    documentos_proyectados = []\n",
    "    for j in range(len(documentos_tokens)):\n",
    "        peso = 0.0\n",
    "        for i, fila in enumerate(X):\n",
    "            peso += fila.get(j, 0.0) * V_k[i]\n",
    "        documentos_proyectados.append(peso)\n",
    "    \n",
    "    print(f\"Documentos proyectados en {k} dimensión(es).\")\n",
    "    print(f\"Primeros 10 documentos proyectados: {documentos_proyectados[:10]}\")\n",
    "    \n",
    "    # Representación de términos en el espacio reducido\n",
    "    terminos_proyectados = []\n",
    "    for term, idx in term_to_index.items():\n",
    "        peso = U_k[idx] * Sigma_k\n",
    "        terminos_proyectados.append((term, peso))\n",
    "    print(f\"Primeros 10 términos proyectados: {terminos_proyectados[:10]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db237c2",
   "metadata": {},
   "source": [
    "**LSA**\n",
    "\n",
    "Se presenta código optimizado en Python que realiza LSA desde cero utilizando únicamente las librerías estándar siguiendo lo solicitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bee307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import copy\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. Preprocesamiento del corpus\n",
    "\n",
    "\n",
    "def leer_corpus(ruta_archivo):\n",
    "    \"\"\"\n",
    "    Lee el corpus desde un archivo de texto.\n",
    "    Supone que cada línea es un documento.\n",
    "    \"\"\"\n",
    "    documentos = []\n",
    "    try:\n",
    "        with open(ruta_archivo, 'r', encoding='utf-8') as archivo:\n",
    "            for linea in archivo:\n",
    "                if linea.strip():  # Ignorar líneas vacías\n",
    "                    documentos.append(linea.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Advertencia: El archivo '{ruta_archivo}' no se encontró. Usando un corpus de ejemplo.\")\n",
    "        documentos = [\n",
    "            \"Este es el primer documento.\",\n",
    "            \"Este documento es el segundo documento.\",\n",
    "            \"Y este es el tercer documento.\",\n",
    "            \"¿Está este documento funcionando correctamente?\",\n",
    "            \"El documento de prueba es este.\"\n",
    "        ]\n",
    "    return documentos\n",
    "\n",
    "def tokenizer(texto):\n",
    "    \"\"\"\n",
    "    Tokeniza el texto en palabras, eliminando puntuación y convirtiendo a minúsculas.\n",
    "    \"\"\"\n",
    "    texto = texto.lower()\n",
    "    # Reemplazar caracteres no alfabéticos por espacios\n",
    "    texto = re.sub(r'[^a-záéíóúñü]+', ' ', texto)\n",
    "    tokens = texto.split()\n",
    "    return tokens\n",
    "\n",
    "def stemming(palabra):\n",
    "    \"\"\"\n",
    "    Aplica una versión simplificada de stemming para palabras en español.\n",
    "    \"\"\"\n",
    "    sufijos = ['ción', 'sión', 'mente', 'ando', 'iendo', 'ado', 'ido', 'ar', 'er', 'ir',\n",
    "               'es', 'os', 'as', 'e', 'a', 'o']\n",
    "    for sufijo in sufijos:\n",
    "        if palabra.endswith(sufijo) and len(palabra) > len(sufijo) + 2:\n",
    "            return palabra[:-len(sufijo)]\n",
    "    return palabra\n",
    "\n",
    "def aplicar_stemming(tokens):\n",
    "    \"\"\"\n",
    "    Aplica stemming a una lista de tokens.\n",
    "    \"\"\"\n",
    "    return [stemming(token) for token in tokens]\n",
    "\n",
    "# Lista de stopwords en español (parcial)\n",
    "STOPWORDS = {\n",
    "    'de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las',\n",
    "    'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como',\n",
    "    'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta',\n",
    "    'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta',\n",
    "    'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos',\n",
    "    'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos',\n",
    "    'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro',\n",
    "    'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes',\n",
    "    'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas',\n",
    "    'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus',\n",
    "    'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía',\n",
    "    'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya',\n",
    "    'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras',\n",
    "    'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy',\n",
    "    'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés',\n",
    "    'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará',\n",
    "    'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías',\n",
    "    'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas',\n",
    "    'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste',\n",
    "    'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera',\n",
    "    'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran',\n",
    "    'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis',\n",
    "    'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas',\n",
    "    'estad'\n",
    "}\n",
    "\n",
    "def remover_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remueve las stopwords de una lista de tokens.\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token not in STOPWORDS]\n",
    "\n",
    "def filtrar_palabras_raras(tokens, umbral=5):\n",
    "    \"\"\"\n",
    "    Filtra las palabras que aparecen menos de 'umbral' veces.\n",
    "    \"\"\"\n",
    "    contador = Counter(tokens)\n",
    "    vocabulario = {palabra for palabra, freq in contador.items() if freq >= umbral}\n",
    "    tokens_filtrados = [token for token in tokens if token in vocabulario]\n",
    "    return tokens_filtrados, vocabulario\n",
    "\n",
    "def preprocesar_documentos(documentos, umbral=5):\n",
    "    \"\"\"\n",
    "    Preprocesa cada documento: tokenización, stemming, remoción de stopwords y filtrado.\n",
    "    \"\"\"\n",
    "    documentos_tokens = []\n",
    "    contador_total = Counter()\n",
    "    for doc in documentos:\n",
    "        tokens = tokenizer(doc)\n",
    "        tokens = aplicar_stemming(tokens)\n",
    "        tokens = remover_stopwords(tokens)\n",
    "        documentos_tokens.append(tokens)\n",
    "        contador_total.update(tokens)\n",
    "    # Filtrar palabras raras\n",
    "    vocabulario = {palabra for palabra, freq in contador_total.items() if freq >= umbral}\n",
    "    if not vocabulario:\n",
    "        print(\"Advertencia: El vocabulario está vacío después del filtrado. Considera reducir el umbral.\")\n",
    "    documentos_tokens_filtrados = [\n",
    "        [token for token in tokens if token in vocabulario]\n",
    "        for tokens in documentos_tokens\n",
    "    ]\n",
    "    return documentos_tokens_filtrados, vocabulario\n",
    "\n",
    "\n",
    "# 2. Latent Semantic Analysis (LSA)\n",
    "\n",
    "def calcular_tf(documentos_tokens):\n",
    "    \"\"\"\n",
    "    Calcula el término frecuencia (TF) para cada documento.\n",
    "    \"\"\"\n",
    "    tf = []\n",
    "    for tokens in documentos_tokens:\n",
    "        contador = Counter(tokens)\n",
    "        tf_doc = {term: count for term, count in contador.items()}\n",
    "        tf.append(tf_doc)\n",
    "    return tf\n",
    "\n",
    "def calcular_idf(documentos_tokens, vocabulario):\n",
    "    \"\"\"\n",
    "    Calcula el inverso de la frecuencia de documentos (IDF) para cada término.\n",
    "    \"\"\"\n",
    "    N = len(documentos_tokens)\n",
    "    df = Counter()\n",
    "    for tokens in documentos_tokens:\n",
    "        tokens_unicos = set(tokens)\n",
    "        for term in tokens_unicos:\n",
    "            df[term] += 1\n",
    "    idf = {}\n",
    "    for term in vocabulario:\n",
    "        df_term = df.get(term, 0)\n",
    "        idf[term] = math.log((N + 1) / (df_term + 1)) + 1  # Evitar división por cero\n",
    "    return idf\n",
    "\n",
    "def construir_matriz_tf_idf(documentos_tokens, vocabulario):\n",
    "    \"\"\"\n",
    "    Construye la matriz TF-IDF.\n",
    "    Retorna una matriz dispersa representada como una lista de diccionarios por término.\n",
    "    \"\"\"\n",
    "    tf = calcular_tf(documentos_tokens)\n",
    "    idf = calcular_idf(documentos_tokens, vocabulario)\n",
    "    term_to_index = {term: idx for idx, term in enumerate(sorted(vocabulario))}\n",
    "    doc_to_index = {idx: idx for idx in range(len(documentos_tokens))}\n",
    "    m = len(vocabulario)\n",
    "    n = len(documentos_tokens)\n",
    "    if m == 0 or n == 0:\n",
    "        print(\"Advertencia: La matriz TF-IDF no puede ser construida debido a dimensiones inválidas.\")\n",
    "        return [], term_to_index, doc_to_index\n",
    "    # Inicializar matriz con diccionarios\n",
    "    X = [dict() for _ in range(m)]\n",
    "    for j, tf_doc in enumerate(tf):\n",
    "        for term, freq in tf_doc.items():\n",
    "            if term in term_to_index:\n",
    "                i = term_to_index[term]\n",
    "                X[i][j] = freq * idf[term]\n",
    "    return X, term_to_index, doc_to_index\n",
    "\n",
    "def matriz_discreta_transpuesta(X):\n",
    "    \"\"\"\n",
    "    Calcula la transpuesta de una matriz dispersa.\n",
    "    \"\"\"\n",
    "    if not X:\n",
    "        return []\n",
    "    num_cols = max((max(fila.keys()) if fila else -1) for fila in X) + 1\n",
    "    Xt = [dict() for _ in range(num_cols)]\n",
    "    for i, fila in enumerate(X):\n",
    "        for j, valor in fila.items():\n",
    "            Xt[j][i] = valor\n",
    "    return Xt\n",
    "\n",
    "def multiplicar_matriz_discreta_vector(X, v):\n",
    "    \"\"\"\n",
    "    Multiplica una matriz dispersa X con un vector v.\n",
    "    X: lista de diccionarios (filas)\n",
    "    v: lista\n",
    "    \"\"\"\n",
    "    resultado = []\n",
    "    for fila in X:\n",
    "        suma = 0.0\n",
    "        for j, valor in fila.items():\n",
    "            if j < len(v):\n",
    "                suma += valor * v[j]\n",
    "        resultado.append(suma)\n",
    "    return resultado\n",
    "\n",
    "def vector_norm(v):\n",
    "    \"\"\"\n",
    "    Calcula la norma Euclidiana de un vector.\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([x**2 for x in v]))\n",
    "\n",
    "def multiplicar_vector_matriz(Xt, v):\n",
    "    \"\"\"\n",
    "    Multiplica una matriz transpuesta dispersa Xt con un vector v.\n",
    "    Xt: lista de diccionarios (columnas de la original)\n",
    "    v: lista\n",
    "    \"\"\"\n",
    "    resultado = [0.0 for _ in range(len(Xt))]\n",
    "    for j, columna in enumerate(Xt):\n",
    "        for i, valor in columna.items():\n",
    "            if i < len(v):\n",
    "                resultado[j] += valor * v[i]\n",
    "    return resultado\n",
    "\n",
    "def obtener_principal_autovector(X, num_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Calcula el principal autovector y autovalor de la matriz X^T X utilizando el método de potencia.\n",
    "    X: lista de diccionarios (filas)\n",
    "    \"\"\"\n",
    "    Xt = matriz_discreta_transpuesta(X)\n",
    "    if not Xt:\n",
    "        print(\"Advertencia: La matriz transpuesta está vacía.\")\n",
    "        return None, None\n",
    "    n = len(Xt)\n",
    "    if n == 0:\n",
    "        print(\"Advertencia: El número de documentos es cero.\")\n",
    "        return None, None\n",
    "    # Inicializar vector aleatorio\n",
    "    random.seed(42)  # Para reproducibilidad\n",
    "    v = [random.random() for _ in range(n)]\n",
    "    # Normalizar\n",
    "    norm = vector_norm(v)\n",
    "    if norm == 0:\n",
    "        print(\"Advertencia: Vector inicial tiene norma cero.\")\n",
    "        return None, None\n",
    "    v = [x / norm for x in v]\n",
    "    for iteracion in range(num_iter):\n",
    "        # v_new = X^T * (X * v)\n",
    "        Xv = multiplicar_matriz_discreta_vector(X, v)\n",
    "        v_new = multiplicar_vector_matriz(Xt, Xv)\n",
    "        norm = vector_norm(v_new)\n",
    "        if norm == 0:\n",
    "            print(\"Advertencia: Vector resultante tiene norma cero durante la iteración.\")\n",
    "            return None, None\n",
    "        v_new = [x / norm for x in v_new]\n",
    "        # Verificar convergencia\n",
    "        diff = sum([abs(x - y) for x, y in zip(v, v_new)])\n",
    "        if diff < tol:\n",
    "            print(f\"Convergencia alcanzada en iteración {iteracion+1}\")\n",
    "            break\n",
    "        v = v_new\n",
    "    else:\n",
    "        print(\"Advertencia: El método de potencia no ha convergido dentro del número máximo de iteraciones.\")\n",
    "    # Calcular autovalor\n",
    "    Xv = multiplicar_matriz_discreta_vector(X, v)\n",
    "    autovalor = sum([x * y for x, y in zip(v, Xv)])\n",
    "    return autovalor, v\n",
    "\n",
    "def reducir_dimensionalidad(U, Sigma, V, k):\n",
    "    \"\"\"\n",
    "    Reduce la dimensionalidad a 'k' dimensiones.\n",
    "    \"\"\"\n",
    "    U_k = U[:k]\n",
    "    Sigma_k = Sigma[:k]\n",
    "    V_k = V[:k]\n",
    "    return U_k, Sigma_k, V_k\n",
    "\n",
    "def proyectar(U_k, Sigma_k, V_k, k):\n",
    "    \"\"\"\n",
    "    Proyecta la matriz original en el espacio reducido.\n",
    "    \"\"\"\n",
    "    # En una implementación completa, se calcularían U_k, Sigma_k y V_k\n",
    "    # Aquí, como simplificación, se considera solo el primer componente\n",
    "    X_k = []\n",
    "    for i in range(len(U_k)):\n",
    "        fila = []\n",
    "        for j in range(len(V_k)):\n",
    "            fila.append(U_k[i] * Sigma_k * V_k[j])\n",
    "        X_k.append(fila)\n",
    "    return X_k\n",
    "\n",
    "\n",
    "# 3. Ejecución completa de LSA\n",
    "\n",
    "def main():\n",
    "    # Ruta al archivo del corpus\n",
    "    ruta_corpus = 'corpus1.txt'  # Reemplazar con la ruta real\n",
    "    \n",
    "    # Leer el corpus\n",
    "    documentos = leer_corpus(ruta_corpus)\n",
    "    print(f\"Total de documentos leídos: {len(documentos)}\")\n",
    "    \n",
    "    if not documentos:\n",
    "        print(\"Error: El corpus está vacío. Por favor, proporciona un corpus válido.\")\n",
    "        return\n",
    "    \n",
    "    # Preprocesar documentos\n",
    "    print(\"Preprocesando documentos...\")\n",
    "    documentos_tokens, vocabulario = preprocesar_documentos(documentos, umbral=2)  # Umbral reducido para prueba\n",
    "    print(f\"Vocabulario después de filtrado: {len(vocabulario)} términos\")\n",
    "    \n",
    "    if not vocabulario:\n",
    "        print(\"Error: El vocabulario está vacío después del preprocesamiento. Ajusta el umbral o revisa el corpus.\")\n",
    "        return\n",
    "    \n",
    "    # Construcción de la matriz TF-IDF\n",
    "    print(\"\\nConstruyendo la matriz TF-IDF...\")\n",
    "    X, term_to_index, doc_to_index = construir_matriz_tf_idf(documentos_tokens, vocabulario)\n",
    "    if not X:\n",
    "        print(\"Error: La matriz TF-IDF no se pudo construir.\")\n",
    "        return\n",
    "    print(f\"Matriz TF-IDF construida con dimensiones: {len(X)} términos x {len(documentos_tokens)} documentos\")\n",
    "    \n",
    "    # Implementación de SVD (Método de Potencia)\n",
    "    print(\"\\nCalculando el principal autovector y autovalor usando el método de potencia...\")\n",
    "    autovalor, autovector = obtener_principal_autovector(X)\n",
    "    if autovalor is None or autovector is None:\n",
    "        print(\"Error: No se pudo calcular el principal autovector y autovalor.\")\n",
    "        return\n",
    "    print(f\"Principal Autovalor: {autovalor}\")\n",
    "    print(f\"Principal Autovector (primeros 10 elementos): {autovector[:10]}\")\n",
    "    \n",
    "    # Reducción de dimensionalidad a k=1 (puedes ajustar 'k' según tus necesidades)\n",
    "    k = 1\n",
    "    print(f\"\\nReduciendo la dimensionalidad a k={k}...\")\n",
    "    \n",
    "    # Seleccionar los primeros k elementos de V_k (autovector)\n",
    "    V_k = autovector[:k]  # Lista de k elementos\n",
    "    Sigma_k = autovalor\n",
    "    \n",
    "    # Calcular U_k = (X * V_k) / Sigma_k\n",
    "    # Multiplicar X por V_k\n",
    "    U_k = multiplicar_matriz_discreta_vector(X, V_k)  # Lista de m elementos\n",
    "    # Normalizar U_k\n",
    "    U_k = [x / Sigma_k for x in U_k]\n",
    "    \n",
    "    # Proyectar términos y documentos en el espacio reducido\n",
    "    print(\"\\nProyectando términos y documentos en el espacio reducido...\")\n",
    "    \n",
    "    # Proyección de términos: U_k * Sigma_k\n",
    "    terminos_proyectados = [u * Sigma_k for u in U_k]  # Lista de m elementos\n",
    "    \n",
    "    # Proyección de documentos: V_k * Sigma_k\n",
    "    documentos_proyectados = [v * Sigma_k for v in V_k]  # Lista de k elementos\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"Documentos proyectados en {k} dimensión(es):\")\n",
    "    for idx, peso in enumerate(documentos_proyectados):\n",
    "        print(f\"Documento {idx+1}: {peso}\")\n",
    "    \n",
    "    print(f\"\\nTérminos proyectados en {k} dimensión(es):\")\n",
    "    for term, peso in zip(sorted(vocabulario), terminos_proyectados):\n",
    "        print(f\"Término '{term}': {peso}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5117e42",
   "metadata": {},
   "source": [
    "Esta implementación utiliza el método de potencia para calcular el principal autovector y autovalor, lo cual es adecuado para conjuntos de datos pequeños. Sin embargo, no es eficiente para matrices grandes.\n",
    "Para múltiples componentes ($k > 1$), se necesitaría una implementación más avanzada que calcule múltiples autovectores y autovalores, posiblemente utilizando técnicas de deflación.\n",
    "\n",
    "La matriz TF-IDF se representa como una lista de diccionarios, donde cada diccionario corresponde a una fila (término) y mapea índices de documentos a pesos. Esto optimiza el almacenamiento y las operaciones al evitar almacenar ceros.\n",
    "Las funciones de multiplicación de matrices y vectores están adaptadas para manejar esta representación dispersa.\n",
    "\n",
    "Puedes ajustar el valor de `k` para reducir la dimensionalidad a más dimensiones si implementas la capacidad de manejar múltiples componentes.\n",
    "Para mejorar la precisión y eficiencia, considera implementar técnicas adicionales como la normalización de la matriz TF-IDF o el uso de otros métodos de descomposición.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30742e",
   "metadata": {},
   "source": [
    "### Word2vec\n",
    "\n",
    "Código completo de Word2vec implementado en Python utilizando únicamente las librerías estándar y numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "# 1. Definición de Stopwords en español\n",
    "STOPWORDS = {\n",
    "    'de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por',\n",
    "    'un', 'para', 'con', 'no', 'una', 'su', 'al', 'es', 'lo', 'como', 'más',\n",
    "    'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre',\n",
    "    'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde',\n",
    "    'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni',\n",
    "    'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes',\n",
    "    'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa',\n",
    "    'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella',\n",
    "    'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te',\n",
    "    'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os',\n",
    "    'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo',\n",
    "    'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras',\n",
    "    'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy',\n",
    "    'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos',\n",
    "    'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis',\n",
    "    'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían',\n",
    "    'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve',\n",
    "    'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera',\n",
    "    'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese',\n",
    "    'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando',\n",
    "    'estado', 'estada', 'estados', 'estadas', 'estad'\n",
    "}\n",
    "\n",
    "# 2. Funciones de preprocesamiento\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokeniza el texto dividiéndolo en palabras, manejando puntuación y mayúsculas.\n",
    "    \"\"\"\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Reemplazar caracteres de puntuación por espacios\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "    # Reemplazar dígitos por espacios\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    # Dividir en palabras\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def simple_stem(word):\n",
    "    \"\"\"\n",
    "    Realiza un stemming básico eliminando sufijos comunes en español.\n",
    "    Esta implementación es muy simplificada y puede no ser precisa.\n",
    "    \"\"\"\n",
    "    suffixes = ['ando', 'iendo', 'ar', 'er', 'ir', 'ado', 'ido', 'ción', 'sión', 'mente']\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "def preprocess_corpus(file_path, min_freq=1):\n",
    "    \"\"\"\n",
    "    Preprocesa el corpus:\n",
    "    - Tokenización\n",
    "    - Stemming\n",
    "    - Remoción de stopwords\n",
    "    - Filtrado de palabras raras\n",
    "    \"\"\"\n",
    "    word_freq = defaultdict(int)\n",
    "    processed_tokens = []\n",
    "\n",
    "    print(\"Iniciando el preprocesamiento del corpus...\")\n",
    "\n",
    "    # Primera pasada: contar frecuencias\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = tokenize(line)\n",
    "            tokens = [simple_stem(token) for token in tokens]\n",
    "            tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "            for token in tokens:\n",
    "                word_freq[token] += 1\n",
    "\n",
    "    print(\"Frecuencias de palabras calculadas.\")\n",
    "\n",
    "    # Filtrar palabras raras\n",
    "    vocab = {word for word, freq in word_freq.items() if freq >= min_freq}\n",
    "    print(f\"Vocabulario filtrado a {len(vocab)} palabras.\")\n",
    "\n",
    "    # Segunda pasada: recolectar tokens válidos\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = tokenize(line)\n",
    "            tokens = [simple_stem(token) for token in tokens]\n",
    "            tokens = [token for token in tokens if token not in STOPWORDS and token in vocab]\n",
    "            processed_tokens.extend(tokens)\n",
    "\n",
    "    print(\"Preprocesamiento completado.\")\n",
    "    return processed_tokens, vocab\n",
    "\n",
    "# 3. Construcción del vocabulario y generación de datos de entrenamiento\n",
    "\n",
    "def build_vocab(tokens):\n",
    "    \"\"\"\n",
    "    Construye un mapeo de palabras a índices y viceversa.\n",
    "    \"\"\"\n",
    "    word_to_ix = {word: idx for idx, word in enumerate(set(tokens))}\n",
    "    ix_to_word = {idx: word for word, idx in word_to_ix.items()}\n",
    "    return word_to_ix, ix_to_word\n",
    "\n",
    "def generate_training_data_cbow(tokens, word_to_ix, window_size=2):\n",
    "    \"\"\"\n",
    "    Genera pares de (contexto, palabra objetivo) para CBOW.\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    for i in range(len(tokens)):\n",
    "        target = word_to_ix[tokens[i]]\n",
    "        context = []\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j != 0 and 0 <= i + j < len(tokens):\n",
    "                context.append(word_to_ix[tokens[i + j]])\n",
    "        training_data.append((context, target))\n",
    "    return training_data\n",
    "\n",
    "def generate_training_data_skipgram(tokens, word_to_ix, window_size=2):\n",
    "    \"\"\"\n",
    "    Genera pares de (palabra objetivo, contexto) para Skip-Gram.\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    for i in range(len(tokens)):\n",
    "        target = word_to_ix[tokens[i]]\n",
    "        context = []\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j != 0 and 0 <= i + j < len(tokens):\n",
    "                context.append(word_to_ix[tokens[i + j]])\n",
    "        training_data.append((target, context))\n",
    "    return training_data\n",
    "\n",
    "# 4. Implementación de los modelos Word2vec\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Inicializa los vectores de embeddings.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # Vectores de entrada (W)\n",
    "        self.W = np.random.uniform(-0.5 / embedding_dim, 0.5 / embedding_dim, (vocab_size, embedding_dim))\n",
    "        # Vectores de salida (W')\n",
    "        self.W_prime = np.random.uniform(-0.5 / embedding_dim, 0.5 / embedding_dim, (vocab_size, embedding_dim))\n",
    "    \n",
    "    def forward(self, context_indices):\n",
    "        \"\"\"\n",
    "        Calcula la representación del contexto.\n",
    "        \"\"\"\n",
    "        # Promedio de los vectores de contexto\n",
    "        v_context = np.mean(self.W[context_indices], axis=0)\n",
    "        return v_context\n",
    "    \n",
    "    def predict(self, v_context):\n",
    "        \"\"\"\n",
    "        Calcula las puntuaciones para todas las palabras en el vocabulario.\n",
    "        \"\"\"\n",
    "        scores = np.dot(self.W_prime, v_context)\n",
    "        return scores\n",
    "    \n",
    "    def softmax(self, scores):\n",
    "        \"\"\"\n",
    "        Calcula la distribución de probabilidad usando softmax.\n",
    "        \"\"\"\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Estabilidad numérica\n",
    "        return exp_scores / np.sum(exp_scores)\n",
    "    \n",
    "    def train(self, training_data, epochs=5, learning_rate=0.05):\n",
    "        \"\"\"\n",
    "        Entrena el modelo CBOW usando descenso de gradiente.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for context, target in training_data:\n",
    "                # Forward pass\n",
    "                v_context = self.forward(context)\n",
    "                scores = self.predict(v_context)\n",
    "                probs = self.softmax(scores)\n",
    "                \n",
    "                # Calculando la pérdida (-log probabilidad del objetivo)\n",
    "                loss += -math.log(probs[target] + 1e-7)\n",
    "                \n",
    "                # Gradiente de la pérdida respecto a scores\n",
    "                d_scores = probs.copy()\n",
    "                d_scores[target] -= 1  # y_hat - y\n",
    "                \n",
    "                # Gradiente respecto a W'\n",
    "                dW_prime = np.outer(d_scores, v_context)\n",
    "                \n",
    "                # Gradiente respecto a v_context\n",
    "                dv_context = np.dot(self.W_prime.T, d_scores)\n",
    "                dv_context /= len(context)  # Promedio\n",
    "                \n",
    "                # Actualización de W' y W\n",
    "                self.W_prime -= learning_rate * dW_prime\n",
    "                for idx in context:\n",
    "                    self.W[idx] -= learning_rate * dv_context\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Pérdida: {loss:.4f}\")\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim, K=5):\n",
    "        \"\"\"\n",
    "        Inicializa los vectores de embeddings.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.K = K  # Número de muestras negativas\n",
    "        # Vectores de entrada (W)\n",
    "        self.W = np.random.uniform(-0.5 / embedding_dim, 0.5 / embedding_dim, (vocab_size, embedding_dim))\n",
    "        # Vectores de salida (W')\n",
    "        self.W_prime = np.random.uniform(-0.5 / embedding_dim, 0.5 / embedding_dim, (vocab_size, embedding_dim))\n",
    "        # Pre-computar la distribución para Negative Sampling\n",
    "        self.word_freq = defaultdict(int)\n",
    "        self.neg_sampling_prob = None\n",
    "    \n",
    "    def build_negative_sampling_distribution(self, tokens, word_to_ix):\n",
    "        \"\"\"\n",
    "        Construye una distribución para muestreo negativo basada en la frecuencia de palabras.\n",
    "        Utiliza la fórmula de unigram raised to the 3/4 power.\n",
    "        \"\"\"\n",
    "        for token in tokens:\n",
    "            self.word_freq[word_to_ix[token]] += 1\n",
    "        # Calcular la frecuencia normalizada\n",
    "        self.neg_sampling_prob = np.array([self.word_freq[i]**0.75 for i in range(self.vocab_size)])\n",
    "        self.neg_sampling_prob /= np.sum(self.neg_sampling_prob)\n",
    "    \n",
    "    def sample_negative(self, exclude, num_samples):\n",
    "        \"\"\"\n",
    "        Muestra palabras negativas, excluyendo ciertas palabras.\n",
    "        \"\"\"\n",
    "        negatives = []\n",
    "        while len(negatives) < num_samples:\n",
    "            sampled = np.random.choice(self.vocab_size, p=self.neg_sampling_prob)\n",
    "            if sampled not in exclude:\n",
    "                negatives.append(sampled)\n",
    "        return negatives\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Calcula la función sigmoide.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, training_data, epochs=5, learning_rate=0.05):\n",
    "        \"\"\"\n",
    "        Entrena el modelo Skip-Gram con Negative Sampling.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for target, context in training_data:\n",
    "                # Para cada palabra de contexto\n",
    "                for context_word in context:\n",
    "                    # Positivo\n",
    "                    z = np.dot(self.W_prime[context_word], self.W[target])\n",
    "                    sigma = self.sigmoid(z)\n",
    "                    loss += -math.log(sigma + 1e-7)\n",
    "                    grad_z = sigma - 1\n",
    "                    # Gradientes\n",
    "                    grad_W_prime = grad_z * self.W[target]\n",
    "                    grad_W = grad_z * self.W_prime[context_word]\n",
    "                    # Actualizar W' y W\n",
    "                    self.W_prime[context_word] -= learning_rate * grad_W_prime\n",
    "                    self.W[target] -= learning_rate * grad_W\n",
    "                    \n",
    "                    # Negative Sampling\n",
    "                    negatives = self.sample_negative(exclude={context_word}, num_samples=self.K)\n",
    "                    for negative in negatives:\n",
    "                        z_neg = np.dot(self.W_prime[negative], self.W[target])\n",
    "                        sigma_neg = self.sigmoid(-z_neg)\n",
    "                        loss += -math.log(sigma_neg + 1e-7)\n",
    "                        grad_z_neg = sigma_neg  # derivada de -log(sigmoid(-z_neg)) respecto a z_neg\n",
    "                        # Gradientes\n",
    "                        grad_W_prime_neg = grad_z_neg * self.W[target]\n",
    "                        grad_W_neg = grad_z_neg * self.W_prime[negative]\n",
    "                        # Actualizar W' y W\n",
    "                        self.W_prime[negative] -= learning_rate * grad_W_prime_neg\n",
    "                        self.W[target] -= learning_rate * grad_W_neg\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Pérdida: {loss:.4f}\")\n",
    "\n",
    "# 5. Función principal para ejecutar el modelo\n",
    "\n",
    "def main():\n",
    "    # 1. Ruta al archivo de corpus\n",
    "    corpus_path = 'corpus_es.txt'  # Asegúrate de tener este archivo en el mismo directorio\n",
    "    \n",
    "    # 2. Preprocesamiento\n",
    "    tokens, vocabulario = preprocess_corpus(corpus_path, min_freq=1)  # Cambiado a min_freq=1 para el ejemplo\n",
    "    print(f\"Total de tokens después del preprocesamiento: {len(tokens)}\")\n",
    "    print(f\"Tamaño del vocabulario: {len(vocabulario)}\")\n",
    "    \n",
    "    # 3. Construir mapeos\n",
    "    word_to_ix, ix_to_word = build_vocab(tokens)\n",
    "    \n",
    "    # 4. Generar datos de entrenamiento para CBOW\n",
    "    training_data_cbow = generate_training_data_cbow(tokens, word_to_ix, window_size=2)\n",
    "    \n",
    "    # 5. Entrenar CBOW\n",
    "    print(\"\\nEntrenando el modelo CBOW...\")\n",
    "    cbow_model = CBOW(vocab_size=len(word_to_ix), embedding_dim=100)\n",
    "    cbow_model.train(training_data_cbow, epochs=5, learning_rate=0.05)\n",
    "    \n",
    "    # 6. Generar datos de entrenamiento para Skip-Gram\n",
    "    training_data_skipgram = generate_training_data_skipgram(tokens, word_to_ix, window_size=2)\n",
    "    \n",
    "    # 7. Entrenar Skip-Gram con Negative Sampling\n",
    "    print(\"\\nEntrenando el modelo Skip-Gram con Negative Sampling...\")\n",
    "    skipgram_model = SkipGram(vocab_size=len(word_to_ix), embedding_dim=100, K=5)\n",
    "    skipgram_model.build_negative_sampling_distribution(tokens, word_to_ix)\n",
    "    skipgram_model.train(training_data_skipgram, epochs=5, learning_rate=0.05)\n",
    "    \n",
    "    # 8. Guardar los embeddings\n",
    "    np.save('cbow_embeddings_W.npy', cbow_model.W)\n",
    "    np.save('cbow_embeddings_W_prime.npy', cbow_model.W_prime)\n",
    "    np.save('skipgram_embeddings_W.npy', skipgram_model.W)\n",
    "    np.save('skipgram_embeddings_W_prime.npy', skipgram_model.W_prime)\n",
    "    print(\"\\nEmbeddings guardados exitosamente.\")\n",
    "    \n",
    "    # 9. Guardar el mapeo de índices a palabras\n",
    "    with open('ix_to_word.txt', 'w', encoding='utf-8') as f:\n",
    "        for idx in sorted(ix_to_word.keys()):\n",
    "            f.write(f\"{idx}\\t{ix_to_word[idx]}\\n\")\n",
    "    print(\"Mapa de índices a palabras guardado exitosamente.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90b02f",
   "metadata": {},
   "source": [
    "El código presenta una versión donde el `min_freq` se establece en 1. Esto garantiza que incluso con el corpus de ejemplo pequeño, obtendrás un vocabulario no vacío. \n",
    "\n",
    "**Observación:**\n",
    "\n",
    "Un problema a afrontar con un corpus de ejemplo pequeño y un umbral de frecuencia alto (`min_freq=5`), es que ninguna palabra cumplió con el criterio para ser incluida en el vocabulario. Al reducir `min_freq` a 1 o ampliar el corpus para que las palabras clave se repitan al menos 5 veces, se resuelve el problema y se permite que el modelo entrenara correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ac51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# 1. Definición de stopwords\n",
    "\n",
    "STOPWORDS_ES = {\n",
    "    'de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se',\n",
    "    'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al',\n",
    "    'es', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o',\n",
    "    'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy',\n",
    "    'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde',\n",
    "    'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno',\n",
    "    'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos',\n",
    "    'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo',\n",
    "    'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho',\n",
    "    'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar',\n",
    "    'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú',\n",
    "    'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosostros',\n",
    "    'vosostras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo',\n",
    "    'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas',\n",
    "    'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro',\n",
    "    'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy',\n",
    "    'estás', 'está', 'estamos', 'estáis', 'están', 'esté',\n",
    "    'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás',\n",
    "    'estará', 'estaremos', 'estaréis', 'estarán', 'estaría',\n",
    "    'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba',\n",
    "    'estabas', 'estábamos', 'estabais', 'estaban', 'estuve',\n",
    "    'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron',\n",
    "    'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais',\n",
    "    'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos',\n",
    "    'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada',\n",
    "    'estados', 'estadas', 'estad'\n",
    "}\n",
    "\n",
    "\n",
    "# 2. Funciones de preprocesamiento\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokeniza el texto: convierte a minúsculas, elimina puntuación y divide en palabras.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Reemplazar caracteres no alfabéticos por espacios\n",
    "    text = re.sub(r'[^a-záéíóúüñ\\s]', ' ', text)\n",
    "    # Dividir por espacios\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def simple_stem(word):\n",
    "    \"\"\"\n",
    "    Realiza un stemming muy básico eliminando sufijos comunes en español.\n",
    "    \"\"\"\n",
    "    suffixes = ['ando', 'iendo', 'ar', 'er', 'ir', 'ado', 'ido', 'es', 'as', 'os', 'a', 'e', 'o']\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "def preprocess_corpus(file_path, stopwords, min_freq=5):\n",
    "    \"\"\"\n",
    "    Preprocesa el corpus: tokenización, stemming, eliminación de stopwords y filtrado de palabras raras.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo de texto del corpus.\n",
    "        stopwords (set): Conjunto de palabras vacías en español.\n",
    "        min_freq (int): Umbral de frecuencia mínima para incluir una palabra en el vocabulario.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de palabras preprocesadas.\n",
    "        dict: Mapeo de palabras a índices.\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    corpus = []\n",
    "    \n",
    "    print(\"Iniciando preprocesamiento del corpus...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            tokens = tokenize(line)\n",
    "            tokens = [simple_stem(token) for token in tokens]\n",
    "            tokens = [token for token in tokens if token not in stopwords]\n",
    "            corpus.extend(tokens)\n",
    "            word_counts.update(tokens)\n",
    "            \n",
    "            if line_num % 10000 == 0:\n",
    "                print(f\"Procesadas {line_num} líneas...\")\n",
    "    \n",
    "    print(\"Filtrando palabras raras...\")\n",
    "    # Filtrar palabras raras\n",
    "    corpus = [word for word in corpus if word_counts[word] >= min_freq]\n",
    "    \n",
    "    # Crear vocabulario\n",
    "    vocab = {word: idx for idx, (word, count) in enumerate(word_counts.items()) if count >= min_freq}\n",
    "    \n",
    "    print(f'Tamaño del vocabulario después del filtrado: {len(vocab)}')\n",
    "    return corpus, vocab\n",
    "\n",
    "\n",
    "# 3. Construcción de la matriz de co-ocurrencia\n",
    "\n",
    "\n",
    "def build_cooccurrence_matrix(corpus, vocab, window_size=5):\n",
    "    \"\"\"\n",
    "    Construye la matriz de co-ocurrencia.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list): Lista de palabras preprocesadas.\n",
    "        vocab (dict): Mapeo de palabras a índices.\n",
    "        window_size (int): Tamaño de la ventana de contexto.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Matriz de co-ocurrencia representada como diccionario de diccionarios.\n",
    "    \"\"\"\n",
    "    cooc_matrix = defaultdict(lambda: defaultdict(int))\n",
    "    corpus_length = len(corpus)\n",
    "    \n",
    "    print(\"Construyendo la matriz de co-ocurrencia...\")\n",
    "    for idx, word in enumerate(corpus):\n",
    "        word_id = vocab[word]\n",
    "        start = max(idx - window_size, 0)\n",
    "        end = min(idx + window_size + 1, corpus_length)\n",
    "        for i in range(start, end):\n",
    "            if i != idx:\n",
    "                context_word = corpus[i]\n",
    "                context_id = vocab[context_word]\n",
    "                cooc_matrix[word_id][context_id] += 1\n",
    "                \n",
    "        if (idx + 1) % 100000 == 0:\n",
    "            print(f\"Procesadas {idx + 1} palabras del corpus...\")\n",
    "    \n",
    "    print(\"Matriz de co-ocurrencia construida.\")\n",
    "    return cooc_matrix\n",
    "\n",
    "\n",
    "# 4. Implementación del modelo GloVe\n",
    "\n",
    "\n",
    "def initialize_parameters(vocab_size, embedding_dim):\n",
    "    \"\"\"\n",
    "    Inicializa los vectores de palabras, vectores de contexto y sesgos.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): Número de palabras en el vocabulario.\n",
    "        embedding_dim (int): Dimensionalidad de los embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        dict, dict, list, list: Vectores de palabras, vectores de contexto, sesgos de palabras, sesgos de contexto.\n",
    "    \"\"\"\n",
    "    W = {i: [random.uniform(-0.5, 0.5) for _ in range(embedding_dim)] for i in range(vocab_size)}\n",
    "    W_tilde = {i: [random.uniform(-0.5, 0.5) for _ in range(embedding_dim)] for i in range(vocab_size)}\n",
    "    b = [0.0 for _ in range(vocab_size)]\n",
    "    b_tilde = [0.0 for _ in range(vocab_size)]\n",
    "    return W, W_tilde, b, b_tilde\n",
    "\n",
    "def weighting_function(x, x_max=100, alpha=0.75):\n",
    "    if x < x_max:\n",
    "        return (x / x_max) ** alpha\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def train_glove(cooc_matrix, vocab_size, embedding_dim=50, x_max=100, alpha=0.75, learning_rate=0.05, epochs=25):\n",
    "    \"\"\"\n",
    "    Entrena el modelo GloVe.\n",
    "    \n",
    "    Args:\n",
    "        cooc_matrix (dict): Matriz de co-ocurrencia.\n",
    "        vocab_size (int): Tamaño del vocabulario.\n",
    "        embedding_dim (int): Dimensionalidad de los embeddings.\n",
    "        x_max (int): Parámetro para la función de ponderación.\n",
    "        alpha (float): Parámetro para la función de ponderación.\n",
    "        learning_rate (float): Tasa de aprendizaje.\n",
    "        epochs (int): Número de épocas de entrenamiento.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Vectores de palabras entrenados.\n",
    "    \"\"\"\n",
    "    W, W_tilde, b, b_tilde = initialize_parameters(vocab_size, embedding_dim)\n",
    "    \n",
    "    print(\"Iniciando entrenamiento del modelo GloVe...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_cost = 0.0\n",
    "        count = 0\n",
    "        for i, context_dict in cooc_matrix.items():\n",
    "            for j, X_ij in context_dict.items():\n",
    "                f_X_ij = weighting_function(X_ij, x_max, alpha)\n",
    "                log_X_ij = math.log(X_ij)\n",
    "                \n",
    "                # Predicción\n",
    "                dot = sum([W[i][k] * W_tilde[j][k] for k in range(embedding_dim)])\n",
    "                prediction = dot + b[i] + b_tilde[j]\n",
    "                \n",
    "                # Error\n",
    "                error = prediction - log_X_ij\n",
    "                cost = f_X_ij * (error ** 2)\n",
    "                total_cost += cost\n",
    "                count +=1\n",
    "                \n",
    "                # Gradientes\n",
    "                grad_common = 2 * f_X_ij * error\n",
    "                for k in range(embedding_dim):\n",
    "                    W[i][k] -= learning_rate * (grad_common * W_tilde[j][k])\n",
    "                    W_tilde[j][k] -= learning_rate * (grad_common * W[i][k])\n",
    "                b[i] -= learning_rate * grad_common\n",
    "                b_tilde[j] -= learning_rate * grad_common\n",
    "                \n",
    "            if i % 10000 == 0:\n",
    "                print(f\"Procesadas {i} palabras en la época {epoch}...\")\n",
    "        \n",
    "        avg_cost = total_cost / count if count != 0 else 0\n",
    "        print(f'Época {epoch}/{epochs}, Costo Promedio: {avg_cost}')\n",
    "    \n",
    "    # Combinar W y W_tilde para obtener los embeddings finales\n",
    "    embeddings = {}\n",
    "    for i in range(vocab_size):\n",
    "        embeddings[i] = [W[i][k] + W_tilde[i][k] for k in range(embedding_dim)]\n",
    "    \n",
    "    print(\"Entrenamiento completado.\")\n",
    "    return embeddings\n",
    "\n",
    "# 5. Evaluación de los embeddings\n",
    "\n",
    "\n",
    "def find_analogy(word_a, word_b, word_c, embeddings, vocab, index_to_word, top_n=1):\n",
    "    \"\"\"\n",
    "    Resuelve analogías del tipo: word_a es a word_b como word_c es a ?\n",
    "    \n",
    "    Args:\n",
    "        word_a (str): Primera palabra de la analogía.\n",
    "        word_b (str): Segunda palabra de la analogía.\n",
    "        word_c (str): Tercera palabra de la analogía.\n",
    "        embeddings (dict): Vectores de palabras.\n",
    "        vocab (dict): Mapeo de palabras a índices.\n",
    "        index_to_word (dict): Mapeo de índices a palabras.\n",
    "        top_n (int): Número de resultados a retornar.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de palabras que completan la analogía.\n",
    "    \"\"\"\n",
    "    if word_a not in vocab or word_b not in vocab or word_c not in vocab:\n",
    "        print('Una de las palabras no está en el vocabulario.')\n",
    "        return []\n",
    "    \n",
    "    vec_a = embeddings[vocab[word_a]]\n",
    "    vec_b = embeddings[vocab[word_b]]\n",
    "    vec_c = embeddings[vocab[word_c]]\n",
    "    \n",
    "    # Calcular el vector objetivo\n",
    "    target_vec = [vec_b[i] - vec_a[i] + vec_c[i] for i in range(len(vec_a))]\n",
    "    \n",
    "    # Calcular similitud coseno\n",
    "    similarities = {}\n",
    "    norm_target = math.sqrt(sum([x**2 for x in target_vec]))\n",
    "    for idx, vec in embeddings.items():\n",
    "        numerator = sum([target_vec[i] * vec[i] for i in range(len(vec))])\n",
    "        norm_vec = math.sqrt(sum([x**2 for x in vec]))\n",
    "        if norm_vec == 0:\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = numerator / (norm_target * norm_vec)\n",
    "        similarities[idx] = similarity\n",
    "    \n",
    "    # Ordenar y retornar las mejores coincidencias\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # Excluir las palabras de entrada\n",
    "    results = []\n",
    "    for idx, sim in sorted_similarities:\n",
    "        if idx not in [vocab[word_a], vocab[word_b], vocab[word_c]]:\n",
    "            results.append(index_to_word[idx])\n",
    "            if len(results) == top_n:\n",
    "                break\n",
    "    return results\n",
    "\n",
    "# 6. Función principal\n",
    "\n",
    "\n",
    "def main():\n",
    "    corpus_file = 'corpus1.txt'  # Ruta al archivo del corpus\n",
    "    min_frequency = 1          # Umbral de frecuencia mínima\n",
    "    window_size = 5            # Tamaño de la ventana de contexto\n",
    "    embedding_dim = 50         # Dimensionalidad de los embeddings\n",
    "    epochs = 25                # Número de épocas de entrenamiento\n",
    "    \n",
    "    # Preprocesamiento del corpus\n",
    "    corpus, vocab = preprocess_corpus(corpus_file, STOPWORDS_ES, min_freq=min_frequency)\n",
    "    \n",
    "    # Construcción de la matriz de co-ocurrencia\n",
    "    cooc_matrix = build_cooccurrence_matrix(corpus, vocab, window_size=window_size)\n",
    "    \n",
    "    # Entrenamiento del modelo GloVe\n",
    "    embeddings = train_glove(cooc_matrix, len(vocab), embedding_dim=embedding_dim, epochs=epochs)\n",
    "    \n",
    "    # Crear mapeo de índices a palabras\n",
    "    index_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    # Ejemplo de analogía: 'rey' es a 'reina' como 'hombre' es a 'mujer'\n",
    "    # Nota: Asegúrate de que estas palabras existan en tu vocabulario después del preprocesamiento\n",
    "    analogy_words = ('re', 'reina', 'hombre')  # 'rey' se puede haber convertido a 're' por stemming\n",
    "    word_a, word_b, word_c = analogy_words\n",
    "    analogy_result = find_analogy(word_a, word_b, word_c, embeddings, vocab, index_to_word)\n",
    "    if analogy_result:\n",
    "        print(f\"'{word_a}' es a '{word_b}' como '{word_c}' es a '{analogy_result[0]}'\")\n",
    "    else:\n",
    "        print(\"No se pudo encontrar una palabra para completar la analogía.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b5e783",
   "metadata": {},
   "source": [
    "Este código puede no ser eficiente para corpus muy grandes como el de Wikipedia. Para manejar grandes volúmenes de datos, se recomienda optimizar el almacenamiento de la matriz de co-ocurrencia, posiblemente utilizando estructuras de datos más eficientes o almacenándola en disco.\n",
    "El entrenamiento del modelo GloVe con descenso de gradiente básico puede ser muy lento para vocabularios grandes. Implementaciones más eficientes podrían utilizar métodos de optimización avanzados como AdaGrad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
