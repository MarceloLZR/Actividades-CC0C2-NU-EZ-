{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37124f83-ce23-46f5-82a6-081f0f450082",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "BERT, acrónimo de Bidirectional Encoder Representations from Transformers, es un modelo de aprendizaje profundo para el procesamiento del lenguaje natural (NLP) desarrollado por Google en 2018. BERT se basa en la arquitectura de transformadores y ha revolucionado el campo del NLP al proporcionar un enfoque bidireccional en el preentrenamiento de modelos de lenguaje. La principal ventaja de BERT radica en su capacidad para comprender el contexto completo de una palabra al considerar tanto el texto que la precede como el que la sigue, lo que contrasta con los modelos unidireccionales anteriores que solo podían tener en cuenta un contexto parcial.\n",
    "\n",
    "Para entender BERT, es fundamental comprender la arquitectura de los transformers, en el artículo \"Attention is All You Need\" en 2017. Los transformadores son una arquitectura de red neuronal diseñada para manejar secuencias de datos, como texto natural, sin recurrir a estructuras recurrentes o convolucionales.\n",
    "\n",
    "BERT utiliza solo la parte del codificador de la arquitectura de transformadores. A diferencia de los modelos de lenguaje tradicionales que leen el texto de izquierda a derecha o de derecha a izquierda, BERT emplea un enfoque bidireccional. Esto significa que en cada paso, BERT puede considerar tanto el contexto anterior como el posterior a la palabra en cuestión, lo que te permite capturar matices más complejos del lenguaje.\n",
    "\n",
    "La implementación de BERT, como se observa en el código proporcionado, incluye varios componentes clave:\n",
    "\n",
    "**Embedding**:\n",
    "\n",
    "Transforma los IDs de tokens en vectores de embedding que contienen información sobre los tokens, sus posiciones y segmentos. Esto es esencial para que el modelo comprenda la estructura de las oraciones y las relaciones entre las palabras.\n",
    "\n",
    "**Capas del codificador**:\n",
    "\n",
    "Consisten en múltiples capas de atención y redes de alimentación directa, lo que permite al modelo capturar dependencias a largo plazo en la secuencia de entrada.\n",
    "\n",
    "**Clasificador:**\n",
    "\n",
    "Una red neuronal adicional que toma la salida del primer token ([CLS]) para tareas de clasificación, como determinar si una oración sigue lógicamente a otra.\n",
    "\n",
    "**Decodificador:**\n",
    "\n",
    "Compartido con la capa de embedding, este componente predice los tokens enmascarados durante el preentrenamiento, permitiendo al modelo aprender representaciones contextuales ricas.\n",
    "\n",
    "**Preentrenamiento y Fine-Tuning:**\n",
    "\n",
    "El entrenamiento de BERT se divide en dos fases:\n",
    "\n",
    "Preentrenamiento:\n",
    "- BERT se entrena en grandes corpus de texto sin etiquetas utilizando dos tareas: Modelado de Lenguaje Enmascarado (MLM) y Predicción de la Siguiente Oración (NSP). En MLM, ciertos tokens en la entrada se enmascaran y el modelo debe predecirlos, lo que obliga a BERT a comprender el contexto bidireccional. En NSP, el modelo aprende a predecir si dos oraciones en secuencia son coherentes.\n",
    "\n",
    "Fine-Tuning:\n",
    "\n",
    "- BERT se ajusta para tareas específicas de NLP (como clasificación de texto, etiquetado de entidades nombradas, y preguntas y respuestas) \n",
    "utilizando conjuntos de datos etiquetados más pequeños. Durante esta fase, todas las capas del modelo se entrenan conjuntamente en la tarea objetivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7073e2-52b5-41a8-99fb-ac2d32b751e7",
   "metadata": {},
   "source": [
    "### Código de ejemplo \n",
    "\n",
    "El siguiente código implementa un modelo BERT (Bidirectional Encoder Representations from Transformers) para dos tareas de preentrenamiento clave: el Modelado de Lenguaje Enmascarado (MLM) y la Predicción de la Siguiente Oración (NSP). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3241741-3ed2-4e99-97c5-7a6c8196e8e4",
   "metadata": {},
   "source": [
    "La clase `BERT` hereda de nn.Module y representa el modelo BERT, que se utiliza para tareas de procesamiento de lenguaje natural. BERT se basa en una arquitectura de transformers y es conocido por su capacidad para entender el contexto bidireccional en el texto.\n",
    "\n",
    "\n",
    "**Embedding**:\n",
    "\n",
    "- `self.embedding = Embedding()`: Esta instancia de la clase `Embedding` se encarga de convertir los IDs de los tokens en vectores de embedding que contienen información de posición y segmento.\n",
    "Capas del Codificador:\n",
    "\n",
    "- `self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])`: Una lista de capas del codificador (EncoderLayer). Cada una de estas capas incluye una atención multi-cabecera y una red neuronal de alimentación directa.\n",
    "\n",
    "**Clasificador**:\n",
    "\n",
    "- `self.fc = nn.Linear(d_model, d_model)`\n",
    "- `self.activ1 = nn.Tanh()`\n",
    "- `self.linear = nn.Linear(d_model, d_model)`\n",
    "- `self.activ2 = gelu`\n",
    "- `self.norm = nn.LayerNorm(d_model)`\n",
    "- `self.classifier = nn.Linear(d_model, 2)`\n",
    "\n",
    "Estas capas se utilizan para la clasificación final. `self.fc` y `self.linear` son capas lineales, `self.activ1` y `self.activ2` son funciones de activación, y `self.norm` es una capa de normalización.\n",
    "\n",
    "**Decodificador**:\n",
    "\n",
    "- `embed_weight = self.embedding.tok_embed.weight`\n",
    "- `n_vocab, n_dim = embed_weight.size()`\n",
    "- `self.decoder = nn.Linear(n_dim, n_vocab, bias=False)`\n",
    "- `self.decoder.weight = embed_weight`\n",
    "- `self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))`\n",
    "\n",
    "El decodificador comparte los pesos con la capa de embedding de tokens para predecir los tokens enmascarados. Esto ayuda a mantener la consistencia entre la entrada y la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6e7f8-4415-41b6-b196-a5625388b954",
   "metadata": {},
   "source": [
    "Para el método forward, se tiene lo siguiente: \n",
    "\n",
    "**Embeddings**:\n",
    "\n",
    "- `output = self.embedding(input_ids, segment_ids)`: Convierte los `input_ids` y `segment_ids` en embeddings utilizando la capa de `Embedding`.\n",
    "\n",
    "**Máscara de atención**:\n",
    "\n",
    "- `enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)`: Crea una máscara de atención para evitar que el modelo preste atención a los tokens de relleno (PAD).\n",
    "\n",
    "**Capa del codificador**:\n",
    "\n",
    "- `for layer in self.layers: output, enc_self_attn = layer(output, enc_self_attn_mask)`: Pasa las salidas a través de las capas del codificador, aplicando la atención y la red neuronal de alimentación directa en cada capa.\n",
    "\n",
    "**Clasificación**:\n",
    "\n",
    "- `h_pooled = self.activ1(self.fc(output[:, 0]))`: Utiliza la salida correspondiente al primer token ([CLS]) para la clasificación.\n",
    "- `logits_clsf = self.classifier(h_pooled)`: Realiza la clasificación binaria.\n",
    "\n",
    "**Predicción de tokens enmascarados**:\n",
    "\n",
    "- `masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))`: Expande las posiciones enmascaradas para que coincidan con la dimensión de la salida.\n",
    "- `h_masked = torch.gather(output, 1, masked_pos)`: Recolecta las salidas en las posiciones enmascaradas.\n",
    "- `h_masked = self.norm(self.activ2(self.linear(h_masked)))`: Aplica una capa lineal, una función de activación (GELU) y una capa de normalización.\n",
    "- `logits_lm = self.decoder(h_masked) + self.decoder_bias`: Decodifica las salidas para predecir los tokens enmascarados.\n",
    "\n",
    "**Salida**:\n",
    "\n",
    "- `return logits_lm, logits_clsf`: Devuelve las predicciones de los tokens enmascarados (`logits_lm`) y las predicciones de clasificación de la oración (`logits_clsf`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778258d-15f4-4fb1-a3a7-b6fe12da83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Función para crear un batch de datos de entrada\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
    "        # Selecciona índices aleatorios para las oraciones\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # Máscara LM\n",
    "        n_pred = min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % de los tokens en una oración\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word_dict['[MASK]'] # hacer máscara\n",
    "            elif random() < 0.5:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # índice aleatorio en el vocabulario\n",
    "                input_ids[pos] = word_dict[number_dict[index]] # reemplazar\n",
    "\n",
    "        # Relleno con ceros\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Relleno con ceros para el resto de los tokens (100% - 15%)\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        # Agregar ejemplos positivos y negativos al batch\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # EsSiguiente\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NoEsSiguiente\n",
    "            negative += 1\n",
    "    return batch\n",
    "# Procesamiento terminado\n",
    "\n",
    "# Función para obtener la máscara de atención para los pads\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) es el token PAD\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), uno es máscara\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "# Función de activación gelu\n",
    "def gelu(x):\n",
    "    \"Implementación de la función de activación gelu por Hugging Face\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "# Clase de Embedding\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # embedding de tokens\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # embedding de posición\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # embedding de segmento (tipo de token)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n",
    "\n",
    "# Clase de Atención con Producto Escalar\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Llena elementos del tensor con valor donde la máscara es uno.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "# Clase de Atención Multi-Cabecera\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n",
    "\n",
    "# Clase de Red Neuronal de Alimentación Directa Posicional\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))\n",
    "\n",
    "# Clase de Capa de Codificador\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs a Q,K,V iguales\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "# Clase BERT (Bidirectional Encoder Representations from Transformers)\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # El decodificador se comparte con la capa de embedding\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        # Se decide por el primer token (CLS)\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        # Obtener la posición enmascarada de la salida final del transformer.\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # posición enmascarada [batch_size, max_pred, d_model]\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_clsf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parámetros de BERT\n",
    "    maxlen = 30 # longitud máxima\n",
    "    batch_size = 6\n",
    "    max_pred = 5  # máximo de tokens de predicción\n",
    "    n_layers = 6 # número de codificadores en la capa del codificador\n",
    "    n_heads = 12 # número de cabeceras en la atención multi-cabecera\n",
    "    d_model = 768 # tamaño del embedding\n",
    "    d_ff = 768 * 4  # 4*d_model, dimensión de FeedForward\n",
    "    d_k = d_v = 64  # dimensión de K(=Q), V\n",
    "    n_segments = 2\n",
    "\n",
    "    text = (\n",
    "        'Hello, how are you? I am Romeo.\\n'\n",
    "        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "        'Nice meet you too. How are you today?\\n'\n",
    "        'Great. My baseball team won the competition.\\n'\n",
    "        'Oh Congratulations, Juliet\\n'\n",
    "        'Thanks you Romeo'\n",
    "    )\n",
    "    sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filtrar '.', ',', '?', '!'\n",
    "    word_list = list(set(\" \".join(sentences).split()))\n",
    "    word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "    for i, w in enumerate(word_list):\n",
    "        word_dict[w] = i + 4\n",
    "    number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "    vocab_size = len(word_dict)\n",
    "\n",
    "    token_list = list()\n",
    "    for sentence in sentences:\n",
    "        arr = [word_dict[s] for s in sentence.split()]\n",
    "        token_list.append(arr)\n",
    "\n",
    "    model = BERT()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    batch = make_batch()\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "        loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # para LM enmascarado\n",
    "        loss_lm = (loss_lm.float()).mean()\n",
    "        loss_clsf = criterion(logits_clsf, isNext) # para clasificación de oraciones\n",
    "        loss = loss_lm + loss_clsf\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Predecir tokens enmascarados y esSiguiente\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "    print(text)\n",
    "    print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "    print('lista de tokens enmascarados : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "    print('lista de tokens enmascarados predichos : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "    logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "    print('esNext : ', True if isNext else False)\n",
    "    print('predecir esNext : ',True if logits_clsf else False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872de27-c976-44a2-8392-0c9b819a8c97",
   "metadata": {},
   "source": [
    "El código crea un conjunto de datos donde se combinan pares de oraciones para formar secuencias de entrada que el modelo BERT utilizará para el entrenamiento. A continuación se muestra un ejemplo de una secuencia de tokens generada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79483db3-51f2-4c50-86df-a3e3f63edd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#['[CLS]', 'hello', '[MASK]', 'are', 'you', 'i', 'am', 'hello', '[SEP]', 'thanks', 'you', 'romeo', '[SEP]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0888f85-9b21-4301-8905-922a55ce587f",
   "metadata": {},
   "source": [
    "Aquí:\n",
    "\n",
    "- [CLS] es el token de clasificación utilizado al principio de cada secuencia.\n",
    "- [SEP] es el token separador que se utiliza para dividir dos oraciones diferentes dentro de la misma secuencia.\n",
    "- [MASK] es el token de máscara que el modelo debe predecir durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e612a886-efc0-4f13-b9d9-15b7000132df",
   "metadata": {},
   "source": [
    "La lista de tokens enmascarados y sus posiciones son las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b9f88-1ca0-4885-b27e-995d94a50736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de tokens enmascarados : [21, 20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184effe5-db8f-49f9-b2f8-c4269d85daed",
   "metadata": {},
   "source": [
    "Esto indica que se enmascararon dos posiciones en la secuencia original. En este caso, el token en la posición 21 fue enmascarado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3568183b-4801-491d-ac12-148a1a59b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de tokens enmascarados predichos : [20, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5d08b-b31e-46cc-8ed8-4563b7ed5a31",
   "metadata": {},
   "source": [
    "Esto significa que el modelo predijo incorrectamente el token en la posición enmascarada, repitiendo el mismo token dos veces en lugar de encontrar el token correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb47b8d-fc45-4388-9b42-9332c4349b34",
   "metadata": {},
   "source": [
    "La clasificación de si la segunda oración sigue lógicamente a la primera es indicada por los valores de `esNext` y `predecir esNext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c164b5-6a29-4b3a-80a7-4718bf697bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#esNext : False\n",
    "#predecir esNext : True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a3a72-de1b-4f8a-82ee-307e0aee9e99",
   "metadata": {},
   "source": [
    "Aquí, `esNext` : `False` significa que, en el conjunto de datos original, las dos oraciones no son secuenciales (no están una después de la otra en el texto original). Sin embargo, predecir `esNext : True` muestra que el modelo BERT predijo incorrectamente que las oraciones eran secuenciales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85885d79-7de3-4966-96d1-fdaa332a135b",
   "metadata": {},
   "source": [
    "#### Ejercicios: \n",
    "\n",
    "1 . Ajusta el preprocesamiento de datos para manejar un nuevo conjunto de datos y experimentar con diferentes técnicas de tokenización.\n",
    "\n",
    "Tareas:\n",
    "- Cambia el texto de entrada por un nuevo corpus de datos.\n",
    "- Implementa una nueva técnica de tokenización (por ejemplo, tokenización basada en subpalabras usando Byte Pair Encoding).\n",
    "- Ajusta el código para que utilice el nuevo tokenizador y diccionario de palabras.\n",
    "\n",
    "2 . Experimentar con diferentes hiperparámetros para observar su impacto en el rendimiento del modelo.\n",
    "\n",
    "Tareas:\n",
    "- Cambia los parámetros del modelo como `n_layers`, `n_heads`, `d_model`, `d_ff`.\n",
    "- Realiza entrenamientos con diferentes configuraciones y registra el costo del modelo durante el entrenamiento.\n",
    "- Compara el rendimiento en términos de pérdida y precisión para diferentes configuraciones de parámetros.\n",
    "\n",
    "3  Implementación de una nueva función de pérdida\n",
    "\n",
    "Tareas:\n",
    "\n",
    "- Implementa la pérdida de Kullback-Leibler Divergence en lugar de CrossEntropyLoss para la tarea de MLM.\n",
    "- Ajusta el código de entrenamiento para utilizar la nueva función de pérdida.\n",
    "- Compara el rendimiento del modelo utilizando la nueva función de pérdida.\n",
    "\n",
    "4 . Realiza fine-tuning del modelo BERT en una tarea específica de NLP, como clasificación de texto o reconocimiento de entidades nombradas.\n",
    "\n",
    "Tareas:\n",
    "- Selecciona un conjunto de datos para una tarea específica (por ejemplo, IMDB dataset para clasificación de sentimientos).\n",
    "- Ajusta el modelo BERT y el código de entrenamiento para realizar fine-tuning en la nueva tarea.\n",
    "- Entrena el modelo y evalúa su rendimiento en la tarea seleccionada.\n",
    "\n",
    "5 . Visualiza las matrices de atención para entender cómo el modelo BERT presta atención a diferentes partes de la secuencia de entrada.\n",
    "\n",
    "Tareas:\n",
    "- Modifica el código para almacenar las matrices de atención durante la inferencia.\n",
    "- Utiliza una librería de visualización (por ejemplo, matplotlib o seaborn) para visualizar las matrices de atención.\n",
    "- Analiza y comenta las visualizaciones para entender mejor el comportamiento del modelo.\n",
    "\n",
    "6 .Implementa técnicas de regularización para prevenir el sobreajuste durante el entrenamiento.\n",
    "\n",
    "Tareas:\n",
    "- Añade Dropout a las capas del modelo BERT.\n",
    "- Ajusta los hiperparámetros del Dropout y realiza entrenamientos con diferentes tasas de Dropout.\n",
    "- Evalúa el impacto de la regularización en el rendimiento del modelo.\n",
    "\n",
    "7 . Utiliza un modelo preentrenado BERT de Hugging Face y realizar fine-tuning en una nueva tarea.\n",
    "\n",
    "Tareas:\n",
    "\n",
    "- Carga un modelo preentrenado BERT utilizando la librería transformers de Hugging Face.\n",
    "- Realiza fine-tuning del modelo en una tarea específica (por ejemplo, MRPC o CoLA).\n",
    "- Evalúa y compara el rendimiento del modelo preentrenado con el modelo entrenado desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14817f53-c290-4e71-834b-2e4446ec170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d9c30-0eb3-4abb-bd16-56fb63c79c25",
   "metadata": {},
   "source": [
    "### Variantes de BERT\n",
    "\n",
    "Desde su introducción en 2018, BERT (Bidirectional Encoder Representations from Transformers) ha revolucionado el campo del procesamiento de lenguaje natural (NLP). Su capacidad para capturar contextos bidireccionales ha permitido mejoras significativas en diversas tareas de NLP. A raíz del éxito de BERT, se han desarrollado múltiples variantes que optimizan diferentes aspectos del modelo original, como la eficiencia, la capacidad de generalización y la adecuación a contextos específicos. Veamos algunas de las variantes más destacadas de BERT, incluyendo RoBERTa, DistilBERT, ALBERT, TinyBERT y otras adaptaciones especializadas.\n",
    "\n",
    "**RoBERTa (Robustly Optimized BERT Pretraining Approach)**\n",
    "\n",
    "[RoBERTa](https://arxiv.org/abs/1907.11692), introducido por Facebook AI, es una mejora sobre el modelo BERT original. Este modelo se basa en la misma arquitectura de transformadores, pero optimiza el preentrenamiento eliminando la tarea de predicción de la siguiente oración (Next Sentence Prediction, NSP). En su lugar, RoBERTa se entrena con secuencias más largas y en volúmenes de datos significativamente mayores.\n",
    "\n",
    "Características principales de RoBERTa:\n",
    "\n",
    "- Eliminación de NSP: La eliminación de la tarea NSP permitió a RoBERTa enfocarse exclusivamente en el modelado de lenguaje enmascarado (MLM), lo que mejoró la eficiencia y el rendimiento.\n",
    "- Aumento de datos de entrenamiento: RoBERTa se entrenó en un corpus de datos diez veces mayor que el utilizado por BERT, incluyendo datos adicionales de libros, Wikipedia y otros recursos.\n",
    "- Tamaño de secuencia: Utiliza secuencias de texto más largas (hasta 512 tokens) durante el entrenamiento, lo que mejora la capacidad del modelo para capturar dependencias a largo plazo.\n",
    "- Ajustes de hiperparámetros: Se realizaron ajustes finos en los hiperparámetros de entrenamiento, como el tamaño del batch y la tasa de aprendizaje, para mejorar la eficacia del modelo.\n",
    "\n",
    "RoBERTa ha demostrado un rendimiento superior en varias tareas de NLP, superando consistentemente a BERT en benchmarks estándar como GLUE, RACE y SQuAD.\n",
    "\n",
    "**DistilBERT**\n",
    "\n",
    "Desarrollado por Hugging Face, [DistilBERT](https://arxiv.org/abs/1910.01108) es una versión más pequeña y eficiente de BERT. Utiliza técnicas de destilación de conocimientos para reducir el tamaño del modelo en aproximadamente un 40%, manteniendo el 97% del rendimiento de BERT en diversas tareas de NLP.\n",
    "\n",
    "Características principales de DistilBERT:\n",
    "\n",
    "- Destilación de conocimientos: El proceso de destilación implica entrenar un modelo más pequeño (el estudiante) para imitar el comportamiento de un modelo más grande (el maestro), en este caso, BERT.\n",
    "- Reducción de parámetros: DistilBERT tiene menos capas (6 en lugar de 12), lo que reduce significativamente el número de parámetros y la complejidad computacional.\n",
    "- Velocidad y eficiencia: Al ser más compacto, DistilBERT es más rápido y menos costoso en términos de recursos computacionales, lo que lo hace ideal para aplicaciones con limitaciones de hardware.\n",
    "\n",
    "DistilBERT es especialmente útil para aplicaciones móviles y de tiempo real, donde la latencia y el consumo de recursos son críticos.\n",
    "\n",
    "**ALBERT (A Lite BERT)**\n",
    "\n",
    "[ALBERT](https://www.arxiv.org/abs/1909.11942), desarrollado por Google Research, es una versión ligera de BERT que introduce varias técnicas innovadoras para reducir el tamaño del modelo sin sacrificar el rendimiento.\n",
    "\n",
    "Características principales de ALBERT:\n",
    "\n",
    "- Factorización de embeddings: ALBERT reduce el tamaño de los embeddings de palabras al factorizar la matriz de embeddings en dos matrices más pequeñas. Esto reduce el número de parámetros y mejora la eficiencia.\n",
    "- Parámetros compartidos: Utiliza parámetros compartidos entre las capas del codificador, lo que reduce aún más el número de parámetros sin afectar la capacidad de representación del modelo.\n",
    "- Mejora en el preentrenamiento: ALBERT introduce una nueva tarea de predicción de la ordenación de oraciones (Sentence Order Prediction, SOP) en lugar de NSP, lo que mejora la capacidad del modelo para comprender la coherencia del texto.\n",
    "\n",
    "ALBERT ha demostrado un rendimiento comparable al de BERT en benchmarks estándar con un costo computacional significativamente menor.\n",
    "\n",
    "**TinyBERT**\n",
    "\n",
    "[TinyBERT](https://arxiv.org/abs/1909.10351) es otra variante optimizada para la eficiencia, desarrollada mediante técnicas de destilación de conocimientos. Este modelo es aún más pequeño que DistilBERT y está diseñado para ser altamente eficiente sin comprometer demasiado el rendimiento.\n",
    "\n",
    "Características principales de TinyBERT:\n",
    "\n",
    "- Destilación de dos etapas: TinyBERT utiliza un proceso de destilación en dos etapas que incluye la destilación general en un corpus grande y la destilación específica para una tarea particular.\n",
    "- Reducción de tamaño: TinyBERT es extremadamente compacto, con menos parámetros y una estructura más simple, lo que lo hace ideal para dispositivos con recursos limitados.\n",
    "- Rendimiento competitivo: A pesar de su pequeño tamaño, TinyBERT mantiene un rendimiento competitivo en varias tareas de NLP.\n",
    "\n",
    "TinyBERT es adecuado para aplicaciones donde la eficiencia y la velocidad son cruciales, como en dispositivos móviles y aplicaciones de tiempo real.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88602f7b-4808-42db-b89c-0d3eb6e2c3d9",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 . Implementa una variante de BERT, como DistilBERT o ALBERT, y comparar su rendimiento con el modelo original.\n",
    "\n",
    "Tareas:\n",
    "- Implementa DistilBERT o ALBERT basándote en la arquitectura del modelo BERT original.\n",
    "- Ajusta el código de entrenamiento para entrenar la nueva variante del modelo.\n",
    "- Compara el rendimiento y la eficiencia de la variante con el modelo BERT original en términos de precisión, tamaño del modelo y tiempo de entrenamiento.\n",
    "\n",
    "2 . Evalua el rendimiento del modelo entrenado en un conjunto de datos completamente nuevo y no visto durante el entrenamiento.\n",
    "\n",
    "Tareas:\n",
    "- Prepara un nuevo conjunto de datos para la evaluación.\n",
    "- Ajusta el código de evaluación para utilizar el nuevo conjunto de datos.\n",
    "- Evalúa el rendimiento del modelo y analiza las predicciones.\n",
    "\n",
    "3 . Optimiza el código de entrenamiento para mejorar la eficiencia computacional y reducir el tiempo de entrenamiento.\n",
    "\n",
    "Tareas:\n",
    "- Identifica posibles cuellos de botella en el código de entrenamiento.\n",
    "- Implementa optimizaciones como el uso de mixed precision training o gradient accumulation.\n",
    "- Mide el tiempo de entrenamiento antes y después de las optimizaciones y compara los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eedaed-2e12-4292-b671-4bdad4aac38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b85f7-201d-4723-8d53-03d07cc53a5e",
   "metadata": {},
   "source": [
    "### Otras adaptaciones y variantes de BERT\n",
    "\n",
    "Además de las variantes mencionadas, existen otras adaptaciones de BERT que están diseñadas para contextos específicos y tareas especializadas:\n",
    "\n",
    "**BERTweet**:\n",
    "\n",
    "[BERTweet](https://aclanthology.org/2020.emnlp-demos.2.pdf) es una adaptación de BERT entrenada específicamente en datos de Twitter. Este modelo está optimizado para comprender y generar texto en el estilo y contexto particular de las redes sociales.\n",
    "\n",
    "**BioBERT**:\n",
    "\n",
    "[BioBERT](https://arxiv.org/abs/1901.08746) está entrenado en grandes corpus de texto biomédico y está diseñado para tareas en el dominio de la biomedicina y la biología, como la extracción de información biomédica y el reconocimiento de entidades nombradas en texto científico.\n",
    "\n",
    "**SciBERT**:\n",
    "\n",
    "[SciBERT](https://arxiv.org/abs/1903.10676) es una variante de BERT entrenada en artículos científicos de varias disciplinas. Está optimizado para tareas de NLP en el contexto de la literatura científica, como la extracción de términos técnicos y la clasificación de texto académico.\n",
    "\n",
    "**Multilingual BERT (mBERT)**:\n",
    "\n",
    "[mBERT](https://aclanthology.org/P19-1493/) es una versión de BERT entrenada en múltiples idiomas. Este modelo es capaz de manejar tareas de NLP en varios idiomas sin necesidad de entrenamientos adicionales específicos para cada lengua.\n",
    "\n",
    "**SpanBERT**:\n",
    "\n",
    "[SpanBERT](https://arxiv.org/abs/1907.10529) es una mejora sobre BERT que se enfoca en el modelado de spans (fragmentos de texto) en lugar de palabras individuales. Este modelo está diseñado para mejorar el rendimiento en tareas que involucran la predicción de relaciones entre spans de texto, como la extracción de relaciones y la respuesta a preguntas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71ee4b-53ff-45d8-8e12-7066acda1b0f",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 .Entrena BERTweet para una tarea específica de análisis de sentimientos en datos de Twitter.\n",
    "\n",
    "Tareas:\n",
    "\n",
    "- Descarga un conjunto de datos de tweets etiquetados con sentimientos (por ejemplo, Sentiment140).\n",
    "- Carga el modelo preentrenado BERTweet utilizando la librería transformers de Hugging Face.\n",
    "- Ajusta el código de entrenamiento para realizar fine-tuning en el conjunto de datos de análisis de sentimientos.\n",
    "- Evalúa el rendimiento del modelo en un conjunto de datos de prueba y analiza las predicciones.\n",
    "\n",
    "2 . Utiliza BioBERT para la tarea de reconocimiento de entidades nombradas (NER) en el dominio biomédico.\n",
    "\n",
    "Tareas:\n",
    "- Descarga un conjunto de datos etiquetados con entidades biomédicas (por ejemplo, BC5CDR).\n",
    "- Carga el modelo preentrenado BioBERT.\n",
    "- Ajusta el código de entrenamiento para realizar fine-tuning en la tarea de NER biomédico.\n",
    "- Evalúa el rendimiento del modelo y analiza la precisión de las entidades reconocidas.\n",
    "\n",
    "3 . Utiliza SciBERT para la tarea de clasificación de artículos científicos en diferentes categorías.\n",
    "\n",
    "Tareas:\n",
    "- Descarga un conjunto de datos de artículos científicos etiquetados por categorías (por ejemplo, arXiv dataset).\n",
    "- Carga el modelo preentrenado SciBERT.\n",
    "- Ajusta el código de entrenamiento para realizar fine-tuning en la tarea de clasificación de artículos científicos.\n",
    "- Evalúa el rendimiento del modelo y analiza las predicciones de categorías.\n",
    "\n",
    "4 .Utiliza mBERT para la tarea de reconocimiento de entidades nombradas en múltiples idiomas.\n",
    "\n",
    "Tareas:\n",
    "- Descarga conjuntos de datos de NER en varios idiomas (por ejemplo, CoNLL-2002 y CoNLL-2003).\n",
    "- Carga el modelo preentrenado mBERT.\n",
    "- Ajusta el código de entrenamiento para realizar fine-tuning en las tareas de NER en diferentes idiomas.\n",
    "- Evalúa el rendimiento del modelo en cada idioma y compara los resultados.\n",
    "\n",
    "5 .Utiliza SpanBERT para la tarea de extracción de relaciones entre entidades en textos.\n",
    "\n",
    "Tareas:\n",
    "\n",
    "- Descarga un conjunto de datos de extracción de relaciones (por ejemplo, TACRED).\n",
    "- Carga el modelo preentrenado SpanBERT.\n",
    "- Ajusta el código de entrenamiento para realizar fine-tuning en la tarea de extracción de relaciones.\n",
    "- Evalúa el rendimiento del modelo y analiza la precisión de las relaciones extraídas.\n",
    "\n",
    "6 .Compara el rendimiento de diferentes variantes de BERT en una tarea común.\n",
    "\n",
    "Tareas:\n",
    "- Selecciona una tarea común de NLP (por ejemplo, clasificación de texto).\n",
    "- Carga los modelos preentrenados BERT, BERTweet, BioBERT, SciBERT, mBERT y SpanBERT.\n",
    "- Realiza fine-tuning de cada modelo en el mismo conjunto de datos.\n",
    "- Evalúa y compara el rendimiento de los modelos en términos de precisión, recall y F1-score.\n",
    "\n",
    "7 . Visualizar las matrices de atención de diferentes variantes de BERT para entender cómo cada modelo presta atención a diferentes partes del texto.\n",
    "\n",
    "Tareas:\n",
    "- Selecciona un texto de entrada y aplica diferentes variantes de BERT (BERT, BERTweet, BioBERT, SciBERT, mBERT, SpanBERT).\n",
    "- Modifica el código para almacenar las matrices de atención de cada modelo durante la inferencia.\n",
    "- Utiliza una librería de visualización (por ejemplo, matplotlib o seaborn) para visualizar las matrices de atención.\n",
    "- Analiza y comenta las visualizaciones para entender mejor las diferencias en el comportamiento de los modelos.\n",
    "\n",
    "8 . Adapta una variante de BERT a un nuevo dominio específico utilizando un pequeño conjunto de datos etiquetados.\n",
    "\n",
    "Tareas:\n",
    "- Selecciona un dominio específico (por ejemplo, legal, financiero).\n",
    "- Recopila un pequeño conjunto de datos etiquetados en el dominio seleccionado.\n",
    "- Carga una variante de BERT adecuada (por ejemplo, SciBERT para textos científicos).\n",
    "- Realiza fine-tuning del modelo en el nuevo conjunto de datos.\n",
    "- Evalúa el rendimiento del modelo en el nuevo dominio y analiza las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6231ed0-95ca-412d-8773-892634557174",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sugerencia\n",
    "\n",
    "#pip install transformers torch datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el conjunto de datos Sentiment140\n",
    "dataset = load_dataset('sentiment140')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Cargar el tokenizador BERTweet\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Aplicar el tokenizador a los datos de entrenamiento y prueba\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Remover columnas no utilizadas y establecer el formato de los tensores\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['text'])\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=batch_size)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Cargar el modelo BERTweet\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=2)\n",
    "\n",
    "from transformers import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configurar el optimizador\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Mover el modelo a la GPU si está disponible\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train():\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['sentiment'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Función de evaluación\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['sentiment'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f'Exactitud: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoca {epoch + 1}/{epochs}')\n",
    "    train()\n",
    "    evaluate()\n",
    "\n",
    "evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231034ef-2462-4451-b622-c46366bc234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
