{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b83363d-5d80-4333-a2d3-a75c7c867093",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "Los transformers son una arquitectura de modelo en aprendizaje profundo que ha revolucionado el campo del procesamiento del lenguaje natural (NLP) y otras áreas de la inteligencia artificial. Propuestos por [Vaswani et al.](https://arxiv.org/abs/1706.03762) en 2017, los transformers utilizan mecanismos de atención para procesar secuencias de datos, eliminando la necesidad de la estructura recurrente o convolucional utilizada en modelos anteriores como RNNs y CNNs. \n",
    "\n",
    "La principal innovación de los transformers es su capacidad para manejar la dependencia a largo plazo de manera más eficiente y efectiva a través de la atención.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874098f6-eed5-484d-88de-97d54eef1d8e",
   "metadata": {},
   "source": [
    "**Conceptos fundamentales**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da86edd-c78d-493e-9d7a-c17e34dfc315",
   "metadata": {},
   "source": [
    "**Codificación posicional**\n",
    "  \n",
    "La codificación posicional en el transformer es un mecanismo que se utiliza para introducir información sobre el orden de las palabras en una secuencia. Esto es necesario porque a diferencia de las redes neuronales recurrentes (RNNs) que procesan las secuencias de manera secuencial y tienen un conocimiento implícito del orden, los transformers procesan todas las palabras de una secuencia en paralelo, lo que significa que no tienen una forma intrínseca de capturar la posición de las palabras.\n",
    "\n",
    "En tareas de procesamiento de lenguaje natural (NLP), el orden de las palabras en una oración es crucial para entender su significado. Por ejemplo, \"el gato persigue al ratón\" tiene un significado muy diferente de \"el ratón persigue al gato\". Sin información posicional, un transformer no podría distinguir entre estas dos oraciones, ya que vería ambas como una bolsa de palabras sin orden. La codificación posicional se suma a las embeddings de las palabras para que el modelo pueda tener información sobre la posición de cada palabra en la secuencia. La codificación sinusoidal es una de las formas en que se realiza esta codificación posicional.\n",
    "\n",
    "La función `get_sinusoid_encoding_table(n_position, d_model)` en el siguiente código crea una tabla de codificación sinusoidal. Cada posición `pos` tiene una representación vectorial que usa funciones seno y coseno de diferentes frecuencias. Esta técnica asegura que cada posición en la secuencia tenga una representación única y que las posiciones cercanas tengan representaciones similares, capturando así las relaciones posicionales.\n",
    "\n",
    "Una forma común de implementar la codificación posicional es mediante el uso de funciones sinusoidales. \n",
    "\n",
    "A continuación, se explican las ecuaciones y la lógica detrás de esta técnica:\n",
    "\n",
    "Para una posición pos y una dimensión i en el vector de codificación de dimensiones d_model, la codificación posicional se define como:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Las funciones seno y coseno se usan para generar patrones que varían a diferentes frecuencias a lo largo de las dimensiones del vector. Las frecuencias se escalan exponencialmente con la posición y la dimensión. Se utiliza `sin` para las dimensiones pares y `cos` para las dimensiones impares. Esto asegura que cada posición tenga una representación única y que las diferencias relativas entre posiciones se codifiquen de manera consistente.\n",
    "\n",
    "Propiedades de la codificación sinusoidal\n",
    "\n",
    "- Las funciones sinusoidales permiten que el modelo generalice a secuencias más largas o más cortas que aquellas vistas durante el entrenamiento.\n",
    "- Las representaciones de posiciones cercanas serán similares, lo cual es útil para capturar relaciones locales, mientras que las representaciones de posiciones más distantes serán suficientemente distintas.\n",
    "\n",
    "En este ejemplo, `get_sinusoid_encoding_table` calcula la tabla de codificación sinusoidal para `n_position` posiciones y `d_model` dimensiones. La codificación posicional se aplica a la secuencia de entrada antes de pasarla al modelo Transformer, agregando estos vectores de codificación a los embeddings de las palabras.\n",
    "\n",
    "En la arquitectura Transformer, la codificación posicional se suma a los embeddings de las palabras en la secuencia de entrada antes de ser procesados por las capas de atención. Esto se puede describir como:\n",
    "\n",
    "$$\n",
    "\\text{Input Embedding} + \\text{Positional Encoding}\n",
    "$$\n",
    "\n",
    "Este enfoque asegura que cada palabra en la secuencia tenga una representación que incluye tanto su identidad como su posición relativa, permitiendo que el modelo Transformer mantenga el orden de la secuencia durante el procesamiento.\n",
    "\n",
    "**Ejercicio** Es útil visualizar las codificaciones posicionales para entender cómo varían a lo largo de las posiciones y dimensiones. Modifica el código siguiente para como se podría verse esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941323c-b4b1-4385-bd26-43767f5fe5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualizar codificaciones posicionales\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.heatmap(sinusoid_table.numpy(), cmap=\"viridis\")\n",
    "plt.xlabel('Dimensiones del Vector de Codificación')\n",
    "plt.ylabel('Posiciones en la Secuencia')\n",
    "plt.title('Codificaciones Posicionales Sinusoidales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235bcdf-5e78-445b-a08e-0cfa05f33641",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6babfc-5ba2-4ac9-a816-906ba8110a48",
   "metadata": {},
   "source": [
    "#### Máscara de atención\n",
    "\n",
    "Las máscaras de atención son una parte crucial de los modelos Transformer, y se utilizan para controlar qué partes de la secuencia de entrada deben ser atendidas (prestadas atención) durante el proceso de atención. Existen principalmente dos tipos de máscaras de atención utilizadas en Transformers:\n",
    "\n",
    "**Máscara de atención de padding**\n",
    "En el procesamiento de secuencias, es común que las secuencias tengan diferentes longitudes. Para manejar esto, se rellenan las secuencias más cortas con un símbolo de padding (P). Sin embargo, estos símbolos de padding no deben influir en el proceso de atención. La máscara de atención de padding (`get_attn_pad_mask(seq_q, seq_k)`) asegura que los tokens de padding sean ignorados en el cálculo de la atención. Esta función crea una máscara que es `True` en las posiciones de padding y `False` en las demás posiciones. Al aplicar esta máscara a los puntajes de atención, se pueden asignar valores negativos muy grandes (como -1e9) a las posiciones de padding, de modo que su influencia sea minimizada.\n",
    "\n",
    "\n",
    "**Máscara de atención subsecuente (causal)**\n",
    "\n",
    "En los decodificadores de los transformers, durante el entrenamiento, es crucial que cada posición solo pueda atender a posiciones anteriores (y no a futuras) para evitar la fuga de información. La máscara de atención subsecuente (`get_attn_subsequent_mask(seq)`) se utiliza para este propósito. Esta máscara es una matriz triangular superior que asegura que la atención en una posición específica solo considere las posiciones anteriores y la posición actual, enmascarando todas las futuras posiciones.  Esto es importante en tareas de generación de texto, donde el modelo debe predecir el siguiente token sin conocer los tokens futuros.\n",
    "\n",
    "Las máscaras de atención se aplican en la etapa de cálculo de las puntuaciones de atención (`attention scores`) para ajustar las puntuaciones de los tokens que deben ser ignorados. Los valores enmascarados se establecen en un valor muy negativo (por ejemplo, -inf) para que las puntuaciones de atención después de la aplicación de softmax sean prácticamente cero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8babf-14c2-4737-a6fa-de4a5d9c71df",
   "metadata": {},
   "source": [
    "**Atención utilizando el producto punto escalado**\n",
    "\n",
    "La atención es el componente clave en los transformers. El mecanismo de atención de producto punto escalado (`ScaledDotProductAttention`) calcula la importancia de cada palabra en una secuencia con respecto a otra palabra utilizando el producto punto escalado. Dado un vector de consulta `Q`, un vector de claves `K`, y un vector de valores `V`, se calcula primero el puntaje de atención como el producto punto de `Q` y `K` transpuesto, dividido por la raíz cuadrada de la dimensión de las claves `(d_k)`. Esto se hace para estabilizar los gradientes y evitar valores extremadamente grandes de la función softmax. Después, se aplica la máscara de atención (si existe) y se normalizan los puntajes utilizando una función softmax. Finalmente, se obtiene el contexto multiplicando los pesos de atención con `V`.\n",
    "\n",
    "El mecanismo de atención de producto punto escalado calcula la relevancia entre las palabras en la secuencia. La fórmula es:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $Q$ es la matriz de consultas.\n",
    "- $K$ es la matriz de claves.\n",
    "- $V$ es la matriz de valores.\n",
    "- $d_k$ es la dimensión de las claves y consultas, y se usa para escalar el producto punto.\n",
    "\n",
    "El código `ScaledDotProductAttention` implementa la atención de producto punto escalado para calcular la importancia de cada palabra en la secuencia de entrada con respecto a otra.\n",
    "\n",
    "- La clase `ScaledDotProductAttention`: Esta clase define el mecanismo de atención de producto punto escalado, que se utiliza para calcular las puntuaciones de atención entre las consultas (`Q`), las claves (`K`) y los valores (`V`).\n",
    "- El método `forward` de esta clase toma como entradas los tensores `Q`, `K` y `V`, así como una máscara de atención (`attn_mask`). A continuación, se describen los pasos que realiza el método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de5531-8f3d-4263-b7dc-02b668e15dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe0758-e612-418d-80be-4a304280a249",
   "metadata": {},
   "source": [
    "- Multiplicación de matrices: Calcula el producto punto de las consultas (`Q`) y las claves (`K`). Esto da una matriz de puntuaciones que representa la afinidad entre cada par de elementos de `Q` y `K`.\n",
    "- Escalado: Divide las puntuaciones por la raíz cuadrada de `d_k` (dimensión de las claves). Esto ayuda a estabilizar los gradientes y evita que las puntuaciones de atención sean demasiado grandes, lo que podría hacer que los valores softmax sean extremadamente pequeños y difíciles de manejar numéricamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93640d4-1873-4758-b11d-15d52d6db8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores.masked_fill_(attn_mask, -1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972b4db-5ed9-4b56-9ac8-95078db61e21",
   "metadata": {},
   "source": [
    "Se utiliza `masked_fill_` para llenar las posiciones especificadas por la máscara de atención (`attn_mask`) con un valor muy bajo (-1e9). Este valor tan bajo asegura que después de la aplicación de la función softmax, las posiciones enmascaradas tendrán una probabilidad cercana a cero y no influirán en el resultado de la atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4e6cb-351f-472f-a4a6-b75e8026d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn = nn.Softmax(dim=-1)(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f289b-5f4c-4cbb-a0c1-cdf51409c7ff",
   "metadata": {},
   "source": [
    "Se aplica la función softmax a las puntuaciones de atención a lo largo del último eje (`dim=-1`). Esto convierte las puntuaciones en probabilidades, de modo que sumen 1 en la dimensión de las claves. La función softmax resalta las puntuaciones más altas (las claves más relevantes) y atenúa las puntuaciones más bajas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16a3ef-67e1-4e96-a497-0307645eb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#context = torch.matmul(attn, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f68968-1040-436d-b129-1cdfc1e82320",
   "metadata": {},
   "source": [
    "Calcula el contexto como el producto punto de las probabilidades de atención (`attn`) con los valores (`V`). Esto produce una representación ponderada de los valores donde las posiciones más relevantes (según las puntuaciones de atención) contribuyen más al contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c73884e-2773-4cdf-8bf8-bf595626f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return context, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc92e6f-9e9f-4c31-8066-959417c00790",
   "metadata": {},
   "source": [
    "Devuelve el contexto calculado y las puntuaciones de atención. El contexto se utilizará como entrada para las siguientes capas del transformer, y las puntuaciones de atención pueden ser útiles para interpretar qué partes de la secuencia de entrada fueron más importantes para cada predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411f811-849e-4e69-b8a5-fe986a7e668a",
   "metadata": {},
   "source": [
    "### Atención multi-cabecera\n",
    "\n",
    "La atención multi-cabecera (`MultiHeadAttention`) es una extensión del mecanismo de atención que permite al modelo enfocarse en diferentes partes de la secuencia de entrada de manera simultánea. En lugar de realizar una única atención con grandes dimensiones, el mecanismo divide estas dimensiones en múltiples cabeceras más pequeñas y realiza el proceso de atención en paralelo. Cada cabecera de atención produce su propio conjunto de valores de contexto, que luego se concatenan y se proyectan de nuevo a la dimensión original. Esto permite al modelo capturar diferentes tipos de relaciones y patrones dentro de la secuencia. \n",
    "\n",
    "La función `MultiHeadAttention` define este mecanismo, utilizando capas lineales para proyectar las entradas en las subespacios de atención y luego combinar las salidas de cada cabecera.\n",
    "\n",
    "Observación: La proyección se utiliza para mapear los datos de entrada de una dimensión a otra. En el caso de la atención multi-cabecera, la proyección se usa para transformar las consultas (`Q`), claves (`K`) y valores (`V`) de su dimensión original a una nueva dimensión que es adecuada para el cálculo de la atención.\n",
    "\n",
    "Proceso de proyección\n",
    "\n",
    "1. Los datos de entrada tienen una dimensión d_model.\n",
    "2. Una capa lineal con parámetros (pesos W y sesgos b) transforma la entrada. La dimensión de salida de esta capa lineal es `d_k * n_heads` para las consultas y las claves, y `d_v * n_heads` para los valores.\n",
    "3. La entrada se multiplica por la matriz de pesos, lo que efectúa una transformación lineal.\n",
    "4. Suma del Sesgo: Se agrega el vector de sesgo para completar la transformación.\n",
    "Matemáticamente, para una entrada X, la transformación lineal se expresa como: $Y=XW+b$\n",
    "\n",
    "Donde:\n",
    "- X es la entrada.\n",
    "- W es la matriz de pesos.\n",
    "- b es el vector de sesgo.\n",
    "- Y es la salida proyectada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acb607-b389-4ba9-9a14-02bdeb86400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def __init__(self):\n",
    "#    super(MultiHeadAttention, self).__init__()\n",
    "#    self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "#    self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "#    self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "#    self.linear = nn.Linear(n_heads * d_v, d_model)\n",
    "#    self.layer_norm = nn.LayerNorm(d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2edb4d-d008-43d7-841e-b3bd01076823",
   "metadata": {},
   "source": [
    "- `self.W_Q`: Proyecta las consultas (`Q`) del espacio de dimensión `d_model` al espacio de dimensión `d_k * n_heads`.\n",
    "- `self.W_K`: Proyecta las claves (`K`) del espacio de dimensión `d_model` al espacio de dimensión `d_k * n_heads`.\n",
    "- `self.W_V`: Proyecta los valores (`V`) del espacio de dimensión `d_model` al espacio de dimensión `d_v * n_heads`.\n",
    "- `self.linear`: Combina las salidas de todas las cabeceras de atención y las proyecta de vuelta al espacio de dimensión `d_model`.\n",
    "- `self.layer_norm`: Normaliza la salida para estabilizar y acelerar el entrenamiento.\n",
    "\n",
    "Aquí, `d_model` es la dimensión de entrada, mientras que `d_k * n_heads` y `d_v * n_heads` son las dimensiones de salida de las capas lineales para las consultas/claves y valores, respectivamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f674da-5531-4b6b-ae6b-4415a22c2c2f",
   "metadata": {},
   "source": [
    "El método `forward` en el código define cómo se procesan las entradas a través de la capa de atención multi-cabecera. Detallemos cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9db385-9179-4c3d-9b85-a760dd2a499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)  # [batch_size x n_heads x len_q x d_k]\n",
    "#k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)  # [batch_size x n_heads x len_k x d_k]\n",
    "#v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)  # [batch_size x n_heads x len_k x d_v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9032b027-614c-4f33-bf4f-16fb2d0d1bd5",
   "metadata": {},
   "source": [
    "- Las consultas (`Q`), claves (`K`) y valores (`V`) se proyectan a dimensiones más altas (`d_k * n_heads`, `d_k * n_heads` y `d_v * n_heads`, respectivamente) utilizando las capas lineales `W_Q`, `W_K` y `W_V`.\n",
    "- Luego, se reconfiguran las dimensiones para dividir estas proyecciones en `n_heads` diferentes cabeceras de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21ccbf-e914-426f-a2aa-8b5d4dbf23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)  # [batch_size x n_heads x len_q x len_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be517c63-4ebc-4160-be06-8e9c5ef21469",
   "metadata": {},
   "source": [
    "La máscara de atención (attn_mask) se expande y se repite para cada cabecera de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5ab47-90d9-489b-985c-f4ed07571f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)  # [batch_size x n_heads x len_q x d_v]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3e891-5b7a-4510-89f7-4c79310ae8f1",
   "metadata": {},
   "source": [
    "Se aplica la atención de producto punto escalado a las consultas, claves y valores divididos en múltiples cabeceras.\n",
    "`ScaledDotProductAttention` calcula las puntuaciones de atención, aplica la máscara y genera el contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3beb2a-9d38-4907-b557-e63183cc53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)  # [batch_size x len_q x n_heads * d_v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937ea5f-f601-4dbb-ae05-24fe0d7a9dfe",
   "metadata": {},
   "source": [
    "Las salidas de las múltiples cabeceras se vuelven a configurar para concatenarlas en una sola representación de contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692312b-0147-4451-a692-42b32cef26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = self.linear(context)\n",
    "#return self.layer_norm(output + residual), attn  # [batch_size x len_q x d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d2256-8410-41e5-abc5-c3ceee738451",
   "metadata": {},
   "source": [
    "* El contexto concatenado se proyecta de nuevo a la dimensión original (d_model) usando una capa lineal.\n",
    "* Se aplica la normalización de capa para estabilizar el entrenamiento.\n",
    "* La suma residual se utiliza para mejorar el flujo de gradientes y la estabilidad del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe566ea-d012-4171-b29f-cbde63a318af",
   "metadata": {},
   "source": [
    "La **autoatención**, o **self-attention**, permite que cada posición en una secuencia de entrada o salida se relacione con otras posiciones en la misma secuencia. En el transformer, la autoatención se implementa en las clases `ScaledDotProductAttention` y `MultiHeadAttention` y se utiliza tanto en el codificador como en el decodificador. Este mecanismo es fundamental para capturar dependencias a largo plazo y relaciones complejas entre palabras, mejorando así la capacidad del modelo para comprender y generar lenguaje natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0d386-1fd9-41ea-b87c-37ccbc77c548",
   "metadata": {},
   "source": [
    "La clase `PoswiseFeedForwardNet` en el código siguiente implementa una red de alimentación hacia adelante (Feed-Forward Network, FFN) que se aplica punto a punto (posicionalmente) en cada posición de la secuencia de entrada en un Transformer. Esta red consiste en dos capas convolucionales con una activación no lineal entre ellas, seguidas de una normalización de capa (Layer Normalization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274f3e9-362c-4edc-934e-c992314bd9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def __init__(self):\n",
    "#    super(PoswiseFeedForwardNet, self).__init__()\n",
    "#    self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "#    self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "#    self.layer_norm = nn.LayerNorm(d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece2520-0764-4501-b6b8-46359ca9dc6c",
   "metadata": {},
   "source": [
    "- `self.conv1`: Una capa convolucional con d_model canales de entrada y `d_ff` canales de salida, y un tamaño de kernel de 1. Esto actúa como una transformación lineal que aumenta la dimensión de `d_model` a `d_ff`.\n",
    "- `self.conv2`: Otra capa convolucional con `d_ff` canales de entrada y `d_model` canales de salida, y un tamaño de kernel de 1. Esto reduce la dimensión de vuelta a `d_model`.\n",
    "- `self.layer_norm`: Una capa de normalización que normaliza las activaciones a lo largo de la dimensión de características d_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596de447-986b-4c4b-a1b9-55a1d109335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def forward(self, inputs):\n",
    "#    residual = inputs  # inputs: [batch_size, len_q, d_model]\n",
    "#    output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "#    output = self.conv2(output).transpose(1, 2)\n",
    "#    return self.layer_norm(output + residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6484aad-6c63-49ac-8ddd-fe1d505d0f4c",
   "metadata": {},
   "source": [
    "- La entrada original se guarda en la variable residual para la conexión residual posterior.\n",
    "- La entrada se transpone para que las dimensiones sean `[batch_size, d_model, len_q]`. Esto es necesario porque nn.Conv1d espera que la dimensión de características esté en el segundo lugar.\n",
    "- `self.conv1` aplica una transformación lineal aumentando la dimensión de `d_model` a `d_ff`.\n",
    "- La activación ReLU se aplica para introducir no linealidad en el modelo.\n",
    "- Después de estos pasos, la dimensión de output es `[batch_size, d_ff, len_q]`.\n",
    "- `self.conv2` aplica una segunda transformación lineal que reduce la dimensión de `d_ff` de vuelta a d_model.\n",
    "-  La salida se transpone de nuevo a la forma `[batch_size, len_q, d_model]` para que coincida con la forma de la entrada original.\n",
    "- La salida de las capas convolucionales se suma a la entrada original (residual), implementando una conexión residual que ayuda a mitigar el problema del desvanecimiento del gradiente y facilita el entrenamiento de redes profundas.\n",
    "- La salida combinada se normaliza a lo largo de la dimensión de características d_model para estabilizar y acelerar el entrenamiento.\n",
    "- La salida final del método `forward` es la representación normalizada con la suma residual aplicada.\n",
    "\n",
    "Este mecanismo permite al transformer transformar las representaciones de manera no lineal y preservar las características importantes a través de las capas, mejorando la capacidad del modelo para aprender relaciones complejas en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9047b3-fbe8-4d4f-9247-01402364f207",
   "metadata": {},
   "source": [
    "La clase `EncoderLayer` define una capa del codificador en el transformer. Cada capa del codificador en un transformer consiste en dos subcomponentes principales: una capa de atención multi-cabecera y una red de alimentación hacia adelante (Feed-Forward Network, FFN). Ambas subcomponentes utilizan conexiones residuales y normalización de capa.\n",
    "\n",
    "Por ejemplo aquí:\n",
    "\n",
    "**Atención multi-cabecera:**\n",
    "\n",
    "- `self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)`: Se aplica atención multi-cabecera donde las entradas `(enc_inputs)` se utilizan como consultas (Q), claves (K) y valores (V). La máscara de atención `(enc_self_attn_mask)` se aplica para ignorar los tokens de relleno.\n",
    "- La salida es una tupla `(enc_outputs, attn)`, donde `enc_outputs` es el resultado de la atención y attn son las puntuaciones de atención.\n",
    "\n",
    "**Red de alimentación hacia adelante:**\n",
    "\n",
    "- `self.pos_ffn(enc_outputs)`: La salida de la atención se pasa a través de la red de alimentación hacia adelante.\n",
    "La salida `enc_outputs` tiene la misma dimensión que la entrada debido a las conexiones residuales y la normalización de capa.\n",
    "\n",
    "Se devuelve `enc_outputs` y las puntuaciones de atención attn.\n",
    "\n",
    "La clase `DecoderLayer` define una capa del decodificador en el transformer. Cada capa del decodificador en un transformer consiste en tres subcomponentes principales: una capa de auto-atención multi-cabecera, una capa de atención multi-cabecera entre el decodificador y el codificador, y una red de alimentación hacia adelante (FFN). Todas estas subcomponentes utilizan conexiones residuales y normalización de capa. \n",
    "\n",
    "Por ejemplo aquí:\n",
    "\n",
    "**Auto-atención multicabecera**:\n",
    "\n",
    "- `self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)`: Se aplica auto-atención multi-cabecera donde las entradas del decodificador (dec_inputs) se utilizan como consultas (Q), claves (K) y valores (V). La máscara de auto-atención (dec_self_attn_mask) asegura que cada posición en la salida del decodificador solo puede atender a posiciones anteriores.\n",
    "- La salida es una tupla `(dec_outputs, dec_self_attn)`, donde `dec_outputs` es el resultado de la auto-atención y `dec_self_attn` son las puntuaciones de auto-atención.\n",
    "\n",
    "**Atención multi-cabecera decodificador-codificador:**\n",
    "\n",
    "- `self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)`: Se aplica atención multi-cabecera entre el decodificador y el codificador donde las salidas del decodificador (dec_outputs) se utilizan como consultas (Q), y las salidas del codificador (enc_outputs) se utilizan como claves (K) y valores (V). La máscara de atención decodificador-codificador `(dec_enc_attn_mask)` se aplica para ignorar los tokens de relleno en el codificador.\n",
    "- La salida es una tupla `(dec_outputs, dec_enc_attn)`, donde `dec_outputs` es el resultado de la atención decodificador-codificador y `dec_enc_attn` son las puntuaciones de atención.\n",
    "\n",
    "**Red de alimentación hacia adelante:**\n",
    "\n",
    "- `self.pos_ffn(dec_outputs)`: La salida de la atención decodificador-codificador se pasa a través de la red de alimentación hacia adelante.\n",
    "- La salida dec_outputs tiene la misma dimensión que la entrada debido a las conexiones residuales y la normalización de capa.\n",
    "\n",
    "Se devuelven `dec_outputs`, las puntuaciones de auto-atención `dec_self_attn` y las puntuaciones de atención decodificador-codificador `dec_enc_attn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002382a8-407b-43a2-bc44-c0c1a78e6f7d",
   "metadata": {},
   "source": [
    "La clase `Encoder` define el componente de codificación en un modelo Transformer. Esta clase toma una secuencia de entrada y la procesa a través de varias capas de codificación, cada una de las cuales aplica atención multi-cabecera y una red de alimentación hacia adelante posicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1629e1-7889-4c81-8293-6e9d4f1e63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def __init__(self):\n",
    "#    super(Encoder, self).__init__()\n",
    "#    self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "#    self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model), freeze=True)\n",
    "#    self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552e2e3-b55d-41f2-b5f2-b431db613a19",
   "metadata": {},
   "source": [
    "- `self.src_emb:` Capa de embeddings que transforma los índices de las palabras en vectores de dimensión `d_model`.\n",
    "- `self.pos_emb`: Capa de embeddings que proporciona información posicional. Se inicializa con una tabla de codificación sinusoidal preentrenada y se congela (no se entrena).\n",
    "- `self.layers`: Lista de capas de codificación (EncoderLayer). Se crean `n_layers` instancias de `EncoderLayer`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500ab71-44f4-485c-8ff3-b72c2d71310b",
   "metadata": {},
   "source": [
    "El método `forward` define cómo se procesan las entradas a través del codificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d789f4-7709-4913-ac46-6f2df60382e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def forward(self, enc_inputs):  # enc_inputs: [batch_size x source_len]\n",
    "#    enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n",
    "#    enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "#    enc_self_attns = []\n",
    "#    for layer in self.layers:\n",
    "#        enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "#        enc_self_attns.append(enc_self_attn)\n",
    "#    return enc_outputs, enc_self_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5d956-5877-4cfc-b7be-7fc83e897a4c",
   "metadata": {},
   "source": [
    "- Se calculan los embeddings de las palabras y se suman a los embeddings posicionales para incorporar información de posición.\n",
    "- Se crea una máscara de atención para los tokens de relleno (padding) para asegurarse de que estos no contribuyan a las puntuaciones de atención.\n",
    "- Cada capa del codificador toma las salidas de la capa anterior y aplica atención multi-cabecera y una red de alimentación hacia adelante. Las puntuaciones de atención se guardan para cada capa.\n",
    "- Se devuelven las salidas finales del codificador `(enc_outputs)` y las puntuaciones de atención de todas las capas `(enc_self_attns)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f61ef-d1a1-447a-a05d-4245c6cec6ee",
   "metadata": {},
   "source": [
    "La clase `Decoder` define el componente de decodificación en un modelo transformer. Esta clase toma una secuencia de entrada del decodificador y las salidas del codificador, y las procesa a través de varias capas de decodificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56429f-c356-4371-93ef-3d3b3d17b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def __init__(self):\n",
    "#    super(Decoder, self).__init__()\n",
    "#    self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "#    self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model), freeze=True)\n",
    "#    self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc994813-b5cc-48c2-99a5-345377110020",
   "metadata": {},
   "source": [
    "- `self.tgt_emb`: Capa de embeddings que transforma los índices de las palabras en vectores de dimensión `d_model`.\n",
    "- `self.pos_emb`: Capa de embeddings que proporciona información posicional. Se inicializa con una tabla de codificación sinusoidal preentrenada y se congela (no se entrena).\n",
    "- `self.layers`: Lista de capas de decodificación (DecoderLayer). Se crean `n_layers` instancias de `DecoderLayer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1e64f-260c-413d-81ea-caaa7f788fca",
   "metadata": {},
   "source": [
    "El método `forward` define cómo se procesan las entradas a través del decodificador.\n",
    "\n",
    "- Se calculan los embeddings de las palabras y se suman a los embeddings posicionales para incorporar información de posición.\n",
    "- Se crea una máscara de atención para los tokens de relleno (padding) en las entradas del decodificador.\n",
    "- Se crea una máscara para asegurarse de que cada posición en la salida del decodificador solo pueda atender a posiciones anteriores (y no futuras).\n",
    "- Se crea una máscara para ignorar los tokens de relleno en las salidas del codificador durante la atención entre el decodificador y el codificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b1e2e-e8b4-4b28-a313-0d979e783c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dec_self_attns, dec_enc_attns = []\n",
    "#for layer in self.layers:\n",
    "#    dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "#    dec_self_attns.append(dec_self_attn)\n",
    "#    dec_enc_attns.append(dec_enc_attn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b1d2a-7ddf-47d6-b5c1-d073afdc15fb",
   "metadata": {},
   "source": [
    "Cada capa del decodificador toma las salidas de la capa anterior y aplica auto-atención multi-cabecera, atención entre el decodificador y el codificador, y una red de alimentación hacia adelante. Las puntuaciones de auto-atención y atención decodificador-codificador se guardan para cada capa.\n",
    "\n",
    "\n",
    "Se devuelven las salidas finales del decodificador `(dec_outputs)`, las puntuaciones de auto-atención de todas las capas `(dec_self_attns)` y las puntuaciones de atención decodificador-codificador de todas las capas `(dec_enc_attns)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17830b63-a714-4dc3-ae4c-1eaa91a950b6",
   "metadata": {},
   "source": [
    "La clase `Transformer` implementa el modelo transformer completo, que consiste en un codificador (encoder) y un decodificador (decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9df91-13e6-443e-b03e-a3df59a49ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def __init__(self):\n",
    "#    super(Transformer, self).__init__()\n",
    "#    self.encoder = Encoder()\n",
    "#    self.decoder = Decoder()\n",
    "#    self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653cfb6-06f0-480b-834b-13a4f0cb31b9",
   "metadata": {},
   "source": [
    "- `self.encoder`: Instancia de la clase `Encoder` que procesa las entradas de la secuencia fuente.\n",
    "- `self.decoder`: Instancia de la clase `Decoder` que genera la secuencia de salida basándose en las salidas del codificador y las entradas del decodificador.\n",
    "- `self.projection`: Una capa lineal que proyecta las salidas del decodificador a un espacio de dimensión igual al tamaño del vocabulario del objetivo (`tgt_vocab_size`). No se utiliza un sesgo en esta capa (`bias=False`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b774d02-d8fa-4946-be6f-a63046bca4ca",
   "metadata": {},
   "source": [
    "El método `forward` define cómo se procesan las entradas a través del modelo transformer completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245c67f-28d9-4535-8c8c-0d2d618c06d6",
   "metadata": {},
   "source": [
    "- `self.encoder(enc_inputs)`: Se procesan las entradas del codificador `(enc_inputs)` a través del codificador. Esto devuelve las salidas del codificador (enc_outputs) y las puntuaciones de auto-atención del codificador `(enc_self_attns)`.\n",
    "- `self.decoder(dec_inputs, enc_inputs, enc_outputs)`: Se procesan las entradas del decodificador `(dec_inputs)` junto con las entradas del codificador `(enc_inputs)` y las salidas del codificador `(enc_outputs)` a través del decodificador. Esto devuelve las salidas del decodificador `(dec_outputs)`, las puntuaciones de auto-atención del decodificador `(dec_self_attns)` y las puntuaciones de atención decodificador-codificador `(dec_enc_attns)`.\n",
    "- `self.projection(dec_outputs)`: Las salidas del decodificador se proyectan a un espacio de dimensión igual al tamaño del vocabulario del objetivo `(tgt_vocab_size)` utilizando una capa lineal. Esto genera los logits de decodificación `(dec_logits)`.\n",
    "- `dec_logits.view(-1, dec_logits.size(-1))`: Se ajusta la forma de los logits de decodificación para que se puedan comparar con las etiquetas de la secuencia objetivo durante el cálculo de la pérdida.\n",
    "\n",
    "Se devuelven los logits de decodificación `(dec_logits)`, las puntuaciones de auto-atención del codificador `(enc_self_attns)`, las puntuaciones de auto-atención del decodificador `(dec_self_attns)` y las puntuaciones de atención decodificador-codificador `(dec_enc_attns)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be59b7-0d9d-499d-a684-89cf18811d6f",
   "metadata": {},
   "source": [
    "El código `greedy_decoder` implementa un decodificador greedy para un modelo `Transformer`. Este decodificador genera la secuencia de salida palabra por palabra durante la inferencia. El enfoque greedy significa que en cada paso se elige la palabra con la mayor probabilidad, sin considerar otras posibles secuencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8a43e-144a-464f-8842-22d026b4dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    for i in range(0, 5):\n",
    "#        dec_input[0][i] = next_symbol\n",
    "#        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
    "#        projected = model.projection(dec_outputs)\n",
    "#        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "#        next_word = prob.data[i]\n",
    "#        next_symbol = next_word.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a30ae9-0365-44df-9e22-7ea188757ac3",
   "metadata": {},
   "source": [
    "- El bucle recorre cada posición de la secuencia de salida (de 0 a 5).\n",
    "- En cada iteración, next_symbol se asigna a la posición actual en `dec_input`.\n",
    "- Se pasa `dec_input` (junto con `enc_input` y `enc_outputs`) a través del decodificador del modelo para obtener `dec_outputs`, que son las representaciones decodificadas.\n",
    "- Las salidas del decodificador `(dec_outputs)` se pasan por una capa de proyección para mapearlas al espacio del vocabulario del objetivo.\n",
    "- `projected` se reduce a dos dimensiones, eliminando el primer eje (batch).\n",
    "- `prob` es el índice de la palabra con la mayor probabilidad en cada posición.\n",
    "- `next_word` se selecciona como la palabra con la mayor probabilidad en la posición actual.\n",
    "- `next_symbol` se actualiza con el índice de next_word para la siguiente iteración del bucle.\n",
    "\n",
    "La función devuelve `dec_input`, que contiene la secuencia de salida generada palabra por palabra utilizando el enfoque greedy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1ac42-d56a-42eb-8876-ba1a8287082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "# S: Símbolo que indica el inicio de la entrada de decodificación\n",
    "# E: Símbolo que indica el inicio de la salida de decodificación\n",
    "# P: Símbolo que rellenará la secuencia en blanco si el tamaño de los datos del lote actual es menor que los pasos de tiempo\n",
    "\n",
    "def make_batch():\n",
    "    # Crea el lote de entrada, salida y objetivo\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
    "    return torch.LongTensor(input_batch), torch.LongTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_model):\n",
    "    # Calcula la tabla de codificación sinusoidal\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    # Obtiene la máscara de atención de padding\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) es el token de PAD\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), uno es enmascarado\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "def get_attn_subsequent_mask(seq):\n",
    "    # Obtiene la máscara de atención subsecuente\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    return subsequent_mask\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9)  # Rellena elementos del tensor con valor donde la máscara es uno.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_v, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # Q: [batch_size x len_q x d_model], K: [batch_size x len_k x d_model], V: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)  # attn_mask: [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)  # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = self.linear(context)\n",
    "        return self.layer_norm(output + residual), attn  # output: [batch_size x len_q x d_model]\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs  # inputs: [batch_size, len_q, d_model]\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        return self.layer_norm(output + residual)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)  # enc_inputs se usa como Q, K, V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)  # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model), freeze=True)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):  # enc_inputs: [batch_size x source_len]\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model), freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):  # dec_inputs: [batch_size x target_len]\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n",
    "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
    "\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs)  # dec_logits: [batch_size x src_vocab_size x tgt_vocab_size]\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "def greedy_decoder(model, enc_input, start_symbol):\n",
    "    \"\"\"\n",
    "    Para simplificar, un Decodificador Greedy es una búsqueda de Beam cuando K=1. Esto es necesario para la inferencia, ya que no \n",
    "    conocemos la secuencia de entrada objetivo. Por lo tanto, tratamos de generar la entrada objetivo palabra por palabra y luego \n",
    "    la alimentamos en el transformer. \n",
    "    Referencia inicial: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
    "    :param model: Modelo Transformer\n",
    "    :param enc_input: La entrada del codificador\n",
    "    :param start_symbol: El símbolo de inicio. En este ejemplo es 'S', que corresponde al índice 4\n",
    "    :return: La entrada objetivo\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
    "    dec_input = torch.zeros(1, 5).type_as(enc_input.data)\n",
    "    next_symbol = start_symbol\n",
    "    for i in range(0, 5):\n",
    "        dec_input[0][i] = next_symbol\n",
    "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
    "        projected = model.projection(dec_outputs)\n",
    "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        next_word = prob.data[i]\n",
    "        next_symbol = next_word.item()\n",
    "    return dec_input\n",
    "\n",
    "def showgraph(attn):\n",
    "    # Muestra el gráfico de la atención\n",
    "    attn = attn[-1].squeeze(0)[0]\n",
    "    attn = attn.squeeze(0).data.numpy()\n",
    "    fig = plt.figure(figsize=(n_heads, n_heads))  # [n_heads, n_heads]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attn, cmap='viridis')\n",
    "\n",
    "    # Usando FixedLocator y FixedFormatter para evitar los warnings\n",
    "    ax.set_xticks(range(len([''] + sentences[0].split())))\n",
    "    ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n",
    "    ax.xaxis.set_major_locator(FixedLocator(ax.get_xticks()))\n",
    "    ax.xaxis.set_major_formatter(FixedFormatter([''] + sentences[0].split()))\n",
    "\n",
    "    ax.set_yticks(range(len([''] + sentences[2].split())))\n",
    "    ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
    "    ax.yaxis.set_major_locator(FixedLocator(ax.get_yticks()))\n",
    "    ax.yaxis.set_major_formatter(FixedFormatter([''] + sentences[2].split()))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "    # Parámetros del Transformer\n",
    "    # El Padding debe ser el índice cero\n",
    "    src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4}\n",
    "    src_vocab_size = len(src_vocab)\n",
    "\n",
    "    tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'S': 5, 'E': 6}\n",
    "    number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "    tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "    src_len = 5  # longitud de la fuente\n",
    "    tgt_len = 5  # longitud del objetivo\n",
    "\n",
    "    d_model = 512  # Tamaño del Embedding\n",
    "    d_ff = 2048  # Dimensión de la FeedForward\n",
    "    d_k = d_v = 64  # dimensión de K(=Q), V\n",
    "    n_layers = 6  # número de capas del Codificador y Decodificador\n",
    "    n_heads = 8  # número de cabeceras en la atención Multi-Cabecera\n",
    "\n",
    "    model = Transformer()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    enc_inputs, dec_inputs, target_batch = make_batch()\n",
    "\n",
    "    for epoch in range(20):\n",
    "        optimizer.zero_grad()\n",
    "        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "        loss = criterion(outputs, target_batch.contiguous().view(-1))\n",
    "        print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Prueba\n",
    "    greedy_dec_input = greedy_decoder(model, enc_inputs, start_symbol=tgt_vocab[\"S\"])\n",
    "    predict, _, _, _ = model(enc_inputs, greedy_dec_input)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "    print('Primera cabecera del último estado enc_self_attns')\n",
    "    showgraph(enc_self_attns)\n",
    "\n",
    "    print('Primera cabecera del último estado dec_self_attns')\n",
    "    showgraph(dec_self_attns)\n",
    "\n",
    "    print('Primera cabecera del último estado dec_enc_attns')\n",
    "    showgraph(dec_enc_attns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f59db-fe5f-4b09-82dd-b6aef1cccca3",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "\n",
    "* Modifica la función `greedy_decoder` para implementar Beam Search. Ajusta el tamaño del beam y compara los resultados con la decodificación greedy.\n",
    "* Implementa técnicas de regularización como dropout y weight decay en el modelo transformer. Observa cómo afectan al rendimiento y la estabilidad del entrenamiento.\n",
    "* Implementa variantes de atención, como atención relativa posicional o local, y compara su impacto en la calidad de la traducción.\n",
    "* Modifica el transformer para utilizar una memoria dinámica, similar al modelo Transformer-XL. Evalúa cómo esta modificación afecta a la longitud de las secuencias que el modelo puede manejar.\n",
    "* Añade capas de normalización como layer normalization en diferentes partes del modelo y compara los resultados con el modelo original.\n",
    "* Implementa un modelo transformer que pueda manejar múltiples modalidades de entrada, como texto e imágenes, y realiza una tarea de generación de descripciones a partir de imágenes.\n",
    "* Preentrena el modelo transformer en un gran corpus de texto y luego realiza fine-tuning en un conjunto de datos específico para tareas de traducción o generación de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e368aa-dabd-457a-ae6f-e91e86337788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(model, enc_input, start_symbol, beam_size=3):\n",
    "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
    "    dec_inputs = torch.zeros(1, 5).type_as(enc_input.data)\n",
    "    dec_inputs[0][0] = start_symbol\n",
    "    end_symbol = tgt_vocab[\"E\"]\n",
    "    hypotheses = [(dec_inputs, 0.0)]  # (dec_input, log_prob)\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        candidates = []\n",
    "        for dec_input, log_prob in hypotheses:\n",
    "            dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
    "            projected = model.projection(dec_outputs)\n",
    "            probs = nn.Softmax(dim=-1)(projected[:, -1])\n",
    "            topk_probs, topk_indices = probs.topk(beam_size)\n",
    "\n",
    "            for j in range(beam_size):\n",
    "                next_dec_input = dec_input.clone()\n",
    "                next_dec_input[0][i] = topk_indices[0][j]\n",
    "                candidates.append((next_dec_input, log_prob + torch.log(topk_probs[0][j])))\n",
    "\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        hypotheses = candidates[:beam_size]\n",
    "\n",
    "    return hypotheses[0][0]\n",
    "\n",
    "# Uso:\n",
    "beam_dec_input = beam_search_decoder(model, enc_inputs, start_symbol=tgt_vocab[\"S\"], beam_size=3)\n",
    "predict, _, _, _ = model(enc_inputs, beam_dec_input)\n",
    "predict = predict.data.max(1, keepdim=True)[1]\n",
    "print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f11391-a180-4c7c-8af7-ba2754bd70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcae00-e513-45c3-ba37-3fd4a410ed23",
   "metadata": {},
   "source": [
    "### Otros transformers\n",
    "\n",
    "Además del transformer original, han surgido varias variantes y extensiones que mejoran y adaptan el modelo a diferentes tareas y dominios. Algunas de las variantes más conocidas incluyen:\n",
    "\n",
    "* BERT (Bidirectional Encoder Representations from Transformers): Un modelo preentrenado que se utiliza principalmente para tareas de clasificación y comprensión del lenguaje. BERT se entrena utilizando una técnica llamada enmascaramiento de tokens, donde algunas palabras en la secuencia de entrada se enmascaran y el modelo aprende a predecir estas palabras basándose en el contexto bidireccional.\n",
    "\n",
    "* GPT (Generative Pre-trained Transformer): Un modelo de generación de texto que se entrena de manera autoregresiva. A diferencia de BERT, GPT se entrena para predecir la siguiente palabra en una secuencia, lo que lo hace ideal para tareas de generación de texto.\n",
    "\n",
    "* T5 (Text-to-Text Transfer Transformer): Un modelo que convierte todas las tareas de procesamiento del lenguaje en un problema de generación de texto. T5 puede manejar una amplia variedad de tareas como traducción, resumen, y clasificación, formulando cada tarea como un problema de generación de secuencias.\n",
    "\n",
    "* RoBERTa (Robustly optimized BERT approach): Una variante de BERT que mejora el rendimiento mediante el ajuste de los hiperparámetros de entrenamiento y el uso de un corpus más grande y diverso para el preentrenamiento.\n",
    "\n",
    "* Transformer-XL: Una extensión que introduce conexiones recurrentes en el modelo transformer, permitiendo que maneje dependencias a largo plazo de manera más efectiva y eficiente.\n",
    "\n",
    "* XLNet: Una variante que combina las ventajas de BERT y Transformer-XL, entrenando el modelo de manera autoregresiva y bidireccional al mismo tiempo.\n",
    "\n",
    "Estas variantes han demostrado ser extremadamente efectivas en una amplia gama de tareas de NLP, estableciendo nuevos estándares en benchmarks y aplicaciones del mundo real."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
