{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f6837c",
   "metadata": {},
   "source": [
    "### Repaso RL, RLHF para NLP\n",
    "---\n",
    "\n",
    "\n",
    "El aprendizaje por refuerzo sin modelo (Model-Free RL) se centra en aprender directamente una política o un valor de acción sin construir un modelo explícito del entorno. Esto se logra a través de métodos que optimizan políticas y funciones de valor utilizando la experiencia directa de interacción con el entorno.\n",
    "\n",
    "### Conceptos básicos y notación\n",
    "\n",
    "Consideremos un problema de RL formulado como un Proceso de Decisión de Markov (MDP), que se define por un conjunto de estados $S$, un conjunto de acciones $A$, una función de recompensa $R$, y una función de transición $P$. El objetivo es encontrar una política $\\pi$ que maximice la recompensa acumulada esperada.\n",
    "\n",
    "#### Definiciones formales\n",
    "\n",
    "1. **Valor del Estado**: $V^\\pi(s)$ es el valor esperado de seguir la política $\\pi$ desde el estado $s$.\n",
    "\n",
    "   $$\n",
    "   V^\\pi(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\mid s_0 = s, \\pi\\right]\n",
    "   $$\n",
    "   \n",
    "   Esta ecuación se genera a partir de la expectativa de la suma de las recompensas futuras descontadas ($\\gamma$ es el factor de descuento) al seguir la política $\\pi$ comenzando en el estado $s$.\n",
    "\n",
    "2. **Valor de la Acción**: $Q^\\pi(s, a)$ es el valor esperado de tomar la acción $a$ en el estado $s$ y luego seguir la política $\\pi$.\n",
    "   \n",
    "   $$\n",
    "   Q^\\pi(s, a) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\mid s_0 = s, a_0 = a, \\pi\\right]\n",
    "   $$\n",
    "   Aquí, $Q^\\pi(s, a)$ representa el valor esperado de la recompensa futura descontada después de tomar la acción $a$ en el estado $s$ y seguir la política $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "## Optimización de políticas\n",
    "\n",
    "La optimización de políticas se refiere a la mejora continua de la política $\\pi$ para maximizar la recompensa acumulada esperada.\n",
    "\n",
    "### Policy Gradient\n",
    "\n",
    "Los métodos de Policy Gradient optimizan directamente la política parametrizada $\\pi_\\theta$ mediante gradientes de la recompensa acumulada esperada.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "La función objetivo a maximizar es el retorno esperado:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "El gradiente de la política se calcula como:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) Q^\\pi(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "Esta ecuación se deriva utilizando la regla del gradiente logarítmico (Log-Likelihood Ratio) y el teorema de la expectativa, lo que permite que el gradiente de la política sea calculado como la expectativa de los gradientes ponderados por el valor de la acción $Q^\\pi(s_t, a_t)$.\n",
    "\n",
    "### A2C/A3C (Asynchronous Advantage Actor-Critic)\n",
    "\n",
    "A2C y A3C son algoritmos actor-crítico que combinan un actor que aprende la política y un crítico que evalúa la política.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "El objetivo es minimizar la pérdida:\n",
    "\n",
    "$$\n",
    "L(\\theta) = -\\log \\pi_\\theta(a_t \\mid s_t) \\left( R_t - V^\\pi(s_t) \\right) + \\frac{1}{2} \\left( R_t - V^\\pi(s_t) \\right)^2\n",
    "$$\n",
    "\n",
    "Donde $R_t$ es el retorno observado y $V^\\pi(s_t)$ es el valor estimado del estado. La primera parte de la ecuación es la pérdida del actor, que se minimiza cuando las acciones que lleva a cabo la política son buenas según la estimación del crítico. La segunda parte es la pérdida del crítico, que se minimiza ajustando el valor del estado para que coincida con el retorno observado.\n",
    "\n",
    "### PPO (Proximal Policy Optimization)\n",
    "\n",
    "PPO es un método que restringe la actualización de la política para evitar grandes cambios que desestabilicen el entrenamiento.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "La función objetivo para PPO es:\n",
    "\n",
    "$$\n",
    "L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]\n",
    "$$\n",
    "\n",
    "Donde $r_t(\\theta) = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}$ y $\\hat{A}_t$ es la ventaja estimada. El objetivo es maximizar la recompensa esperada mientras se evita que $r_t(\\theta)$, que es la relación de probabilidad, se aleje demasiado de 1 mediante la operación de \"clipping\".\n",
    "\n",
    "### TRPO (Trust Region Policy Optimization)\n",
    "\n",
    "TRPO optimiza la política dentro de una región de confianza para garantizar la mejora de la política.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "El objetivo de TRPO es maximizar:\n",
    "\n",
    "$$\n",
    "L^{\\text{TRPO}}(\\theta) = \\mathbb{E}_t \\left[ \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)} \\hat{A}_t \\right]\n",
    "$$\n",
    "\n",
    "Sujeto a:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_t \\left[ \\text{KL} \\left[ \\pi_{\\theta_{\\text{old}}} (\\cdot \\mid s_t) \\parallel \\pi_\\theta (\\cdot \\mid s_t) \\right] \\right] \\leq \\delta\n",
    "$$\n",
    "\n",
    "Donde la función objetivo maximiza el rendimiento esperado, y la restricción asegura que el cambio en la política (medido por la divergencia KL) no sea mayor que un valor umbral $\\delta$.\n",
    "\n",
    "### DPPO (Distributed Proximal Policy Optimization)\n",
    "\n",
    "DPPO extiende PPO para entornos distribuidos, permitiendo el entrenamiento paralelo de múltiples agentes.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "DPPO utiliza la misma función objetivo que PPO, pero con actualizaciones distribuidas y sincronizadas de los parámetros de la política a través de múltiples trabajadores. La clave está en la gestión de la sincronización y agregación de las actualizaciones para asegurar una convergencia estable y eficiente.\n",
    "\n",
    "### TD3 (Twin Delayed Deep Deterministic Policy Gradient)\n",
    "\n",
    "TD3 es una mejora del DDPG (Deep Deterministic Policy Gradient) que reduce la sobreestimación del valor de la acción mediante el uso de dos críticos y actualizaciones retrasadas.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "Las actualizaciones de los críticos son:\n",
    "\n",
    "$$\n",
    "y_t = r_t + \\gamma \\min_{i=1,2} Q_{\\theta_i'}(s_{t+1}, \\pi_{\\phi'}(s_{t+1}))\n",
    "$$\n",
    "\n",
    "Donde $y_t$ es la estimación de la recompensa futura, $Q_{\\theta_i'}$ son las funciones de valor de los dos críticos, y $\\pi_{\\phi'}$ es la política actualizada.\n",
    "\n",
    "Las actualizaciones del actor son retrasadas:\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi J(\\phi) = \\mathbb{E}_t \\left[ \\nabla_a Q_{\\theta_1}(s_t, a) \\mid_{a = \\pi_\\phi(s_t)} \\nabla_\\phi \\pi_\\phi(s_t) \\right]\n",
    "$$\n",
    "\n",
    "El actor se actualiza usando el gradiente de la función de valor del primer crítico, asegurando que las actualizaciones sean más estables al introducir un retraso.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3b3f5",
   "metadata": {},
   "source": [
    "## Aprendizaje Q\n",
    "\n",
    "El aprendizaje Q (Q-Learning) es uno de los algoritmos más fundamentales en el campo del Aprendizaje por Refuerzo (RL). Este algoritmo aprende una política óptima para un MDP (Proceso de Decisión de Markov) al estimar el valor esperado de las acciones en cada estado, denominado valor Q.\n",
    "\n",
    "Los avances recientes han extendido Q-Learning a través de técnicas de redes neuronales profundas y enfoques estadísticos avanzados, mejorando su capacidad para resolver problemas complejos y de alta dimensionalidad.\n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "Q-Learning es un método off-policy que busca aprender la función de valor Q $Q(s, a)$, que representa el valor esperado de realizar una acción $ a $ en un estado $ s $ y seguir la política óptima a partir de ese punto.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "La actualización de la función Q en Q-Learning se basa en la ecuación de Bellman:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\alpha $ es la tasa de aprendizaje.\n",
    "- $\\gamma $ es el factor de descuento.\n",
    "- $r_t$ es la recompensa obtenida al realizar la acción $a_t$ en el estado $s_t$.\n",
    "\n",
    "Esta ecuación ajusta el valor Q actual hacia el valor Q estimado del estado siguiente $s_{t+1}$.\n",
    "\n",
    "---\n",
    "\n",
    "### Deep Q-Network (DQN)\n",
    "\n",
    "DQN extiende Q-Learning utilizando redes neuronales profundas para aproximar la función Q. Esto permite que el algoritmo maneje espacios de estado continuos y de alta dimensionalidad.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "La función de pérdida en DQN se define como:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \\sim D} \\left[ \\left( y_t - Q(s_t, a_t; \\theta) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $y_t = r_t + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta^{-})$\n",
    "- $\\theta $ son los parámetros de la red neuronal.\n",
    "- $\\theta^{-}$ son los parámetros de la red de destino, que se actualizan periódicamente.\n",
    "\n",
    "El gradiente de esta función de pérdida se utiliza para actualizar los parámetros $\\theta$ de la red Q mediante retropropagación.\n",
    "\n",
    "---\n",
    "\n",
    "### C51 (Categorical DQN)\n",
    "\n",
    "C51 es una extensión de DQN que modela la distribución completa de los retornos futuros en lugar de solo su valor esperado. Utiliza una representación categórica de la distribución de valores.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "La distribución de valores se representa mediante una función de probabilidad discreta con $N$ categorías:\n",
    "\n",
    "$$\n",
    "Z_{\\theta}(s, a) = \\sum_{i=1}^{N} p_i \\delta(z_i)\n",
    "$$\n",
    "\n",
    "Donde $z_i$ son los valores de soporte y $p_i$ son las probabilidades asociadas. La función de pérdida se basa en la divergencia Kullback-Leibler:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \\sim D} \\left[ D_{\\text{KL}}(P^{\\pi}_{\\theta^{-}}(s_{t+1}, r_t + \\gamma z) \\parallel Z_{\\theta}(s_t, a_t)) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### QR-DQN (Quantile Regression DQN)\n",
    "\n",
    "QR-DQN extiende DQN utilizando regresión cuantílica para aproximar la distribución de retornos futuros. En lugar de modelar la media de los retornos, QR-DQN modela múltiples cuantiles de la distribución.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "QR-DQN utiliza $K$ cuantiles para representar la distribución de valores:\n",
    "\n",
    "$$\n",
    "Z_{\\theta}(s, a) = \\sum_{i=1}^{K} \\hat{Q}_\\theta^{\\tau_i}(s, a) \\delta(\\tau_i)\n",
    "$$\n",
    "\n",
    "Donde $\\tau_i$ son los cuantiles, y $\\hat{Q}_\\theta^{\\tau_i}(s, a)$ son las estimaciones de los valores de cuantiles. La función de pérdida se basa en la pérdida de cuantiles:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{K} \\sum_{i=1}^{K} \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \\sim D} \\left[ \\rho_{\\tau_i} \\left( r_t + \\gamma \\max_{a'} \\hat{Q}_{\\theta^{-}}^{\\tau_i}(s_{t+1}, a') - \\hat{Q}_{\\theta}^{\\tau_i}(s_t, a_t) \\right) \\right]\n",
    "$$\n",
    "\n",
    "Donde $\\rho_{\\tau_i}$ es la función de pérdida cuantílica de Huber.\n",
    "\n",
    "---\n",
    "\n",
    "### HER (Hindsight Experience Replay)\n",
    "\n",
    "HER es una técnica que modifica la forma en que se almacenan y reutilizan las experiencias en el aprendizaje por refuerzo. HER permite que los agentes aprendan de los fracasos transformando los objetivos de las trayectorias fallidas en objetivos alcanzables.\n",
    "\n",
    "#### Ecuaciones clave\n",
    "\n",
    "La idea principal es reutilizar cada transición observada $(s, a, r, s')$ como si se hubiera perseguido un objetivo diferente $g'$. Para cada transición original, se crean múltiples transiciones \"ficticias\" con diferentes objetivos:\n",
    "\n",
    "$$\n",
    "Q(s, a, g) \\leftarrow Q(s, a, g) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a', g) - Q(s, a, g) \\right)\n",
    "$$\n",
    "\n",
    "Donde $g$ es el objetivo original, y $g'$ es el nuevo objetivo. Esta técnica aumenta la eficiencia del uso de datos, especialmente en entornos de alta dimensión y con sparse rewards.\n",
    "\n",
    "---\n",
    "\n",
    "Este informe proporciona una visión detallada de varios métodos avanzados de aprendizaje Q, con un enfoque en las ecuaciones clave que permiten el entrenamiento y mejora de políticas en entornos complejos y de alta dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0254f",
   "metadata": {},
   "source": [
    "## Códigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b2634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Gradient\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "def update_policy(policy_network, optimizer, rewards, log_probs):\n",
    "    discounted_rewards = []\n",
    "    cumulative_reward = 0\n",
    "    for reward in rewards[::-1]:\n",
    "        cumulative_reward = reward + 0.99 * cumulative_reward\n",
    "        discounted_rewards.insert(0, cumulative_reward)\n",
    "    \n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, reward in zip(log_probs, discounted_rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A2C/A3C\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.actor = nn.Linear(128, action_dim)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return torch.softmax(self.actor(x), dim=-1), self.critic(x)\n",
    "\n",
    "def worker(global_model, optimizer, global_ep, res_queue):\n",
    "    local_model = ActorCritic(state_dim, action_dim)\n",
    "    local_model.load_state_dict(global_model.state_dict())\n",
    "    \n",
    "    while global_ep.value < max_episodes:\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards, log_probs, values = [], [], []\n",
    "        while not done:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            action_probs, value = local_model(state)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            log_prob = torch.log(action_probs.squeeze(0)[action])\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        R = torch.zeros(1, 1)\n",
    "        if not done:\n",
    "            R = local_model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))[1]\n",
    "        \n",
    "        policy_loss, value_loss = 0, 0\n",
    "        gae = torch.zeros(1, 1)\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = 0.99 * R + rewards[i]\n",
    "            advantage = R - values[i]\n",
    "            value_loss += advantage.pow(2)\n",
    "            \n",
    "            delta_t = rewards[i] + 0.99 * values[i + 1].data - values[i].data\n",
    "            gae = gae * 0.95 * 0.99 + delta_t\n",
    "            \n",
    "            policy_loss -= log_probs[i] * gae - 0.01 * log_probs[i].exp() * log_probs[i]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "        loss.backward()\n",
    "        for local_param, global_param in zip(local_model.parameters(), global_model.parameters()):\n",
    "            global_param._grad = local_param.grad\n",
    "        optimizer.step()\n",
    "        \n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        with global_ep.get_lock():\n",
    "            global_ep.value += 1\n",
    "        res_queue.put(global_ep.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO (Proximal Policy Optimization)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPO, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.actor = nn.Linear(128, action_dim)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        action_probs = torch.softmax(self.actor(x), dim=-1)\n",
    "        state_values = self.critic(x)\n",
    "        return action_probs, state_values\n",
    "\n",
    "# Función para recolectar trayectorias\n",
    "def collect_trajectories(env, policy_network, max_steps=200):\n",
    "    state = env.reset()\n",
    "    states, actions, log_probs, rewards = [], [], [], []\n",
    "    for _ in range(max_steps):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_probs, _ = policy_network(state_tensor)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        log_prob = torch.log(action_probs.squeeze(0)[action])\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        states.append(state_tensor)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return states, actions, log_probs, rewards\n",
    "\n",
    "# Función para calcular las ventajas\n",
    "def compute_advantages(rewards, values, gamma=0.99, lam=0.95):\n",
    "    advantages, returns = [], []\n",
    "    gae = 0\n",
    "    # Asegurarse de que values incluya el valor final (0 si el episodio ha terminado)\n",
    "    values.append(0)  # Añadir un valor adicional para manejar el último estado\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] - values[i]\n",
    "        gae = delta + gamma * lam * gae\n",
    "        advantages.insert(0, gae)\n",
    "        returns.insert(0, gae + values[i])\n",
    "    return returns, advantages\n",
    "\n",
    "\n",
    "def update_policy(policy_network, old_policy_network, optimizer, states, actions, log_probs, returns, advantages, epsilon=0.2):\n",
    "    new_log_probs = []\n",
    "    state_values = []\n",
    "    for state, action in zip(states, actions):\n",
    "        action_probs, state_value = policy_network(state)\n",
    "        new_log_prob = torch.log(action_probs.squeeze(0)[action])\n",
    "        new_log_probs.append(new_log_prob)\n",
    "        state_values.append(state_value)\n",
    "    \n",
    "    new_log_probs = torch.stack(new_log_probs)\n",
    "    state_values = torch.stack(state_values).squeeze(1)\n",
    "    returns = torch.tensor(returns)\n",
    "    advantages = torch.tensor(advantages)\n",
    "\n",
    "    ratio = torch.exp(new_log_probs - torch.stack(log_probs))\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    value_loss = (returns - state_values).pow(2).mean()\n",
    "    loss = policy_loss + 0.5 * value_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Ejemplo de uso\n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    model = PPO(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        states, actions, old_log_probs, rewards = collect_trajectories(env, model)\n",
    "        # Evaluar todos los estados a la vez si es posible para eficiencia\n",
    "        state_tensors = torch.cat(states)\n",
    "        _, state_values = model(state_tensors)\n",
    "        state_values = state_values.squeeze().tolist()  # Convertir los valores del tensor a lista\n",
    "        returns, advantages = compute_advantages(rewards, state_values)\n",
    "        update_policy(model, model, optimizer, states, actions, old_log_probs, returns, advantages)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRPO (Trust Region Policy Optimization) - estructura de codigo elemental\n",
    "\n",
    "\n",
    "def conjugate_gradient(Ax, b, nsteps, residual_tol=1e-10):\n",
    "    x = torch.zeros_like(b)\n",
    "    r = b.clone()\n",
    "    p = r.clone()\n",
    "    r_dot_r = torch.dot(r, r)\n",
    "    for i in range(nsteps):\n",
    "        Ap = Ax(p)\n",
    "        alpha = r_dot_r / torch.dot(p, Ap)\n",
    "        x += alpha * p\n",
    "        r -= alpha * Ap\n",
    "        new_r_dot_r = torch.dot(r, r)\n",
    "        beta = new_r_dot_r / r_dot_r\n",
    "        p = r + beta * p\n",
    "        r_dot_r = new_r_dot_r\n",
    "        if r_dot_r < residual_tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "def trpo_step(policy_network, states, actions, advantages, max_kl=1e-2):\n",
    "    def get_loss_and_kl():\n",
    "        action_probs, _ = policy_network(states)\n",
    "        log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1)))\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        kl = (old_action_probs * (old_log_probs - log_probs)).mean()\n",
    "        return policy_loss, kl\n",
    "    \n",
    "    policy_loss, kl = get_loss_and_kl()\n",
    "    \n",
    "    grads = torch.autograd.grad(policy_loss, policy_network.parameters(), create_graph=True)\n",
    "    grads = torch.cat([grad.view(-1) for grad in grads])\n",
    "    \n",
    "    Hvp = hessian_vector_product(kl, policy_network.parameters())\n",
    "    \n",
    "    step_direction = conjugate_gradient(Hvp, grads, 10)\n",
    "    \n",
    "    step_size = torch.sqrt(2 * max_kl / (torch.dot(step_direction, Hvp(step_direction)) + 1e-8))\n",
    "    final_step = step_size * step_direction\n",
    "    \n",
    "    apply_update(policy_network, final_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Proximal Policy Optimization (DPPO)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPO, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.actor = nn.Linear(128, action_dim)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return torch.softmax(self.actor(x), dim=-1), self.critic(x)\n",
    "\n",
    "def collect_trajectories(policy_network, env, max_steps=200):\n",
    "    state = env.reset()\n",
    "    states, actions, log_probs, rewards = [], [], [], []\n",
    "    for _ in range(max_steps):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action_probs, _ = policy_network(state_tensor)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        log_prob = torch.log(action_probs.squeeze(0)[action])\n",
    "        \n",
    "        states.append(state_tensor)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, log_probs, rewards\n",
    "\n",
    "def compute_advantages(rewards, values, gamma=0.99, lam=0.95):\n",
    "    returns, advantages = [], []\n",
    "    R = 0\n",
    "    A = 0\n",
    "    for reward, value in zip(rewards[::-1], values[::-1]):\n",
    "        R = reward + gamma * R\n",
    "        delta = reward + gamma * value - value\n",
    "        A = delta + gamma * lam * A\n",
    "        returns.insert(0, R)\n",
    "        advantages.insert(0, A)\n",
    "    return returns, advantages\n",
    "\n",
    "def update_policy(policy_network, optimizer, states, actions, log_probs, returns, advantages, epsilon=0.2):\n",
    "    new_log_probs, state_values = [], []\n",
    "    for state, action in zip(states, actions):\n",
    "        action_probs, state_value = policy_network(state)\n",
    "        new_log_prob = torch.log(action_probs.squeeze(0)[action])\n",
    "        new_log_probs.append(new_log_prob)\n",
    "        state_values.append(state_value)\n",
    "\n",
    "    new_log_probs = torch.stack(new_log_probs)\n",
    "    state_values = torch.stack(state_values).squeeze(1)\n",
    "    returns = torch.tensor(returns)\n",
    "    advantages = torch.tensor(advantages)\n",
    "\n",
    "    ratio = torch.exp(new_log_probs - torch.stack(log_probs))\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "    value_loss = (returns - state_values).pow(2).mean()\n",
    "    loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def dppo_worker(global_policy_params, global_ep, global_ep_lock, env_name, state_dim, action_dim, max_episodes=1000):\n",
    "    env = gym.make(env_name)\n",
    "    local_model = PPO(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(local_model.parameters(), lr=3e-4)\n",
    "\n",
    "    while True:\n",
    "        with global_policy_params['lock']:\n",
    "            for local_param, global_param in zip(local_model.parameters(), global_policy_params['params']):\n",
    "                param_data = np.array(global_param[:]).reshape(local_param.data.shape)\n",
    "                local_param.data.copy_(torch.from_numpy(param_data))\n",
    "\n",
    "        states, actions, log_probs, rewards = collect_trajectories(local_model, env)\n",
    "        values = [local_model(state)[1] for state in states]\n",
    "        advantages, returns = compute_advantages(rewards, values)\n",
    "\n",
    "        update_policy(local_model, optimizer, states, actions, log_probs, returns, advantages)\n",
    "\n",
    "        with global_policy_params['lock']:\n",
    "            for global_param, local_param in zip(global_policy_params['params'], local_model.parameters()):\n",
    "                global_param[:] = local_param.data.cpu().numpy()\n",
    "\n",
    "        with global_ep_lock:\n",
    "            if global_ep.value >= max_episodes:\n",
    "                break\n",
    "            global_ep.value += 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    env_name = \"CartPole-v1\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    with mp.Manager() as manager:\n",
    "        global_policy_params = manager.dict()\n",
    "        global_policy_params['lock'] = manager.Lock()\n",
    "        global_policy_params['params'] = [manager.list(param.data.cpu().numpy().flatten()) for param in PPO(state_dim, action_dim).parameters()]\n",
    "\n",
    "        global_ep = manager.Value('i', 0)\n",
    "        global_ep_lock = manager.Lock()\n",
    "\n",
    "        num_workers = mp.cpu_count()\n",
    "        workers = [mp.Process(target=dppo_worker, args=(global_policy_params, global_ep, global_ep_lock, env_name, state_dim, action_dim)) for _ in range(num_workers)]\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "        for worker in workers:\n",
    "            worker.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd499e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twin Delayed Deep Deterministic Policy Gradient (TD3) \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Red Actor \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.fc3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.max_action * torch.tanh(self.fc3(x))\n",
    "\n",
    "# Red Critic\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # First Q-function network\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.q1 = nn.Linear(300, 1)\n",
    "        # Second Q-function network\n",
    "        self.fc3 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.fc4 = nn.Linear(400, 300)\n",
    "        self.q2 = nn.Linear(300, 1)\n",
    "    \n",
    "    def forward(self, x, a):\n",
    "        xu = torch.cat([x, a], 1)  # Concatena estado y accion\n",
    "        # Primera Q-function\n",
    "        x1 = torch.relu(self.fc1(xu))\n",
    "        x1 = torch.relu(self.fc2(x1))\n",
    "        q1 = self.q1(x1)\n",
    "        # Segunda Q-function\n",
    "        x2 = torch.relu(self.fc3(xu))\n",
    "        x2 = torch.relu(self.fc4(x2))\n",
    "        q2 = self.q2(x2)\n",
    "        return q1, q2\n",
    "\n",
    "# Actualiza la funcion para TD3\n",
    "def update_td3(actor, critic, target_actor, target_critic, replay_buffer, optimizer_actor, optimizer_critic, it, batch_size=100, gamma=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Ruido para smoothing\n",
    "        noise = (torch.randn_like(action) * policy_noise).clamp(-noise_clip, noise_clip)\n",
    "        # Smoothing de proximas acciones\n",
    "        next_action = (target_actor(next_state) + noise).clamp(-actor.max_action, actor.max_action)\n",
    "        \n",
    "        # Calcula el valor Q objetivo\n",
    "        target_q1, target_q2 = target_critic(next_state, next_action)\n",
    "        target_q = reward + (1 - done) * gamma * torch.min(target_q1, target_q2)\n",
    "    \n",
    "    # Obtiene estimaciones Q actuales\n",
    "    current_q1, current_q2 = critic(state, action)\n",
    "    critic_loss = torch.nn.functional.mse_loss(current_q1, target_q) + torch.nn.functional.mse_loss(current_q2, target_q)\n",
    "    \n",
    "    # Optimiza el critic\n",
    "    optimizer_critic.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    optimizer_critic.step()\n",
    "    \n",
    "    # Actualizaciones de políticas retrasadas\n",
    "    if it % policy_freq == 0:\n",
    "        # Compute actor loss\n",
    "        actor_loss = -critic.q1(state, actor(state)).mean()\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "        \n",
    "        # actualizar los modelos objetivos congelados\n",
    "        for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad87b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN (Deep Q-Network)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def update_dqn(policy_network, target_network, optimizer, replay_buffer, batch_size=64, gamma=0.99):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    q_values = policy_network(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_values = target_network(next_state).max(1)[0]\n",
    "    expected_q_values = reward + gamma * next_q_values * (1 - done)\n",
    "    \n",
    "    loss = torch.nn.functional.mse_loss(q_values, expected_q_values)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ddbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C51\n",
    "class C51(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_atoms, v_min, v_max):\n",
    "        super(C51, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim * num_atoms)\n",
    "        self.num_atoms = num_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.delta_z = (v_max - v_min) / (num_atoms - 1)\n",
    "        self.z = torch.linspace(v_min, v_max, num_atoms)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x).view(-1, action_dim, num_atoms)\n",
    "        return torch.nn.functional.softmax(x, dim=-1)\n",
    "\n",
    "def update_c51(policy_network, target_network, optimizer, replay_buffer, batch_size=64, gamma=0.99):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_dist = target_network(next_state)\n",
    "        next_action = next_dist.sum(-1).max(1)[1]\n",
    "        next_dist = next_dist[range(batch_size), next_action]\n",
    "        \n",
    "        t_z = reward + (1 - done) * gamma * policy_network.z\n",
    "        t_z = t_z.clamp(policy_network.v_min, policy_network.v_max)\n",
    "        b = (t_z - policy_network.v_min) / policy_network.delta_z\n",
    "        l = b.floor().long()\n",
    "        u = b.ceil().long()\n",
    "        \n",
    "        m = torch.zeros(batch_size, policy_network.num_atoms)\n",
    "        offset = torch.linspace(0, (batch_size - 1) * policy_network.num_atoms, batch_size).long().unsqueeze(1).expand(batch_size, policy_network.num_atoms)\n",
    "        \n",
    "        m.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u - b)).view(-1))\n",
    "        m.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l)).view(-1))\n",
    "    \n",
    "    dist = policy_network(state)\n",
    "    log_p = torch.log(dist[range(batch_size), action])\n",
    "    loss = -(m * log_p).sum(1).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QR-DQN (Quantile Regression DQN)\n",
    "class QRDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_quantiles):\n",
    "        super(QRDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim * num_quantiles)\n",
    "        self.num_quantiles = num_quantiles\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x).view(-1, action_dim, self.num_quantiles)\n",
    "\n",
    "def update_qrdqn(policy_network, target_network, optimizer, replay_buffer, batch_size=64, gamma=0.99):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    quantiles = policy_network(state).gather(1, action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, policy_network.num_quantiles)).squeeze(1)\n",
    "    next_quantiles = target_network(next_state)\n",
    "    next_action = next_quantiles.mean(2).max(1)[1]\n",
    "    next_quantiles = next_quantiles[range(batch_size), next_action]\n",
    "    \n",
    "    target_quantiles = reward.unsqueeze(1) + (1 - done).unsqueeze(1) * gamma * next_quantiles\n",
    "    target_quantiles = target_quantiles.unsqueeze(1).expand(batch_size, policy_network.num_quantiles, policy_network.num_quantiles)\n",
    "    quantiles = quantiles.unsqueeze(2).expand(batch_size, policy_network.num_quantiles, policy_network.num_quantiles)\n",
    "    \n",
    "    diff = target_quantiles - quantiles\n",
    "    loss = torch.where(diff > 0, diff * 0.5, -diff * 0.5)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HER (Hindsight Experience Replay)\n",
    "class HERReplayBuffer:\n",
    "    def __init__(self, capacity, k=4):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.k = k\n",
    "    \n",
    "    def add(self, episode):\n",
    "        self.buffer.append(episode)\n",
    "        if len(self.buffer) > self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        sampled_episodes = random.sample(self.buffer, batch_size)\n",
    "        return self._create_transitions(sampled_episodes)\n",
    "    \n",
    "    def _create_transitions(self, episodes):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for episode in episodes:\n",
    "            for i in range(len(episode['states'])):\n",
    "                states.append(episode['states'][i])\n",
    "                actions.append(episode['actions'][i])\n",
    "                next_states.append(episode['next_states'][i])\n",
    "                dones.append(episode['dones'][i])\n",
    "                \n",
    "                if dones[-1]:\n",
    "                    break\n",
    "                \n",
    "                for _ in range(self.k):\n",
    "                    future = random.randint(i, len(episode['states']) - 1)\n",
    "                    goal = episode['states'][future]\n",
    "                    \n",
    "                    new_state = episode['states'][i]\n",
    "                    new_action = episode['actions'][i]\n",
    "                    new_next_state = episode['next_states'][i]\n",
    "                    new_done = episode['dones'][i]\n",
    "                    new_reward = self._compute_reward(new_next_state, goal)\n",
    "                    \n",
    "                    states.append(new_state)\n",
    "                    actions.append(new_action)\n",
    "                    next_states.append(new_next_state)\n",
    "                    dones.append(new_done)\n",
    "                    rewards.append(new_reward)\n",
    "        \n",
    "        return torch.tensor(states), torch.tensor(actions), torch.tensor(rewards), torch.tensor(next_states), torch.tensor(dones)\n",
    "    \n",
    "    def _compute_reward(self, state, goal):\n",
    "        return -np.linalg.norm(state - goal)\n",
    "\n",
    "def update_dqn_with_her(policy_network, target_network, optimizer, her_replay_buffer, batch_size=64, gamma=0.99):\n",
    "    state, action, reward, next_state, done = her_replay_buffer.sample(batch_size)\n",
    "    \n",
    "    q_values = policy_network(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_values = target_network(next_state).max(1)[0]\n",
    "    expected_q_values = reward + gamma * next_q_values * (1 - done)\n",
    "    \n",
    "    loss = torch.nn.functional.mse_loss(q_values, expected_q_values)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e3794",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 . Policy Gradient\n",
    "Descripción: Implementa un algoritmo básico de gradientes de política para un agente que aprende a jugar un juego simple como CartPole usando Gym de OpenAI. Integra feedback humano simulado para mejorar las decisiones del agente.\n",
    "\n",
    "Tareas:\n",
    "\n",
    "* Define y entrena una red neuronal para representar la política del agente.\n",
    "* Recolecta trayectorias de entrenamiento y calcula los retornos.\n",
    "* Usa el método de gradientes de política para actualizar los pesos de la red.\n",
    "\n",
    "2 . A2C/A3C\n",
    "\n",
    "Descripción: Modifica el ejercicio anterior para utilizar el algoritmo Actor-Critic, específicamente A2C y A3C, para mejorar la estabilidad y eficiencia del entrenamiento.\n",
    "\n",
    "Tareas:\n",
    "* Implementa una arquitectura Actor-Critic con PyTorch.\n",
    "* Añade soporte para operaciones asíncronas en el caso de A3C.\n",
    "* Evalúa la diferencia en el rendimiento y la convergencia entre A2C y A3C.\n",
    "\n",
    "3 . PPO (Proximal Policy Optimization)\n",
    "\n",
    "Descripción: Implementa PPO para entrenar un agente que puede interactuar con un entorno más complejo, como BipedalWalker de Gym.\n",
    "\n",
    "Tareas:\n",
    "\n",
    "* Implementa la función de recorte de ventajas que caracteriza a PPO.\n",
    "* Realiza múltiples actualizaciones de política usando el mismo conjunto de datos para mejorar la eficiencia de los datos.\n",
    "Integra técnicas de normalización para estabilizar el aprendizaje.\n",
    "\n",
    "4 . TRPO (Trust Region Policy Optimization) y DPPO\n",
    "Descripción: Utiliza TRPO y luego extiéndelo a DPPO para manejar múltiples agentes en un entorno distribuido.\n",
    "\n",
    "Tareas:\n",
    "\n",
    "- Implementa TRPO con restricciones de región de confianza para asegurar actualizaciones de política seguras.\n",
    "- Escala la solución usando DPPO, gestionando varios workers para recopilar datos y actualizar una política central.\n",
    "\n",
    "5 . TD3 (Twin Delayed Deep Deterministic Policy Gradient)\n",
    "Descripción: Aplica TD3 para un problema de control continuo como Pendulum.\n",
    "\n",
    "Tareas:\n",
    "\n",
    "- Implementa redes gemelas para la estimación Q y añade ruido a las acciones para exploración.\n",
    "- Utiliza actualizaciones retardadas para los parámetros del actor y el crítico para estabilizar el entrenamiento.\n",
    "\n",
    "6 . Q-Learning: DQN, C51, QR-DQN, y HER\n",
    "Descripción: Implementa diferentes variantes de DQN y explora cómo HER puede mejorar el aprendizaje en entornos con objetivos esparsos.\n",
    "\n",
    "Tareas:\n",
    "\n",
    "- Implementa un DQN estándar para SpaceInvaders.\n",
    "- Extiende DQN a C51 y QR-DQN para aprender distribuciones de valor.\n",
    "- Aplica HER en un entorno como FetchPickAndPlace para mejorar la eficiencia en tareas con recompensas esparsas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d16561",
   "metadata": {},
   "source": [
    "Ejercicio 1: Policy Gradient\n",
    "\n",
    "Objetivo: Implementa el método Policy Gradient utilizando un transformer para la predicción de la política."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae3b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "class TransformerPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, nhead, num_layers):\n",
    "        super(TransformerPolicy, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=state_dim, nhead=nhead, num_encoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(state_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = state.unsqueeze(1)  # Add sequence dimension\n",
    "        transformer_out = self.transformer(state)\n",
    "        policy = torch.softmax(self.fc(transformer_out.squeeze(1)), dim=-1)\n",
    "        return policy\n",
    "\n",
    "def policy_gradient(env, policy_network, optimizer, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            action_probs = policy_network(state)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            log_prob = torch.log(action_probs.squeeze(0)[action])\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        G = 0\n",
    "        policy_loss = []\n",
    "        for r, log_prob in zip(rewards[::-1], log_probs[::-1]):\n",
    "            G = r + 0.99 * G\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "nhead = 2\n",
    "num_layers = 2\n",
    "\n",
    "policy_network = TransformerPolicy(state_dim, action_dim, nhead, num_layers)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=1e-3)\n",
    "\n",
    "policy_gradient(env, policy_network, optimizer, num_episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21370556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f985de6",
   "metadata": {},
   "source": [
    "Ejercicio 2: A2C/A3C (Asynchronous Advantage Actor-Critic)\n",
    "\n",
    "Objetivo: Implementa A2C/A3C utilizando un transformer para la predicción de la política y el valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb589d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import gym\n",
    "\n",
    "class TransformerActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, nhead, num_layers):\n",
    "        super(TransformerActorCritic, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=state_dim, nhead=nhead, num_encoder_layers=num_layers)\n",
    "        self.actor = nn.Linear(state_dim, action_dim)\n",
    "        self.critic = nn.Linear(state_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = state.unsqueeze(1)  # Add sequence dimension\n",
    "        transformer_out = self.transformer(state)\n",
    "        policy = torch.softmax(self.actor(transformer_out.squeeze(1)), dim=-1)\n",
    "        value = self.critic(transformer_out.squeeze(1))\n",
    "        return policy, value\n",
    "\n",
    "def worker(worker_id, env_name, global_policy, optimizer, global_episode, gamma=0.99):\n",
    "    env = gym.make(env_name)\n",
    "    local_policy = TransformerActorCritic(state_dim, action_dim, nhead, num_layers)\n",
    "    local_policy.load_state_dict(global_policy.state_dict())\n",
    "    \n",
    "    while global_episode.value < 1000:\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        \n",
    "        while not done:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            policy, value = local_policy(state)\n",
    "            action = torch.multinomial(policy, 1).item()\n",
    "            log_prob = torch.log(policy.squeeze(0)[action])\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        G = 0\n",
    "        actor_loss = 0\n",
    "        critic_loss = 0\n",
    "        for r, log_prob, value in zip(rewards[::-1], log_probs[::-1], values[::-1]):\n",
    "            G = r + gamma * G\n",
    "            advantage = G - value.item()\n",
    "            actor_loss += -log_prob * advantage\n",
    "            critic_loss += advantage ** 2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = actor_loss + critic_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with global_episode.get_lock():\n",
    "            global_episode.value += 1\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "nhead = 2\n",
    "num_layers = 2\n",
    "\n",
    "global_policy = TransformerActorCritic(state_dim, action_dim, nhead, num_layers)\n",
    "global_policy.share_memory()\n",
    "optimizer = optim.Adam(global_policy.parameters(), lr=1e-3)\n",
    "\n",
    "global_episode = mp.Value('i', 0)\n",
    "num_workers = mp.cpu_count()\n",
    "workers = [mp.Process(target=worker, args=(i, env_name, global_policy, optimizer, global_episode)) for i in range(num_workers)]\n",
    "\n",
    "for worker in workers:\n",
    "    worker.start()\n",
    "for worker in workers:\n",
    "    worker.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62ff10",
   "metadata": {},
   "source": [
    "Ejercicio 3: PPO (Proximal Policy Optimization)\n",
    "    \n",
    "Objetivo: Implementa PPO utilizando un transformer para la predicción de la política y el valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "class TransformerPPO(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, nhead, num_layers):\n",
    "        super(TransformerPPO, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=state_dim, nhead=nhead, num_encoder_layers=num_layers)\n",
    "        self.actor = nn.Linear(state_dim, action_dim)\n",
    "        self.critic = nn.Linear(state_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = state.unsqueeze(1)  # Add sequence dimension\n",
    "        transformer_out = self.transformer(state)\n",
    "        policy = torch.softmax(self.actor(transformer_out.squeeze(1)), dim=-1)\n",
    "        value = self.critic(transformer_out.squeeze(1))\n",
    "        return policy, value\n",
    "\n",
    "def ppo(env, policy_network, optimizer, num_episodes, clip_epsilon=0.2, gamma=0.99):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        dones = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            policy, value = policy_network(state)\n",
    "            action = torch.multinomial(policy, 1).item()\n",
    "            log_prob = torch.log(policy.squeeze(0)[action])\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            dones.append(done)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        G = 0\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns)\n",
    "        \n",
    "        states = torch.cat(states)\n",
    "        actions = torch.tensor(actions)\n",
    "        log_probs = torch.tensor(log_probs)\n",
    "        values = torch.cat(values)\n",
    "        \n",
    "        advantages = returns - values.squeeze(1)\n",
    "        \n",
    "        for _ in range(4):  # PPO multiple epochs\n",
    "            new_log_probs = []\n",
    "            new_values = []\n",
    "            for state, action in zip(states, actions):\n",
    "                policy, value = policy_network(state)\n",
    "                new_log_probs.append(torch.log(policy.squeeze(0)[action]))\n",
    "                new_values.append(value)\n",
    "            \n",
    "            new_log_probs = torch.stack(new_log_probs)\n",
    "            new_values = torch.stack(new_values).squeeze(1)\n",
    "            \n",
    "            ratios = torch.exp(new_log_probs - log_probs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (returns - new_values).pow(2).mean()\n",
    "            loss = actor_loss + 0.5 * critic_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "nhead = 2\n",
    "num_layers = 2\n",
    "\n",
    "policy_network = TransformerPPO(state_dim, action_dim, nhead, num_layers)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=1e-3)\n",
    "\n",
    "ppo(env, policy_network, optimizer, num_episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995f60f",
   "metadata": {},
   "source": [
    "Ejercicio 4: TRPO (Trust Region Policy Optimization)\n",
    "\n",
    "Objetivo: Implementa TRPO utilizando un transformer para la predicción de la política y el valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f109124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "class TransformerTRPO(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, nhead, num_layers):\n",
    "        super(TransformerTRPO, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=state_dim, nhead=nhead, num_encoder_layers=num_layers)\n",
    "        self.actor = nn.Linear(state_dim, action_dim)\n",
    "        self.critic = nn.Linear(state_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = state.unsqueeze(1)  # Add sequence dimension\n",
    "        transformer_out = self.transformer(state)\n",
    "        policy = torch.softmax(self.actor(transformer_out.squeeze(1)), dim=-1)\n",
    "        value = self.critic(transformer_out.squeeze(1))\n",
    "        return policy, value\n",
    "\n",
    "def trpo(env, policy_network, optimizer, num_episodes, max_kl=0.01, gamma=0.99):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        dones = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            policy, value = policy_network(state)\n",
    "            action = torch.multinomial(policy, 1).item()\n",
    "            log_prob = torch.log(policy.squeeze(0)[action])\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            dones.append(done)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        G = 0\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns)\n",
    "        \n",
    "        states = torch.cat(states)\n",
    "        actions = torch.tensor(actions)\n",
    "        log_probs = torch.tensor(log_probs)\n",
    "        values = torch.cat(values)\n",
    "        \n",
    "        advantages = returns - values.squeeze(1)\n",
    "        \n",
    "        def get_loss():\n",
    "            new_log_probs = []\n",
    "            new_values = []\n",
    "            for state, action in zip(states, actions):\n",
    "                policy, value = policy_network(state)\n",
    "                new_log_probs.append(torch.log(policy.squeeze(0)[action]))\n",
    "                new_values.append(value)\n",
    "            \n",
    "            new_log_probs = torch.stack(new_log_probs)\n",
    "            new_values = torch.stack(new_values).squeeze(1)\n",
    "            \n",
    "            ratios = torch.exp(new_log_probs - log_probs)\n",
    "            surr1 = ratios * advantages\n",
    "            actor_loss = -surr1.mean()\n",
    "            critic_loss = (returns - new_values).pow(2).mean()\n",
    "            loss = actor_loss + 0.5 * critic_loss\n",
    "            return loss\n",
    "        \n",
    "        loss = get_loss()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "nhead = 2\n",
    "num_layers = 2\n",
    "\n",
    "policy_network = TransformerTRPO(state_dim, action_dim, nhead, num_layers)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=1e-3)\n",
    "\n",
    "trpo(env, policy_network, optimizer, num_episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac12f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a836a",
   "metadata": {},
   "source": [
    "Ejercicio 5: Utilizando LLMs para Generar Feedback para RLHF\n",
    "    \n",
    "Objetivo: Implementa un agente que utilice un LLM para generar feedback y mejorar la política.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import gym\n",
    "\n",
    "class TransformerPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, nhead, num_layers):\n",
    "        super(TransformerPolicy, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=state_dim, nhead=nhead, num_encoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(state_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = state.unsqueeze(1)  # Add sequence dimension\n",
    "        transformer_out = self.transformer(state)\n",
    "        policy = torch.softmax(self.fc(transformer_out.squeeze(1)), dim=-1)\n",
    "        return policy\n",
    "\n",
    "def generate_feedback(llm, tokenizer, state, action):\n",
    "    input_text = f\"State: {state.tolist()}, Action: {action}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = llm(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss.item()\n",
    "    feedback = torch.tensor([loss])\n",
    "    return feedback\n",
    "\n",
    "def train_policy_with_llm_feedback(policy_network, env, llm, tokenizer, optimizer, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            action_probs = policy_network(state_tensor)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            feedback = generate_feedback(llm, tokenizer, state, action)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            action_one_hot = torch.zeros(action_probs.shape)\n",
    "            action_one_hot[0, action] = 1.0\n",
    "            loss = -torch.mean(feedback * torch.sum(action_probs * action_one_hot, dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "# Inicializar entorno y modelo de LLM\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "nhead = 2\n",
    "num_layers = 2\n",
    "\n",
    "policy_network = TransformerPolicy(state_dim, action_dim, nhead, num_layers)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=1e-3)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "llm = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Entrenar la política con feedback del LLM\n",
    "train_policy_with_llm_feedback(policy_network, env, llm, tokenizer, optimizer, num_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e694b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24d9b1",
   "metadata": {},
   "source": [
    "Ejercicio 6: PPO (Proximal Policy Optimization) con Feedback de LLM\n",
    "\n",
    "Objetivo: Implementa PPO utilizando un transformador para la predicción de la política y el valor, y un LLM para generar feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcbc0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import gym\n",
    "\n",
    "class TransformerPPO(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, nhead, num_layers):\n",
    "        super(TransformerPPO, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=state_dim, nhead=nhead, num_encoder_layers=num_layers)\n",
    "        self.actor = nn.Linear(state_dim, action_dim)\n",
    "        self.critic = nn.Linear(state_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = state.unsqueeze(1)  # Add sequence dimension\n",
    "        transformer_out = self.transformer(state)\n",
    "        policy = torch.softmax(self.actor(transformer_out.squeeze(1)), dim=-1)\n",
    "        value = self.critic(transformer_out.squeeze(1))\n",
    "        return policy, value\n",
    "\n",
    "def generate_feedback(llm, tokenizer, state, action):\n",
    "    input_text = f\"State: {state.tolist()}, Action: {action}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = llm(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss.item()\n",
    "    feedback = torch.tensor([loss])\n",
    "    return feedback\n",
    "\n",
    "def ppo_with_llm_feedback(env, policy_network, llm, tokenizer, optimizer, num_episodes, clip_epsilon=0.2, gamma=0.99):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        dones = []\n",
    "        feedbacks = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            policy, value = policy_network(state_tensor)\n",
    "            action = torch.multinomial(policy, 1).item()\n",
    "            log_prob = torch.log(policy.squeeze(0)[action])\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            feedback = generate_feedback(llm, tokenizer, state, action)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            states.append(state_tensor)\n",
    "            actions.append(action)\n",
    "            dones.append(done)\n",
    "            feedbacks.append(feedback)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        G = 0\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns)\n",
    "        \n",
    "        states = torch.cat(states)\n",
    "        actions = torch.tensor(actions)\n",
    "        log_probs = torch.tensor(log_probs)\n",
    "        values = torch.cat(values)\n",
    "        \n",
    "        advantages = returns - values.squeeze(1)\n",
    "        \n",
    "        for _ in range(4):  # PPO multiple epochs\n",
    "            new_log_probs = []\n",
    "            new_values = []\n",
    "            for state, action in zip(states, actions):\n",
    "                policy, value = policy_network(state)\n",
    "                new_log_probs.append(torch.log(policy.squeeze(0)[action]))\n",
    "                new_values.append(value)\n",
    "            \n",
    "            new_log_probs = torch.stack(new_log_probs)\n",
    "            new_values = torch.stack(new_values).squeeze(1)\n",
    "            \n",
    "            ratios = torch.exp(new_log_probs - log_probs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (returns - new_values).pow(2).mean()\n",
    "            feedback_loss = torch.mean(torch.stack(feedbacks))  # Adding feedback loss\n",
    "            loss = actor_loss + 0.5 * critic_loss + feedback_loss  # Combined loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Inicializar entorno y modelo de LLM\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "nhead = 2\n",
    "num_layers = 2\n",
    "\n",
    "policy_network = TransformerPPO(state_dim, action_dim, nhead, num_layers)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=1e-3)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "llm = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Entrenar PPO con feedback del LLM\n",
    "ppo_with_llm_feedback(env, policy_network, llm, tokenizer, optimizer, num_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda8482",
   "metadata": {},
   "source": [
    "Ejercicio 7: DQN (Deep Q-Network) con Transformers\n",
    "\n",
    "Objetivo: Implementa DQN utilizando un transformer para la predicción de la Q-valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7b2be",
   "metadata": {},
   "source": [
    "Ejercicio 8: Implementa C51 utilizando un transformer para la predicción de la Q-valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3531ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class TransformerC51(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, nhead, num_layers, num_atoms, v_min, v_max):\n",
    "        super(TransformerC51, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=state_dim, nhead=nhead, num_encoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(state_dim, action_dim * num_atoms)\n",
    "        self.num_atoms = num_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.delta_z = (v_max - v_min) / (num_atoms - 1)\n",
    "        self.z = torch.linspace(v_min, v_max, num_atoms)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = state.unsqueeze(1)  # Add sequence dimension\n",
    "        transformer_out = self.transformer(state)\n",
    "        q_values = self.fc(transformer_out.squeeze(1))\n",
    "        q_values = q_values.view(q_values.size(0), -1, self.num_atoms)\n",
    "        return q_values\n",
    "\n",
    "def projection_distribution(next_q_values, rewards, dones, gamma, num_atoms, v_min, v_max, delta_z):\n",
    "    next_z = rewards + gamma * (1 - dones) * z\n",
    "    next_z = next_z.clamp(v_min, v_max)\n",
    "    b = (next_z - v_min) / delta_z\n",
    "    l = b.floor().long()\n",
    "    u = b.ceil().long()\n",
    "    m = torch.zeros_like(next_q_values)\n",
    "    for i in range(num_atoms):\n",
    "        m[:, i] += next_q_values[:, i] * (u - b)\n",
    "        m[:, i] += next_q_values[:, i] * (b - l)\n",
    "    return m\n",
    "\n",
    "def train_c51(env, policy_network, target_network, optimizer, replay_buffer, batch_size=64, gamma=0.99):\n",
    "    for _ in range(1000):  # Number of training iterations\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        \n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        done = torch.tensor(done, dtype=torch.float32)\n",
    "        \n",
    "        q_values = policy_network(state)\n",
    "        q_values = q_values[range(batch_size), action].squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_network(next_state)\n",
    "            next_q_values = next_q_values[range(batch_size), next_q_values.argmax(1)]\n",
    "            target_q_values = projection_distribution(next_q_values, reward, done, gamma, policy_network.num_atoms, policy_network.v_min, policy_network.v_max, policy_network.delta_z)\n",
    "        \n",
    "        loss = -torch.sum(target_q_values * torch.log(q_values), dim=1).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "nhead = 2\n",
    "num_layers = 2\n",
    "num_atoms = 51\n",
    "v_min = -10\n",
    "v_max = 10\n",
    "\n",
    "policy_network = TransformerC51(state_dim, action_dim, nhead, num_layers, num_atoms, v_min, v_max)\n",
    "target_network = TransformerC51(state_dim, action_dim, nhead, num_layers, num_atoms, v_min, v_max)\n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=1e-3)\n",
    "\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "num_episodes = 500\n",
    "batch_size = 64\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for t in range(200):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = policy_network(state_tensor)\n",
    "        action = q_values.sum(-1).argmax().item()\n",
    "        \n",
    "        if random.random() < 0.1:  # Epsilon-greedy policy\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    train_c51(env, policy_network, target_network, optimizer, replay_buffer, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdecbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7a809",
   "metadata": {},
   "source": [
    "El aprendizaje por refuerzo Basado en Modelo (Model-Based RL) implica la construcción y utilización de un modelo explícito del entorno para la planificación y toma de decisiones. Este enfoque puede ser más eficiente en cuanto a muestras que los métodos sin modelo (Model-Free RL), ya que permite que el agente utilice simulaciones del modelo para aprender y mejorar su política.\n",
    "\n",
    "### Aprender el modelo\n",
    "\n",
    "Aprender un modelo del entorno implica construir una representación interna que capture las dinámicas del entorno. Este modelo puede ser utilizado para predecir las transiciones de estado y recompensas, facilitando la planificación y la toma de decisiones.\n",
    "\n",
    "#### World Models\n",
    "\n",
    "Los World Models son un enfoque en el que el agente aprende una representación compacta del entorno y utiliza esta representación para planificar y tomar decisiones.\n",
    "\n",
    "##### Componentes clave\n",
    "\n",
    "1. **VAE (Variational Autoencoder)**: Se utiliza para aprender una representación latente de los estados del entorno.\n",
    "\n",
    "   - **Función de pérdida del VAE**:\n",
    "     $$\n",
    "     \\mathcal{L}_{\\text{VAE}} = \\mathbb{E}_{q_\\phi(z|s)} \\left[ \\log p_\\theta(s|z) \\right] - D_{\\text{KL}}(q_\\phi(z|s) \\| p(z))\n",
    "     $$\n",
    "     \n",
    "     - **Término de reconstrucción**: $\\mathbb{E}_{q_\\phi(z|s)} \\left[ \\log p_\\theta(s|z) \\right]$ mide cuán bien el VAE puede reconstruir el estado original $s$ a partir de la representación latente $z$.\n",
    "     - **Término de regularización**: $D_{\\text{KL}}(q_\\phi(z|s) \\| p(z))$ es la divergencia KL entre la distribución aproximada $q_\\phi(z|s)$ y una distribución prior $p(z)$ (normalmente una distribución gaussiana).\n",
    "\n",
    "2. **MDN-RNN (Mixture Density Network - Recurrent Neural Network)**: Modelo recurrente que predice la próxima representación latente y recompensa dadas las representaciones latentes actuales y acciones.\n",
    "\n",
    "   - **Función de pérdida del MDN-RNN**:\n",
    "     $$\n",
    "     p(z_{t+1}, r_t | z_t, a_t) = \\text{MDN-RNN}(z_t, a_t)\n",
    "     $$\n",
    "     - La salida del MDN-RNN es una combinación de varias distribuciones gausianas que modelan las posibles próximas representaciones latentes $z_{t+1}$ y recompensas $r_t$.\n",
    "\n",
    "3. **Controller**: Utiliza la representación latente y las predicciones del MDN-RNN para tomar decisiones de acción.\n",
    "\n",
    "#### I2A (Imagination-Augmented Agents)\n",
    "\n",
    "I2A utiliza un modelo imaginativo para simular futuros posibles y mejorar la toma de decisiones.\n",
    "\n",
    "##### Componentes clave\n",
    "\n",
    "1. **Modelo imaginativo**: Simula transiciones futuras del entorno.\n",
    "\n",
    "   - **Modelo de transición**:\n",
    "     $$\n",
    "     \\hat{s}_{t+1}, \\hat{r}_t = f(s_t, a_t)\n",
    "     $$\n",
    "     - Aquí, $f(s_t, a_t)$ representa un modelo que predice el siguiente estado $\\hat{s}_{t+1}$ y la recompensa $\\hat{r}_t$ dado el estado actual $s_t$ y la acción $a_t$.\n",
    "\n",
    "2. **Imagination Core**: Genera múltiples trayectorias imaginadas a partir del estado actual y posibles acciones.\n",
    "\n",
    "   - **Trayectorias imaginadas**:\n",
    "     $$\n",
    "     \\{ (\\hat{s}_{t+1}^i, \\hat{r}_t^i) \\}_{i=1}^n\n",
    "     $$\n",
    "     - Este conjunto de trayectorias imaginadas permite que el agente considere varios futuros posibles y sus respectivas recompensas.\n",
    "\n",
    "3. **Policy Network**: Integra las trayectorias imaginadas para seleccionar la acción óptima.\n",
    "\n",
    "   - **Selección de acción**:\n",
    "     $$\n",
    "     a_t = \\pi(s_t, \\{\\hat{s}_{t+1}^i, \\hat{r}_t^i \\}_{i=1}^n)\n",
    "     $$\n",
    "     - La red de políticas $\\pi$ toma como entrada el estado actual $s_t$ y las trayectorias imaginadas para seleccionar la mejor acción $a_t$.\n",
    "\n",
    "#### MBMF (Model-Based Model-Free)\n",
    "\n",
    "MBMF combina elementos de RL basado en modelo y sin modelo para aprovechar las ventajas de ambos.\n",
    "\n",
    "##### Componentes clave\n",
    "\n",
    "1. **Modelo dinámico**: Predice transiciones y recompensas.\n",
    "\n",
    "   - **Modelo de transición**:\n",
    "     $$\n",
    "     \\hat{s}_{t+1}, \\hat{r}_t = f(s_t, a_t)\n",
    "     $$\n",
    "\n",
    "2. **Planificación corta**: Utiliza el modelo dinámico para planificar a corto plazo y generar rollouts.\n",
    "\n",
    "   - **Rollouts a corto plazo**:\n",
    "     $$\n",
    "     \\{ (\\hat{s}_{t+1}^k, \\hat{r}_t^k) \\}_{k=1}^K\n",
    "     $$\n",
    "     - Se generan múltiples secuencias de estados y recompensas simuladas para un horizonte de planificación corto.\n",
    "\n",
    "3. **Entrenamiento sin modelo**: Utiliza los rollouts generados para actualizar una política o función de valor sin modelo.\n",
    "\n",
    "   - **Actualización de la política**:\n",
    "     $$\n",
    "     \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\pi_\\theta, \\{ (\\hat{s}_{t+1}^k, \\hat{r}_t^k) \\}_{k=1}^K)\n",
    "     $$\n",
    "     - Los rollouts simulados se utilizan para calcular las gradientes y actualizar los parámetros de la política $\\theta$.\n",
    "\n",
    "#### MBVE (Model-Based Value Expansion)\n",
    "\n",
    "MBVE utiliza un modelo del entorno para expandir la estimación de valor más allá del horizonte de planificación actual.\n",
    "\n",
    "##### Componentes clave\n",
    "\n",
    "1. **Modelo dinámico**: Predice transiciones y recompensas.\n",
    "\n",
    "   - **Modelo de transición**:\n",
    "     $$\n",
    "     \\hat{s}_{t+1}, \\hat{r}_t = f(s_t, a_t)\n",
    "     $$\n",
    "\n",
    "2. **Expansión de valor**: Utiliza el modelo para expandir la estimación de valor más allá del horizonte.\n",
    "\n",
    "   - **Expansión de valor**:\n",
    "     $$\n",
    "     V(s_t) = r_t + \\gamma \\hat{r}_{t+1} + \\gamma^2 \\hat{r}_{t+2} + \\dots + \\gamma^H V_\\text{target}(\\hat{s}_{t+H})\n",
    "     $$\n",
    "     - Esta ecuación combina las recompensas inmediatas predichas $\\hat{r}_t$ con el valor estimado a largo plazo $V_\\text{target}$.\n",
    "\n",
    "3. **Entrenamiento de Valor**: Actualiza la función de valor utilizando la expansión de valor.\n",
    "\n",
    "   - **Actualización de la función de valor**:\n",
    "     $$\n",
    "     \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(V_\\theta, V(s_t))\n",
    "     $$\n",
    "     - Se utiliza la expansión de valor para calcular las gradientes y actualizar los parámetros de la función de valor $\\theta$.\n",
    "\n",
    "### Usar un modelo dado\n",
    "\n",
    "En algunos casos, se utiliza un modelo predefinido del entorno para facilitar la toma de decisiones.\n",
    "\n",
    "#### AlphaZero\n",
    "\n",
    "AlphaZero es un algoritmo que combina búsqueda en árbol de Monte Carlo (MCTS) con redes neuronales profundas para jugar juegos de tablero de manera superhumana.\n",
    "\n",
    "##### Componentes clave\n",
    "\n",
    "1. **Red neuronal**: Estima la política y el valor del estado.\n",
    "\n",
    "   - **Red neuronal**:\n",
    "     $$\n",
    "     \\pi(a|s), v(s) = f_\\theta(s)\n",
    "     $$\n",
    "     - La red neuronal parametrizada por $\\theta$ estima la probabilidad de cada acción $\\pi(a|s)$ y el valor del estado $v(s)$.\n",
    "\n",
    "2. **Búsqueda MCTS**: Realiza simulaciones para explorar posibles futuras secuencias de jugadas.\n",
    "\n",
    "   - **Selección**: Elige el nodo con el mayor valor de UCB.\n",
    "     $$\n",
    "     \\text{UCB}(s, a) = Q(s, a) + c \\sqrt{\\frac{\\log N(s)}{N(s, a)}}\n",
    "     $$\n",
    "     - Aquí, $Q(s, a)$ es el valor esperado de la acción $a$ en el estado $s$, $N(s)$ es el número total de visitas al nodo $s$, y $N(s, a)$ es el número de visitas al nodo hijo correspondiente a la acción $a$.\n",
    "\n",
    "   - **Expansión**: Añade un nuevo nodo al árbol de búsqueda.\n",
    "\n",
    "   - **Simulación**: Realiza un rollout desde el nodo expandido hasta un estado terminal o un número fijo de pasos.\n",
    "\n",
    "   - **Backup**: Actualiza los valores de $Q(s, a)$ hacia atrás en el árbol.\n",
    "     $$\n",
    "     Q(s, a) \\leftarrow Q(s, a) + \\frac{1}{N(s, a)} \\left( v - Q(s, a) \\right)\n",
    "     $$\n",
    "     - Aquí, $v$ es el valor simulado desde el nodo expandido.\n",
    "\n",
    "3. **Actualización de la política**: Utiliza las estadísticas de las simulaciones MCTS para actualizar la política.\n",
    "\n",
    "   - **Nueva política**:\n",
    "     $$\n",
    "     \\pi_\\text{new}(a|s) \\propto \\text{visit count}(a|s)\n",
    "     $$\n",
    "     - La nueva política se actualiza en función del conteo de visitas de las acciones durante la búsqueda MCTS.\n",
    "\n",
    "4. **Entrenamiento**: Actualiza los parámetros de la red neuronal usando las jugadas simuladas y el valor de las jugadas.\n",
    "\n",
    "   - **Pérdida de entrenamiento**:\n",
    "     $$\n",
    "     \\mathcal{L}(\\pi_\\theta, \\pi_\\text{new}, v_\\theta, z) = (z - v_\\theta)^2 - \\pi_\\text{new} \\cdot \\log \\pi_\\theta\n",
    "     $$\n",
    "     - Aquí, $z$ es el resultado del juego (1, 0, -1) y $\\pi_\\text{new}$ es la política mejorada de MCTS.\n",
    "\n",
    "   - **Actualización de los parámetros**:\n",
    "     $$\n",
    "     \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\pi_\\theta, \\pi_\\text{new}, v_\\theta, z)\n",
    "     $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b58d7a",
   "metadata": {},
   "source": [
    "**World models**\n",
    "\n",
    "Los World Models combinan Variational Autoencoders (VAE), Mixture Density Networks - Recurrent Neural Networks (MDN-RNN), y un controlador para la toma de decisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, state_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim * 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, state_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        mu_logvar = self.encoder(x)\n",
    "        mu, logvar = mu_logvar.chunk(2, dim=-1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "class MDNRNN(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim, hidden_dim, num_gaussians):\n",
    "        super(MDNRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(latent_dim + action_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_pi = nn.Linear(hidden_dim, num_gaussians)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, num_gaussians * latent_dim)\n",
    "        self.fc_sigma = nn.Linear(hidden_dim, num_gaussians * latent_dim)\n",
    "\n",
    "    def forward(self, z, a, h):\n",
    "        x = torch.cat([z, a], dim=-1).unsqueeze(1)\n",
    "        out, h = self.rnn(x, h)\n",
    "        pi = self.fc_pi(out).squeeze(1)\n",
    "        mu = self.fc_mu(out).squeeze(1)\n",
    "        sigma = self.fc_sigma(out).squeeze(1)\n",
    "        return pi, mu, sigma, h\n",
    "\n",
    "def mdn_loss(pi, mu, sigma, z):\n",
    "    m = Normal(mu, sigma)\n",
    "    z = z.unsqueeze(2).expand_as(m.loc)\n",
    "    log_prob = m.log_prob(z)\n",
    "    log_prob = log_prob.sum(dim=1)\n",
    "    log_sum_exp = torch.logsumexp(log_prob + torch.log_softmax(pi, dim=1), dim=1)\n",
    "    return -log_sum_exp.mean()\n",
    "\n",
    "class Controller(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super(Controller, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "# Implementación de entrenamiento para World Models\n",
    "def train_world_model(vae, mdnrnn, controller, data, vae_optimizer, mdnrnn_optimizer, controller_optimizer):\n",
    "    # Asume que `data` es un DataLoader que proporciona estados y acciones.\n",
    "    for states, actions in data:\n",
    "        # VAE Training\n",
    "        vae_optimizer.zero_grad()\n",
    "        recon_x, mu, logvar = vae(states)\n",
    "        loss_vae = vae_loss(recon_x, states, mu, logvar)\n",
    "        loss_vae.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "        # MDN-RNN Training\n",
    "        mdnrnn_optimizer.zero_grad()\n",
    "        z = vae.encode(states)[0]\n",
    "        pi, mu, sigma, _ = mdnrnn(z, actions, None)\n",
    "        loss_mdnrnn = mdn_loss(pi, mu, sigma, z)\n",
    "        loss_mdnrnn.backward()\n",
    "        mdnrnn_optimizer.step()\n",
    "\n",
    "        # Controller Training (usualmente requiere un ciclo de simulación)\n",
    "        # controller_optimizer.zero_grad()\n",
    "        # action_probs = controller(z)\n",
    "        # loss_controller = ...\n",
    "        # loss_controller.backward()\n",
    "        # controller_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304dd092",
   "metadata": {},
   "source": [
    "I2A utiliza un modelo imaginativo para simular futuros posibles y mejorar la toma de decisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423fc67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImaginationCore(nn.Module):\n",
    "    def __init__(self, model, policy, value):\n",
    "        super(ImaginationCore, self).__init__()\n",
    "        self.model = model\n",
    "        self.policy = policy\n",
    "        self.value = value\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        next_state, reward = self.model(state, action)\n",
    "        next_action = self.policy(next_state)\n",
    "        next_value = self.value(next_state)\n",
    "        return next_state, reward, next_action, next_value\n",
    "\n",
    "# Define modelos de política y valor\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Value, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "\n",
    "# Define el modelo de transición\n",
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(TransitionModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, state_dim + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        out = self.fc(x)\n",
    "        next_state = out[:, :-1]\n",
    "        reward = out[:, -1]\n",
    "        return next_state, reward\n",
    "\n",
    "# Implementación de entrenamiento para I2A\n",
    "def train_i2a(imagination_core, policy, value, data, policy_optimizer, value_optimizer):\n",
    "    # Asume que `data` es un DataLoader que proporciona estados y acciones.\n",
    "    for states, actions in data:\n",
    "        # Forward pass\n",
    "        imagined_states, imagined_rewards, imagined_actions, imagined_values = imagination_core(states, actions)\n",
    "        \n",
    "        # Policy Training\n",
    "        policy_optimizer.zero_grad()\n",
    "        action_probs = policy(states)\n",
    "        loss_policy = ...  # Calcula la pérdida de política utilizando las imaginaciones\n",
    "        loss_policy.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # Value Training\n",
    "        value_optimizer.zero_grad()\n",
    "        state_values = value(states)\n",
    "        loss_value = ...  # Calcula la pérdida de valor utilizando las imaginaciones\n",
    "        loss_value.backward()\n",
    "        value_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c87a7",
   "metadata": {},
   "source": [
    "AlphaZero combina la búsqueda en árbol de Monte Carlo (MCTS) con redes neuronales profundas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dabce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(AlphaZeroNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim, 256, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.policy_head = nn.Conv2d(256, action_dim, kernel_size=1)\n",
    "        self.value_head = nn.Conv2d(256, 1, kernel_size=1)\n",
    "        self.fc_value = nn.Linear(state_dim, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.conv1(state))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        policy = torch.softmax(self.policy_head(x).view(x.size(0), -1), dim=-1)\n",
    "        value = torch.tanh(self.fc_value(self.value_head(x).view(x.size(0), -1)))\n",
    "        return policy, value\n",
    "\n",
    "def mcts_policy_value(network, state, num_simulations):\n",
    "    for _ in range(num_simulations):\n",
    "        mcts_simulation(network, state)\n",
    "    return compute_policy_and_value_from_mcts()\n",
    "\n",
    "# Implementación de entrenamiento para AlphaZero\n",
    "def train_alphazero(network, data, optimizer):\n",
    "    # Asume que `data` es un DataLoader que proporciona estados, políticas y valores.\n",
    "    for states, target_policies, target_values in data:\n",
    "        optimizer.zero_grad()\n",
    "        policies, values = network(states)\n",
    "        loss_policy = nn.CrossEntropyLoss()(policies, target_policies)\n",
    "        loss_value = nn.MSELoss()(values, target_values)\n",
    "        loss = loss_policy + loss_value\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d195c0c",
   "metadata": {},
   "source": [
    "#### Ejercicio 1: World Models\n",
    "\n",
    "Objetivo: Implementa un World Model que aprenda una representación latente del entorno usando un VAE, predecir las transiciones de estado y recompensas con un MDN-RNN, y tomar decisiones con un controlador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de931a",
   "metadata": {},
   "source": [
    "#### Ejercicio 2: I2A (Imagination-Augmented Agents)\n",
    "\n",
    "Objetivo: Implementa I2A que utiliza un modelo imaginativo para simular futuros posibles y mejorar la toma de decisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e712c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd463b",
   "metadata": {},
   "source": [
    "#### Ejercicio 3: MBMF (Model-Based Model-Free)\n",
    "Objetivo: Implementa MBMF que combina RL basado en modelo y sin modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(TransitionModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, state_dim + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        out = self.fc(x)\n",
    "        next_state = out[:, :-1]\n",
    "        reward = out[:, -1]\n",
    "        return next_state, reward\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Value, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "\n",
    "def train_mbmf(transition_model, policy, value, data, policy_optimizer, value_optimizer, num_rollouts):\n",
    "    for states, actions in data:\n",
    "        rollouts = []\n",
    "        for _ in range(num_rollouts):\n",
    "            next_states, rewards = transition_model(states, actions)\n",
    "            rollouts.append((next_states, rewards))\n",
    "\n",
    "        policy_optimizer.zero_grad()\n",
    "        action_probs = policy(states)\n",
    "        loss_policy = -torch.mean(torch.sum(action_probs * torch.log(actions + 1e-10), dim=1))\n",
    "        loss_policy.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        state_values = value(states)\n",
    "        rollout_values = [value(ns) for ns, _ in rollouts]\n",
    "        loss_value = nn.MSELoss()(state_values, sum(rollout_values) / len(rollout_values))\n",
    "        loss_value.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "# Inicializar entorno y datos ficticios\n",
    "state_dim = 10\n",
    "action_dim = 5\n",
    "\n",
    "transition_model = TransitionModel(state_dim, action_dim)\n",
    "policy = Policy(state_dim, action_dim)\n",
    "value = Value(state_dim)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "value_optimizer = optim.Adam(value.parameters(), lr=1e-3)\n",
    "\n",
    "states = torch.randn(1000, state_dim)\n",
    "actions = torch.randint(0, action_dim, (1000,))\n",
    "\n",
    "data = DataLoader(TensorDataset(states, actions), batch_size=32, shuffle=True)\n",
    "\n",
    "# Entrenar MBMF\n",
    "train_mbmf(transition_model, policy, value, data, policy_optimizer, value_optimizer, num_rollouts=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12808fe2",
   "metadata": {},
   "source": [
    "#### Ejercicio 4: MBVE (Model-Based Value Expansion)\n",
    "Objetivo: Implementa MBVE que utiliza un modelo del entorno para expandir la estimación de valor más allá del horizonte de planificación actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(TransitionModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, state_dim + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        out = self.fc(x)\n",
    "        next_state = out[:, :-1]\n",
    "        reward = out[:, -1]\n",
    "        return next_state, reward\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Value, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "\n",
    "def train_mbve(transition_model, value, data, value_optimizer, gamma, horizon):\n",
    "    for states, actions in data:\n",
    "        expanded_values = []\n",
    "        for t in range(horizon):\n",
    "            next_states, rewards = transition_model(states, actions)\n",
    "            state_values = value(next_states)\n",
    "            expanded_values.append(rewards + gamma * state_values)\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        state_values = value(states)\n",
    "        loss_value = nn.MSELoss()(state_values, sum(expanded_values) / len(expanded_values))\n",
    "        loss_value.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "# Inicializar entorno y datos ficticios\n",
    "state_dim = 10\n",
    "action_dim = 5\n",
    "\n",
    "transition_model = TransitionModel(state_dim, action_dim)\n",
    "value = Value(state_dim)\n",
    "\n",
    "value_optimizer = optim.Adam(value.parameters(), lr=1e-3)\n",
    "\n",
    "states = torch.randn(1000, state_dim)\n",
    "actions = torch.randint(0, action_dim, (1000,))\n",
    "\n",
    "data = DataLoader(TensorDataset(states, actions), batch_size=32, shuffle=True)\n",
    "\n",
    "# Entrenar MBVE\n",
    "train_mbve(transition_model, value, data, value_optimizer, gamma=0.99, horizon=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d627ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f74d13",
   "metadata": {},
   "source": [
    "#### Ejercicio 5: AlphaZero\n",
    "\n",
    "Objetivo: Implementa AlphaZero que combina búsqueda en árbol de Monte Carlo (MCTS) con redes neuronales profundas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f012bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class AlphaZeroNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(AlphaZeroNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim, 256, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.policy_head = nn.Conv2d(256, action_dim, kernel_size=1)\n",
    "        self.value_head = nn.Conv2d(256, 1, kernel_size=1)\n",
    "        self.fc_value = nn.Linear(state_dim, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.conv1(state))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        policy = torch.softmax(self.policy_head(x).view(x.size(0), -1), dim=-1)\n",
    "        value = torch.tanh(self.fc_value(self.value_head(x).view(x.size(0), -1)))\n",
    "        return policy, value\n",
    "\n",
    "def mcts_policy_value(network, state, num_simulations):\n",
    "    # Aquí se implementa MCTS (omisión por simplicidad)\n",
    "    pass\n",
    "\n",
    "def train_alphazero(network, data, optimizer):\n",
    "    for states, target_policies, target_values in data:\n",
    "        optimizer.zero_grad()\n",
    "        policies, values = network(states)\n",
    "        loss_policy = nn.CrossEntropyLoss()(policies, target_policies)\n",
    "        loss_value = nn.MSELoss()(values, target_values)\n",
    "        loss = loss_policy + loss_value\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Inicializar entorno y datos ficticios\n",
    "state_dim = (3, 19, 19)\n",
    "action_dim = 362\n",
    "\n",
    "network = AlphaZeroNetwork(state_dim[0], action_dim)\n",
    "optimizer = optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "states = torch.randn(1000, *state_dim)\n",
    "target_policies = torch.randint(0, action_dim, (1000,))\n",
    "target_values = torch.randn(1000, 1)\n",
    "\n",
    "data = DataLoader(TensorDataset(states, target_policies, target_values), batch_size=32, shuffle=True)\n",
    "\n",
    "# Entrenar AlphaZero\n",
    "train_alphazero(network, data, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c8d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690c5d4",
   "metadata": {},
   "source": [
    "#### Ejercicio 6: World Models con transformers\n",
    "\n",
    "Objetivo: Implementa un World Model utilizando transformers para la predicción de secuencias en lugar de un MDN-RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73185f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gym\n",
    "\n",
    "# VAE para aprender la representación latente\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, state_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim * 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, state_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        mu_logvar = self.encoder(x)\n",
    "        mu, logvar = mu_logvar.chunk(2, dim=-1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Transformer para predecir las transiciones\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim, nhead, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=latent_dim + action_dim, nhead=nhead, num_encoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(latent_dim + action_dim, latent_dim + 1)\n",
    "\n",
    "    def forward(self, z, a):\n",
    "        x = torch.cat([z, a], dim=-1).unsqueeze(1)\n",
    "        out = self.transformer(x)\n",
    "        out = self.fc(out.squeeze(1))\n",
    "        next_z = out[:, :-1]\n",
    "        reward = out[:, -1]\n",
    "        return next_z, reward\n",
    "\n",
    "# Controlador para tomar decisiones\n",
    "class Controller(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super(Controller, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "# Entrenamiento del World Model\n",
    "def train_world_model(vae, transformer, controller, data, vae_optimizer, transformer_optimizer, controller_optimizer):\n",
    "    for states, actions in data:\n",
    "        # VAE Training\n",
    "        vae_optimizer.zero_grad()\n",
    "        recon_x, mu, logvar = vae(states)\n",
    "        loss_vae = vae_loss(recon_x, states, mu, logvar)\n",
    "        loss_vae.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "        # Transformer Training\n",
    "        transformer_optimizer.zero_grad()\n",
    "        z = vae.encode(states)[0]\n",
    "        next_z, rewards = transformer(z, actions)\n",
    "        loss_transformer = nn.MSELoss()(next_z, z) + nn.MSELoss()(rewards, torch.randn_like(rewards))  # Fake target\n",
    "        loss_transformer.backward()\n",
    "        transformer_optimizer.step()\n",
    "\n",
    "        # Controller Training\n",
    "        controller_optimizer.zero_grad()\n",
    "        z = vae.encode(states)[0]\n",
    "        action_probs = controller(z)\n",
    "        loss_controller = -torch.mean(torch.sum(action_probs * torch.log(actions + 1e-10), dim=1))\n",
    "        loss_controller.backward()\n",
    "        controller_optimizer.step()\n",
    "\n",
    "# Inicializar entorno y datos\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "latent_dim = 32\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "\n",
    "vae = VAE(state_dim, latent_dim)\n",
    "transformer = TransformerModel(latent_dim, action_dim, nhead, num_layers)\n",
    "controller = Controller(latent_dim, action_dim)\n",
    "\n",
    "vae_optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "transformer_optimizer = optim.Adam(transformer.parameters(), lr=1e-3)\n",
    "controller_optimizer = optim.Adam(controller.parameters(), lr=1e-3)\n",
    "\n",
    "# Generar datos ficticios para el entrenamiento\n",
    "states = torch.randn(1000, state_dim)\n",
    "actions = torch.randint(0, action_dim, (1000,))\n",
    "\n",
    "data = DataLoader(TensorDataset(states, actions), batch_size=32, shuffle=True)\n",
    "\n",
    "# Entrenar el World Model\n",
    "train_world_model(vae, transformer, controller, data, vae_optimizer, transformer_optimizer, controller_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff98b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add7d7e",
   "metadata": {},
   "source": [
    "Ejercicio 7: RLHF (Reinforcement Learning with Human Feedback)\n",
    "Objetivo: Implementa un agente que utilice feedback humano para mejorar la política."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gym\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_policy_with_human_feedback(policy_network, feedback_data, optimizer):\n",
    "    for states, actions, feedback in feedback_data:\n",
    "        optimizer.zero_grad()\n",
    "        action_probs = policy_network(states)\n",
    "        loss = -torch.mean(feedback * torch.sum(action_probs * actions, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Inicializar entorno y datos ficticios\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy_network = PolicyNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=1e-3)\n",
    "\n",
    "# Generar datos ficticios para el entrenamiento\n",
    "states = torch.randn(1000, state_dim)\n",
    "actions = torch.randint(0, action_dim, (1000,))\n",
    "feedback = torch.randn(1000, 1)  # Fake feedback from human\n",
    "\n",
    "feedback_data = DataLoader(TensorDataset(states, actions, feedback), batch_size=32, shuffle=True)\n",
    "\n",
    "# Entrenar la política con feedback humano\n",
    "train_policy_with_human_feedback(policy_network, feedback_data, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3dd02b",
   "metadata": {},
   "source": [
    "#### Ejercicio 8: AlphaZero con Transformers\n",
    "Objetivo: Implementa AlphaZero utilizando transformers para la predicción de políticas y valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class TransformerAlphaZero(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, nhead, num_layers):\n",
    "        super(TransformerAlphaZero, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=state_dim, nhead=nhead, num_encoder_layers=num_layers)\n",
    "        self.policy_head = nn.Linear(state_dim, action_dim)\n",
    "        self.value_head = nn.Linear(state_dim, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.transformer(state.unsqueeze(1)).squeeze(1)\n",
    "        policy = torch.softmax(self.policy_head(x), dim=-1)\n",
    "        value = torch.tanh(self.value_head(x))\n",
    "        return policy, value\n",
    "\n",
    "def train_alphazero_transformer(network, data, optimizer):\n",
    "    for states, target_policies, target_values in data:\n",
    "        optimizer.zero_grad()\n",
    "        policies, values = network(states)\n",
    "        loss_policy = nn.CrossEntropyLoss()(policies, target_policies)\n",
    "        loss_value = nn.MSELoss()(values, target_values)\n",
    "        loss = loss_policy + loss_value\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Inicializar entorno y datos ficticios\n",
    "state_dim = 128\n",
    "action_dim = 10\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "\n",
    "network = TransformerAlphaZero(state_dim, action_dim, nhead, num_layers)\n",
    "optimizer = optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "states = torch.randn(1000, state_dim)\n",
    "target_policies = torch.randint(0, action_dim, (1000,))\n",
    "target_values = torch.randn(1000, 1)\n",
    "\n",
    "data = DataLoader(TensorDataset(states, target_policies, target_values), batch_size=32, shuffle=True)\n",
    "\n",
    "# Entrenar AlphaZero con transformers\n",
    "train_alphazero_transformer(network, data, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
