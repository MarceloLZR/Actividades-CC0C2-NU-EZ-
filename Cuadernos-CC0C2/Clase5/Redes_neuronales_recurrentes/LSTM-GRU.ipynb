{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9754276-8fb2-4e52-99f0-ea134cf627e1",
   "metadata": {},
   "source": [
    "### Preliminares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf301f-f696-4600-bee4-b8bcea8f8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "#plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab39538-6273-4f0f-b615-e58b66518063",
   "metadata": {},
   "source": [
    "#### Definimos las funciones de activación a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aace101-3333-4df1-bbfe-ff951dcc9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: ndarray):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x: ndarray):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def tanh(x: ndarray):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(x: ndarray):\n",
    "    return 1 - np.tanh(x) * np.tanh(x)\n",
    "\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
    "\n",
    "\n",
    "def batch_softmax(input_array: ndarray):\n",
    "    out = []\n",
    "    for row in input_array:\n",
    "        out.append(softmax(row, axis=1))\n",
    "    return np.stack(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d66ad-1ebe-45b8-b928-0d8f0f54b41f",
   "metadata": {},
   "source": [
    "##### Optimizador de redes neuronales recurrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc5b71-2bce-499b-8b38-b87711a3b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNOptimizer(object):\n",
    "    # Constructor de la clase optimizador para redes neuronales recurrentes.\n",
    "    # Args:\n",
    "    #   lr (float): Tasa de aprendizaje, con un valor por defecto de 0.01.\n",
    "    #   gradient_clipping (bool): Indica si se aplica clipping a los gradientes,\n",
    "    #                             activado por defecto.\n",
    "    def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -> None:\n",
    "        self.lr = lr  # Almacena la tasa de aprendizaje.\n",
    "        self.gradient_clipping = gradient_clipping  # Almacena la configuración de clipping.\n",
    "        self.first = True  # Variable auxiliar, posiblemente para controlar la primera actualización.\n",
    "\n",
    "    # Método que ejecuta un paso de optimización sobre todos los parámetros del modelo.\n",
    "    def step(self) -> None:\n",
    "        # Itera sobre cada capa del modelo.\n",
    "        for layer in self.model.layers:\n",
    "            # Itera sobre cada parámetro de la capa.\n",
    "            for key in layer.params.keys():\n",
    "                # Si el clipping de gradientes está activado, aplica clipping.\n",
    "                if self.gradient_clipping:\n",
    "                    # Los gradientes se limitan a estar entre -2 y 2 para evitar la explosión de gradientes.\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "\n",
    "                # Llama a la regla de actualización para ajustar el valor del parámetro\n",
    "                # usando la tasa de aprendizaje y el gradiente actual.\n",
    "                self._update_rule(param=layer.params[key]['value'],\n",
    "                                  grad=layer.params[key]['deriv'])\n",
    "\n",
    "    # Método abstracto para definir la regla de actualización de parámetros.\n",
    "    # Debe ser implementado por subclases.\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        raise NotImplementedError(\"Este método debe ser implementado por subclases.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff18f2-df0c-47e7-87ee-1a64478cc3df",
   "metadata": {},
   "source": [
    "#### Algunos optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da952c52-241f-4660-835b-ebd0569d56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(RNNOptimizer):\n",
    "    # Constructor de la clase SGD (Stochastic Gradient Descent).\n",
    "    # Args:\n",
    "    #   lr (float): Tasa de aprendizaje.\n",
    "    #   gradient_clipping (bool): Indica si se activa el clipping de gradientes.\n",
    "    def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -> None:\n",
    "        super().__init__(lr, gradient_clipping)  # Inicializa la clase base\n",
    "\n",
    "    # Método específico de SGD para actualizar los parámetros según el gradiente.\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        update = self.lr * kwargs['grad']  # Calcula la actualización del parámetro.\n",
    "        kwargs['param'] -= update  # Actualiza el parámetro restando la actualización.\n",
    "        \n",
    "class AdaGrad(RNNOptimizer):\n",
    "    # Constructor de la clase AdaGrad.\n",
    "    # Args:\n",
    "    #   lr (float): Tasa de aprendizaje.\n",
    "    #   gradient_clipping (bool): Indica si se activa el clipping de gradientes.\n",
    "    def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -> None:\n",
    "        super().__init__(lr, gradient_clipping)  # Inicializa la clase base\n",
    "        self.eps = 1e-7  # Un pequeño número para evitar división por cero en la actualización.\n",
    "\n",
    "    # Método que ejecuta un paso de optimización sobre todos los parámetros del modelo.\n",
    "    def step(self) -> None:\n",
    "        if self.first:\n",
    "            self.sum_squares = {}  # Diccionario para almacenar la suma acumulada de los cuadrados de los gradientes.\n",
    "            # Inicializa sum_squares para cada parámetro en cada capa.\n",
    "            for i, layer in enumerate(self.model.layers):\n",
    "                self.sum_squares[i] = {}\n",
    "                for key in layer.params.keys():\n",
    "                    self.sum_squares[i][key] = np.zeros_like(layer.params[key]['value'])\n",
    "            self.first = False  # Marca la inicialización como completa.\n",
    "\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            for key in layer.params.keys():\n",
    "                if self.gradient_clipping:\n",
    "                    # Si el clipping de gradientes está activado, aplica clipping.\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "\n",
    "                # Actualiza cada parámetro utilizando la regla específica de AdaGrad.\n",
    "                self._update_rule(param=layer.params[key]['value'],\n",
    "                                  grad=layer.params[key]['deriv'],\n",
    "                                  sum_square=self.sum_squares[i][key])\n",
    "\n",
    "    # Método específico de AdaGrad para actualizar los parámetros.\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        # Actualiza la suma acumulada de los cuadrados de los gradientes.\n",
    "        kwargs['sum_square'] += self.eps + np.power(kwargs['grad'], 2)\n",
    "        # Calcula la tasa de aprendizaje escalada.\n",
    "        lr_scaled = self.lr / np.sqrt(kwargs['sum_square'])\n",
    "        # Utiliza la tasa de aprendizaje escalada para actualizar los parámetros.\n",
    "        kwargs['param'] -= lr_scaled * kwargs['grad']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f9936-c0e7-4294-88b8-6659aee3d1ef",
   "metadata": {},
   "source": [
    "#### Funciones de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9e0d4-e4d9-46c3-b719-f0faadf2b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def assert_same_shape(output: np.ndarray, output_grad: np.ndarray):\n",
    "    assert output.shape == output_grad.shape, \\\n",
    "        '''\n",
    "        Dos tensores deben tener la misma forma;\n",
    "        en cambio, la forma del primer tensor es {0}\n",
    "        y la forma del segundo tensor es {1}.\n",
    "        '''.format(tuple(output_grad.shape), tuple(output.shape))\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68577fda-7a5b-4145-867d-d29bdc700294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    # Constructor de la clase base para las pérdidas.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Método forward para calcular la pérdida entre las predicciones y los objetivos.\n",
    "    def forward(self,\n",
    "                prediction: ndarray,\n",
    "                target: ndarray) -> float:\n",
    "        # Asegura que las predicciones y los objetivos tengan la misma forma.\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction  # Almacena las predicciones.\n",
    "        self.target = target  # Almacena los objetivos.\n",
    "\n",
    "        self.output = self._output()  # Calcula la pérdida utilizando una función interna.\n",
    "\n",
    "        return self.output  # Devuelve el valor de la pérdida.\n",
    "    \n",
    "    # Método backward para calcular el gradiente de la pérdida respecto a las entradas.\n",
    "    def backward(self) -> ndarray:\n",
    "        self.input_grad = self._input_grad()  # Calcula el gradiente utilizando una función interna.\n",
    "\n",
    "        # Asegura que las predicciones y el gradiente tengan la misma forma.\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad  # Devuelve el gradiente de la entrada.\n",
    "\n",
    "    # Método abstracto para calcular la pérdida; debe ser implementado por subclases.\n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Método abstracto para calcular el gradiente de la entrada; debe ser implementado por subclases.\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    # Constructor de la clase para la pérdida de entropía cruzada con softmax.\n",
    "    def __init__(self, eps: float=1e-9) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps  # Un pequeño número para evitar inestabilidad numérica.\n",
    "\n",
    "    # Método para calcular la salida de la pérdida de entropía cruzada con softmax.\n",
    "    def _output(self) -> float:\n",
    "        out = []\n",
    "        # Aplica softmax a cada fila de la predicción.\n",
    "        for row in self.prediction:\n",
    "            out.append(softmax(row, axis=1))\n",
    "        softmax_preds = np.stack(out)\n",
    "\n",
    "        # Recorta la salida de softmax para prevenir inestabilidad numérica.\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # Calcula la pérdida real de entropía cruzada.\n",
    "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "            (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss)  # Retorna la suma total de la pérdida.\n",
    "\n",
    "    # Método para calcular el gradiente de la entrada basado en la salida de softmax.\n",
    "    def _input_grad(self) -> np.ndarray:\n",
    "        return self.softmax_preds - self.target  # Gradiente de la pérdida respecto a la entrada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672d134-e2fd-48a3-8e73-1de5671ddaf8",
   "metadata": {},
   "source": [
    " ## Redes neuronales recurrentes\n",
    "Una red neuronal recurrente (RNN) es una clase de redes neuronales diseñadas para manejar secuencias de datos, como series temporales o secuencias lingüísticas. \n",
    "\n",
    "A diferencia de las redes neuronales tradicionales que asumen independencia entre las entradas, las RNNs tienen \"memoria\" sobre entradas anteriores. Esto les permite retener información a través del tiempo y utilizar esta información para influir en la salida actual, lo cual es crucial para tareas donde el contexto y el orden de los datos son importantes, como el procesamiento del lenguaje natural o el análisis de series temporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fac0e-6e07-4c7d-8489-58590dd6dac6",
   "metadata": {},
   "source": [
    "#### Clase RNNNode\n",
    "La clase RNNNode define un nodo dentro de una red neuronal recurrente (RNN), capaz de realizar cálculos hacia adelante y hacia atrás. El método forward calcula la salida y el nuevo estado oculto del nodo basado en la entrada actual y el estado oculto anterior, utilizando matrices de pesos y sesgos. \n",
    "\n",
    "El método backward se encarga de la retropropagación del error, calculando los gradientes respecto a las entradas y actualizando los parámetros del modelo basados en estos gradientes. Esto permite entrenar la red para ajustar sus pesos y mejorar la precisión de sus predicciones a lo largo del tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5009e-d6cc-401d-8167-944bfb994d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNode(object):\n",
    "    # Constructor del nodo RNN, inicialmente no realiza ninguna operación específica.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Método forward para calcular la salida del nodo RNN a partir de la entrada y el estado oculto anterior.\n",
    "    def forward(self,\n",
    "                x_in: ndarray, \n",
    "                H_in: ndarray,\n",
    "                params_dict: Dict[str, Dict[str, ndarray]]\n",
    "                ) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        Calcula la salida del nodo RNN y el nuevo estado oculto.\n",
    "        Args:\n",
    "        x_in: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        H_in: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        Retorna x_out: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        Retorna H_out: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        '''\n",
    "        self.X_in = x_in\n",
    "        self.H_in = H_in\n",
    "    \n",
    "        # Concatena la entrada x con el estado oculto anterior H.\n",
    "        self.Z = np.column_stack((x_in, H_in))\n",
    "        \n",
    "        # Calcula el nuevo estado oculto intermedio usando los pesos y biases.\n",
    "        self.H_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
    "        \n",
    "        # Aplica la función de activación tanh al estado oculto intermedio.\n",
    "        self.H_out = tanh(self.H_int)\n",
    "\n",
    "        # Calcula la salida del nodo RNN.\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out\n",
    "\n",
    "    # Método backward para la propagación hacia atrás del error a través del nodo RNN.\n",
    "    def backward(self, \n",
    "                 X_out_grad: ndarray, \n",
    "                 H_out_grad: ndarray,\n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        Retropropaga el gradiente a través del nodo RNN.\n",
    "        Args:\n",
    "        X_out_grad: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        H_out_grad: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        Retorna X_in_grad: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        Retorna H_in_grad: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        '''\n",
    "        \n",
    "        # Verifica que los gradientes y las salidas tengan la misma forma.\n",
    "        assert_same_shape(X_out_grad, self.X_out)\n",
    "        assert_same_shape(H_out_grad, self.H_out)\n",
    "\n",
    "        # Calcula los gradientes para los parámetros de salida y acumula en los derivados.\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        \n",
    "        # Propaga el gradiente hacia atrás a través de la red.\n",
    "        dh = np.dot(X_out_grad, params_dict['W_v']['value'].T)\n",
    "        dh += H_out_grad\n",
    "        \n",
    "        # Calcula el gradiente del estado oculto intermedio.\n",
    "        dH_int = dh * dtanh(self.H_int)\n",
    "        \n",
    "        # Acumula gradientes en los parámetros del estado oculto.\n",
    "        params_dict['B_f']['deriv'] += dH_int.sum(axis=0)\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, dH_int)     \n",
    "        \n",
    "        # Calcula los gradientes de entrada.\n",
    "        dz = np.dot(dH_int, params_dict['W_f']['value'].T)\n",
    "        X_in_grad = dz[:, :self.X_in.shape[1]]\n",
    "        H_in_grad = dz[:, self.X_in.shape[1]:]\n",
    "        \n",
    "        return X_in_grad, H_in_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1207c7e3-a638-4604-aa50-63ed3bbc89ff",
   "metadata": {},
   "source": [
    "#### RNNLayer\n",
    "\n",
    "La clase RNNLayer es una capa de red neuronal recurrente que maneja la propagación hacia adelante y hacia atrás de los datos a través de una secuencia de tiempo. En el método forward, la capa procesa secuencialmente la entrada utilizando nodos RNN internos, cada uno correspondiente a un paso de tiempo, manteniendo un estado oculto que pasa de un nodo a otro. Esta capa es capaz de ajustar sus pesos y sesgos para mejorar la predicción del siguiente carácter en una secuencia.\n",
    "\n",
    "Durante la retropropagación, calcula los gradientes para actualizar los parámetros con el fin de minimizar el error en las predicciones. Esta estructura es fundamental para tareas de procesamiento de secuencias como la generación de texto, donde la dependencia temporal entre los datos es crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46601f36-22ce-4830-b739-0156ec964868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(object):\n",
    "    # Constructor de la clase de la capa RNN.\n",
    "    # Args:\n",
    "    #   hidden_size: int - Número de neuronas ocultas en la capa RNN.\n",
    "    #   output_size: int - Número de caracteres en el vocabulario para predecir el siguiente carácter.\n",
    "    #   weight_scale: float - Escala para la inicialización de los pesos.\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = None):\n",
    "        self.hidden_size = hidden_size  # Almacena el tamaño del estado oculto.\n",
    "        self.output_size = output_size  # Almacena el tamaño de salida.\n",
    "        self.weight_scale = weight_scale  # Escala de inicialización de los pesos.\n",
    "        self.start_H = np.zeros((1, hidden_size))  # Estado oculto inicial.\n",
    "        self.first = True  # Bandera para inicialización en el primer paso forward.\n",
    "\n",
    "    # Método para inicializar los parámetros de la capa.\n",
    "    def _init_params(self, input_: ndarray):\n",
    "        self.vocab_size = input_.shape[2]  # Tamaño del vocabulario a partir de la entrada.\n",
    "        # Establece la escala de peso si no se proporcionó.\n",
    "        if not self.weight_scale:\n",
    "            self.weight_scale = 2 / (self.vocab_size + self.output_size)\n",
    "        \n",
    "        self.params = {'W_f': {}, 'B_f': {}, 'W_v': {}, 'B_v': {}}\n",
    "        # Inicializa pesos y sesgos con valores aleatorios normalizados.\n",
    "        self.params['W_f']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                       (self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(0.0, self.weight_scale, (1, self.hidden_size))\n",
    "        self.params['W_v']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                       (self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(0.0, self.weight_scale, (1, self.output_size))\n",
    "\n",
    "        self.params['W_f']['deriv'] = np.zeros_like(self.params['W_f']['value'])\n",
    "        self.params['B_f']['deriv'] = np.zeros_like(self.params['B_f']['value'])\n",
    "        self.params['W_v']['deriv'] = np.zeros_like(self.params['W_v']['value'])\n",
    "        self.params['B_v']['deriv'] = np.zeros_like(self.params['B_v']['value'])\n",
    "        \n",
    "        self.cells = [RNNNode() for _ in range(input_.shape[1])]  # Inicializa nodos RNN por cada paso de secuencia.\n",
    "\n",
    "    # Limpia los gradientes acumulados en los parámetros.\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "\n",
    "    # Procesa la entrada a través de la capa RNN y calcula la salida para cada paso de tiempo.\n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)  # Inicializa parámetros en el primer paso.\n",
    "            self.first = False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        H_in = np.repeat(self.start_H, batch_size, axis=0)\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)  # Actualiza el estado oculto inicial para la próxima ejecución.\n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "    # Retropropaga el error desde la salida hacia las entradas.\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        sequence_length = x_seq_out_grad.shape[1]\n",
    "        x_seq_in_grad = np.zeros((batch_size, sequence_length, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(sequence_length)):\n",
    "            x_out_grad = x_seq_out_grad[:, t, :]\n",
    "            grad_out, h_in_grad = self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        \n",
    "        return x_seq_in_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef54c0-a343-4280-b8f5-758e9d8ca1e3",
   "metadata": {},
   "source": [
    "#### RNNModel\n",
    "\n",
    "El código define un modelo de red neuronal recurrente (RNN) que es capaz de procesar secuencias de datos, como series temporales o texto. El modelo está compuesto por varias capas (RNNLayer), cada una procesando la entrada y pasándola a la siguiente. El proceso de entrenamiento ocurre en pasos, donde cada paso involucra:\n",
    "\n",
    "* Paso hacia adelante (forward): Cada entrada de la secuencia es procesada por todas las capas de la red, pasando de una a otra. Esta operación se utiliza para obtener la salida de la red que luego se compara con el objetivo real para calcular la pérdida.\n",
    "* Cálculo de la pérdida: Se usa un objeto de pérdida para evaluar qué tan bien la salida de la red coincide con los objetivos esperados.\n",
    "\n",
    "* Paso hacia atrás (backward): Una vez calculada la pérdida, se calcula el gradiente de la pérdida respecto a las salidas, y este gradiente se propaga hacia atrás a través de la red para actualizar los pesos de las neuronas en cada capa, lo que permite que la red aprenda.\n",
    "\n",
    "* Actualización de parámetros: Basándose en los gradientes obtenidos de la retropropagación, se actualizan los parámetros de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340dcf7-9330-4e5d-af6a-0067c0ef7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(object):\n",
    "    '''\n",
    "    Clase Modelo que recibe entradas y objetivos, entrena la red y calcula la pérdida.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[RNNLayer],\n",
    "                 sequence_length: int, \n",
    "                 vocab_size: int, \n",
    "                 loss: Loss):\n",
    "        '''\n",
    "        Inicializa el modelo de red neuronal recurrente.\n",
    "        Args:\n",
    "        layers: Lista de capas RNN en la red.\n",
    "        sequence_length: Longitud de la secuencia que pasa a través de la red.\n",
    "        vocab_size: Número de caracteres en el vocabulario.\n",
    "        loss: Objeto de pérdida utilizado para calcular la pérdida durante el entrenamiento.\n",
    "        '''\n",
    "        self.layers = layers  # Lista de capas RNN.\n",
    "        self.vocab_size = vocab_size  # Tamaño del vocabulario.\n",
    "        self.sequence_length = sequence_length  # Longitud de la secuencia.\n",
    "        self.loss = loss  # Objeto de pérdida.\n",
    "        # Establece la longitud de la secuencia para cada capa.\n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "    def forward(self, \n",
    "                x_batch: ndarray):\n",
    "        '''\n",
    "        Realiza la propagación hacia adelante a través de la red.\n",
    "        Args:\n",
    "        x_batch: Array de entrada con forma (batch_size, sequence_length, vocab_size)\n",
    "        Returns:\n",
    "        x_batch_in: Array de salida de la última capa.\n",
    "        '''       \n",
    "        for layer in self.layers:\n",
    "            x_batch = layer.forward(x_batch)  # Propaga la entrada a través de cada capa.\n",
    "        return x_batch\n",
    "        \n",
    "    def backward(self, \n",
    "                 loss_grad: ndarray):\n",
    "        '''\n",
    "        Realiza la retropropagación a través de la red utilizando el gradiente de la pérdida.\n",
    "        Args:\n",
    "        loss_grad: Gradiente de la pérdida con forma (batch_size, sequence_length, vocab_size)\n",
    "        Returns:\n",
    "        loss_grad: Propaga el gradiente a través de todas las capas.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(loss_grad)  # Retropropaga a través de cada capa en orden inverso.\n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, \n",
    "                    x_batch: ndarray, \n",
    "                    y_batch: ndarray):\n",
    "        '''\n",
    "        Ejecuta un único paso de entrenamiento completo:\n",
    "        1. Paso hacia adelante y aplicación de softmax.\n",
    "        2. Calcula la pérdida y su gradiente.\n",
    "        3. Paso hacia atrás.\n",
    "        4. Actualización de parámetros.\n",
    "        Args:\n",
    "        x_batch: Array de entrada con forma (batch_size, sequence_length, vocab_size)\n",
    "        y_batch: Array objetivo correspondiente.\n",
    "        Returns:\n",
    "        loss: Pérdida calculada para el lote actual.\n",
    "        '''\n",
    "        x_batch_out = self.forward(x_batch)  # Paso hacia adelante.\n",
    "        loss = self.loss.forward(x_batch_out, y_batch)  # Calcula la pérdida.\n",
    "        loss_grad = self.loss.backward()  # Calcula el gradiente de la pérdida.\n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()  # Limpia los gradientes en cada capa.\n",
    "        self.backward(loss_grad)  # Retropropaga el gradiente de la pérdida.\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3a356-02a5-4e5e-8375-8d8a1bd092d6",
   "metadata": {},
   "source": [
    "#### RNNTrainer\n",
    "\n",
    "Este código define una clase RNNTrainer que se utiliza para entrenar un modelo de red neuronal recurrente (RNN) para la generación de texto. Utiliza un archivo de texto como datos de entrada y realiza las siguientes tareas principales:\n",
    "\n",
    "* Inicialización: Prepara el modelo, el optimizador y los datos necesarios para el entrenamiento, incluyendo la creación de mapeos de caracteres a índices y viceversa.\n",
    "\n",
    "* Generación de entradas y objetivos: Crea los lotes de datos de entrada y los objetivos (targets) correspondientes que el modelo intentará predecir.\n",
    "\n",
    "* Entrenamiento: Ejecuta el proceso de entrenamiento en varias iteraciones, donde cada iteración incluye un paso hacia adelante (forward), el cálculo de la pérdida, un paso hacia atrás (backward) para la propagación del error, y la actualización de los parámetros del modelo.\n",
    "\n",
    "* Muestreo de salidas: Opcionalmente, genera texto basado en el modelo entrenado para visualizar cómo está aprendiendo el modelo durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14ab9c-a999-4705-90e7-0a3c22e17411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    '''\n",
    "    Clase que toma un archivo de texto y un modelo, y comienza a generar caracteres.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 text_file: str, \n",
    "                 model: RNNModel,\n",
    "                 optim: RNNOptimizer,\n",
    "                 batch_size: int = 32):\n",
    "        # Leer los datos del archivo de texto.\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = model  # Modelo de red neuronal recurrente.\n",
    "        self.chars = list(set(self.data))  # Lista de caracteres únicos en el texto.\n",
    "        self.vocab_size = len(self.chars)  # Tamaño del vocabulario.\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}  # Diccionario de caracteres a índices.\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}  # Diccionario inverso de índices a caracteres.\n",
    "        self.sequence_length = self.model.sequence_length  # Longitud de la secuencia usada en el modelo.\n",
    "        self.batch_size = batch_size  # Tamaño del lote para el entrenamiento.\n",
    "        self.optim = optim  # Optimizador para ajustar los parámetros del modelo.\n",
    "        setattr(self.optim, 'model', self.model)  # Establece el modelo en el optimizador.\n",
    "\n",
    "    def _generate_inputs_targets(self, start_pos: int):\n",
    "        # Genera índices para los lotes de entradas y objetivos desde una posición inicial.\n",
    "        inputs_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
    "        targets_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            inputs_indices[i, :] = np.array([self.char_to_idx[ch] \n",
    "                            for ch in self.data[start_pos + i: start_pos + self.sequence_length  + i]])\n",
    "            targets_indices[i, :] = np.array([self.char_to_idx[ch] \n",
    "                         for ch in self.data[start_pos + 1 + i: start_pos + self.sequence_length + 1 + i]])\n",
    "\n",
    "        return inputs_indices, targets_indices\n",
    "\n",
    "    def _generate_one_hot_array(self, indices: ndarray):\n",
    "        # Convierte los índices de caracteres a una representación one-hot.\n",
    "        batch = []\n",
    "        for seq in indices:\n",
    "            one_hot_sequence = np.zeros((self.sequence_length, self.vocab_size))\n",
    "            for i in range(self.sequence_length):\n",
    "                one_hot_sequence[i, seq[i]] = 1.0\n",
    "            batch.append(one_hot_sequence) \n",
    "        return np.stack(batch)\n",
    "\n",
    "    def sample_output(self, input_char: int, sample_length: int):\n",
    "        # Genera una muestra de salida del modelo actual, caracter por caracter.\n",
    "        indices = []\n",
    "        sample_model = deepcopy(self.model)  # Hace una copia del modelo para usar en muestreo.\n",
    "        for i in range(sample_length):\n",
    "            input_char_batch = np.zeros((1, 1, self.vocab_size))\n",
    "            input_char_batch[0, 0, input_char] = 1.0\n",
    "            x_batch_out = sample_model.forward(input_char_batch)\n",
    "            x_softmax = batch_softmax(x_batch_out)\n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
    "            indices.append(input_char)\n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "\n",
    "    def train(self, num_iterations: int, sample_every: int=100):\n",
    "        # Entrena el modelo para generar caracteres.\n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        num_iter = 0\n",
    "        start_pos = 0\n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            if start_pos + self.sequence_length + self.batch_size + 1 > len(self.data):\n",
    "                start_pos = 0\n",
    "            inputs_indices, targets_indices = self._generate_inputs_targets(start_pos)\n",
    "            inputs_batch, targets_batch = \\\n",
    "                self._generate_one_hot_array(inputs_indices), self._generate_one_hot_array(targets_indices)\n",
    "            loss = self.model.single_step(inputs_batch, targets_batch)\n",
    "            self.optim.step()\n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            start_pos += self.batch_size\n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            if num_iter % sample_every == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], 200)\n",
    "                print(sample_text)\n",
    "            num_iter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6df46d-d512-496a-a7d4-b48ce873c01a",
   "metadata": {},
   "source": [
    "#### Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4e3f6-a27a-4b51-8df5-c95ead849313",
   "metadata": {},
   "outputs": [],
   "source": [
    "capas = [RNNLayer(hidden_size=256, output_size=62)]\n",
    "mod = RNNModel(layers=capas,\n",
    "               vocab_size=62, sequence_length=10,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = SGD(lr=0.001, gradient_clipping=True)\n",
    "trainer = RNNTrainer('Ejemplo.txt', mod, optim)\n",
    "trainer.train(100, sample_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8062f62c-faa4-43ed-b927-fc55282fb34c",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "**Clase LSTMNode**\n",
    "\n",
    "- Clase `LSTMNode`: Representa un nodo en una capa de LSTM, encargado de procesar datos secuenciales.\n",
    "- Método `forward`: Implementa el paso hacia adelante. Realiza cálculos para las puertas de la LSTM (olvido, entrada y salida) y actualiza el estado de memoria y el estado oculto.\n",
    "- Método `backward`: Realiza el paso hacia atrás o retropropagación para ajustar los gradientes de los parámetros. Calcula los gradientes para cada puerta y los propaga hacia atrás, permitiendo actualizar los parámetros de la red LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f3ae6-bc6b-4c25-97f7-6fe6da2b35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNode:\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor de la clase LSTMNode. \n",
    "        Inicializa los parámetros que se utilizarán en el nodo de la LSTM.\n",
    "        \n",
    "        param hidden_size: int - número de neuronas ocultas en la capa LSTM.\n",
    "        param vocab_size: int - tamaño del vocabulario, es decir, el número de caracteres o palabras posibles.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def forward(self, \n",
    "                X_in: ndarray, \n",
    "                H_in: ndarray, \n",
    "                C_in: ndarray, \n",
    "                params_dict: Dict[str, Dict[str, ndarray]]):\n",
    "        '''\n",
    "        Realiza el paso hacia adelante de la LSTM.\n",
    "        \n",
    "        param X_in: numpy array con forma (batch_size, vocab_size), representa la entrada al nodo LSTM.\n",
    "        param H_in: numpy array con forma (batch_size, hidden_size), representa el estado oculto anterior.\n",
    "        param C_in: numpy array con forma (batch_size, hidden_size), representa el estado de memoria anterior.\n",
    "        param params_dict: Diccionario que contiene los pesos y sesgos de la LSTM.\n",
    "        \n",
    "        return self.X_out: numpy array con forma (batch_size, output_size), salida de la red.\n",
    "        return self.H_out: numpy array con forma (batch_size, hidden_size), nuevo estado oculto.\n",
    "        return self.C_out: numpy array con forma (batch_size, hidden_size), nuevo estado de memoria.\n",
    "        '''\n",
    "        # Guarda la entrada y el estado de memoria anterior.\n",
    "        self.X_in = X_in\n",
    "        self.C_in = C_in\n",
    "\n",
    "        # Concatenación de la entrada y el estado oculto para el cálculo de las puertas.\n",
    "        self.Z = np.column_stack((X_in, H_in))\n",
    "        \n",
    "        # Cálculo de la puerta de olvido (forget gate).\n",
    "        self.f_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
    "        self.f = sigmoid(self.f_int)\n",
    "        \n",
    "        # Cálculo de la puerta de entrada (input gate).\n",
    "        self.i_int = np.dot(self.Z, params_dict['W_i']['value']) + params_dict['B_i']['value']\n",
    "        self.i = sigmoid(self.i_int)\n",
    "        self.C_bar_int = np.dot(self.Z, params_dict['W_c']['value']) + params_dict['B_c']['value']\n",
    "        self.C_bar = tanh(self.C_bar_int)\n",
    "\n",
    "        # Cálculo del nuevo estado de memoria.\n",
    "        self.C_out = self.f * C_in + self.i * self.C_bar\n",
    "        \n",
    "        # Cálculo de la puerta de salida (output gate).\n",
    "        self.o_int = np.dot(self.Z, params_dict['W_o']['value']) + params_dict['B_o']['value']\n",
    "        self.o = sigmoid(self.o_int)\n",
    "        \n",
    "        # Cálculo del nuevo estado oculto.\n",
    "        self.H_out = self.o * tanh(self.C_out)\n",
    "\n",
    "        # Cálculo de la salida de la red.\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out, self.C_out \n",
    "\n",
    "    def backward(self, \n",
    "                 X_out_grad: ndarray, \n",
    "                 H_out_grad: ndarray, \n",
    "                 C_out_grad: ndarray, \n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]):\n",
    "        '''\n",
    "        Realiza el paso hacia atrás (backpropagation) de la LSTM.\n",
    "        \n",
    "        param X_out_grad: Gradiente de la pérdida con respecto a la salida.\n",
    "        param H_out_grad: Gradiente de la pérdida con respecto al estado oculto.\n",
    "        param C_out_grad: Gradiente de la pérdida con respecto al estado de memoria.\n",
    "        param params_dict: Diccionario que contiene los pesos, sesgos y sus derivadas para actualizar los parámetros.\n",
    "        \n",
    "        return dx_prev: numpy array con forma (1, vocab_size), gradiente con respecto a la entrada anterior.\n",
    "        return dH_prev: numpy array con forma (1, hidden_size), gradiente con respecto al estado oculto anterior.\n",
    "        return dC_prev: numpy array con forma (1, hidden_size), gradiente con respecto al estado de memoria anterior.\n",
    "        '''\n",
    "        \n",
    "        # Asegura que las formas de los gradientes coinciden con las salidas.\n",
    "        assert_same_shape(X_out_grad, self.X_out)\n",
    "        assert_same_shape(H_out_grad, self.H_out)\n",
    "        assert_same_shape(C_out_grad, self.C_out)\n",
    "\n",
    "        # Cálculo de las derivadas para los parámetros de la salida.\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "\n",
    "        # Gradiente con respecto al estado oculto.\n",
    "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)        \n",
    "        dh_out += H_out_grad\n",
    "                         \n",
    "        # Gradiente de la puerta de salida.\n",
    "        do = dh_out * tanh(self.C_out)\n",
    "        do_int = dsigmoid(self.o_int) * do\n",
    "        params_dict['W_o']['deriv'] += np.dot(self.Z.T, do_int)\n",
    "        params_dict['B_o']['deriv'] += do_int.sum(axis=0)\n",
    "\n",
    "        # Gradiente del estado de memoria.\n",
    "        dC_out = dh_out * self.o * dtanh(self.C_out)\n",
    "        dC_out += C_out_grad\n",
    "        dC_bar = dC_out * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar_int) * dC_bar\n",
    "        params_dict['W_c']['deriv'] += np.dot(self.Z.T, dC_bar_int)\n",
    "        params_dict['B_c']['deriv'] += dC_bar_int.sum(axis=0)\n",
    "\n",
    "        # Gradiente de la puerta de entrada.\n",
    "        di = dC_out * self.C_bar\n",
    "        di_int = dsigmoid(self.i_int) * di\n",
    "        params_dict['W_i']['deriv'] += np.dot(self.Z.T, di_int)\n",
    "        params_dict['B_i']['deriv'] += di_int.sum(axis=0)\n",
    "\n",
    "        # Gradiente de la puerta de olvido.\n",
    "        df = dC_out * self.C_in\n",
    "        df_int = dsigmoid(self.f_int) * df\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, df_int)\n",
    "        params_dict['B_f']['deriv'] += df_int.sum(axis=0)\n",
    "\n",
    "        # Cálculo del gradiente con respecto a la entrada y al estado oculto.\n",
    "        dz = (np.dot(df_int, params_dict['W_f']['value'].T)\n",
    "             + np.dot(di_int, params_dict['W_i']['value'].T)\n",
    "             + np.dot(dC_bar_int, params_dict['W_c']['value'].T)\n",
    "             + np.dot(do_int, params_dict['W_o']['value'].T))\n",
    "    \n",
    "        dx_prev = dz[:, :self.X_in.shape[1]]\n",
    "        dH_prev = dz[:, self.X_in.shape[1]:]\n",
    "        dC_prev = self.f * dC_out\n",
    "\n",
    "        return dx_prev, dH_prev, dC_prev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41689b58-de75-4404-8ffb-182c14213823",
   "metadata": {},
   "source": [
    "**LSTMLayer**\n",
    "\n",
    "- Clase `LSTMLayer`: Representa una capa completa de nodos LSTM que procesa secuencias de datos.\n",
    "- Método `__init__`: Configura los parámetros iniciales y los estados ocultos y de memoria.\n",
    "- Método `_init_params`: Inicializa los parámetros de los pesos y sesgos de cada puerta (olvido, entrada, memoria, y salida) y los gradientes.\n",
    "- Método `_clear_gradients`: Reinicia los gradientes a cero para evitar acumulación no deseada.\n",
    "- Método `forward`: Ejecuta la secuencia de entrada a través de los nodos LSTM en orden, actualizando los estados ocultos y de memoria en cada paso de tiempo.\n",
    "    Método `backward`: Ejecuta el paso hacia atrás para propagar los gradientes en orden inverso a través de cada paso de tiempo, actualizando los gradientes de los parámetros y acumulando el gradiente de la entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5921eb-6c20-4562-aeff-911e4a6b114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = 0.01):\n",
    "        '''\n",
    "        Constructor de la clase LSTMLayer. \n",
    "        Inicializa los parámetros y variables de estado de la capa LSTM.\n",
    "\n",
    "        param hidden_size: int - número de neuronas ocultas en la capa LSTM.\n",
    "        param output_size: int - tamaño de la salida de la red.\n",
    "        param weight_scale: float - escala para inicializar los pesos.\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))  # Estado oculto inicial\n",
    "        self.start_C = np.zeros((1, hidden_size))  # Estado de memoria inicial        \n",
    "        self.first = True  # Bandera para inicializar parámetros la primera vez\n",
    "\n",
    "    def _init_params(self, input_: ndarray):\n",
    "        '''\n",
    "        Inicializa los parámetros de la capa LSTM.\n",
    "        \n",
    "        param input_: ndarray - entrada de la secuencia de datos con forma (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        # Determina el tamaño del vocabulario basado en la entrada\n",
    "        self.vocab_size = input_.shape[2]\n",
    "\n",
    "        # Diccionario para almacenar los parámetros de las puertas de la LSTM\n",
    "        self.params = {\n",
    "            'W_f': {}, 'B_f': {}, 'W_i': {}, 'B_i': {}, 'W_c': {}, 'B_c': {},\n",
    "            'W_o': {}, 'B_o': {}, 'W_v': {}, 'B_v': {}\n",
    "        }\n",
    "        \n",
    "        # Inicializa los pesos y sesgos para cada puerta\n",
    "        self.params['W_f']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                       (self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                       (1, self.hidden_size))\n",
    "        self.params['W_i']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                       (self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_i']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                      (1, self.hidden_size))\n",
    "        self.params['W_c']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                      (self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_c']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                      (1, self.hidden_size))\n",
    "        self.params['W_o']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                      (self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_o']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                      (1, self.hidden_size))       \n",
    "        self.params['W_v']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                      (self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                      (1, self.output_size))\n",
    "        \n",
    "        # Inicializa los gradientes de los parámetros\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
    "        \n",
    "        # Crea una lista de nodos LSTM para cada paso de tiempo en la secuencia\n",
    "        self.cells = [LSTMNode() for x in range(input_.shape[1])]\n",
    "\n",
    "    def _clear_gradients(self):\n",
    "        '''\n",
    "        Reinicia los gradientes a cero para evitar acumulaciones no deseadas entre épocas de entrenamiento.\n",
    "        '''\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "                    \n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        '''\n",
    "        Realiza el paso hacia adelante para la secuencia de entrada.\n",
    "\n",
    "        param x_seq_in: numpy array con forma (batch_size, sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array con forma (batch_size, sequence_length, output_size)\n",
    "        '''\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)  # Inicializa parámetros en la primera ejecución\n",
    "            self.first = False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        # Inicializa el estado oculto y de memoria para cada batch\n",
    "        H_in = np.repeat(self.start_H, batch_size, axis=0)\n",
    "        C_in = np.repeat(self.start_C, batch_size, axis=0)        \n",
    "\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        \n",
    "        # Inicializa la salida para cada paso de la secuencia\n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        # Itera sobre cada paso de tiempo en la secuencia\n",
    "        for t in range(sequence_length):\n",
    "            x_in = x_seq_in[:, t, :]  # Entrada en el paso de tiempo t\n",
    "            \n",
    "            # Propaga el paso hacia adelante en el nodo LSTM correspondiente\n",
    "            y_out, H_in, C_in = self.cells[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_seq_out[:, t, :] = y_out  # Guarda la salida en la secuencia de salida\n",
    "    \n",
    "        # Actualiza los estados iniciales para el próximo batch\n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
    "        self.start_C = C_in.mean(axis=0, keepdims=True)        \n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        '''\n",
    "        Realiza el paso hacia atrás (backpropagation) para la secuencia de salida.\n",
    "\n",
    "        param x_seq_out_grad: numpy array con forma (batch_size, sequence_length, output_size)\n",
    "        return x_seq_in_grad: numpy array con forma (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        \n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        # Inicializa los gradientes del estado oculto y del estado de memoria\n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        c_in_grad = np.zeros((batch_size, self.hidden_size))        \n",
    "        \n",
    "        num_chars = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        # Inicializa el gradiente de la entrada de la secuencia\n",
    "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
    "        \n",
    "        # Itera en reversa a través de cada paso de tiempo en la secuencia\n",
    "        for t in reversed(range(num_chars)):\n",
    "            x_out_grad = x_seq_out_grad[:, t, :]  # Gradiente de la salida en el paso t\n",
    "\n",
    "            # Propaga el paso hacia atrás en el nodo LSTM correspondiente\n",
    "            grad_out, h_in_grad, c_in_grad = \\\n",
    "                self.cells[t].backward(x_out_grad, h_in_grad, c_in_grad, self.params)\n",
    "        \n",
    "            x_seq_in_grad[:, t, :] = grad_out  # Guarda el gradiente de la entrada en el paso t\n",
    "        \n",
    "        return x_seq_in_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8b203-f1d9-4631-9ebb-0e2b172168e7",
   "metadata": {},
   "source": [
    "**LSTMModel**\n",
    "\n",
    "- Clase `LSTMModel`: Define el modelo de red LSTM que incluye múltiples capas y se encarga de entrenar el modelo y calcular la pérdida.\n",
    "- Método `__init__`: Inicializa el modelo, asigna las capas y la función de pérdida, y establece la longitud de secuencia en cada capa.\n",
    "- Método `forward`: Realiza el paso hacia adelante a través de todas las capas, generando la salida final del modelo.\n",
    "- Método `backward`: Realiza el paso hacia atrás para propagar los gradientes de error a través de todas las capas en orden inverso, actualizando gradientes de cada capa.\n",
    "- Método `single_step`: Realiza un solo paso de entrenamiento, incluyendo el paso hacia adelante, cálculo de la pérdida, propagación hacia atrás, y la actualización de parámetros en cada capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746b9b7-c3de-48b8-b99a-edc182aad880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(object):\n",
    "    '''\n",
    "    Clase del modelo que recibe las entradas y objetivos, entrena la red y calcula la pérdida.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[LSTMLayer],\n",
    "                 sequence_length: int, \n",
    "                 vocab_size: int, \n",
    "                 hidden_size: int,\n",
    "                 loss: Loss):\n",
    "        '''\n",
    "        Inicializa el modelo con las capas, el tamaño de la secuencia, el vocabulario y la función de pérdida.\n",
    "\n",
    "        param layers: List[LSTMLayer] - lista de capas LSTM que forman la red.\n",
    "        param sequence_length: int - longitud de la secuencia que se pasa a través de la red.\n",
    "        param vocab_size: int - número de caracteres en el vocabulario.\n",
    "        param hidden_size: int - número de neuronas ocultas en cada capa.\n",
    "        param loss: Loss - objeto que representa la función de pérdida.\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.loss = loss\n",
    "        \n",
    "        # Asigna la longitud de la secuencia a cada capa\n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "    def forward(self, x_batch: ndarray):\n",
    "        '''\n",
    "        Realiza el paso hacia adelante para todo el modelo, propagando la entrada a través de cada capa.\n",
    "\n",
    "        param x_batch: numpy array con forma (batch_size, sequence_length, vocab_size)\n",
    "        returns x_batch: numpy array con forma (batch_size, sequence_length, vocab_size), la salida de la red.\n",
    "        '''       \n",
    "        for layer in self.layers:\n",
    "            x_batch = layer.forward(x_batch)  # Pasa la salida de cada capa como entrada a la siguiente capa\n",
    "                \n",
    "        return x_batch\n",
    "        \n",
    "    def backward(self, loss_grad: ndarray):\n",
    "        '''\n",
    "        Realiza el paso hacia atrás para todo el modelo, propagando el gradiente de pérdida hacia atrás.\n",
    "\n",
    "        param loss_grad: numpy array con forma (batch_size, sequence_length, vocab_size)\n",
    "        returns loss_grad: numpy array, el gradiente de pérdida tras propagarse a través de las capas.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):  # Itera en orden inverso sobre las capas\n",
    "            loss_grad = layer.backward(loss_grad)  # Pasa el gradiente hacia atrás en cada capa\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, x_batch: ndarray, y_batch: ndarray):\n",
    "        '''\n",
    "        Ejecuta un solo paso de entrenamiento:\n",
    "        1. Paso hacia adelante y cálculo de softmax\n",
    "        2. Computa la pérdida y el gradiente de la pérdida\n",
    "        3. Paso hacia atrás (backpropagation)\n",
    "        4. Actualiza los parámetros de las capas\n",
    "\n",
    "        param x_batch: ndarray - entrada de la secuencia con forma (batch_size, sequence_length, vocab_size)\n",
    "        param y_batch: ndarray - objetivos de la secuencia con la misma forma que x_batch\n",
    "        return loss: float - pérdida calculada en este paso\n",
    "        '''  \n",
    "        # Paso hacia adelante\n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        \n",
    "        # Calcula la pérdida entre la salida y las etiquetas objetivo\n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        \n",
    "        # Calcula el gradiente de la pérdida\n",
    "        loss_grad = self.loss.backward()\n",
    "        \n",
    "        # Limpia los gradientes acumulados en cada capa\n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()\n",
    "        \n",
    "        # Paso hacia atrás para actualizar los gradientes en cada capa\n",
    "        self.backward(loss_grad)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429445da-e21e-40a6-8659-533551f397eb",
   "metadata": {},
   "source": [
    "### GRU\n",
    "\n",
    "**GRUNode**\n",
    "\n",
    "- Clase `GRUNode`: Representa un nodo de una capa GRU, encargado de calcular los estados internos de una red GRU para cada paso temporal en una secuencia de entrada.\n",
    "- Método `__init__`: Inicializa el nodo sin configurar ningún parámetro en el constructor.\n",
    "- Método `forward`: Realiza el paso hacia adelante (forward) del nodo GRU, calculando las puertas de reinicio y actualización, así como el nuevo estado oculto. La salida final del nodo depende de una combinación de la entrada y el estado oculto anterior.\n",
    "- Método `backward`: Realiza el paso hacia atrás (backpropagation) para actualizar los gradientes de los parámetros de la GRU, pasando los gradientes hacia atrás para ajustar los pesos de cada puerta y los estados de salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c42199-424d-4a95-988c-113d78a8bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNode(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Inicializa un nodo GRU sin definir parámetros en el constructor.\n",
    "        \n",
    "        param hidden_size: int - número de neuronas ocultas en la capa de GRU.\n",
    "        param vocab_size: int - tamaño del vocabulario para la predicción del próximo carácter.\n",
    "        '''\n",
    "        pass\n",
    "        \n",
    "    def forward(self, \n",
    "                X_in: ndarray, \n",
    "                H_in: ndarray,\n",
    "                params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        Realiza el paso hacia adelante del nodo GRU.\n",
    "\n",
    "        param X_in: numpy array con forma (batch_size, vocab_size), representa la entrada actual.\n",
    "        param H_in: numpy array con forma (batch_size, hidden_size), representa el estado oculto anterior.\n",
    "        param params_dict: Diccionario que contiene los pesos y sesgos.\n",
    "        \n",
    "        return self.X_out: numpy array con forma (batch_size, vocab_size), la salida de la red.\n",
    "        return self.H_out: numpy array con forma (batch_size, hidden_size), el nuevo estado oculto.\n",
    "        '''\n",
    "        self.X_in = X_in\n",
    "        self.H_in = H_in        \n",
    "        \n",
    "        # Cálculo de la puerta de reinicio (reset gate)\n",
    "        self.X_r = np.dot(X_in, params_dict['W_xr']['value'])\n",
    "        self.H_r = np.dot(H_in, params_dict['W_hr']['value'])\n",
    "\n",
    "        # Cálculo de la puerta de actualización (update gate)        \n",
    "        self.X_u = np.dot(X_in, params_dict['W_xu']['value'])\n",
    "        self.H_u = np.dot(H_in, params_dict['W_hu']['value'])        \n",
    "        \n",
    "        # Aplicación de las funciones de activación para las puertas\n",
    "        self.r_int = self.X_r + self.H_r + params_dict['B_r']['value']\n",
    "        self.r = sigmoid(self.r_int)\n",
    "        \n",
    "        self.u_int = self.X_r + self.H_r + params_dict['B_u']['value']\n",
    "        self.u = sigmoid(self.u_int)\n",
    "\n",
    "        # Cálculo del nuevo estado\n",
    "        self.h_reset = self.r * H_in\n",
    "        self.X_h = np.dot(X_in, params_dict['W_xh']['value'])\n",
    "        self.H_h = np.dot(self.h_reset, params_dict['W_hh']['value']) \n",
    "        self.h_bar_int = self.X_h + self.H_h + params_dict['B_h']['value']\n",
    "        self.h_bar = tanh(self.h_bar_int)        \n",
    "        \n",
    "        # Cálculo del estado oculto de salida\n",
    "        self.H_out = self.u * self.H_in + (1 - self.u) * self.h_bar\n",
    "\n",
    "        # Cálculo de la salida de la red\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out\n",
    "\n",
    "\n",
    "    def backward(self, \n",
    "                 X_out_grad: ndarray, \n",
    "                 H_out_grad: ndarray, \n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]):\n",
    "        '''\n",
    "        Realiza el paso hacia atrás (backpropagation) para el nodo GRU.\n",
    "\n",
    "        param X_out_grad: Gradiente de la pérdida respecto a la salida.\n",
    "        param H_out_grad: Gradiente de la pérdida respecto al estado oculto.\n",
    "        param params_dict: Diccionario con los pesos y sus derivadas para actualizar los parámetros.\n",
    "        \n",
    "        return dX_in: Gradiente con respecto a la entrada X_in.\n",
    "        return dH_in: Gradiente con respecto al estado oculto H_in.\n",
    "        '''\n",
    "        \n",
    "        # Gradiente de la capa de salida\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "\n",
    "        # Gradiente con respecto al estado oculto\n",
    "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)        \n",
    "        dh_out += H_out_grad\n",
    "                         \n",
    "        # Gradiente de la puerta de actualización\n",
    "        du = self.H_in * H_out_grad - self.h_bar * H_out_grad \n",
    "        dh_bar = (1 - self.u) * H_out_grad\n",
    "        \n",
    "        dh_bar_int = dh_bar * dtanh(self.h_bar_int)\n",
    "        params_dict['B_h']['deriv'] += dh_bar_int.sum(axis=0)\n",
    "        params_dict['W_xh']['deriv'] += np.dot(self.X_in.T, dh_bar_int)\n",
    "        \n",
    "        dX_in = np.dot(dh_bar_int, params_dict['W_xh']['value'].T)\n",
    " \n",
    "        params_dict['W_hh']['deriv'] += np.dot(self.h_reset.T, dh_bar_int)\n",
    "        dh_reset = np.dot(dh_bar_int, params_dict['W_hh']['value'].T)   \n",
    "        \n",
    "        # Gradiente de la puerta de reinicio\n",
    "        dr = dh_reset * self.H_in\n",
    "        dH_in = dh_reset * self.r        \n",
    "        \n",
    "        # Rama de actualización\n",
    "        du_int = dsigmoid(self.u_int) * du\n",
    "        params_dict['B_u']['deriv'] += du_int.sum(axis=0)\n",
    "\n",
    "        dX_in += np.dot(du_int, params_dict['W_xu']['value'].T)\n",
    "        params_dict['W_xu']['deriv'] += np.dot(self.X_in.T, du_int)\n",
    "        \n",
    "        dH_in += np.dot(du_int, params_dict['W_hu']['value'].T)\n",
    "        params_dict['W_hu']['deriv'] += np.dot(self.H_in.T, du_int)        \n",
    "\n",
    "        # Rama de reinicio\n",
    "        dr_int = dsigmoid(self.r_int) * dr\n",
    "        params_dict['B_r']['deriv'] += dr_int.sum(axis=0)\n",
    "\n",
    "        dX_in += np.dot(dr_int, params_dict['W_xr']['value'].T)\n",
    "        params_dict['W_xr']['deriv'] += np.dot(self.X_in.T, dr_int)\n",
    "        \n",
    "        dH_in += np.dot(dr_int, params_dict['W_hr']['value'].T)\n",
    "        params_dict['W_hr']['deriv'] += np.dot(self.H_in.T, dr_int)   \n",
    "        \n",
    "        return dX_in, dH_in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce662c8-10ca-48fe-a52b-9f9c8e8e3f31",
   "metadata": {},
   "source": [
    "**GRULayer**\n",
    "\n",
    "- Clase `GRULayer`: Define una capa GRU compuesta de múltiples nodos GRU, que se encargan de procesar una secuencia de datos en varias etapas de tiempo.\n",
    "- Método `__init__`: Configura los parámetros iniciales de la capa, incluyendo el tamaño de la salida, la escala de pesos, y el estado oculto inicial.\n",
    "- Método `_init_params`: Inicializa los pesos y sesgos de cada puerta en la capa GRU para la entrada y el estado oculto. Los pesos están asociados a las puertas de reinicio, actualización y salida de cada nodo.\n",
    "- Método `_clear_gradients`: Reinicia los gradientes de cada parámetro de la capa a cero para evitar acumulaciones.\n",
    "- Método `forward`: Realiza el paso hacia adelante en toda la capa GRU, procesando la secuencia de entrada. En cada paso de tiempo, actualiza el estado oculto y almacena la salida correspondiente.\n",
    "- Método `backward`: Realiza el paso hacia atrás para calcular los gradientes de la pérdida y actualiza los gradientes de los parámetros para cada nodo en la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553cd0bf-adce-462d-9950-dd8100fc2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = 0.01):\n",
    "        '''\n",
    "        Inicializa una capa de nodos GRU.\n",
    "\n",
    "        param hidden_size: int - número de neuronas ocultas en la capa GRU.\n",
    "        param output_size: int - tamaño de la salida de la capa GRU.\n",
    "        param weight_scale: float - escala para inicializar los pesos aleatoriamente.\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))  # Estado inicial oculto\n",
    "        self.first = True  # Indica si es la primera vez que se ejecuta el forward para inicializar parámetros\n",
    "\n",
    "        \n",
    "    def _init_params(self, input_: ndarray):\n",
    "        '''\n",
    "        Inicializa los parámetros de la capa GRU.\n",
    "\n",
    "        param input_: ndarray - entrada de la secuencia con forma (batch_size, sequence_length, vocab_size).\n",
    "        '''\n",
    "        \n",
    "        self.vocab_size = input_.shape[2]\n",
    "\n",
    "        # Diccionario que almacena los pesos y sesgos para cada puerta en la GRU\n",
    "        self.params = {\n",
    "            'W_xr': {}, 'W_hr': {}, 'B_r': {},\n",
    "            'W_xu': {}, 'W_hu': {}, 'B_u': {},\n",
    "            'W_xh': {}, 'W_hh': {}, 'B_h': {},\n",
    "            'W_v': {}, 'B_v': {}\n",
    "        }\n",
    "        \n",
    "        # Inicialización de los pesos y sesgos para las puertas de reinicio, actualización y salida\n",
    "        self.params['W_xr']['value'] = np.random.normal(0.0, self.weight_scale, (self.vocab_size, self.hidden_size))\n",
    "        self.params['W_hr']['value'] = np.random.normal(0.0, self.weight_scale, (self.hidden_size, self.hidden_size))\n",
    "        self.params['B_r']['value'] = np.random.normal(0.0, self.weight_scale, (1, self.hidden_size))\n",
    "        \n",
    "        self.params['W_xu']['value'] = np.random.normal(0.0, self.weight_scale, (self.vocab_size, self.hidden_size))\n",
    "        self.params['W_hu']['value'] = np.random.normal(0.0, self.weight_scale, (self.hidden_size, self.hidden_size))\n",
    "        self.params['B_u']['value'] = np.random.normal(0.0, self.weight_scale, (1, self.hidden_size))\n",
    "        \n",
    "        self.params['W_xh']['value'] = np.random.normal(0.0, self.weight_scale, (self.vocab_size, self.hidden_size))\n",
    "        self.params['W_hh']['value'] = np.random.normal(0.0, self.weight_scale, (self.hidden_size, self.hidden_size))\n",
    "        self.params['B_h']['value'] = np.random.normal(0.0, 1.0, (1, self.hidden_size))\n",
    "        \n",
    "        self.params['W_v']['value'] = np.random.normal(0.0, 1.0, (self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(0.0, 1.0, (1, self.output_size))    \n",
    "\n",
    "        # Inicialización de los gradientes para cada parámetro\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
    "        \n",
    "        # Crea una lista de nodos GRU, uno por cada paso de tiempo en la secuencia de entrada\n",
    "        self.cells = [GRUNode() for x in range(input_.shape[1])]\n",
    "\n",
    "\n",
    "    def _clear_gradients(self):\n",
    "        '''\n",
    "        Reinicia los gradientes a cero para evitar acumulación no deseada entre lotes (batches).\n",
    "        '''\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "                    \n",
    "        \n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        '''\n",
    "        Realiza el paso hacia adelante en la capa GRU para una secuencia de entrada.\n",
    "\n",
    "        param x_seq_in: numpy array con forma (batch_size, sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array con forma (batch_size, sequence_length, output_size)\n",
    "        '''\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)  # Inicializa los parámetros en la primera ejecución\n",
    "            self.first = False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        # Inicializa el estado oculto para cada lote\n",
    "        H_in = np.repeat(self.start_H, batch_size, axis=0)\n",
    "\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        \n",
    "        # Inicializa la salida para cada paso de la secuencia\n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        # Itera sobre cada paso de tiempo en la secuencia\n",
    "        for t in range(sequence_length):\n",
    "            x_in = x_seq_in[:, t, :]  # Entrada en el paso de tiempo t\n",
    "            \n",
    "            # Ejecuta el paso hacia adelante en el nodo GRU correspondiente\n",
    "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
    "      \n",
    "            x_seq_out[:, t, :] = y_out  # Guarda la salida en la secuencia de salida\n",
    "    \n",
    "        # Actualiza el estado inicial para el próximo lote\n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        '''\n",
    "        Realiza el paso hacia atrás (backpropagation) para la secuencia de salida de la capa GRU.\n",
    "\n",
    "        param x_seq_out_grad: numpy array con forma (batch_size, sequence_length, output_size), el gradiente de la pérdida.\n",
    "        return x_seq_in_grad: numpy array con forma (batch_size, sequence_length, vocab_size), el gradiente de la entrada.\n",
    "        '''\n",
    "        \n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        # Inicializa los gradientes del estado oculto\n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))        \n",
    "        \n",
    "        num_chars = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        # Inicializa el gradiente de la entrada de la secuencia\n",
    "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
    "        \n",
    "        # Itera en reversa a través de cada paso de tiempo en la secuencia\n",
    "        for t in reversed(range(num_chars)):\n",
    "            x_out_grad = x_seq_out_grad[:, t, :]  # Gradiente de la salida en el paso t\n",
    "\n",
    "            # Realiza el paso hacia atrás en el nodo GRU correspondiente\n",
    "            grad_out, h_in_grad = self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
    "        \n",
    "            x_seq_in_grad[:, t, :] = grad_out  # Guarda el gradiente de la entrada en el paso t\n",
    "        \n",
    "        return x_seq_in_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aafde0-e60c-46cb-9518-76e70613c974",
   "metadata": {},
   "source": [
    "### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649c6c1-8859-4d7a-96ee-492dbf861abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa única de LSTM\n",
    "\n",
    "capas1 = [LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
    "mod = RNNModel(layers=capas1,\n",
    "               vocab_size=62, sequence_length=25,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
    "trainer = RNNTrainer('input.txt', mod, optim, batch_size=3)\n",
    "trainer.train(1000, sample_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476de119-92b4-4ac9-b439-1c41352d0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modelos con multiples capas\n",
    "\n",
    "capas2 = [RNNLayer(hidden_size=256, output_size=128, weight_scale=0.1),\n",
    "           LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
    "mod = RNNModel(layers=capas2,\n",
    "               vocab_size=62, sequence_length=25,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
    "trainer = RNNTrainer('input.txt', mod, optim, batch_size=32)\n",
    "trainer.train(2000, sample_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec928f-bf1b-4d73-a6e2-7d7a5785bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "capas2 = [LSTMLayer(hidden_size=256, output_size=128, weight_scale=0.1),\n",
    "           LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
    "mod = RNNModel(layers=capas2,\n",
    "               vocab_size=62, sequence_length=25,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = SGD(lr=0.01, gradient_clipping=True)\n",
    "trainer = RNNTrainer('input.txt', mod, optim, batch_size=32)\n",
    "trainer.train(2000, sample_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f59aa-5d21-49b2-ace7-d76301014330",
   "metadata": {},
   "outputs": [],
   "source": [
    "capas3 = [GRULayer(hidden_size=256, output_size=128, weight_scale=0.1),\n",
    "           LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
    "mod = RNNModel(layers=capas3,\n",
    "               vocab_size=62, sequence_length=25,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
    "trainer = RNNTrainer('input.txt', mod, optim, batch_size=32)\n",
    "trainer.train(2000, sample_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33011b3-02da-4ea2-a96e-10b431069a5b",
   "metadata": {},
   "source": [
    "Los ejemplos anteriores configuran y entrenan un modelo de red neuronal recurrente que opera a nivel de carácter, utilizando diferentes combinaciones de capas RNN, LSTM y GRU. La salida generada indica que el modelo está generando texto basado en secuencias de caracteres, replicando en parte el estilo del texto de entrada de \"King Lear\" de Shakespeare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10658191-c442-429d-ab7d-0a344ce46c7e",
   "metadata": {},
   "source": [
    "#### **Ejercicio 1: Comprensión de la arquitectura del modelo**\n",
    "\n",
    "**Objetivo:** Entender la estructura del modelo configurado en el código y cómo se interrelacionan las diferentes capas.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Identificación de capas:**\n",
    "   - Observa las diferentes configuraciones de capas (`capas2`, `capas3`) en el código proporcionado.\n",
    "   - Describe la arquitectura de cada configuración, especificando el tipo de capas utilizadas y sus parámetros (`hidden_size`, `output_size`, `weight_scale`).\n",
    "\n",
    "2. **Flujo de datos:**\n",
    "   - Explica cómo fluye la información desde la entrada hasta la salida en cada una de las configuraciones.\n",
    "   - ¿Cómo interactúan las capas entre sí durante el paso hacia adelante?\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Diferencias entre `RNNLayer`, `LSTMLayer` y `GRULayer`.\n",
    "- Cómo se combinan múltiples capas para formar una red más profunda.\n",
    "- La importancia de `output_size` en la última capa para la generación de salidas.\n",
    "\n",
    "\n",
    "#### **Ejercicio 2: Nivel de análisis del modelo**\n",
    "\n",
    "**Objetivo:** Determinar si el modelo opera a nivel de **carácter** o de **palabra** y comprender las implicaciones de esta elección.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Análisis del tamaño del vocabulario:**\n",
    "   - Basándote en `vocab_size=62`, ¿a qué nivel está operando el modelo? Justifica tu respuesta.\n",
    "\n",
    "2. **Interpretación de la salida generada:**\n",
    "   - Observa la salida proporcionada:\n",
    "     ```\n",
    "     he lots I kyour bjt?n, uld ofpyo: lruysAfoV havehe,\n",
    "     ve and , Vminne, vay, ' -your Lralns Stry, Bear of's, I bvall.\n",
    "     I cew,ew, vadd\n",
    "     Le my Vruny:\n",
    "     I O spraiJ, bu larrtwes a ' hat! I be the !ue\n",
    "     co a parlde y el texto de input.txt tiene la forma de: That, poor contempt, or claim'd thou slept so faithful,\n",
    "     ...\n",
    "     ```\n",
    "     - ¿Confirma esta salida tu conclusión sobre el nivel de análisis? Explica por qué.\n",
    "\n",
    "3. **Ventajas y desventajas:**\n",
    "   - Discute las ventajas y desventajas de entrenar un modelo a nivel de carácter frente a a nivel de palabra.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo el tamaño del vocabulario influye en el nivel de análisis.\n",
    "- Coherencia semántica vs. creatividad en la generación de texto.\n",
    "- Complejidad computacional y requisitos de memoria.\n",
    "\n",
    "\n",
    "#### **Ejercicio 3: Modificación de la arquitectura**\n",
    "\n",
    "**Objetivo:** Modificar la arquitectura del modelo para experimentar con diferentes configuraciones y observar sus efectos.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Añadir una capa adicional:**\n",
    "   - Modifica la segunda configuración de `capas2` para incluir una tercera capa `GRULayer` con `hidden_size=128` y `output_size=256`.\n",
    "   - Actualiza la creación del modelo `RNNModel` para incluir esta nueva capa.\n",
    "\n",
    "2. **Cambiar el tamaño de la secuencia:**\n",
    "   - Ajusta `sequence_length` de 25 a 50 en el modelo modificado.\n",
    "   - Observa y describe cómo este cambio podría afectar el entrenamiento y la generación de texto.\n",
    "\n",
    "3. **Implementar una capa de dropout:**\n",
    "   - Introduce una capa de Dropout entre las capas GRU y LSTM para reducir el sobreajuste.\n",
    "   - Explica cómo y por qué el Dropout puede ayudar en este contexto.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Impacto de agregar más capas en la capacidad del modelo para aprender patrones complejos.\n",
    "- Cómo el tamaño de la secuencia afecta la captura de dependencias a largo plazo.\n",
    "- Beneficios de la regularización mediante Dropout.\n",
    "\n",
    "#### **Ejercicio 4: Implementación de una nueva función de activación**\n",
    "\n",
    "**Objetivo:** Experimentar con diferentes funciones de activación para potencialmente mejorar el rendimiento del modelo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Seleccionar una función de activación alternativa:**\n",
    "   - Elige una función de activación diferente a la sigmoide y tangente hiperbólica (por ejemplo, **ReLU** o **Leaky ReLU**).\n",
    "\n",
    "2. **Modificar el código de las capas:**\n",
    "   - Implementa la función de activación seleccionada en las puertas y candidatos de estado oculto de las clases `LSTMNode` y `GRUNode`.\n",
    "   - Asegúrate de actualizar también las derivadas correspondientes para el paso hacia atrás.\n",
    "\n",
    "3. **Entrenar el modelo modificado:**\n",
    "   - Entrena el modelo con la nueva función de activación utilizando el mismo conjunto de datos y parámetros de entrenamiento.\n",
    "   - Compara el rendimiento (pérdida y calidad del texto generado) con la configuración original.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo diferentes funciones de activación afectan la capacidad del modelo para aprender.\n",
    "- Posibles mejoras en la velocidad de convergencia y en la capacidad de manejar gradientes.\n",
    "\n",
    "\n",
    "#### **Ejercicio 5: Evaluación del impacto del optimizer**\n",
    "\n",
    "**Objetivo:** Analizar cómo diferentes algoritmos de optimización afectan el entrenamiento del modelo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Comparar AdaGrad y SGD:**\n",
    "   - Utiliza ambas configuraciones de capas (`capas2` con `AdaGrad` y `capas2` con `SGD`) para entrenar el modelo en el mismo conjunto de datos.\n",
    "\n",
    "2. **Registrar métricas de entrenamiento:**\n",
    "   - Durante el entrenamiento, registra la pérdida y cualquier otra métrica relevante para ambas configuraciones.\n",
    "\n",
    "3. **Analizar los resultados:**\n",
    "   - Compara la convergencia de la pérdida entre AdaGrad y SGD.\n",
    "   - Discute cuál optimizador muestra un mejor rendimiento y por qué podría ser así en este contexto.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Características de AdaGrad y SGD que afectan su rendimiento.\n",
    "- Cómo la tasa de aprendizaje y el clipping de gradientes interactúan con cada optimizador.\n",
    "- Escenarios en los que un optimizador puede ser preferible sobre otro.\n",
    "\n",
    "\n",
    "#### **Ejercicio 6: Implementación de regularización por dropout**\n",
    "\n",
    "**Objetivo:** Reducir el sobreajuste mediante la implementación de Dropout en la arquitectura del modelo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Añadir capas de dropout:**\n",
    "   - Inserta capas de Dropout con una tasa de 0.5 después de cada capa recurrente (RNN, LSTM, GRU) en la configuración `capas3`.\n",
    "\n",
    "2. **Modificar el flujo de datos:**\n",
    "   - Asegúrate de que las capas de Dropout se apliquen correctamente durante el paso hacia adelante y hacia atrás.\n",
    "\n",
    "3. **Entrenar y evaluar:**\n",
    "   - Entrena el modelo modificado en el mismo conjunto de datos.\n",
    "   - Compara la pérdida de entrenamiento y validación con la configuración sin Dropout.\n",
    "   - Evalúa si la inclusión de Dropout mejora la generalización del modelo.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo Dropout ayuda a prevenir el sobreajuste al introducir ruido durante el entrenamiento.\n",
    "- Impacto de Dropout en la velocidad de entrenamiento y en la capacidad del modelo para aprender patrones relevantes.\n",
    "\n",
    "\n",
    "#### **Ejercicio 7: Depuración de errores en la implementación del backward**\n",
    "\n",
    "**Objetivo:** Revisar y corregir posibles errores en la implementación del método `backward` de la clase `GRUNode`.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Revisar las derivadas:**\n",
    "   - Analiza el método `backward` de la clase `GRUNode` y verifica si las derivadas calculadas corresponden correctamente a las ecuaciones matemáticas de una GRU.\n",
    "\n",
    "2. **Identificar inconsistencias:**\n",
    "   - Busca posibles inconsistencias o errores en la propagación de gradientes, especialmente en las puertas de reinicio y actualización.\n",
    "\n",
    "3. **Corregir el código:**\n",
    "   - Realiza las modificaciones necesarias para asegurar que las derivadas se calculan correctamente.\n",
    "   - Asegúrate de que las actualizaciones de los parámetros reflejan las derivadas correctas.\n",
    "\n",
    "4. **Validar las correcciones:**\n",
    "   - Entrena el modelo después de las correcciones y verifica si la pérdida disminuye de manera consistente.\n",
    "   - Genera muestras de texto para evaluar si la calidad ha mejorado.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Importancia de las derivadas correctas para el entrenamiento efectivo del modelo.\n",
    "- Cómo los errores en la retropropagación pueden afectar la convergencia y la calidad de las predicciones.\n",
    "\n",
    "\n",
    "#### **Ejercicio 8: Experimentación con el tamaño del lote (Batch Size)**\n",
    "\n",
    "**Objetivo:** Evaluar cómo diferentes tamaños de lote afectan el entrenamiento y el rendimiento del modelo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Probar diferentes tamaños de lote:**\n",
    "   - Reentrena el modelo utilizando diferentes tamaños de lote, por ejemplo, 16, 32, 64 y 128.\n",
    "\n",
    "2. **Registrar el rendimiento:**\n",
    "   - Para cada tamaño de lote, registra la pérdida de entrenamiento y la calidad de las muestras generadas.\n",
    "\n",
    "3. **Analizar los resultados:**\n",
    "   - Compara cómo el tamaño del lote afecta la velocidad de convergencia y la estabilidad del entrenamiento.\n",
    "   - Discute las ventajas y desventajas de usar tamaños de lote más pequeños vs. más grandes.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Influencia del tamaño del lote en la estimación del gradiente.\n",
    "- Compromiso entre velocidad de entrenamiento y calidad de las actualizaciones de parámetros.\n",
    "- Impacto en el uso de memoria y en la capacidad de paralelización.\n",
    "\n",
    "\n",
    "\n",
    "#### **Ejercicio 9: Implementación de gradient clipping**\n",
    "\n",
    "**Objetivo:** Asegurar la estabilidad del entrenamiento mediante la implementación y ajuste del **gradient clipping**.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Entender gradient clipping:**\n",
    "   - Investiga qué es el gradient clipping y por qué es útil en el entrenamiento de redes recurrentes.\n",
    "\n",
    "2. **Implementar gradient clipping:**\n",
    "   - Si no está ya implementado, añade una función de gradient clipping en el optimizador `AdaGrad` y `SGD`.\n",
    "   - Define un umbral adecuado para el clipping, por ejemplo, 5.0.\n",
    "\n",
    "3. **Ajustar el umbral:**\n",
    "   - Experimenta con diferentes valores de umbral para el clipping y observa cómo afectan al entrenamiento.\n",
    "\n",
    "4. **Evaluar la efectividad:**\n",
    "   - Compara la estabilidad y la convergencia del entrenamiento con y sin gradient clipping.\n",
    "   - Observa si el modelo evita problemas como gradientes explosivos.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo gradient clipping ayuda a prevenir gradientes excesivamente grandes que pueden desestabilizar el entrenamiento.\n",
    "- Selección adecuada del umbral para balancear la preservación de información y la estabilidad.\n",
    "\n",
    "\n",
    "#### **Ejercicio 10: Generación de texto con el modelo entrenado**\n",
    "\n",
    "**Objetivo:** Utilizar el modelo entrenado para generar texto y evaluar su capacidad para imitar el estilo del texto de entrada.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Preparar una semilla:**\n",
    "   - Selecciona una secuencia de caracteres inicial (semilla) del texto de entrada para comenzar la generación.\n",
    "\n",
    "2. **Generar texto:**\n",
    "   - Utiliza el modelo entrenado para predecir el siguiente carácter de manera iterativa, extendiendo la secuencia generada.\n",
    "   - Genera al menos 500 caracteres de texto continuo.\n",
    "\n",
    "3. **Evaluar la calidad:**\n",
    "   - Analiza la coherencia, la creatividad y la fidelidad al estilo del texto de entrada.\n",
    "   - Identifica patrones repetitivos o incoherencias.\n",
    "\n",
    "4. **Experimentar con la temperatura:**\n",
    "   - Implementa una función de **temperatura** que ajuste la aleatoriedad en la selección del siguiente carácter.\n",
    "   - Genera texto con diferentes valores de temperatura (por ejemplo, 0.5, 1.0, 1.5) y compara los resultados.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo la semilla inicial influye en la generación de texto.\n",
    "- Impacto de la temperatura en la diversidad y creatividad del texto generado.\n",
    "- Evaluación subjetiva de la calidad del texto en términos de coherencia y estilo.\n",
    "\n",
    "\n",
    "#### **Ejercicio 11: Implementación de early stopping**\n",
    "\n",
    "**Objetivo:** Mejorar la eficiencia del entrenamiento y prevenir el sobreajuste mediante la implementación de **early stopping**.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Definir una métrica de monitoreo:**\n",
    "   - Selecciona una métrica para monitorear durante el entrenamiento, como la pérdida de validación.\n",
    "\n",
    "2. **Implementar early stopping:**\n",
    "   - Modifica la clase `RNNTrainer` para que detenga el entrenamiento si la métrica seleccionada no mejora después de un número determinado de iteraciones (por ejemplo, 10).\n",
    "\n",
    "3. **Dividir el conjunto de datos:**\n",
    "   - Separa una porción del `input.txt` como conjunto de validación.\n",
    "\n",
    "4. **Entrenar con early stopping:**\n",
    "   - Entrena el modelo utilizando early stopping y observa si el entrenamiento se detiene antes de alcanzar las 2000 iteraciones.\n",
    "\n",
    "5. **Comparar resultados:**\n",
    "   - Compara el rendimiento del modelo con y sin early stopping en términos de pérdida y calidad de generación de texto.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo early stopping ayuda a prevenir el sobreajuste al detener el entrenamiento cuando el rendimiento en datos de validación ya no mejora.\n",
    "- Selección adecuada del número de iteraciones de paciencia antes de detener el entrenamiento.\n",
    "- Impacto en el tiempo de entrenamiento y en la generalización del modelo.\n",
    "\n",
    "#### **Ejercicio 12: Adaptación del modelo para datos a nivel de palabra**\n",
    "\n",
    "**Objetivo:** Modificar el modelo para operar a nivel de palabra en lugar de a nivel de carácter.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Aumentar el tamaño del vocabulario:**\n",
    "   - Procesa el archivo `input.txt` para tokenizar el texto en palabras.\n",
    "   - Crea un mapeo de palabras a índices y ajusta `vocab_size` en el modelo para reflejar el número total de palabras únicas.\n",
    "\n",
    "2. **Modificar la entrada y salida:**\n",
    "   - Cambia la representación de la entrada y salida del modelo para que cada unidad corresponda a una palabra en lugar de a un carácter.\n",
    "   - Asegúrate de que las capas de salida tengan un tamaño igual al nuevo `vocab_size`.\n",
    "\n",
    "3. **Actualizar el modelo:**\n",
    "   - Ajusta las capas del modelo para manejar el nuevo tamaño del vocabulario.\n",
    "   - Considera reducir el `hidden_size` si el modelo se vuelve demasiado grande.\n",
    "\n",
    "4. **Entrenar el modelo:**\n",
    "   - Entrena el modelo adaptado en el nuevo conjunto de datos a nivel de palabra.\n",
    "   - Genera muestras de texto y evalúa la coherencia y la calidad en comparación con el modelo a nivel de carácter.\n",
    "\n",
    "5. **Comparar rendimiento:**\n",
    "   - Discute las diferencias en la generación de texto y el rendimiento del modelo al operar a nivel de palabra frente a carácter.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo la tokenización afecta la representación de los datos.\n",
    "- Impacto del aumento del tamaño del vocabulario en la complejidad computacional y en la capacidad del modelo.\n",
    "- Ventajas de trabajar a nivel de palabra en términos de coherencia semántica.\n",
    "\n",
    "\n",
    "#### **Ejercicio 13: Visualización de las activaciones de las capas**\n",
    "\n",
    "**Objetivo:** Comprender cómo las diferentes capas del modelo responden a ciertas entradas mediante la visualización de sus activaciones.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Seleccionar una entrada:**\n",
    "   - Elige una secuencia de caracteres específica del conjunto de datos de entrenamiento.\n",
    "\n",
    "2. **Capturar activaciones:**\n",
    "   - Modifica las clases de las capas (`RNNLayer`, `LSTMLayer`, `GRULayer`) para almacenar las activaciones (salidas intermedias) durante el paso hacia adelante.\n",
    "\n",
    "3. **Visualizar las activaciones:**\n",
    "   - Utiliza herramientas de visualización como **matplotlib** para crear gráficos de las activaciones de las diferentes capas.\n",
    "   - Observa cómo las activaciones varían a lo largo de la secuencia de entrada.\n",
    "\n",
    "4. **Interpretar los resultados:**\n",
    "   - Analiza qué patrones o comportamientos se pueden observar en las activaciones.\n",
    "   - Discute cómo las diferentes capas contribuyen al procesamiento de la información.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo las activaciones de las capas recurrentes reflejan la memoria y la atención del modelo.\n",
    "- Identificación de patrones recurrentes o estados de activación que corresponden a ciertos caracteres o contextos.\n",
    "\n",
    "\n",
    "#### **Ejercicio 14: Implementación de batch normalization**\n",
    "\n",
    "**Objetivo:** Mejorar la estabilidad y velocidad de entrenamiento mediante la implementación de **Batch Normalization** en el modelo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Investigar batch normalization:**\n",
    "   - Comprende qué es Batch Normalization y cómo puede aplicarse en redes recurrentes.\n",
    "\n",
    "2. **Añadir batch normalization:**\n",
    "   - Introduce capas de Batch Normalization después de cada capa recurrente (RNN, LSTM, GRU) en la arquitectura del modelo.\n",
    "   - Implementa las funciones necesarias para normalizar las activaciones durante el paso hacia adelante y ajustar los parámetros durante el entrenamiento.\n",
    "\n",
    "3. **Entrenar el modelo con batch normalization:**\n",
    "   - Entrena el modelo modificado y observa los efectos en la convergencia y en la calidad de las muestras generadas.\n",
    "\n",
    "4. **Comparar con la configuración original:**\n",
    "   - Compara el rendimiento del modelo con Batch Normalization frente al modelo sin ella.\n",
    "   - Discute los beneficios y posibles inconvenientes observados.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo Batch Normalization ayuda a estabilizar las distribuciones de activación y acelera el entrenamiento.\n",
    "- Consideraciones especiales al aplicar Batch Normalization en arquitecturas recurrentes.\n",
    "\n",
    "\n",
    "#### **Ejercicio 15: Guardar y cargar el modelo entrenado**\n",
    "\n",
    "**Objetivo:** Implementar mecanismos para guardar el estado del modelo durante el entrenamiento y cargarlo posteriormente para la generación de texto.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Implementar funciones de guardado:**\n",
    "   - Añade métodos en la clase `RNNModel` para guardar los pesos y sesgos de todas las capas en un archivo (por ejemplo, en formato JSON o pickle).\n",
    "\n",
    "2. **Implementar funciones de carga:**\n",
    "   - Añade métodos para cargar los pesos y sesgos desde un archivo guardado y restaurar el estado del modelo.\n",
    "\n",
    "3. **Entrenar y guardar el modelo:**\n",
    "   - Durante el entrenamiento, guarda el estado del modelo cada cierto número de iteraciones (por ejemplo, cada 500 iteraciones).\n",
    "\n",
    "4. **Cargar el modelo y generar texto:**\n",
    "   - Detén el entrenamiento y carga el modelo desde un archivo guardado.\n",
    "   - Genera texto utilizando el modelo cargado y verifica que las salidas sean consistentes con el estado guardado.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Importancia de guardar el estado del modelo para reproducibilidad y para evitar entrenamientos repetidos.\n",
    "- Manejo adecuado de formatos de archivo para preservar la integridad de los pesos y sesgos.\n",
    "- Verificación de la consistencia entre el modelo antes y después de la carga.\n",
    "\n",
    "\n",
    "#### **Ejercicio 16: Optimización del rendimiento del modelo**\n",
    "\n",
    "**Objetivo:** Mejorar el rendimiento del modelo mediante la optimización de hiperparámetros y la arquitectura.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Ajustar hiperparámetros:**\n",
    "   - Experimenta con diferentes valores de `hidden_size`, `output_size`, y `weight_scale`.\n",
    "   - Observa cómo estos cambios afectan la capacidad del modelo para aprender y generar texto.\n",
    "\n",
    "2. **Implementar técnicas de regularización:**\n",
    "   - Además de Dropout, prueba otras técnicas como **L2 regularization** o **early stopping** (si no lo has hecho en ejercicios anteriores).\n",
    "\n",
    "3. **Reducir el tiempo de entrenamiento:**\n",
    "   - Optimiza el código para mejorar la eficiencia computacional, por ejemplo, utilizando operaciones vectorizadas de NumPy de manera más efectiva.\n",
    "   - Considera implementar técnicas como **mini-batch training** si aún no están implementadas.\n",
    "\n",
    "4. **Evaluar mejoras:**\n",
    "   - Compara el rendimiento del modelo antes y después de las optimizaciones realizadas.\n",
    "   - Discute qué cambios fueron más efectivos y por qué.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Balance entre la complejidad del modelo y la capacidad de generalización.\n",
    "- Impacto de la regularización en la prevención del sobreajuste.\n",
    "- Estrategias para acelerar el entrenamiento sin sacrificar la calidad del modelo.\n",
    "\n",
    "#### **Ejercicio 17: Implementación de una función de activación personalizada**\n",
    "\n",
    "**Objetivo:** Crear e integrar una función de activación personalizada en el modelo para explorar su impacto en el aprendizaje.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Diseñar una función de activación:**\n",
    "   - Crea una nueva función de activación, por ejemplo, una variante de ReLU como **Leaky ReLU**:\n",
    "     $$\n",
    "     f(x) = \n",
    "     \\begin{cases} \n",
    "     x & \\text{si } x > 0 \\\\\n",
    "     \\alpha x & \\text{si } x \\leq 0 \n",
    "     \\end{cases}\n",
    "     $$\n",
    "     donde $\\alpha$ es un pequeño valor, como 0.01.\n",
    "\n",
    "2. **Implementar la función y su derivada:**\n",
    "   - Escribe las funciones en Python para la activación y su derivada.\n",
    "\n",
    "3. **Integrar la función en las capas recurrentes:**\n",
    "   - Modifica las clases `LSTMNode` y `GRUNode` para utilizar la nueva función de activación en lugar de las funciones estándar.\n",
    "\n",
    "4. **Entrenar el modelo con la activación personalizada:**\n",
    "   - Entrena el modelo y evalúa cómo afecta la nueva función de activación al rendimiento y a la calidad del texto generado.\n",
    "\n",
    "5. **Comparar con la función de activación original:**\n",
    "   - Discute las diferencias observadas en el comportamiento del modelo con y sin la función de activación personalizada.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo las diferentes funciones de activación afectan la capacidad del modelo para aprender y manejar el flujo de gradientes.\n",
    "- Beneficios de funciones de activación que permiten pequeñas pendientes en regiones negativas para prevenir gradientes muertos.\n",
    "\n",
    "#### **Ejercicio 18: Evaluación del modelo con datos de prueba**\n",
    "\n",
    "**Objetivo:** Evaluar el desempeño del modelo utilizando un conjunto de datos de prueba independiente.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Dividir el conjunto de datos:**\n",
    "   - Separa una porción del archivo `input.txt` como conjunto de prueba, asegurándote de que no se use durante el entrenamiento.\n",
    "\n",
    "2. **Modificar la clase `RNNTrainer`:**\n",
    "   - Añade funcionalidad para evaluar el modelo en el conjunto de prueba después de cada cierto número de iteraciones de entrenamiento.\n",
    "\n",
    "3. **Calcular métricas de evaluación:**\n",
    "   - Implementa métricas adicionales como **Perplexity** para evaluar la calidad del modelo en datos no vistos.\n",
    "\n",
    "4. **Analizar el rendimiento:**\n",
    "   - Compara las métricas obtenidas en el conjunto de entrenamiento y en el de prueba.\n",
    "   - Discute posibles indicaciones de sobreajuste o subajuste.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Importancia de un conjunto de prueba para evaluar la capacidad de generalización del modelo.\n",
    "- Interpretación de métricas como Perplexity en el contexto de modelos de lenguaje.\n",
    "\n",
    "\n",
    "#### **Ejercicio 19: Implementación de un pipeline de preprocesamiento mejorado**\n",
    "\n",
    "**Objetivo:** Mejorar el preprocesamiento de los datos para optimizar el entrenamiento del modelo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Normalización de datos:**\n",
    "   - Implementa técnicas de normalización para las entradas, como **one-hot encoding** para los caracteres o palabras.\n",
    "\n",
    "2. **Gestión de datos desbalanceados:**\n",
    "   - Si ciertas clases (caracteres o palabras) son significativamente más frecuentes, implementa técnicas para balancear el conjunto de datos, como **undersampling** o **oversampling**.\n",
    "\n",
    "3. **Implementar embeddings:**\n",
    "   - Añade una capa de embeddings que transforma las representaciones one-hot en vectores densos de menor dimensión.\n",
    "   - Ajusta la arquitectura del modelo para integrar esta capa.\n",
    "\n",
    "4. **Entrenar el modelo con el pipeline mejorado:**\n",
    "   - Observa cómo estas mejoras afectan la capacidad del modelo para aprender patrones y generar texto coherente.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo la representación de los datos afecta la eficiencia y efectividad del aprendizaje del modelo.\n",
    "- Beneficios de los embeddings en la captura de relaciones semánticas entre palabras o caracteres.\n",
    "\n",
    "\n",
    "#### **Ejercicio 20: Comparación entre RNN, LSTM y GRU**\n",
    "\n",
    "**Objetivo:** Evaluar y comparar el rendimiento de diferentes arquitecturas recurrentes en la tarea de generación de texto.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Configurar modelos separados:**\n",
    "   - Define tres configuraciones de modelos:\n",
    "     - **Modelo A:** Solo con capas `RNNLayer`.\n",
    "     - **Modelo B:** Solo con capas `LSTMLayer`.\n",
    "     - **Modelo C:** Solo con capas `GRULayer`.\n",
    "\n",
    "2. **Entrenar cada modelo:**\n",
    "   - Entrena cada modelo utilizando el mismo conjunto de datos, optimizador y parámetros de entrenamiento.\n",
    "\n",
    "3. **Generar y comparar salidas:**\n",
    "   - Genera muestras de texto con cada modelo.\n",
    "   - Compara la coherencia, creatividad y fidelidad al estilo del texto de entrada.\n",
    "\n",
    "4. **Analizar métricas de rendimiento:**\n",
    "   - Evalúa y compara métricas como la pérdida de entrenamiento, velocidad de convergencia y estabilidad durante el entrenamiento.\n",
    "\n",
    "5. **Discusión:**\n",
    "   - Discute cuál de las arquitecturas mostró un mejor rendimiento y por qué podría ser así.\n",
    "   - Considera aspectos como la capacidad para capturar dependencias a largo plazo y la eficiencia computacional.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Fortalezas y debilidades de RNN, LSTM y GRU en tareas de modelado secuencial.\n",
    "- Cómo las diferencias en las puertas y el manejo de la memoria afectan el aprendizaje y la generación de texto.\n",
    "\n",
    "\n",
    "#### **Ejercicio 21: Implementación de técnicas de data augmentation**\n",
    "\n",
    "**Objetivo:** Aumentar la diversidad del conjunto de datos de entrenamiento para mejorar la generalización del modelo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Definir Técnicas de data augmentation:**\n",
    "   - Implementa técnicas como **sinónimos de sustitución** (para modelos a nivel de palabra) o **inserción de ruido** (para modelos a nivel de carácter).\n",
    "\n",
    "2. **Aplicar data Augmentation al conjunto de datos:**\n",
    "   - Modifica el archivo `input.txt` aplicando las técnicas definidas para crear un conjunto de datos ampliado.\n",
    "\n",
    "3. **Entrenar el modelo con datos aumentados:**\n",
    "   - Reentrena el modelo utilizando el nuevo conjunto de datos aumentado.\n",
    "   - Observa si hay mejoras en la capacidad del modelo para generar texto variado y coherente.\n",
    "\n",
    "4. **Evaluar el impacto:**\n",
    "   - Compara el rendimiento del modelo con y sin data augmentation en términos de pérdida y calidad de la generación de texto.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo el data augmentation puede ayudar a prevenir el sobreajuste y mejorar la robustez del modelo.\n",
    "- Selección adecuada de técnicas de augmentation que mantengan la coherencia del texto.\n",
    "\n",
    "\n",
    "#### **Ejercicio 22: Implementación de un mecanismo de atención**\n",
    "\n",
    "**Objetivo:** Añadir un mecanismo de atención al modelo para mejorar la capacidad de enfocarse en partes relevantes de la secuencia de entrada.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Investigar mecanismos de atención:**\n",
    "   - Comprende los conceptos básicos de los mecanismos de atención en redes neuronales recurrentes.\n",
    "\n",
    "2. **Diseñar e implementar la atención:**\n",
    "   - Añade una capa de atención después de las capas recurrentes (`RNNLayer`, `LSTMLayer`, `GRULayer`).\n",
    "   - Implementa las operaciones de cálculo de pesos de atención y combinación de contextos.\n",
    "\n",
    "3. **Modificar el flujo de datos:**\n",
    "   - Asegúrate de que las salidas de las capas recurrentes se combinan adecuadamente con la atención para producir la salida final.\n",
    "\n",
    "4. **Entrenar el modelo con atención:**\n",
    "   - Entrena el modelo y evalúa si la atención mejora la calidad de la generación de texto.\n",
    "\n",
    "5. **Visualizar los pesos de atención:**\n",
    "   - Implementa visualizaciones de los pesos de atención para entender en qué partes de la secuencia el modelo está enfocando su atención durante la generación.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo la atención permite que el modelo se enfoque dinámicamente en diferentes partes de la secuencia de entrada.\n",
    "- Beneficios de la atención en la captura de dependencias a largo plazo y en la generación de salidas más coherentes.\n",
    "\n",
    "\n",
    "### **Ejercicio 23: Implementación de bidirectional RNN**\n",
    "\n",
    "**Objetivo:** Mejorar la capacidad del modelo para capturar información contextual desde ambas direcciones de la secuencia.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Comprender bidirectional RNN:**\n",
    "   - Investiga cómo funcionan las RNN bidireccionales y sus ventajas.\n",
    "\n",
    "2. **Modificar la arquitectura del modelo:**\n",
    "   - Cambia las capas recurrentes (`RNNLayer`, `LSTMLayer`, `GRULayer`) para que sean bidireccionales.\n",
    "   - Esto implica crear dos instancias de cada capa: una que procesa la secuencia en orden normal y otra en orden inverso.\n",
    "\n",
    "3. **Concatenar las salidas:**\n",
    "   - Combina las salidas de las direcciones forward y backward antes de pasar a la siguiente capa o a la capa de salida.\n",
    "\n",
    "4. **Entrenar el modelo bidireccional:**\n",
    "   - Entrena el modelo modificado y compara su rendimiento con el modelo unidireccional.\n",
    "\n",
    "5. **Evaluar mejoras:**\n",
    "   - Analiza si la capacidad de capturar contexto bidireccional mejora la coherencia y relevancia del texto generado.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo las RNN bidireccionales pueden capturar mejor el contexto global de la secuencia.\n",
    "- Impacto en la complejidad computacional y en el tiempo de entrenamiento.\n",
    "\n",
    "\n",
    "#### **Ejercicio 24: Implementación de early fusion de características**\n",
    "\n",
    "**Objetivo:** Integrar características adicionales al modelo para enriquecer la representación de la entrada.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Definir características adicionales:**\n",
    "   - Selecciona características que puedan enriquecer la representación de los caracteres o palabras, como **posiciones en la secuencia**, **características gramaticales** (para modelos a nivel de palabra), etc.\n",
    "\n",
    "2. **Modificar el preprocesamiento de datos:**\n",
    "   - Extrae y agrega estas características adicionales a la representación de la entrada.\n",
    "\n",
    "3. **Actualizar las capas de entrada:**\n",
    "   - Ajusta las capas recurrentes para aceptar las características adicionales, aumentando el tamaño de la entrada si es necesario.\n",
    "\n",
    "4. **Entrenar y evaluar el modelo:**\n",
    "   - Entrena el modelo con las nuevas características y evalúa si mejoran la capacidad del modelo para generar texto coherente y estilísticamente consistente.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo las características adicionales pueden aportar información relevante que no está presente en la secuencia de caracteres o palabras por sí sola.\n",
    "- Manejo adecuado de la dimensionalidad aumentada en las capas de entrada.\n",
    "\n",
    "\n",
    "#### **Ejercicio 25: Implementación de regularización por DropConnect**\n",
    "\n",
    "**Objetivo:** Introducir una variante de regularización conocida como **DropConnect** en lugar de Dropout para explorar sus efectos en el modelo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Entender DropConnect:**\n",
    "   - Investiga qué es DropConnect y cómo difiere de Dropout.\n",
    "\n",
    "2. **Implementar DropConnect:**\n",
    "   - Añade DropConnect a las capas de conexión (pesos) de las capas recurrentes (`RNNLayer`, `LSTMLayer`, `GRULayer`).\n",
    "   - Define una tasa de conexión que determine la probabilidad de mantener una conexión activa.\n",
    "\n",
    "3. **Integrar en el flujo de datos:**\n",
    "   - Asegúrate de que DropConnect se aplica durante el entrenamiento pero no durante la inferencia.\n",
    "\n",
    "4. **Entrenar el modelo con DropConnect:**\n",
    "   - Entrena el modelo y observa cómo afecta la regularización al rendimiento y a la generación de texto.\n",
    "\n",
    "5. **Comparar con dropout:**\n",
    "   - Compara el impacto de DropConnect frente a Dropout en términos de pérdida de entrenamiento y calidad del texto generado.\n",
    "\n",
    "**Puntos clave a considerar:**\n",
    "\n",
    "- Cómo DropConnect puede proporcionar una regularización más robusta al aplicar ruido directamente a las conexiones de pesos.\n",
    "- Impacto en la capacidad del modelo para generalizar y en la estabilidad del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5517c58-476e-44d1-a53d-1727ecf6c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
