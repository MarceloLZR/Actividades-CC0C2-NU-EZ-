{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872cbcce",
   "metadata": {},
   "source": [
    "## Redes neuronales recurrentes y otros modelos de secuencia\n",
    "\n",
    "Se crea una instancia de una red neuronal recurrente (RNN). Aquí, torch.nn.RNN es la clase que implementa una RNN básica en PyTorch. Los argumentos 300 y 512 especifican los tamaños de las características de entrada y de las características ocultas, respectivamente.\n",
    "\n",
    "- 300: Este es el tamaño de la característica de cada elemento de entrada en la secuencia. Por ejemplo, si estás procesando datos de texto, esto podría ser el tamaño del vector de características (como embeddings de palabras) para cada palabra.\n",
    "- 512: Este es el tamaño de la característica de los estados ocultos en la RNN. Esto afecta la capacidad del modelo para aprender dependencias complejas en los datos. Un número más alto puede permitir que el modelo capture dependencias más complejas, pero también puede llevar a un mayor riesgo de sobreajuste y requerir más memoria y tiempo de cómputo.\n",
    "\n",
    "Esta línea de código prepara un modelo RNN que puedes entrenar con datos adecuados para realizar tareas como la predicción de series temporales, el modelado de lenguaje, y otras tareas donde los datos de entrada tienen una naturaleza secuencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ce5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.nn.RNN(300, 512)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47aa3d",
   "metadata": {},
   "source": [
    "#### RNN usando Pytorch\n",
    "\n",
    "El código siguiente define y utiliza una celda de red neuronal recurrente (RNN) personalizada usando PyTorch y la biblioteca fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e139d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "# fastai.__version__\n",
    "     \n",
    "from fastai.text.all import *\n",
    "     \n",
    "\n",
    "class RNNCell(nn.Module):    \n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ih = nn.Linear(input_size, hidden_size)\n",
    "        self.hh = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h = None):\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), self.hidden_size)\n",
    "        h = torch.tanh(self.ih(x) + self.hh(h))\n",
    "        return h\n",
    "     \n",
    "\n",
    "#oculto\n",
    "cell = RNNCell(100, 300)\n",
    "cell(torch.randn(1, 100)).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee9594",
   "metadata": {},
   "source": [
    "En el código anterior, la clase RNNCell, define una celda RNN personalizada. La celda tiene dos partes principales:\n",
    "\n",
    "- Inicialización (__init__) que define dos capas lineales, una para transformar la entrada (ih) y otra para transformar el estado oculto anterior (hh).\n",
    "- Paso hacia adelante (forward) que calcula el nuevo estado oculto combinando la entrada actual x y el estado oculto anterior h (si h no se proporciona, se inicializa como un tensor de ceros). La suma de las transformaciones lineales de x y h se pasa a través de una función de activación tanh para obtener el nuevo estado oculto.\n",
    "\n",
    "Luego se crea una instancia de RNNCell con un tamaño de entrada de 100 y un tamaño de estado oculto de 300. Luego, pasa un tensor aleatorio (simulando una entrada única de tamaño 100) a través de la celda RNN para obtener el estado oculto, e imprime la forma del tensor resultante, que sería [1, 300], correspondiente a un lote de tamaño 1 con un vector de estado oculto de tamaño 300."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f48517",
   "metadata": {},
   "source": [
    "Ahora definimos una clase RNN que implementa una red neuronal recurrente completa usando la clase RNNCell personalizada anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd55b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):    \n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.cell = RNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h = None):\n",
    "        \n",
    "        print(x.shape)\n",
    "        for i in range(x.shape[1]):\n",
    "            h = self.cell(x[:,i], h)\n",
    "            \n",
    "        return h\n",
    "     \n",
    "\n",
    "#oculto\n",
    "rnn = RNN(100, 300)\n",
    "rnn(torch.randn(256, 10, 100)).shape\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6ca6c",
   "metadata": {},
   "source": [
    "Se define la clase RNN como una subclase de nn.Module, que es la clase base para todos los módulos de red neuronal en PyTorch. En el constructor (__init__), inicializa una instancia de RNNCell con los tamaños de entrada y ocultos especificados, almacenándola como self.cell.\n",
    "\n",
    "El método forward define cómo los datos pasan a través de la red:\n",
    "\n",
    "- x.shape imprime la forma de la entrada x. Suponiendo que x tiene forma (256, 10, 100), esto indica que estamos procesando 256 secuencias, cada una de longitud 10, donde cada elemento de la secuencia tiene 100 características.\n",
    "\n",
    "- El bucle `for i in range(x.shape[1])` itera sobre la segunda dimensión de x, que es la longitud de la secuencia (10 en este caso). En cada iteración, extrae el i-ésimo elemento de cada secuencia (usando x[:, i]), y lo pasa a self.cell junto con el estado oculto actual h.\n",
    "\n",
    "- h se actualiza en cada paso de tiempo al procesar el elemento correspondiente de la secuencia a través de la celda RNN. Si h es None inicialmente, será inicializado a un tensor de ceros dentro de RNNCell.\n",
    "\n",
    "\n",
    "Luego se crea una instancia de RNN con un tamaño de entrada de 100 y un tamaño de estado oculto de 300. Luego, se pasa un tensor de entrada aleatorio con forma `(256, 10, 100)` a través de la RNN. Este tensor representa 256 secuencias, cada una de longitud 10, con cada elemento de la secuencia teniendo 100 características.\n",
    "\n",
    "El resultado de `rnn(torch.randn(256, 10, 100))`.shape sería la forma del último estado oculto producido, que es [256, 300]. Esto indica que hay un estado oculto de tamaño 300 para cada una de las 256 secuencias después de procesar todos los elementos de cada secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578742a",
   "metadata": {},
   "source": [
    "### Ejemplo de clasificación de texto\n",
    "\n",
    "Definimos una clase TextClassifier que es un modelo de red neuronal para clasificación de texto, utilizando módulos comunes de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739e7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = RNN(hidden_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        x = self.rnn(x)\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n",
    "     \n",
    "\n",
    "path = untar_data(URLs.IMDB)\n",
    "dls = TextDataLoaders.from_folder(path, valid='test', bs=256)\n",
    "     \n",
    "\n",
    "dls.show_batch(max_n=5)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64adc2e4",
   "metadata": {},
   "source": [
    "Explicación del código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = RNN(hidden_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c778c0",
   "metadata": {},
   "source": [
    "- nn.Embedding(vocab_size, hidden_size): Este módulo es una tabla de búsqueda que almacena embeddings de un vocabulario fijo de tamaño vocab_size, cada uno con una dimensión hidden_size. Transforma índices de palabras en vectores densos.\n",
    "\n",
    "- RNN(hidden_size, hidden_size): Se utiliza una RNN, definida previamente, que toma la salida del embedding como entrada. La RNN puede capturar dependencias secuenciales en el texto.\n",
    "\n",
    "- nn.Linear(hidden_size, 10) y nn.Linear(10, 1): Estas son capas lineales que transforman la salida de la RNN a dimensiones más pequeñas, eventualmente produciendo una salida única. Esto es típico para tareas de clasificación binaria donde la salida puede representar la probabilidad de una clase (a menudo después de aplicar una función sigmoide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba08aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = self.emb(x)        # Aplica embedding al texto de entrada\n",
    "    x = self.rnn(x)        # Procesa el texto a través de la RNN\n",
    "    x = self.fc1(x)        # Primera capa lineal\n",
    "    out = self.fc2(x)      # Segunda capa lineal que produce la salida final\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e00095",
   "metadata": {},
   "source": [
    "El método forward define cómo los datos pasan a través del modelo. Los datos de entrada (x) son índices de palabras que primero se convierten en embeddings, luego se procesan a través de una RNN, y finalmente se transforman a través de dos capas lineales para producir la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "dls = TextDataLoaders.from_folder(path, valid='test', bs=256)\n",
    "dls.show_batch(max_n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0979a21",
   "metadata": {},
   "source": [
    "- untar_data(URLs.IMDB): Descarga y descomprime el conjunto de datos IMDB para análisis de sentimientos, que contiene críticas de películas etiquetadas como positivas o negativas.\n",
    "- TextDataLoaders.from_folder(path, valid='test', bs=256): Crea cargadores de datos para el entrenamiento y validación. Los datos se dividen en un conjunto de entrenamiento y un conjunto de validación, con un tamaño de lote de 256.\n",
    "- dls.show_batch(max_n=5): Muestra un lote de datos de ejemplo, que puede ser útil para verificar cómo se están cargando y procesando los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea368f",
   "metadata": {},
   "source": [
    "**Ejercicio 1:** Modifica la arquitectura del TextClassifier para incluir una capa adicional de LSTM en lugar de RNN simple. Investiga y compara los resultados en términos de precisión y pérdida durante el entrenamiento.\n",
    "\n",
    "Pasos:\n",
    "- Define una nueva clase LSTMCell o modifica la clase RNN para utilizar nn.LSTM en lugar de nn.RNN.\n",
    "- Integra esta nueva celda LSTM en el TextClassifier.\n",
    "- Entrena el modelo modificado en el conjunto de datos IMDB y compara los resultados con la versión original.\n",
    "\n",
    "**Ejercicio 2:** Evalúa cómo el tamaño del vocabulario afecta la precisión de la clasificación.\n",
    "\n",
    "Pasos:\n",
    "- Experimenta con diferentes tamaños de vocabulario (por ejemplo, 5000, 10000, 20000) al crear los TextDataLoaders.\n",
    "- Entrena tu modelo TextClassifier para cada tamaño de vocabulario.\n",
    "- Compara los resultados en términos de precisión y rendimiento computacional.\n",
    "\n",
    "**Ejercicio 3:** Visualiza los embeddings de palabras utilizando técnicas como t-SNE o PCA para entender cómo las palabras relacionadas están agrupadas en el espacio vectorial.\n",
    "\n",
    "Pasos:\n",
    "\n",
    "- Entrena el modelo TextClassifier y extrae la matriz de embeddings.\n",
    "- Utiliza t-SNE o PCA (disponible en sklearn) para reducir la dimensionalidad de los embeddings a 2 o 3 dimensiones.\n",
    "- Visualiza los embeddings reducidos usando Matplotlib o Seaborn y analiza si palabras semánticamente similares se agrupan juntas.\n",
    "\n",
    "**Ejercicio 4:** Implementa técnicas de regularización como Dropout o L2 regularization para mejorar la generalización del modelo.\n",
    "\n",
    "Pasos:\n",
    "- Modifica la clase TextClassifier para incluir capas de Dropout después de cada capa RNN y antes de las capas lineales.\n",
    "- Entrena el modelo y compara el rendimiento en el conjunto de validación para evaluar si la regularización reduce el sobreajuste.\n",
    "\n",
    "**Ejercicio 5:** Experimenta con diferentes funciones de activación (ReLU, ELU, LeakyReLU) en las capas lineales para ver cómo afectan al aprendizaje.\n",
    "\n",
    "Pasos:\n",
    "- Modifica las capas lineales en TextClassifier para utilizar diferentes funciones de activación.\n",
    "- Entrena el modelo para cada configuración y evalúa el impacto en la rapidez de la convergencia y la precisión final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e37a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaff5db",
   "metadata": {},
   "source": [
    "Realizamos varias operaciones importantes para construir y entrenar un modelo de clasificación de texto utilizando la biblioteca fastai junto con PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964325f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "??torch.nn.RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489eaad",
   "metadata": {},
   "source": [
    "Recuerda: Este fragmento de código utiliza la sintaxis de Python ?? para mostrar la documentación integrada de la clase torch.nn.RNN. Esto es útil para entender más detalles sobre cómo funciona la implementación de RNN en PyTorch, incluyendo parámetros y métodos disponibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89daf537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        _, x = self.rnn(x)\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "\n",
    "        return out\n",
    "     \n",
    "\n",
    "learn = Learner(dls, TextClassifier(len(dls.vocab[0]), 100),\n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy)\n",
    "learn.fit(10)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed48e483",
   "metadata": {},
   "source": [
    "- nn.Embedding: Transforma índices de palabras en vectores densos de tamaño hidden_size.\n",
    "- nn.RNN: Un módulo RNN que procesa secuencias de embeddings de palabras. El parámetro batch_first=True indica que la primera dimensión del tensor de entrada representa el tamaño del lote.\n",
    "- nn.Linear: Dos capas lineales que transforman la salida de la RNN en logits para clasificación. La última capa tiene 2 unidades de salida, lo que sugiere una tarea de clasificación binaria.\n",
    "- forward: Define cómo pasa la entrada a través del modelo. Nota que sólo se utiliza el último estado oculto de la RNN `(_, x = self.rnn(x))`  para la clasificación final, lo cual es común en tareas donde sólo el resultado después de ver toda la entrada es relevante (como en clasificación de sentimientos al final de la secuencia).\n",
    "\n",
    "Luego se crea una nueva instancia de `Learner` y se entrena el modelo modificado por 10 épocas, Esto muestra cómo puedes iterar sobre la definición del modelo y el proceso de entrenamiento para experimentar con diferentes arquitecturas o configuraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392d128",
   "metadata": {},
   "source": [
    "#### Redes bidireccionales\n",
    "\n",
    "Las redes neuronales bidireccionales combinan dos RNNs que operan en direcciones opuestas sobre la entrada de datos, lo que permite que el modelo tenga en cuenta tanto el contexto pasado como el futuro en cada punto de la secuencia.\n",
    "\n",
    "Al procesar una secuencia tanto en dirección hacia adelante como hacia atrás, una Bi-RNN puede capturar información de todo el contexto alrededor de un punto dado en la secuencia. Esto es especialmente útil para tareas donde la comprensión del contexto posterior es crucial, como puede ser en el etiquetado de partes del habla, reconocimiento de entidades nombradas, y otras tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "Dado que el modelo tiene acceso a más información contextual, a menudo puede hacer predicciones más precisas en comparación con una RNN unidireccional, particularmente en tareas complejas de clasificación o regresión sobre secuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size,\n",
    "                          bidirectional=True, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        _, x = self.rnn(x)\n",
    "        x = torch.cat((x[0], x[1]), dim=-1)\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "\n",
    "        return out\n",
    "     \n",
    "\n",
    "learn = Learner(dls, TextClassifier(len(dls.vocab[0]), 100),\n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy)\n",
    "learn.fit(10)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec40837",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d36228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        x, _ = self.rnn(x)[1]\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n",
    "     \n",
    "\n",
    "learn = Learner(dls, TextClassifier(len(dls.vocab[0]), 100),\n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy)\n",
    "learn.fit(10)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e30fc",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        _, x = self.rnn(x)\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n",
    "     \n",
    "\n",
    "learn = Learner(dls, TextClassifier(len(dls.vocab[0]), 100),\n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy)\n",
    "learn.fit(10)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50458c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
