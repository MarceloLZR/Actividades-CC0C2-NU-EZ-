{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872cbcce",
   "metadata": {},
   "source": [
    "## Redes neuronales recurrentes y otros modelos de secuencia\n",
    "\n",
    "Se crea una instancia de una red neuronal recurrente (RNN). Aquí, torch.nn.RNN es la clase que implementa una RNN básica en PyTorch. Los argumentos 300 y 512 especifican los tamaños de las características de entrada y de las características ocultas, respectivamente.\n",
    "\n",
    "- 300: Este es el tamaño de la característica de cada elemento de entrada en la secuencia. Por ejemplo, si estás procesando datos de texto, esto podría ser el tamaño del vector de características (como embeddings de palabras) para cada palabra.\n",
    "- 512: Este es el tamaño de la característica de los estados ocultos en la RNN. Esto afecta la capacidad del modelo para aprender dependencias complejas en los datos. Un número más alto puede permitir que el modelo capture dependencias más complejas, pero también puede llevar a un mayor riesgo de sobreajuste y requerir más memoria y tiempo de cómputo.\n",
    "\n",
    "Esta línea de código prepara un modelo RNN que puedes entrenar con datos adecuados para realizar tareas como la predicción de series temporales, el modelado de lenguaje, y otras tareas donde los datos de entrada tienen una naturaleza secuencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ce5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.nn.RNN(300, 512)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47aa3d",
   "metadata": {},
   "source": [
    "#### RNN usando Pytorch\n",
    "\n",
    "El código siguiente define y utiliza una celda de red neuronal recurrente (RNN) personalizada usando PyTorch y la biblioteca fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e139d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "# fastai.__version__\n",
    "     \n",
    "from fastai.text.all import *\n",
    "     \n",
    "\n",
    "class RNNCell(nn.Module):    \n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ih = nn.Linear(input_size, hidden_size)\n",
    "        self.hh = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h = None):\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), self.hidden_size)\n",
    "        h = torch.tanh(self.ih(x) + self.hh(h))\n",
    "        return h\n",
    "     \n",
    "\n",
    "#oculto\n",
    "cell = RNNCell(100, 300)\n",
    "cell(torch.randn(1, 100)).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee9594",
   "metadata": {},
   "source": [
    "En el código anterior, la clase RNNCell, define una celda RNN personalizada. La celda tiene dos partes principales:\n",
    "\n",
    "- Inicialización (__init__) que define dos capas lineales, una para transformar la entrada (ih) y otra para transformar el estado oculto anterior (hh).\n",
    "- Paso hacia adelante (forward) que calcula el nuevo estado oculto combinando la entrada actual x y el estado oculto anterior h (si h no se proporciona, se inicializa como un tensor de ceros). La suma de las transformaciones lineales de x y h se pasa a través de una función de activación tanh para obtener el nuevo estado oculto.\n",
    "\n",
    "Luego se crea una instancia de RNNCell con un tamaño de entrada de 100 y un tamaño de estado oculto de 300. Luego, pasa un tensor aleatorio (simulando una entrada única de tamaño 100) a través de la celda RNN para obtener el estado oculto, e imprime la forma del tensor resultante, que sería [1, 300], correspondiente a un lote de tamaño 1 con un vector de estado oculto de tamaño 300."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f48517",
   "metadata": {},
   "source": [
    "Ahora definimos una clase RNN que implementa una red neuronal recurrente completa usando la clase RNNCell personalizada anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd55b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):    \n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.cell = RNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h = None):\n",
    "        \n",
    "        print(x.shape)\n",
    "        for i in range(x.shape[1]):\n",
    "            h = self.cell(x[:,i], h)\n",
    "            \n",
    "        return h\n",
    "     \n",
    "\n",
    "#oculto\n",
    "rnn = RNN(100, 300)\n",
    "rnn(torch.randn(256, 10, 100)).shape\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6ca6c",
   "metadata": {},
   "source": [
    "Se define la clase RNN como una subclase de nn.Module, que es la clase base para todos los módulos de red neuronal en PyTorch. En el constructor (__init__), inicializa una instancia de RNNCell con los tamaños de entrada y ocultos especificados, almacenándola como self.cell.\n",
    "\n",
    "El método forward define cómo los datos pasan a través de la red:\n",
    "\n",
    "- x.shape imprime la forma de la entrada x. Suponiendo que x tiene forma (256, 10, 100), esto indica que estamos procesando 256 secuencias, cada una de longitud 10, donde cada elemento de la secuencia tiene 100 características.\n",
    "\n",
    "- El bucle `for i in range(x.shape[1])` itera sobre la segunda dimensión de x, que es la longitud de la secuencia (10 en este caso). En cada iteración, extrae el i-ésimo elemento de cada secuencia (usando x[:, i]), y lo pasa a self.cell junto con el estado oculto actual h.\n",
    "\n",
    "- h se actualiza en cada paso de tiempo al procesar el elemento correspondiente de la secuencia a través de la celda RNN. Si h es None inicialmente, será inicializado a un tensor de ceros dentro de RNNCell.\n",
    "\n",
    "\n",
    "Luego se crea una instancia de RNN con un tamaño de entrada de 100 y un tamaño de estado oculto de 300. Luego, se pasa un tensor de entrada aleatorio con forma `(256, 10, 100)` a través de la RNN. Este tensor representa 256 secuencias, cada una de longitud 10, con cada elemento de la secuencia teniendo 100 características.\n",
    "\n",
    "El resultado de `rnn(torch.randn(256, 10, 100))`.shape sería la forma del último estado oculto producido, que es [256, 300]. Esto indica que hay un estado oculto de tamaño 300 para cada una de las 256 secuencias después de procesar todos los elementos de cada secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578742a",
   "metadata": {},
   "source": [
    "### Ejemplo de clasificación de texto\n",
    "\n",
    "Definimos una clase TextClassifier que es un modelo de red neuronal para clasificación de texto, utilizando módulos comunes de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739e7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = RNN(hidden_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        x = self.rnn(x)\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n",
    "     \n",
    "\n",
    "path = untar_data(URLs.IMDB)\n",
    "dls = TextDataLoaders.from_folder(path, valid='test', bs=256)\n",
    "     \n",
    "\n",
    "dls.show_batch(max_n=5)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64adc2e4",
   "metadata": {},
   "source": [
    "Explicación del código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = RNN(hidden_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c778c0",
   "metadata": {},
   "source": [
    "- nn.Embedding(vocab_size, hidden_size): Este módulo es una tabla de búsqueda que almacena embeddings de un vocabulario fijo de tamaño vocab_size, cada uno con una dimensión hidden_size. Transforma índices de palabras en vectores densos.\n",
    "\n",
    "- RNN(hidden_size, hidden_size): Se utiliza una RNN, definida previamente, que toma la salida del embedding como entrada. La RNN puede capturar dependencias secuenciales en el texto.\n",
    "\n",
    "- nn.Linear(hidden_size, 10) y nn.Linear(10, 1): Estas son capas lineales que transforman la salida de la RNN a dimensiones más pequeñas, eventualmente produciendo una salida única. Esto es típico para tareas de clasificación binaria donde la salida puede representar la probabilidad de una clase (a menudo después de aplicar una función sigmoide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba08aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = self.emb(x)        # Aplica embedding al texto de entrada\n",
    "    x = self.rnn(x)        # Procesa el texto a través de la RNN\n",
    "    x = self.fc1(x)        # Primera capa lineal\n",
    "    out = self.fc2(x)      # Segunda capa lineal que produce la salida final\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e00095",
   "metadata": {},
   "source": [
    "El método forward define cómo los datos pasan a través del modelo. Los datos de entrada (x) son índices de palabras que primero se convierten en embeddings, luego se procesan a través de una RNN, y finalmente se transforman a través de dos capas lineales para producir la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "dls = TextDataLoaders.from_folder(path, valid='test', bs=256)\n",
    "dls.show_batch(max_n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0979a21",
   "metadata": {},
   "source": [
    "- untar_data(URLs.IMDB): Descarga y descomprime el conjunto de datos IMDB para análisis de sentimientos, que contiene críticas de películas etiquetadas como positivas o negativas.\n",
    "- TextDataLoaders.from_folder(path, valid='test', bs=256): Crea cargadores de datos para el entrenamiento y validación. Los datos se dividen en un conjunto de entrenamiento y un conjunto de validación, con un tamaño de lote de 256.\n",
    "- dls.show_batch(max_n=5): Muestra un lote de datos de ejemplo, que puede ser útil para verificar cómo se están cargando y procesando los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea368f",
   "metadata": {},
   "source": [
    "**Ejercicio 1:** Modifica la arquitectura del TextClassifier para incluir una capa adicional de LSTM en lugar de RNN simple. Investiga y compara los resultados en términos de precisión y pérdida durante el entrenamiento.\n",
    "\n",
    "Pasos:\n",
    "- Define una nueva clase LSTMCell o modifica la clase RNN para utilizar nn.LSTM en lugar de nn.RNN.\n",
    "- Integra esta nueva celda LSTM en el TextClassifier.\n",
    "- Entrena el modelo modificado en el conjunto de datos IMDB y compara los resultados con la versión original.\n",
    "\n",
    "**Ejercicio 2:** Evalúa cómo el tamaño del vocabulario afecta la precisión de la clasificación.\n",
    "\n",
    "Pasos:\n",
    "- Experimenta con diferentes tamaños de vocabulario (por ejemplo, 5000, 10000, 20000) al crear los TextDataLoaders.\n",
    "- Entrena tu modelo TextClassifier para cada tamaño de vocabulario.\n",
    "- Compara los resultados en términos de precisión y rendimiento computacional.\n",
    "\n",
    "**Ejercicio 3:** Visualiza los embeddings de palabras utilizando técnicas como t-SNE o PCA para entender cómo las palabras relacionadas están agrupadas en el espacio vectorial.\n",
    "\n",
    "Pasos:\n",
    "\n",
    "- Entrena el modelo TextClassifier y extrae la matriz de embeddings.\n",
    "- Utiliza t-SNE o PCA (disponible en sklearn) para reducir la dimensionalidad de los embeddings a 2 o 3 dimensiones.\n",
    "- Visualiza los embeddings reducidos usando Matplotlib o Seaborn y analiza si palabras semánticamente similares se agrupan juntas.\n",
    "\n",
    "**Ejercicio 4:** Implementa técnicas de regularización como Dropout o L2 regularization para mejorar la generalización del modelo.\n",
    "\n",
    "Pasos:\n",
    "- Modifica la clase TextClassifier para incluir capas de Dropout después de cada capa RNN y antes de las capas lineales.\n",
    "- Entrena el modelo y compara el rendimiento en el conjunto de validación para evaluar si la regularización reduce el sobreajuste.\n",
    "\n",
    "**Ejercicio 5:** Experimenta con diferentes funciones de activación (ReLU, ELU, LeakyReLU) en las capas lineales para ver cómo afectan al aprendizaje.\n",
    "\n",
    "Pasos:\n",
    "- Modifica las capas lineales en TextClassifier para utilizar diferentes funciones de activación.\n",
    "- Entrena el modelo para cada configuración y evalúa el impacto en la rapidez de la convergencia y la precisión final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e37a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaff5db",
   "metadata": {},
   "source": [
    "Realizamos varias operaciones importantes para construir y entrenar un modelo de clasificación de texto utilizando la biblioteca fastai junto con PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964325f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "??torch.nn.RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489eaad",
   "metadata": {},
   "source": [
    "Recuerda: Este fragmento de código utiliza la sintaxis de Python ?? para mostrar la documentación integrada de la clase torch.nn.RNN. Esto es útil para entender más detalles sobre cómo funciona la implementación de RNN en PyTorch, incluyendo parámetros y métodos disponibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89daf537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        _, x = self.rnn(x)\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "\n",
    "        return out\n",
    "     \n",
    "\n",
    "learn = Learner(dls, TextClassifier(len(dls.vocab[0]), 100),\n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy)\n",
    "learn.fit(10)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed48e483",
   "metadata": {},
   "source": [
    "- nn.Embedding: Transforma índices de palabras en vectores densos de tamaño hidden_size.\n",
    "- nn.RNN: Un módulo RNN que procesa secuencias de embeddings de palabras. El parámetro batch_first=True indica que la primera dimensión del tensor de entrada representa el tamaño del lote.\n",
    "- nn.Linear: Dos capas lineales que transforman la salida de la RNN en logits para clasificación. La última capa tiene 2 unidades de salida, lo que sugiere una tarea de clasificación binaria.\n",
    "- forward: Define cómo pasa la entrada a través del modelo. Nota que sólo se utiliza el último estado oculto de la RNN `(_, x = self.rnn(x))`  para la clasificación final, lo cual es común en tareas donde sólo el resultado después de ver toda la entrada es relevante (como en clasificación de sentimientos al final de la secuencia).\n",
    "\n",
    "Luego se crea una nueva instancia de `Learner` y se entrena el modelo modificado por 10 épocas, Esto muestra cómo puedes iterar sobre la definición del modelo y el proceso de entrenamiento para experimentar con diferentes arquitecturas o configuraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392d128",
   "metadata": {},
   "source": [
    "#### Redes bidireccionales\n",
    "\n",
    "Las redes neuronales bidireccionales combinan dos RNNs que operan en direcciones opuestas sobre la entrada de datos, lo que permite que el modelo tenga en cuenta tanto el contexto pasado como el futuro en cada punto de la secuencia.\n",
    "\n",
    "Al procesar una secuencia tanto en dirección hacia adelante como hacia atrás, una Bi-RNN puede capturar información de todo el contexto alrededor de un punto dado en la secuencia. Esto es especialmente útil para tareas donde la comprensión del contexto posterior es crucial, como puede ser en el etiquetado de partes del habla, reconocimiento de entidades nombradas, y otras tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "Dado que el modelo tiene acceso a más información contextual, a menudo puede hacer predicciones más precisas en comparación con una RNN unidireccional, particularmente en tareas complejas de clasificación o regresión sobre secuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size,\n",
    "                          bidirectional=True, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        _, x = self.rnn(x)\n",
    "        x = torch.cat((x[0], x[1]), dim=-1)\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "\n",
    "        return out\n",
    "     \n",
    "\n",
    "learn = Learner(dls, TextClassifier(len(dls.vocab[0]), 100),\n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy)\n",
    "learn.fit(10)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec40837",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d36228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        x, _ = self.rnn(x)[1]\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n",
    "     \n",
    "\n",
    "learn = Learner(dls, TextClassifier(len(dls.vocab[0]), 100),\n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy)\n",
    "learn.fit(10)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e30fc",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        _, x = self.rnn(x)\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n",
    "     \n",
    "\n",
    "learn = Learner(dls, TextClassifier(len(dls.vocab[0]), 100),\n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy)\n",
    "learn.fit(10)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e179dc3",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "#### **Ejercicio 1: agregar capas de dropout**\n",
    "\n",
    "**Objetivo:** Mejorar la generalización del modelo añadiendo capas de dropout para prevenir el sobreajuste.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Modifica la clase `TextClassifier` para incluir capas de `nn.Dropout` después de la capa de embedding y entre las capas completamente conectadas.\n",
    "- Ajusta la probabilidad de dropout (por ejemplo, `p=0.5`) y observa cómo afecta al rendimiento del modelo.\n",
    "- Entrena el modelo nuevamente y compara los resultados con los obtenidos previamente sin dropout.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- El dropout ayuda a prevenir el sobreajuste al desactivar aleatoriamente neuronas durante el entrenamiento.\n",
    "- Observa si la precisión en el conjunto de validación mejora o si el modelo converge más lentamente.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 2: Implementar un mecanismo de atención (attention)**\n",
    "\n",
    "**Objetivo:** Mejorar el rendimiento del modelo al permitir que preste más atención a partes específicas de la secuencia.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Investiga cómo funciona el mecanismo de atención en el contexto de las RNNs.\n",
    "- Modifica la clase `TextClassifier` para incluir una capa de atención después de la RNN.\n",
    "- Implementa el mecanismo de atención para ponderar las salidas de la RNN antes de pasarlas a las capas totalmente conectadas.\n",
    "- Entrena el modelo y compara su rendimiento con el modelo sin atención.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- El mecanismo de atención puede ayudar al modelo a enfocarse en palabras o tokens importantes para la tarea de clasificación.\n",
    "- Asegúrate de manejar correctamente las dimensiones al implementar la atención.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 3: Experimentar con diferentes tamaños de embedding**\n",
    "\n",
    "**Objetivo:** Analizar cómo el tamaño de las embeddings afecta el rendimiento del modelo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Modifica el tamaño de las embeddings en la capa `nn.Embedding` (por ejemplo, prueba con tamaños de 50, 100, 200).\n",
    "- Entrena el modelo con cada uno de estos tamaños y registra los resultados.\n",
    "- Grafica la precisión del modelo en función del tamaño de las embeddings.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Un tamaño de embedding mayor puede capturar más información semántica pero también puede llevar a un modelo más complejo y propenso al sobreajuste.\n",
    "- Busca un equilibrio entre el rendimiento y la complejidad del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 4: Utilizar embeddings pre-entrenadas (GloVe o FastText)**\n",
    "\n",
    "**Objetivo:** Aprovechar embeddings pre-entrenadas para mejorar el rendimiento del modelo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Descarga embeddings pre-entrenadas como GloVe o FastText.\n",
    "- Modifica la capa de embeddings para inicializarla con estos vectores pre-entrenados.\n",
    "- Asegúrate de que los índices de las palabras en tu vocabulario correspondan a los vectores de las embeddings.\n",
    "- Decide si quieres mantener las embeddings fijas o permitir que se ajusten durante el entrenamiento (`emb.weight.requires_grad = False` o `True`).\n",
    "- Entrena el modelo y compara su rendimiento con el modelo que utiliza embeddings aprendidas desde cero.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Las embeddings pre-entrenadas pueden mejorar el rendimiento especialmente si el conjunto de datos de entrenamiento es pequeño.\n",
    "- Presta atención a cómo manejas las palabras fuera del vocabulario (OOV).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 5: Implementar early stopping**\n",
    "\n",
    "**Objetivo:** Prevenir el sobreentrenamiento deteniendo el entrenamiento cuando el rendimiento en el conjunto de validación deja de mejorar.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Investiga cómo implementar early stopping en fastai utilizando callbacks.\n",
    "- Añade un callback a tu `Learner` para monitorear la pérdida de validación.\n",
    "- Establece un criterio para detener el entrenamiento, como no mejora después de un número determinado de épocas (`patience`).\n",
    "- Entrena el modelo y observa en qué época se detiene el entrenamiento automáticamente.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Early stopping puede ahorrar tiempo de entrenamiento y prevenir que el modelo se ajuste demasiado a los datos de entrenamiento.\n",
    "- Asegúrate de guardar el mejor modelo obtenido durante el proceso.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 6: Visualizar las pérdidas y las métricas durante el entrenamiento**\n",
    "\n",
    "**Objetivo:** Entender mejor el proceso de entrenamiento al visualizar cómo cambian las pérdidas y las métricas.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Utiliza las herramientas de visualización de fastai para graficar las pérdidas de entrenamiento y validación.\n",
    "- Después de entrenar el modelo, llama al método `learn.recorder.plot_loss()` para visualizar las pérdidas.\n",
    "- Observa si hay señales de sobreajuste (por ejemplo, si la pérdida de validación comienza a aumentar mientras que la de entrenamiento sigue disminuyendo).\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Las visualizaciones pueden ayudarte a diagnosticar problemas en el entrenamiento y a ajustar hiperparámetros.\n",
    "- Considera también graficar la precisión en el conjunto de validación.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 7: Reemplazar la RNN con un modelo Transformer**\n",
    "\n",
    "**Objetivo:** Explorar arquitecturas más recientes y potentes para el procesamiento de lenguaje natural.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Investiga sobre los modelos Transformer y cómo se diferencian de las RNNs tradicionales.\n",
    "- Utiliza la implementación de Transformers en PyTorch o fastai.\n",
    "- Modifica la clase `TextClassifier` para utilizar un Transformer en lugar de una RNN.\n",
    "- Entrena el modelo y compara su rendimiento y tiempo de entrenamiento con las RNNs anteriores.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Los Transformers pueden manejar dependencias a largo plazo de manera más efectiva y suelen entrenarse más rápido gracias a su paralelización.\n",
    "- Asegúrate de ajustar los hiperparámetros adecuadamente, ya que los Transformers pueden ser más sensibles a estos.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 8: Crear un conjunto de datos personalizado**\n",
    "\n",
    "**Objetivo:** Aplicar lo aprendido a un nuevo conjunto de datos y enfrentar desafíos de preprocesamiento.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Busca o crea un conjunto de datos de texto para una tarea de clasificación diferente (por ejemplo, categorización de noticias, análisis de sentimientos en tweets).\n",
    "- Prepara el conjunto de datos para su uso con fastai, asegurándote de que las carpetas y los archivos estén organizados correctamente.\n",
    "- Crea `TextDataLoaders` para tu nuevo conjunto de datos.\n",
    "- Entrena uno de los modelos previamente implementados y evalúa su rendimiento.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Es posible que debas preprocesar el texto para eliminar ruido o manejar caracteres especiales.\n",
    "- Presta atención al equilibrio de clases; si las clases están desbalanceadas, podrías necesitar técnicas adicionales como el muestreo estratificado.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 9: Ajustar la tasa de aprendizaje y otros hiperparámetros**\n",
    "\n",
    "**Objetivo:** Entender cómo los hiperparámetros afectan el entrenamiento y encontrar la configuración óptima.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Utiliza el método `learn.lr_find()` de fastai para encontrar una tasa de aprendizaje adecuada.\n",
    "- Ajusta la tasa de aprendizaje (`lr`) en el método `learn.fit_one_cycle()` basándote en los resultados de `lr_find()`.\n",
    "- Experimenta con diferentes tamaños de lote (`bs`), números de épocas y funciones de optimización (por ejemplo, Adam, SGD).\n",
    "- Registra los resultados y determina qué configuraciones producen el mejor rendimiento.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Una tasa de aprendizaje demasiado alta puede hacer que el modelo no converja, mientras que una demasiado baja puede hacer que el entrenamiento sea muy lento.\n",
    "- El tamaño del lote afecta la estabilidad y la eficiencia del entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 10: Implementar regularización adicional**\n",
    "\n",
    "**Objetivo:** Mejorar la generalización del modelo mediante técnicas de regularización.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Aplica regularización L2 (decaimiento de peso) al optimizador.\n",
    "- Experimenta con diferentes valores del parámetro de regularización (`weight_decay`).\n",
    "- Observa cómo afecta al rendimiento y al sobreajuste.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- La regularización ayuda a evitar que los pesos del modelo se vuelvan demasiado grandes.\n",
    "- Un valor de regularización muy alto puede dificultar el aprendizaje del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 11: Analizar los resultados y errores del modelo**\n",
    "\n",
    "**Objetivo:** Comprender dónde y por qué el modelo puede estar cometiendo errores.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Después de entrenar el modelo, utiliza `ClassificationInterpretation.from_learner(learn)` para obtener una interpretación de los resultados.\n",
    "- Usa métodos como `interp.plot_confusion_matrix()` para visualizar la matriz de confusión.\n",
    "- Identifica ejemplos donde el modelo falla y analiza posibles razones (por ejemplo, ambigüedad en el texto, ruido, longitud de la secuencia).\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Analizar los errores puede proporcionar información valiosa para mejorar el modelo.\n",
    "- Podrías considerar técnicas como el aumento de datos para abordar ciertos tipos de errores.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 12: Implementar batch normalization y layer normalization**\n",
    "\n",
    "**Objetivo:** Mejorar la estabilidad y el rendimiento del modelo mediante técnicas de normalización.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Investiga las diferencias entre Batch Normalization y Layer Normalization.\n",
    "- Implementa `nn.BatchNorm1d` o `nn.LayerNorm` en tu modelo, posiblemente después de la capa RNN o entre las capas completamente conectadas.\n",
    "- Entrena el modelo y compara los resultados con el modelo sin normalización.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- La normalización puede acelerar el entrenamiento y mejorar la convergencia.\n",
    "- Considera las implicaciones de utilizar Batch Normalization en datos secuenciales y si Layer Normalization es más apropiado.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 13: Añadir más capas a la RNN**\n",
    "\n",
    "**Objetivo:** Explorar cómo la profundidad de la red afecta al aprendizaje.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Modifica la RNN para que tenga múltiples capas (por ejemplo, `num_layers=2` en `nn.RNN`, `nn.LSTM` o `nn.GRU`).\n",
    "- Asegúrate de manejar correctamente las dimensiones de los estados ocultos cuando hay múltiples capas.\n",
    "- Entrena el modelo y observa cómo cambia el rendimiento.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Las redes más profundas pueden capturar patrones más complejos pero también pueden ser más difíciles de entrenar.\n",
    "- Podrías necesitar ajustar la tasa de aprendizaje u otros hiperparámetros al aumentar la profundidad.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 14: Aplicar técnicas de embedding posicional**\n",
    "\n",
    "**Objetivo:** Mejorar la capacidad del modelo para entender la posición de las palabras en la secuencia.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Investiga cómo funcionan los embeddings posicionales en modelos como Transformers.\n",
    "- Implementa una forma de incorporar información posicional en las embeddings o en las entradas de la RNN.\n",
    "- Entrena el modelo y evalúa si hay mejoras en el rendimiento.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Las RNNs, por su naturaleza, capturan el orden secuencial, pero la información posicional explícita puede reforzar este conocimiento.\n",
    "- Asegúrate de que la implementación de los embeddings posicionales sea compatible con el modelo utilizado.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 15: utilizar técnicas de enseñanza ensembles**\n",
    "\n",
    "**Objetivo:** Mejorar el rendimiento combinando múltiples modelos.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Entrena varios modelos independientes con diferentes inicializaciones o hiperparámetros.\n",
    "- Combina sus predicciones mediante promedio o votación mayoritaria.\n",
    "- Evalúa si el ensemble mejora el rendimiento sobre los modelos individuales.\n",
    "\n",
    "**Puntos a considerar:**\n",
    "\n",
    "- Los ensembles pueden reducir la varianza y mejorar la generalización.\n",
    "- Ten en cuenta el aumento en el costo computacional al utilizar múltiples modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded0b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
