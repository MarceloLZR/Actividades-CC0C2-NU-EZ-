{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548eeb3e-52c7-4a07-b15f-3c47c921ba6b",
   "metadata": {},
   "source": [
    "Este cuaderno son los ejemplos dados en clases.\n",
    "\n",
    "### Flujo en una red neuronal básica con pesos $W_1, W_2, ..., W_n+1$. \n",
    "\n",
    "\n",
    "1. **Entrada (Vector de características numéricas)**: \n",
    "   Supongamos que tenemos una entrada con tres características numéricas. Estas características pueden representar información sobre un cliente, como el ingreso anual, la edad y el puntaje de crédito.\n",
    "   \n",
    "   $$\n",
    "   \\text{Entrada} = \\begin{bmatrix} 50000 \\\\ 30 \\\\ 700 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Capa de \"características de características\" ($W_1$)**:\n",
    "   La entrada pasa por la primera capa de pesos, $W_1$, que convierte las características originales en una representación de \"características de características\". Este proceso aplica una transformación lineal seguida por una función de activación (por ejemplo, ReLU) para captar patrones más complejos en los datos.\n",
    "\n",
    "   Supongamos que $W_1$ tiene dimensiones $3 \\times 4$, lo que convierte la entrada de dimensión $3$ en una salida de dimensión $4$. La transformación sería algo así:\n",
    "   \n",
    "   $$\n",
    "   \\text{Vector de características de características} = \\text{ReLU}(W_1 \\cdot \\text{Entrada})\n",
    "   $$\n",
    "\n",
    "3. **Capas intermedias ($W_2$ a $W_n$)**:\n",
    "   Este proceso se repite para $W_2$ hasta $W_n$. Cada una de estas capas transforma el vector de la capa anterior en una nueva representación, ajustando los pesos de la red para mejorar la capacidad de predicción. En cada paso, se aplica una función de activación no lineal como ReLU para mejorar la expresividad del modelo.\n",
    "\n",
    "   Por ejemplo, la salida de la capa $W_2$ podría ser:\n",
    "   \n",
    "   $$\n",
    "   \\text{Salida}_2 = \\text{ReLU}(W_2 \\cdot \\text{Vector de características de características})\n",
    "   $$\n",
    "\n",
    "   Y así sucesivamente hasta la capa $W_n$.\n",
    "\n",
    "4. **Vector final de representación del conjunto de observación**:\n",
    "   Después de pasar por todas las capas intermedias, llegamos a una representación final de la entrada original en un espacio de características de mayor nivel. Esta representación condensa toda la información relevante que la red ha aprendido de los datos de entrada.\n",
    "\n",
    "5. **Predicción (Capa de salida con $W_{n+1}$)**:\n",
    "   Finalmente, el vector final de la capa anterior se pasa a través de $W_{n+1}$, que mapea esta representación al espacio de salida deseado. Si el objetivo es una clasificación binaria, la salida puede ser un único valor que luego se pasa por una función sigmoide para producir una probabilidad. Si es una clasificación multiclase, se puede usar una función softmax para generar probabilidades para cada clase.\n",
    "\n",
    "   Por ejemplo, si estamos prediciendo si un cliente aprobará un préstamo (1) o no (0), la salida podría ser:\n",
    "\n",
    "   $$\n",
    "   \\text{Predicción} = \\text{Sigmoide}(W_{n+1} \\cdot \\text{Vector final de representación})\n",
    "   $$\n",
    "\n",
    "Este flujo general muestra cómo una red neuronal básica puede tomar un conjunto de características de entrada y transformarlas a través de capas para producir una predicción final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1077a79-736e-45ee-853b-73d5dab3c1a2",
   "metadata": {},
   "source": [
    "### Ejemplo númerico\n",
    "\n",
    "Vamos a utilizar una red simple con un solo nodo en cada capa para simplificar el cálculo. Este ejemplo ilustrará el flujo a través de una red neuronal con una entrada de tres características y dos capas intermedias.\n",
    "\n",
    "\n",
    "1. **Entrada**:\n",
    "   Supongamos que nuestra entrada es un vector de tres características:\n",
    "   $$\n",
    "   \\text{Entrada} = \\begin{bmatrix} 50000 \\\\ 30 \\\\ 700 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Primera Capa de Pesos ($W_1$)**:\n",
    "   Supongamos que $W_1$ es una matriz de $3 \\times 2$ que transforma la entrada de dimensión 3 en una salida de dimensión 2. Usamos los siguientes pesos y un sesgo ($b_1$):\n",
    "\n",
    "   $$\n",
    "   W_1 = \\begin{bmatrix} 0.0001 & 0.0002 \\\\ 0.05 & 0.02 \\\\ 0.01 & 0.03 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   La salida de esta capa sería:\n",
    "   $$\n",
    "   \\text{Salida}_1 = \\text{ReLU}(W_1 \\cdot \\text{Entrada} + b_1)\n",
    "   $$\n",
    "\n",
    "   Calculamos:\n",
    "   $$\n",
    "   W_1 \\cdot \\text{Entrada} = \\begin{bmatrix} 0.0001 & 0.0002 \\\\ 0.05 & 0.02 \\\\ 0.01 & 0.03 \\end{bmatrix} \\cdot \\begin{bmatrix} 50000 \\\\ 30 \\\\ 700 \\end{bmatrix} = \\begin{bmatrix} (0.0001 \\cdot 50000) + (0.05 \\cdot 30) + (0.01 \\cdot 700) \\\\ (0.0002 \\cdot 50000) + (0.02 \\cdot 30) + (0.03 \\cdot 700) \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada elemento:\n",
    "   $$\n",
    "   = \\begin{bmatrix} 5 + 1.5 + 7 \\\\ 10 + 0.6 + 21 \\end{bmatrix} = \\begin{bmatrix} 13.5 \\\\ 31.6 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Luego añadimos el sesgo $b_1$:\n",
    "   $$\n",
    "   \\text{Salida}_1 = \\text{ReLU} \\left( \\begin{bmatrix} 13.5 \\\\ 31.6 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\right) = \\text{ReLU} \\left( \\begin{bmatrix} 14.5 \\\\ 32.6 \\end{bmatrix} \\right)\n",
    "   $$\n",
    "\n",
    "   Aplicamos la función ReLU (max(0, x)), que no cambia los valores positivos:\n",
    "   $$\n",
    "   \\text{Salida}_1 = \\begin{bmatrix} 14.5 \\\\ 32.6 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Segunda capa de pesos ($W_2$)**:\n",
    "   Supongamos que $W_2$ es una matriz de $2 \\times 1$ que convierte la salida en un solo valor escalar de salida. También tiene un sesgo $b_2$.\n",
    "\n",
    "   $$\n",
    "   W_2 = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}, \\quad b_2 = \\begin{bmatrix} 2 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   La salida final de esta capa sería:\n",
    "   $$\n",
    "   \\text{Salida}_2 = W_2 \\cdot \\text{Salida}_1 + b_2\n",
    "   $$\n",
    "\n",
    "   Calculamos:\n",
    "   $$\n",
    "   W_2 \\cdot \\text{Salida}_1 = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} \\cdot \\begin{bmatrix} 14.5 \\\\ 32.6 \\end{bmatrix} = (0.1 \\cdot 14.5) + (0.2 \\cdot 32.6)\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   = 1.45 + 6.52 = 8.97\n",
    "   $$\n",
    "\n",
    "   Luego añadimos el sesgo $b_2$:\n",
    "   $$\n",
    "   \\text{Salida}_2 = 8.97 + 2 = 10.97\n",
    "   $$\n",
    "\n",
    "4. **Predicción final**:\n",
    "   La salida final de la red neuronal para esta entrada es:\n",
    "   $$\n",
    "   \\text{Predicción} = 10.97\n",
    "   $$\n",
    "\n",
    "Este resultado $10.97$ representa la salida de la red neuronal, que puede ser interpretada según el problema. Si fuera un problema de regresión, esta podría ser la predicción de un valor continuo. Si fuera un problema de clasificación binaria, este valor podría pasarse por una función sigmoide para obtener una probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afbe13-be9c-4bd3-86c6-867340ee3541",
   "metadata": {},
   "source": [
    "### Red neuronal recurrente (RNN) \n",
    "\n",
    "En esta red neuronal la información se procesa en pasos de tiempo $t$. Veamos paso a pado como funciona una RNN, utilizando dos pasos de tiempo $t = 1$ y $t = 2$.\n",
    "\n",
    "### Ejemplo paso a paso:\n",
    "\n",
    "Supongamos que estamos utilizando esta red para predecir una secuencia, como una serie de valores en el tiempo (por ejemplo, precios de acciones). Cada paso de tiempo recibe un valor de entrada y genera una predicción basada en esa entrada y en el estado anterior de la red.\n",
    "\n",
    "#### Paso de tiempo 1: $t = 1$\n",
    "\n",
    "1. **Entrada $t = 1$**:\n",
    "   La entrada en el primer paso de tiempo es un vector de características numéricas. Supongamos que tenemos:\n",
    "   $$\n",
    "   \\text{Input} (t=1) = \\begin{bmatrix} 0.5 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Primera capa oculta en $t = 1$**:\n",
    "   Esta entrada pasa a través de la primera capa oculta usando el peso $W_1$ y produce un vector de características en el estado oculto. Supongamos que:\n",
    "   $$\n",
    "   W_1 = \\begin{bmatrix} 0.8 \\end{bmatrix}\n",
    "   $$\n",
    "   Entonces, la salida de la primera capa sería:\n",
    "   $$\n",
    "   \\text{Hidden Vector} (t=1) = W_1 \\cdot \\text{Input} (t=1) = 0.8 \\cdot 0.5 = 0.4\n",
    "   $$\n",
    "\n",
    "3. **Capa intermedia hasta la $n$-ésima en $t = 1$**:\n",
    "   Este vector oculto continúa pasando a través de las capas intermedias con pesos $W_2, W_3, \\ldots, W_n$, donde cada capa aplica una transformación a este vector oculto.\n",
    "\n",
    "   Supongamos que todos los pesos en estas capas son $W_2 = W_3 = \\ldots = W_n = 0.5$. Entonces, al pasar por cada capa, tendríamos:\n",
    "   $$\n",
    "   \\text{Hidden Vector} (n, t=1) = 0.5 \\cdot 0.4 = 0.2\n",
    "   $$\n",
    "\n",
    "4. **Predicción en $t = 1$**:\n",
    "   La salida final de esta capa oculta se usa para hacer la predicción en el primer paso de tiempo, utilizando otro peso $W_{n+1}$.\n",
    "\n",
    "   Supongamos que $W_{n+1} = 1.2$, entonces:\n",
    "   $$\n",
    "   \\text{Prediction Vector} (t=1) = W_{n+1} \\cdot \\text{Hidden Vector} (n, t=1) = 1.2 \\cdot 0.2 = 0.24\n",
    "   $$\n",
    "\n",
    "   La predicción en el primer paso de tiempo es $0.24$.\n",
    "\n",
    "#### Paso de tiempo 2: $t = 2$\n",
    "\n",
    "1. **Entrada $t = 2$**:\n",
    "   La entrada en el segundo paso de tiempo es otro valor de la secuencia. Supongamos que ahora tenemos:\n",
    "   $$\n",
    "   \\text{Input} (t=2) = \\begin{bmatrix} 0.7 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Primera capa oculta en $t = 2$**:\n",
    "   La entrada en $t=2$ pasa a través de la primera capa, donde se usa el peso $W_1$ y el estado oculto del paso anterior ($t = 1$) para calcular el nuevo estado oculto.\n",
    "\n",
    "   Supongamos que usamos el mismo peso $W_1 = 0.8$, entonces:\n",
    "   $$\n",
    "   \\text{Hidden Vector} (t=2) = W_1 \\cdot \\text{Input} (t=2) = 0.8 \\cdot 0.7 = 0.56\n",
    "   $$\n",
    "\n",
    "3. **Capa intermedia hasta la $n$-ésima en $t = 2$**:\n",
    "   Este vector oculto se pasa a través de las capas intermedias, utilizando los mismos pesos $W_2, W_3, \\ldots, W_n$, igual que en el paso anterior.\n",
    "\n",
    "   $$\n",
    "   \\text{Hidden Vector} (n, t=2) = 0.5 \\cdot 0.56 = 0.28\n",
    "   $$\n",
    "\n",
    "4. **Predicción en $t = 2$**:\n",
    "   La salida final de la última capa oculta se usa para hacer la predicción en el segundo paso de tiempo.\n",
    "\n",
    "   Utilizando el mismo peso $W_{n+1} = 1.2$:\n",
    "   $$\n",
    "   \\text{Prediction Vector} (t=2) = W_{n+1} \\cdot \\text{Hidden Vector} (n, t=2) = 1.2 \\cdot 0.28 = 0.336\n",
    "   $$\n",
    "\n",
    "   La predicción en el segundo paso de tiempo es $0.336$.\n",
    "\n",
    "**Conclusión**\n",
    "\n",
    "- **Predicción en $t=1$**: 0.24\n",
    "- **Predicción en $t=2$**: 0.336\n",
    "\n",
    "Este proceso puede continuar por varios pasos de tiempo, y en cada paso, la red utiliza la información del paso anterior (estado oculto) para mejorar la precisión de las predicciones en la secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4e7e6-ba2d-44fb-932a-92ffaac1f637",
   "metadata": {},
   "source": [
    "### Ejemplo: Predicción de palabras en una oración\n",
    "\n",
    "Imaginemos que estamos entrenando una RNN para predecir la siguiente palabra en la frase \"El perro corre en el parque\". La red procesará cada palabra en secuencia y tratará de predecir la siguiente, utilizando el contexto previo acumulado en el estado oculto.\n",
    "\n",
    "Para simplificar, usaremos embeddings (representación numérica) para cada palabra y pesos ficticios.\n",
    "\n",
    "#### Paso a paso\n",
    "\n",
    "1. **Asignación de embeddings (representación numérica)**:\n",
    "   Asignamos un vector de dos dimensiones a cada palabra:\n",
    "   \n",
    "   - \"El\" = [0.1, 0.3]\n",
    "   - \"perro\" = [0.4, 0.2]\n",
    "   - \"corre\" = [0.5, 0.6]\n",
    "   - \"en\" = [0.2, 0.1]\n",
    "   - \"el\" = [0.1, 0.3]\n",
    "   - \"parque\" = [0.7, 0.8]\n",
    "\n",
    "2. **Predicción en el primer paso de tiempo ($t = 1$)**:\n",
    "   \n",
    "   - **Entrada**: La palabra \"El\" representada como el vector `[0.1, 0.3]`.\n",
    "   - **Capa oculta 1**: La entrada se multiplica por el peso $W_1$ para generar el primer vector oculto.\n",
    "   \n",
    "     Supongamos que:\n",
    "     $$\n",
    "     W_1 = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix}\n",
    "     $$\n",
    "     Entonces:\n",
    "     $$\n",
    "     \\text{Hidden Vector} (t=1) = W_1 \\cdot \\text{Input} (t=1) = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.1 \\\\ 0.3 \\end{bmatrix}\n",
    "     $$\n",
    "   \n",
    "     Calculamos cada componente:\n",
    "     $$\n",
    "     = \\begin{bmatrix} (0.2 \\cdot 0.1) + (0.5 \\cdot 0.3) \\\\ (0.3 \\cdot 0.1) + (0.4 \\cdot 0.3) \\end{bmatrix} = \\begin{bmatrix} 0.17 \\\\ 0.15 \\end{bmatrix}\n",
    "     $$\n",
    "   \n",
    "   - **Predicción**: El vector oculto generado se multiplica por un peso $W_{n+1}$ para predecir la siguiente palabra. Supongamos que $W_{n+1} = \\begin{bmatrix} 1.0 & 0.8 \\end{bmatrix}$.\n",
    "     \n",
    "     $$\n",
    "     \\text{Prediction Vector} (t=1) = W_{n+1} \\cdot \\text{Hidden Vector} (t=1) = \\begin{bmatrix} 1.0 & 0.8 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.17 \\\\ 0.15 \\end{bmatrix}\n",
    "     $$\n",
    "     $$\n",
    "     = (1.0 \\cdot 0.17) + (0.8 \\cdot 0.15) = 0.29\n",
    "     $$\n",
    "\n",
    "   En el entrenamiento, este valor se compararía con el embedding de la palabra \"perro\", y los pesos de la red se ajustarían para mejorar la precisión.\n",
    "\n",
    "3. **Predicción en el segundo paso de tiempo ($t = 2$)**:\n",
    "\n",
    "   - **Entrada**: La siguiente palabra, \"perro\" (vector `[0.4, 0.2]`).\n",
    "   - La salida del primer paso de tiempo también se usa como parte del estado oculto actual, ayudando a la red a \"recordar\" el contexto.\n",
    "\n",
    "   - **Capa oculta 2**:\n",
    "     Utilizamos el mismo peso $W_1$:\n",
    "     $$\n",
    "     \\text{Hidden Vector} (t=2) = W_1 \\cdot \\text{Input} (t=2) = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.4 \\\\ 0.2 \\end{bmatrix}\n",
    "     $$\n",
    "   \n",
    "     Calculamos cada componente:\n",
    "     $$\n",
    "     = \\begin{bmatrix} (0.2 \\cdot 0.4) + (0.5 \\cdot 0.2) \\\\ (0.3 \\cdot 0.4) + (0.4 \\cdot 0.2) \\end{bmatrix} = \\begin{bmatrix} 0.18 \\\\ 0.2 \\end{bmatrix}\n",
    "     $$\n",
    "   \n",
    "   - **Predicción**: Esta salida se multiplica por $W_{n+1}$ para predecir la próxima palabra.\n",
    "     \n",
    "     $$\n",
    "     \\text{Prediction Vector} (t=2) = W_{n+1} \\cdot \\text{Hidden Vector} (t=2) = \\begin{bmatrix} 1.0 & 0.8 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.18 \\\\ 0.2 \\end{bmatrix}\n",
    "     $$\n",
    "     $$\n",
    "     = (1.0 \\cdot 0.18) + (0.8 \\cdot 0.2) = 0.34\n",
    "     $$\n",
    "\n",
    "\n",
    "\n",
    "Este mismo proceso se repite para las palabras \"corre\", \"en\", \"el\" y \"parque\", utilizando el contexto acumulado en el estado oculto. La red ajusta sus pesos durante el entrenamiento para minimizar la diferencia entre sus predicciones y los embeddings reales de las palabras siguientes.\n",
    "\n",
    "4. **Predicción en el tercer paso de tiempo: $t = 3$**\n",
    "\n",
    "   **Entrada**: La tercera palabra en la secuencia es \"corre\", representada por su embedding $[0.5, 0.6]$.\n",
    "\n",
    "   **Capa oculta en $t = 3$**:\n",
    "   Usamos el mismo peso $W_1$ para calcular el nuevo vector oculto basado en la entrada actual.\n",
    "\n",
    "   $$\n",
    "   \\text{Hidden Vector} (t=3) = W_1 \\cdot \\text{Input} (t=3) = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 \\\\ 0.6 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.5) + (0.5 \\cdot 0.6) \\\\ (0.3 \\cdot 0.5) + (0.4 \\cdot 0.6) \\end{bmatrix} = \\begin{bmatrix} 0.37 \\\\ 0.39 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   **Predicción en $t = 3$**:\n",
    "   Esta salida pasa a través del peso $W_{n+1}$ para predecir la siguiente palabra.\n",
    "\n",
    "   $$\n",
    "   \\text{Prediction Vector} (t=3) = W_{n+1} \\cdot \\text{Hidden Vector} (t=3) = \\begin{bmatrix} 1.0 & 0.8 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.37 \\\\ 0.39 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   = (1.0 \\cdot 0.37) + (0.8 \\cdot 0.39) = 0.686\n",
    "   $$\n",
    "\n",
    "   La red intenta predecir la palabra \"en\" en función del contexto proporcionado por las palabras anteriores (\"El perro corre\").\n",
    "\n",
    "\n",
    "5. **Predicción en el cuarto paso de tiempo : $t = 4$**\n",
    "\n",
    "   **Entrada**: La cuarta palabra es \"en\", representada por su embedding $[0.2, 0.1]$.\n",
    "\n",
    "   **Capa oculta en  $t = 4$**:\n",
    "   Calculamos el nuevo estado oculto usando $W_1$ nuevamente.\n",
    "\n",
    "   $$\n",
    "   \\text{Hidden Vector} (t=4) = W_1 \\cdot \\text{Input} (t=4) = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.2 \\\\ 0.1 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.2) + (0.5 \\cdot 0.1) \\\\ (0.3 \\cdot 0.2) + (0.4 \\cdot 0.1) \\end{bmatrix} = \\begin{bmatrix} 0.09 \\\\ 0.1 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   **Predicción en $t = 4$**:\n",
    "   La predicción de la próxima palabra se hace de la misma forma, usando $W_{n+1}$.\n",
    "\n",
    "   $$\n",
    "   \\text{Prediction Vector} (t=4) = W_{n+1} \\cdot \\text{Hidden Vector} (t=4) = \\begin{bmatrix} 1.0 & 0.8 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.09 \\\\ 0.1 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   = (1.0 \\cdot 0.09) + (0.8 \\cdot 0.1) = 0.17\n",
    "   $$\n",
    "\n",
    "   La red intenta predecir la palabra \"el\".\n",
    "\n",
    "\n",
    "6. **Predicción en el quinto paso de tiempo: $t = 5$**\n",
    "\n",
    "   **Entrada**: La quinta palabra en la secuencia es \"el\", representada por su embedding $[0.1, 0.3]$.\n",
    "\n",
    "   **Capa oculta en $t = 5$**:\n",
    "   Calculamos el estado oculto.\n",
    "\n",
    "   $$\n",
    "   \\text{Hidden Vector} (t=5) = W_1 \\cdot \\text{Input} (t=5) = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.1 \\\\ 0.3 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.1) + (0.5 \\cdot 0.3) \\\\ (0.3 \\cdot 0.1) + (0.4 \\cdot 0.3) \\end{bmatrix} = \\begin{bmatrix} 0.17 \\\\ 0.15 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Predicción en $t = 5$**:\n",
    "   Usamos $W_{n+1}$ para predecir la siguiente palabra, que debería ser \"parque\".\n",
    "\n",
    "   $$\n",
    "   \\text{Prediction Vector} (t=5) = W_{n+1} \\cdot \\text{Hidden Vector} (t=5) = \\begin{bmatrix} 1.0 & 0.8 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.17 \\\\ 0.15 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   = (1.0 \\cdot 0.17) + (0.8 \\cdot 0.15) = 0.29\n",
    "   $$\n",
    "\n",
    "\n",
    "7. **Predicción en el sexto paso de tiempo: $t = 6$**\n",
    "\n",
    "   **Entrada**: La última palabra en la oración es \"parque\", representada por el vector $[0.7, 0.8]$.\n",
    "\n",
    "   **Capa oculta en $t = 6$**:\n",
    "   Calculamos el estado oculto.\n",
    "\n",
    "   $$\n",
    "   \\text{Hidden Vector} (t=6) = W_1 \\cdot \\text{Input} (t=6) = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.7 \\\\ 0.8 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.7) + (0.5 \\cdot 0.8) \\\\ (0.3 \\cdot 0.7) + (0.4 \\cdot 0.8) \\end{bmatrix} = \\begin{bmatrix} 0.66 \\\\ 0.55 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   **Predicción en $t = 6$**:\n",
    "   Esta salida pasa por $W_{n+1}$ para generar una predicción final.\n",
    "\n",
    "   $$\n",
    "   \\text{Prediction Vector} (t=6) = W_{n+1} \\cdot \\text{Hidden Vector} (t=6) = \\begin{bmatrix} 1.0 & 0.8 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.66 \\\\ 0.55 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   = (1.0 \\cdot 0.66) + (0.8 \\cdot 0.55) = 1.1\n",
    "   $$\n",
    "\n",
    "   Este último paso de tiempo no requiere predecir otra palabra, ya que la oración termina.\n",
    "\n",
    "\n",
    "#### Predicción continua\n",
    "\n",
    "A medida que la red avanza en la secuencia:\n",
    "- En cada paso $t$, la RNN utiliza la entrada de la palabra actual y el estado oculto del paso anterior para generar el siguiente vector oculto.\n",
    "- La salida de cada paso se utiliza para predecir la próxima palabra.\n",
    "- La red mejora su capacidad de predicción al aprender el contexto y ajustarse a las secuencias de palabras observadas en el entrenamiento.\n",
    "\n",
    "Este procedimiento permite que la RNN use tanto el contexto pasado como los pesos entrenados para predecir palabras en una secuencia de texto, haciendo de las RNN una herramienta poderosa para tareas como el modelado de lenguaje y la generación de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafe890-3ecf-4e94-baed-9f90ccc9ab37",
   "metadata": {},
   "source": [
    "### RNN con dos capas ocultas\n",
    "\n",
    "Aquí cada capa tiene una secuencia de nodos (RNNNodes) conectados a lo largo de una longitud de secuencia en el tiempo. \n",
    "\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Supongamos que estamos procesando una secuencia de datos de entrada en mini-lotes (batches). Cada dato de entrada tiene un tamaño de característica de dos valores (como dos números por cada paso en la secuencia). Usaremos una RNN con:\n",
    "\n",
    "- **Entrada**: Tamaño de batch de 1, longitud de secuencia de 5 pasos de tiempo, y tamaño de característica de 2.\n",
    "- **Capa oculta 1**: 5 nodos, denominados `1, 3, 5, 7, 9`.\n",
    "- **Capa oculta 2**: 5 nodos, denominados `2, 4, 6, 8, 10`.\n",
    "- **Output Array**: Matriz de salida final con el tamaño `[batch_size, sequence_length, output_size]`.\n",
    "\n",
    "Para simplificar, asume que la RNN utiliza una función de activación simple y pesos ficticios.\n",
    "\n",
    "#### Paso 1: Definir la entrada y pesos\n",
    "\n",
    "1. **Input Array**: \n",
    "   Supongamos que la secuencia de entrada (Input array) tiene 5 pasos de tiempo, cada uno con un vector de características de tamaño 2. La entrada sería:\n",
    "\n",
    "   $$\n",
    "   \\text{Input Array} = \\begin{bmatrix} [0.5, 0.1], [0.3, 0.2], [0.4, 0.3], [0.6, 0.5], [0.7, 0.6] \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Pesos en la capa oculta 1**:\n",
    "   Supongamos que cada nodo de la primera capa tiene un conjunto de pesos $W_{h1}$ que conecta la entrada de un paso de tiempo a cada nodo de la capa oculta. Por ejemplo, asumamos:\n",
    "   $$\n",
    "   W_{h1} = \\begin{bmatrix} 0.2 & 0.4 \\\\ 0.3 & 0.5 \\end{bmatrix}\n",
    "   $$\n",
    "   \n",
    "3. **Pesos en la capa oculta 2**:\n",
    "   Cada nodo de la segunda capa tiene pesos $W_{h2}$ que conectan la salida de la primera capa a la segunda capa. Supongamos:\n",
    "   $$\n",
    "   W_{h2} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "4. **Pesos de salida**:\n",
    "   Cada nodo de la segunda capa produce una salida que se almacena en el array de salida.\n",
    "\n",
    "#### Paso 2: Cálculo en cada paso de tiempo\n",
    "\n",
    "En cada paso de tiempo, calcularemos la salida de cada nodo en la capa oculta 1, que luego pasa a la capa oculta 2.\n",
    "\n",
    "**Paso de tiempo 1 (t=1)**\n",
    "\n",
    "1. **Capa oculta 1**:\n",
    "   La entrada en el primer paso de tiempo es `[0.5, 0.1]`. Calculamos la salida del primer nodo en la capa oculta 1 (nodo 1):\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 1 (t=1)} = W_{h1} \\cdot \\text{Input} = \\begin{bmatrix} 0.2 & 0.4 \\\\ 0.3 & 0.5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 \\\\ 0.1 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.5) + (0.4 \\cdot 0.1) \\\\ (0.3 \\cdot 0.5) + (0.5 \\cdot 0.1) \\end{bmatrix} = \\begin{bmatrix} 0.1 + 0.04 \\\\ 0.15 + 0.05 \\end{bmatrix} = \\begin{bmatrix} 0.14 \\\\ 0.2 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Aplicamos una función de activación (por ejemplo, ReLU) a cada salida.\n",
    "\n",
    "2. **Capa oculta 2**:\n",
    "   La salida del nodo 1 (vector `[0.14, 0.2]`) se pasa como entrada al nodo 2 en la capa oculta 2:\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 2 (t=1)} = W_{h2} \\cdot \\text{Nodo 1 (t=1)} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.14 \\\\ 0.2 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.6 \\cdot 0.14) + (0.7 \\cdot 0.2) \\\\ (0.5 \\cdot 0.14) + (0.4 \\cdot 0.2) \\end{bmatrix} = \\begin{bmatrix} 0.084 + 0.14 \\\\ 0.07 + 0.08 \\end{bmatrix} = \\begin{bmatrix} 0.224 \\\\ 0.15 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Esta salida se almacena en el array de salida en el primer índice.\n",
    "\n",
    "**Paso de tiempo 2 (t=2)**\n",
    "\n",
    "1. **Capa Oculta 1**:\n",
    "   La entrada en el segundo paso de tiempo es `[0.3, 0.2]`. Calculamos la salida del nodo 3 en la capa oculta 1:\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 3 (t=2)} = W_{h1} \\cdot \\text{Input} = \\begin{bmatrix} 0.2 & 0.4 \\\\ 0.3 & 0.5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.3 \\\\ 0.2 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.3) + (0.4 \\cdot 0.2) \\\\ (0.3 \\cdot 0.3) + (0.5 \\cdot 0.2) \\end{bmatrix} = \\begin{bmatrix} 0.06 + 0.08 \\\\ 0.09 + 0.1 \\end{bmatrix} = \\begin{bmatrix} 0.14 \\\\ 0.19 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Capa oculta 2**:\n",
    "   La salida del nodo 3 (vector `[0.14, 0.19]`) se pasa como entrada al nodo 4 en la capa oculta 2:\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 4 (t=2)} = W_{h2} \\cdot \\text{Nodo 3 (t=2)} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.14 \\\\ 0.19 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.6 \\cdot 0.14) + (0.7 \\cdot 0.19) \\\\ (0.5 \\cdot 0.14) + (0.4 \\cdot 0.19) \\end{bmatrix} = \\begin{bmatrix} 0.084 + 0.133 \\\\ 0.07 + 0.076 \\end{bmatrix} = \\begin{bmatrix} 0.217 \\\\ 0.146 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Esta salida se almacena en el array de salida en el segundo índice.\n",
    "\n",
    "\n",
    "**Paso de tiempo 3 ($t = 3$)**\n",
    "\n",
    "1. **Entrada**: La entrada en $t = 3$ es el vector $[0.4, 0.3]$.\n",
    "\n",
    "2. **Cálculo en la capa oculta 1 (Nodo 5 en $t = 3$)**:\n",
    "   La entrada pasa por la capa oculta 1 usando los pesos $W_{h1}$.\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 5 (t=3)} = W_{h1} \\cdot \\text{Input} = \\begin{bmatrix} 0.2 & 0.4 \\\\ 0.3 & 0.5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.4 \\\\ 0.3 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.4) + (0.4 \\cdot 0.3) \\\\ (0.3 \\cdot 0.4) + (0.5 \\cdot 0.3) \\end{bmatrix} = \\begin{bmatrix} 0.08 + 0.12 \\\\ 0.12 + 0.15 \\end{bmatrix} = \\begin{bmatrix} 0.2 \\\\ 0.27 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Cálculo en la capa oculta 2 (Nodo 6 en $t = 3$)**:\n",
    "   La salida del nodo 5, $[0.2, 0.27]$, pasa a la capa oculta 2 (nodo 6).\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 6 (t=3)} = W_{h2} \\cdot \\text{Nodo 5 (t=3)} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.2 \\\\ 0.27 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.6 \\cdot 0.2) + (0.7 \\cdot 0.27) \\\\ (0.5 \\cdot 0.2) + (0.4 \\cdot 0.27) \\end{bmatrix} = \\begin{bmatrix} 0.12 + 0.189 \\\\ 0.1 + 0.108 \\end{bmatrix} = \\begin{bmatrix} 0.309 \\\\ 0.208 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   La salida $[0.309, 0.208]$ se almacena en el array de salida para $t = 3$.\n",
    "\n",
    "\n",
    "**Paso de tiempo 4 ($t = 4$)**\n",
    "\n",
    "1. **Entrada**: La entrada en $t = 4$ es el vector $[0.6, 0.5]$.\n",
    "\n",
    "2. **Cálculo en la capa oculta 1 (Nodo 7 en $t = 4$)**:\n",
    "   La entrada pasa por la capa oculta 1.\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 7 (t=4)} = W_{h1} \\cdot \\text{Input} = \\begin{bmatrix} 0.2 & 0.4 \\\\ 0.3 & 0.5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.6 \\\\ 0.5 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.6) + (0.4 \\cdot 0.5) \\\\ (0.3 \\cdot 0.6) + (0.5 \\cdot 0.5) \\end{bmatrix} = \\begin{bmatrix} 0.12 + 0.2 \\\\ 0.18 + 0.25 \\end{bmatrix} = \\begin{bmatrix} 0.32 \\\\ 0.43 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Cálculo en la capa oculta 2 (Nodo 8 en $t = 4$)**:\n",
    "   La salida del nodo 7, $[0.32, 0.43]$, pasa a la capa oculta 2 (nodo 8).\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 8 (t=4)} = W_{h2} \\cdot \\text{Nodo 7 (t=4)} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.32 \\\\ 0.43 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.6 \\cdot 0.32) + (0.7 \\cdot 0.43) \\\\ (0.5 \\cdot 0.32) + (0.4 \\cdot 0.43) \\end{bmatrix} = \\begin{bmatrix} 0.192 + 0.301 \\\\ 0.16 + 0.172 \\end{bmatrix} = \\begin{bmatrix} 0.493 \\\\ 0.332 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   La salida $[0.493, 0.332]$ se almacena en el array de salida para $t = 4$.\n",
    "\n",
    "\n",
    "**Paso de tiempo 5 ($t = 5$)**\n",
    "\n",
    "1. **Entrada**: La entrada en $t = 5$ es el vector $[0.7, 0.6]$.\n",
    "\n",
    "2. **Cálculo en la capa oculta 1 (Nodo 9 en $t = 5$)**:\n",
    "   La entrada pasa por la capa oculta 1.\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 9 (t=5)} = W_{h1} \\cdot \\text{Input} = \\begin{bmatrix} 0.2 & 0.4 \\\\ 0.3 & 0.5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.7 \\\\ 0.6 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.7) + (0.4 \\cdot 0.6) \\\\ (0.3 \\cdot 0.7) + (0.5 \\cdot 0.6) \\end{bmatrix} = \\begin{bmatrix} 0.14 + 0.24 \\\\ 0.21 + 0.3 \\end{bmatrix} = \\begin{bmatrix} 0.38 \\\\ 0.51 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Cálculo en la capa oculta 2 (Nodo 10 en $t = 5$)**:\n",
    "   La salida del nodo 9, $[0.38, 0.51]$, pasa a la capa oculta 2 (nodo 10).\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 10 (t=5)} = W_{h2} \\cdot \\text{Nodo 9 (t=5)} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.38 \\\\ 0.51 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.6 \\cdot 0.38) + (0.7 \\cdot 0.51) \\\\ (0.5 \\cdot 0.38) + (0.4 \\cdot 0.51) \\end{bmatrix} = \\begin{bmatrix} 0.228 + 0.357 \\\\ 0.19 + 0.204 \\end{bmatrix} = \\begin{bmatrix} 0.585 \\\\ 0.394 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   La salida $[0.585, 0.394]$ se almacena en el array de salida para $t = 5$.\n",
    "\n",
    "\n",
    "##### Resultado final en el array de salida\n",
    "\n",
    "Después de completar todos los pasos de tiempo, el array de salida final será:\n",
    "\n",
    "$$\n",
    "\\text{Output Array} = \\begin{bmatrix} [0.224, 0.15], [0.217, 0.146], [0.309, 0.208], [0.493, 0.332], [0.585, 0.394] \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Este array contiene las salidas de la capa oculta 2 en cada paso de tiempo, lo que representa la transformación de la secuencia de entrada a lo largo de la red. Este resultado puede ser utilizado para tareas de predicción o clasificación en secuencias, proporcionando un resumen de la información procesada en cada paso de la secuencia.\n",
    "\n",
    "\n",
    "Este proceso ilustra cómo una RNN con múltiples capas ocultas procesa secuencias en pasos de tiempo y permite mantener un contexto a través de la propagación de estados ocultos en cada capa y cada paso de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe52c4-4ceb-496f-8767-92bb9e3a873e",
   "metadata": {},
   "source": [
    "### Ejemplo: Traducción automática de inglés a español\n",
    "\n",
    "Supongamos que queremos traducir una frase en inglés, como \"The dog runs\", a su equivalente en español \"El perro corre\". \n",
    "\n",
    "#### 1. Representación de las palabras en la entrada y en la salida\n",
    "\n",
    "Para que la RNN pueda procesar palabras, necesitamos convertir cada palabra en un vector numérico llamado *embedding*. \n",
    "\n",
    "- Embeddings en inglés:\n",
    "  - \"The\" = $[0.1, 0.2]$\n",
    "  - \"dog\" = $[0.3, 0.5]$\n",
    "  - \"runs\" = $[0.6, 0.7]$\n",
    "\n",
    "- Embeddings en español (con los valores que el modelo debe aprender a producir para cada palabra traducida):\n",
    "  - \"El\" = $[0.2, 0.1]$\n",
    "  - \"perro\" = $[0.5, 0.3]$\n",
    "  - \"corre\" = $[0.7, 0.6]$\n",
    "\n",
    "#### 2. Arquitectura de la RNN para traducción\n",
    "\n",
    "Imaginemos que esta RNN es parte de un modelo secuencia a secuencia (seq2seq), donde tenemos dos partes:\n",
    "- **Codificador (encoder)**: Procesa la frase en inglés y genera un estado oculto que representa la oración.\n",
    "- **Decodificador (decoder)**: Usa este estado oculto para generar la traducción en español.\n",
    "\n",
    "Para este ejemplo, supongamos que tenemos una RNN con dos capas ocultas en el codificador y en el decodificador, como se muestra en la figura.\n",
    "\n",
    "#### Paso a Paso en la traducción\n",
    "\n",
    "1. **Codificación de la oración en inglés (\"The dog runs\")**\n",
    "\n",
    "   La entrada \"The dog runs\" se procesa palabra por palabra. A continuación se muestra cómo sería el flujo de la RNN para esta frase en inglés:\n",
    "\n",
    "   - **Paso de tiempo 1 (t=1)**:\n",
    "     - **Entrada**: \"The\" = $[0.1, 0.2]$\n",
    "     - La entrada pasa a través de la capa oculta 1 y luego la capa oculta 2, generando un vector de estado oculto que captura el contexto de \"The\".\n",
    "     - **Estado oculto final para t=1**: $[0.224, 0.15]$ (como se calculó previamente).\n",
    "\n",
    "   - **Paso de tiempo 2 (t=2)**:\n",
    "     - **Entrada**: \"dog\" = $[0.3, 0.5]$\n",
    "     - La palabra \"dog\" pasa por las capas ocultas, y el estado oculto se actualiza para reflejar \"The dog\".\n",
    "     - **Estado oculto final para t=2**: $[0.217, 0.146]$ (como se calculó previamente).\n",
    "\n",
    "   - **Paso de tiempo 3 (t=3)**:\n",
    "     - **Entrada**: \"runs\" = $[0.6, 0.7]$\n",
    "     - La palabra \"runs\" pasa por las capas ocultas, y el estado oculto final se actualiza para reflejar toda la oración \"The dog runs\".\n",
    "     - **Estado oculto final para t=3**: $[0.309, 0.208]$ (como se calculó previamente).\n",
    "\n",
    "   Al final de este paso, el codificador produce un vector de estado oculto que resume la oración en inglés. Este vector se pasa al decodificador para iniciar la generación de la traducción en español.\n",
    "\n",
    "2. **Decodificación de la oración en español (\"El perro corre\")**\n",
    "\n",
    "   Ahora, el decodificador toma el estado oculto final del codificador como entrada inicial para generar la traducción en español. Para simplificar, supondremos que el decodificador también sigue el mismo flujo con los embeddings en español que establecimos antes.\n",
    "\n",
    "   - **Paso de tiempo 1 (t=1)**:\n",
    "     - **Entrada**: Estado oculto del codificador que representa \"The dog runs\" = $[0.309, 0.208]$.\n",
    "     - Este vector inicial pasa por la primera capa oculta del decodificador y luego por la segunda capa para generar el embedding de la primera palabra en español, \"El\".\n",
    "     - **Predicción de t=1**: $[0.2, 0.1]$, que corresponde a \"El\".\n",
    "\n",
    "   - **Paso de tiempo 2 (t=2)**:\n",
    "     - **Entrada**: La predicción anterior, \"El\" = $[0.2, 0.1]$, se utiliza como entrada para el siguiente paso del decodificador.\n",
    "     - Este vector pasa por las capas ocultas para generar el embedding de la siguiente palabra, \"perro\".\n",
    "     - **Predicción de t=2**: $[0.5, 0.3]$, que corresponde a \"perro\".\n",
    "\n",
    "   - **Paso de tiempo 3 (t=3)**:\n",
    "     - **Entrada**: La predicción anterior, \"perro\" = $[0.5, 0.3]$, se utiliza como entrada para el tercer paso del decodificador.\n",
    "     - Este vector pasa por las capas ocultas para generar el embedding de la última palabra, \"corre\".\n",
    "     - **Predicción de t=3**: $[0.7, 0.6]$, que corresponde a \"corre\".\n",
    "\n",
    "   Al final de la decodificación, el modelo ha producido la secuencia en español \"El perro corre\".\n",
    "\n",
    "\n",
    "Este ejemplo demuestra cómo una RNN puede capturar el contexto de una oración completa en un estado oculto (en el codificador) y luego utilizar ese contexto para generar una secuencia equivalente en otro idioma (en el decodificador). Aunque las RNN fueron pioneras en el campo de la traducción automática, las arquitecturas modernas como los Transformers han reemplazado a las RNN en esta tarea debido a su capacidad para capturar dependencias de largo plazo de manera más eficiente. \n",
    "\n",
    "Sin embargo, la RNN sigue siendo una buena introducción al concepto de secuencia a secuencia (seq2seq), que es fundamental para muchas aplicaciones de NLP, incluida la traducción automática.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f5fb3-db91-44c5-a6be-1c67fe2c3d3e",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "En este ejemplo usaremos la red neuronal recurrente (RNN) de dos capas en la que cada capa procesa secuencialmente los elementos de una secuencia de entrada. Esta estructura se utiliza típicamente para procesar secuencias de datos, como en el procesamiento de lenguaje natural, donde cada paso en el tiempo puede representar una palabra o un elemento de una secuencia.\n",
    "\n",
    "El ejemplo procesa una secuencia de entrada en cinco pasos de tiempo, utilizando una RNN con dos capas ocultas. Esto implica que:\n",
    "\n",
    "- La entrada tiene una dimensión de características definida (llamada `feature_size`).\n",
    "- Cada capa oculta tiene una dimensión especificada por el parámetro `hidden_size`.\n",
    "- La salida final se almacena en un `Output Array`.\n",
    "\n",
    "Para simplificar el cálculo, se utilizará valores ficticios y no se aplicará funciones de activación en este ejemplo, aunque en una implementación real se usaría una activación no lineal (como ReLU o tanh) para añadir complejidad a la red.\n",
    "\n",
    "#### Configuración de la entrada y pesos\n",
    "\n",
    "1. **Dimensiones de la entrada**:\n",
    "   - Tamaño de batch: 1 (procesaremos una secuencia a la vez).\n",
    "   - Longitud de secuencia: 5 pasos de tiempo.\n",
    "   - Tamaño de característica (feature size): 2.\n",
    "\n",
    "   Supongamos que la secuencia de entrada es:\n",
    "   $$\n",
    "   \\text{Input Array} = \\begin{bmatrix} [0.1, 0.3], [0.2, 0.4], [0.3, 0.5], [0.4, 0.6], [0.5, 0.7] \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Pesos en la capa oculta 1**:\n",
    "   Los nodos en la primera capa oculta tienen un conjunto de pesos $ W_{h1}$ que conectan la entrada a los nodos de esta capa. Supongamos:\n",
    "   $$\n",
    "   W_{h1} = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Pesos en la capa oculta 2**:\n",
    "   Los nodos en la segunda capa oculta tienen un conjunto de pesos $ W_{h2} $ que conectan la salida de la primera capa a la segunda capa. Supongamos:\n",
    "   $$\n",
    "   W_{h2} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "4. **Pesos de salida**:\n",
    "   Cada nodo en la segunda capa produce una salida que se almacena en el array de salida, el cual tiene el tamaño `[batch_size, sequence_length, output_size]`.\n",
    "\n",
    "#### Paso a paso por cada paso de tiempo\n",
    "\n",
    "A continuación, procesaremos la entrada a través de la red en cada paso de tiempo, calculando la salida de cada nodo en ambas capas ocultas y guardando la salida de la segunda capa en el array de salida.\n",
    "\n",
    "##### Paso de tiempo 1 ($t = 1$)\n",
    "\n",
    "1. **Entrada**: La entrada en el primer paso de tiempo es el vector $[0.1, 0.3]$.\n",
    "\n",
    "2. **Cálculo en la capa oculta 1 (Nodo 1)**:\n",
    "   La entrada pasa por la capa oculta 1 usando los pesos $ W_{h1} $.\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 1 (t=1)} = W_{h1} \\cdot \\text{Input} = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.1 \\\\ 0.3 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.1) + (0.5 \\cdot 0.3) \\\\ (0.3 \\cdot 0.1) + (0.4 \\cdot 0.3) \\end{bmatrix} = \\begin{bmatrix} 0.1 + 0.15 \\\\ 0.03 + 0.12 \\end{bmatrix} = \\begin{bmatrix} 0.17 \\\\ 0.15 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Cálculo en la capa oculta 2 (Nodo 6)**:\n",
    "   La salida del nodo 1 $[0.17, 0.15]$ se pasa al nodo 6 en la capa oculta 2.\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 6 (t=1)} = W_{h2} \\cdot \\text{Nodo 1 (t=1)} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.17 \\\\ 0.15 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.6 \\cdot 0.17) + (0.7 \\cdot 0.15) \\\\ (0.5 \\cdot 0.17) + (0.4 \\cdot 0.15) \\end{bmatrix} = \\begin{bmatrix} 0.102 + 0.105 \\\\ 0.085 + 0.06 \\end{bmatrix} = \\begin{bmatrix} 0.207 \\\\ 0.145 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Esta salida $[0.207, 0.145]$ se almacena en el array de salida para $ t = 1 $.\n",
    "\n",
    "\n",
    "#### Paso de tiempo 2 ($t = 2$)\n",
    "\n",
    "1. **Entrada**: La entrada en $ t = 2 $ es $[0.2, 0.4]$.\n",
    "\n",
    "2. **Cálculo en la capa oculta 1 (Nodo 2)**:\n",
    "   $$\n",
    "   \\text{Nodo 2 (t=2)} = W_{h1} \\cdot \\text{Input} = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.2 \\\\ 0.4 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.2) + (0.5 \\cdot 0.4) \\\\ (0.3 \\cdot 0.2) + (0.4 \\cdot 0.4) \\end{bmatrix} = \\begin{bmatrix} 0.04 + 0.2 \\\\ 0.06 + 0.16 \\end{bmatrix} = \\begin{bmatrix} 0.24 \\\\ 0.22 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Cálculo en la capa oculta 2 (Nodo 7)**:\n",
    "   $$\n",
    "   \\text{Nodo 7 (t=2)} = W_{h2} \\cdot \\text{Nodo 2 (t=2)} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.24 \\\\ 0.22 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.6 \\cdot 0.24) + (0.7 \\cdot 0.22) \\\\ (0.5 \\cdot 0.24) + (0.4 \\cdot 0.22) \\end{bmatrix} = \\begin{bmatrix} 0.144 + 0.154 \\\\ 0.12 + 0.088 \\end{bmatrix} = \\begin{bmatrix} 0.298 \\\\ 0.208 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   La salida $[0.298, 0.208]$ se almacena en el array de salida para $ t = 2 $.\n",
    "\n",
    "\n",
    "#### Paso de tiempo 3 ($t = 3$)\n",
    "\n",
    "1. **Entrada**: La entrada en $ t = 3 $ es $[0.3, 0.5]$.\n",
    "\n",
    "2. **Cálculo en la capa oculta 1 (Nodo 3)**:\n",
    "   $$\n",
    "   \\text{Nodo 3 (t=3)} = W_{h1} \\cdot \\text{Input} = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.3 \\\\ 0.5 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.3) + (0.5 \\cdot 0.5) \\\\ (0.3 \\cdot 0.3) + (0.4 \\cdot 0.5) \\end{bmatrix} = \\begin{bmatrix} 0.06 + 0.25 \\\\ 0.09 + 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.31 \\\\ 0.29 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Cálculo en la capa oculta 2 (Nodo 8)**:\n",
    "   $$\n",
    "   \\text{Nodo 8 (t=3)} = W_{h2} \\cdot \\text{Nodo 3 (t=3)} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.31 \\\\ 0.29 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.6 \\cdot 0.31) + (0.7 \\cdot 0.29) \\\\ (0.5 \\cdot 0.31) + (0.4 \\cdot 0.29) \\end{bmatrix} = \\begin{bmatrix} 0.186 + 0.203 \\\\ 0.155 + 0.116 \\end{bmatrix} = \\begin{bmatrix} 0.389 \\\\ 0.271 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   La salida $[0.389, 0.271]$ se almacena en el array de salida para $ t = 3 $.\n",
    "\n",
    "#### Paso de tiempo 4 ($t = 4$)\n",
    "\n",
    "1. **Entrada**: La entrada en $ t = 4 $ es el vector $[0.4, 0.6]$.\n",
    "\n",
    "2. **Cálculo en la capa oculta 1 (Nodo 4)**:\n",
    "   La entrada pasa a través de la primera capa oculta usando los pesos $ W_{h1} $.\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 4 (t=4)} = W_{h1} \\cdot \\text{Input} = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.4 \\\\ 0.6 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.4) + (0.5 \\cdot 0.6) \\\\ (0.3 \\cdot 0.4) + (0.4 \\cdot 0.6) \\end{bmatrix} = \\begin{bmatrix} 0.08 + 0.3 \\\\ 0.12 + 0.24 \\end{bmatrix} = \\begin{bmatrix} 0.38 \\\\ 0.36 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Cálculo en la capa oculta 2 (Nodo 9)**:\n",
    "   La salida del nodo 4, $[0.38, 0.36]$, se pasa al nodo 9 en la capa oculta 2.\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 9 (t=4)} = W_{h2} \\cdot \\text{Nodo 4 (t=4)} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.38 \\\\ 0.36 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.6 \\cdot 0.38) + (0.7 \\cdot 0.36) \\\\ (0.5 \\cdot 0.38) + (0.4 \\cdot 0.36) \\end{bmatrix} = \\begin{bmatrix} 0.228 + 0.252 \\\\ 0.19 + 0.144 \\end{bmatrix} = \\begin{bmatrix} 0.48 \\\\ 0.334 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Esta salida $[0.48, 0.334]$ se almacena en el array de salida para $ t = 4 $.\n",
    "\n",
    "\n",
    "#### Paso de tiempo 5 ($t = 5$)\n",
    "\n",
    "1. **Entrada**: La entrada en $ t = 5 $ es el vector $[0.5, 0.7]$.\n",
    "\n",
    "2. **Cálculo en la capa oculta 1 (Nodo 5)**:\n",
    "   La entrada pasa a través de la primera capa oculta usando los pesos $ W_{h1} $.\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 5 (t=5)} = W_{h1} \\cdot \\text{Input} = \\begin{bmatrix} 0.2 & 0.5 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 \\\\ 0.7 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.2 \\cdot 0.5) + (0.5 \\cdot 0.7) \\\\ (0.3 \\cdot 0.5) + (0.4 \\cdot 0.7) \\end{bmatrix} = \\begin{bmatrix} 0.1 + 0.35 \\\\ 0.15 + 0.28 \\end{bmatrix} = \\begin{bmatrix} 0.45 \\\\ 0.43 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Cálculo en la capa oculta 2 (Nodo 10)**:\n",
    "   La salida del nodo 5, $[0.45, 0.43]$, se pasa al nodo 10 en la capa oculta 2.\n",
    "\n",
    "   $$\n",
    "   \\text{Nodo 10 (t=5)} = W_{h2} \\cdot \\text{Nodo 5 (t=5)} = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.5 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.45 \\\\ 0.43 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   $$\n",
    "   = \\begin{bmatrix} (0.6 \\cdot 0.45) + (0.7 \\cdot 0.43) \\\\ (0.5 \\cdot 0.45) + (0.4 \\cdot 0.43) \\end{bmatrix} = \\begin{bmatrix} 0.27 + 0.301 \\\\ 0.225 + 0.172 \\end{bmatrix} = \\begin{bmatrix} 0.571 \\\\ 0.397 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Esta salida $[0.571, 0.397]$ se almacena en el array de salida para $ t = 5 $.\n",
    "\n",
    "\n",
    "#### Resultado final en el array de salida\n",
    "\n",
    "Después de completar todos los pasos de tiempo, el `Output Array` contiene las salidas generadas en cada paso de tiempo desde la segunda capa oculta. Estas salidas son las siguientes:\n",
    "\n",
    "$$\n",
    "\\text{Output Array} = \\begin{bmatrix} [0.207, 0.145], [0.298, 0.208], [0.389, 0.271], [0.48, 0.334], [0.571, 0.397] \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Este array representa la salida procesada a lo largo de los cinco pasos de tiempo y refleja cómo la secuencia de entrada ha sido transformada en una secuencia de salida a través de las capas ocultas de la RNN. \n",
    "\n",
    "Este resultado podría ser utilizado para tareas de secuencia a secuencia, como la clasificación de secuencias, traducción, o generación de secuencias de texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695c1d5-980a-478a-a165-f887120af8e1",
   "metadata": {},
   "source": [
    "### Retropropagación en redes neuronales recurrentes (RNN)\n",
    "\n",
    "Este proceso se denomina *retropropagación a través del tiempo* (Backpropagation Through Time, BPTT) en el contexto de RNNs, y se usa para ajustar los pesos de la red en función del error de salida a lo largo de una secuencia.\n",
    "\n",
    "Usaremos una RNN con dos capas ocultas, cada una compuesta por varios nodos. Para cada paso de tiempo de la secuencia, el modelo procesa una entrada y genera una salida. La retropropagación ajusta los pesos para minimizar la diferencia entre la salida generada y la salida esperada.\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Consideremos una RNN que recibe una secuencia de entrada y genera una secuencia de salida. Suponemos que estamos entrenando la red para predecir la siguiente palabra en una oración, dado el contexto de palabras anteriores. Para simplificar, asumimos una secuencia de 5 pasos de tiempo, y observamos cómo el error de salida en cada paso afecta a los nodos anteriores en el proceso de retropropagación.\n",
    "\n",
    "#### Configuración del ejemplo\n",
    "\n",
    "1. **Dimensiones**:\n",
    "   - **Entrada**: `[batch_size, sequence_length, feature_size]` – Suponemos una entrada con tamaño de batch = 1, longitud de secuencia = 5, y tamaño de característica = 2.\n",
    "   - **Hidden Size**: El tamaño de la capa oculta es 2 para ambas capas.\n",
    "\n",
    "2. **Estructura de las capas**:\n",
    "   - La RNN tiene dos capas ocultas, como se muestra en la figura. La primera capa oculta (nodos 1, 3, 5, 7, 9) procesa la entrada de cada paso de tiempo y pasa su salida a la segunda capa oculta (nodos 2, 4, 6, 8, 10), la cual genera la salida final.\n",
    "\n",
    "3. **Pesos iniciales**:\n",
    "   - **Pesos de la capa oculta 1** ($W_{h1}$)\n",
    "   - **Pesos de la capa oculta 2** ($W_{h2}$)\n",
    "   - **Pesos de salida** ($W_{o}$) para conectar la segunda capa oculta a la salida.\n",
    "\n",
    "4. **Error en la salida**:\n",
    "   - En cada paso de tiempo, se genera un error de salida (diferencia entre la salida generada y la esperada). Este error se retropropaga a través de la red para ajustar los pesos en todas las capas.\n",
    "\n",
    "#### Paso a paso: Retropropagación en la RNN\n",
    "\n",
    "La retropropagación en una RNN implica calcular gradientes de error en cada paso de tiempo y retropropagarlos a través de los nodos para actualizar los pesos. A continuación, se muestra el proceso paso a paso.\n",
    "\n",
    "#### Paso 1: Cálculo de la salida y error en cada paso de tiempo\n",
    "\n",
    "1. **Paso de tiempo 1 (t=1)**:\n",
    "   - La red recibe la primera entrada y genera una salida en el nodo 2 de la segunda capa oculta.\n",
    "   - La salida generada se compara con la salida esperada, y se calcula el **error**.\n",
    "   - Este error se almacena y se utilizará para calcular los gradientes de los pesos.\n",
    "\n",
    "2. **Paso de tiempo 2 (t=2)**:\n",
    "   - La red procesa la segunda entrada y genera una salida en el nodo 4 de la segunda capa oculta.\n",
    "   - Se calcula el **error de salida** para $ t=2 $ comparando la salida generada con la salida esperada.\n",
    "   - Este error se suma al error acumulado para cada peso de la red.\n",
    "\n",
    "3. **Pasos de tiempo 3 a 5 (t=3, t=4, t=5)**:\n",
    "   - El mismo proceso se repite para cada paso de tiempo restante.\n",
    "   - En cada paso, el error se calcula y se suma a los gradientes acumulados para cada peso.\n",
    "\n",
    "#### Paso 2: Retropropagación del error a través del tiempo (BPTT)\n",
    "\n",
    "Ahora que tenemos el error acumulado en cada paso de tiempo, procedemos con la retropropagación. La retropropagación a través del tiempo se hace desde el último paso hasta el primer paso de la secuencia.\n",
    "\n",
    "1. **Retropropagación en el último paso de tiempo ($ t=5 $)**:\n",
    "   - Comenzamos desde el último nodo en la segunda capa oculta (nodo 10) y retropropagamos el error hacia el nodo anterior (nodo 9) en la primera capa.\n",
    "   - Calculamos el gradiente de error respecto a los pesos de la segunda capa ($ W_{h2} $) usando la derivada del error en $ t=5 $.\n",
    "   - Ajustamos los pesos en función de este gradiente.\n",
    "\n",
    "2. **Retropropagación en el paso de tiempo 4 ($ t=4 $)**:\n",
    "   - Retropropagamos el error en el nodo 8 de la segunda capa oculta hacia el nodo 7 en la primera capa.\n",
    "   - Calculamos el gradiente de error respecto a $ W_{h2} $ y $ W_{h1} $ usando la derivada del error en $ t=4 $ y sumando los gradientes acumulados de $ t=5 $.\n",
    "   - Ajustamos los pesos en función del gradiente total acumulado hasta este paso.\n",
    "\n",
    "3. **Repetir la retropropagación para $ t=3, t=2 $, y $ t=1 $**:\n",
    "   - El mismo proceso se repite hacia atrás en el tiempo, propagando los errores acumulados a través de todos los nodos.\n",
    "   - En cada paso, los gradientes acumulados afectan a los pesos de la red.\n",
    "\n",
    "#### Paso 3: Actualización de pesos\n",
    "\n",
    "Después de calcular los gradientes para cada peso en cada paso de tiempo, ajustamos los pesos de la red en función de la tasa de aprendizaje ($ \\alpha $) y el gradiente acumulado:\n",
    "\n",
    "$$\n",
    "W = W - \\alpha \\cdot \\text{gradiente acumulado}\n",
    "$$\n",
    "\n",
    "1. **Ajuste de $W_{o}$**: Los pesos de salida se ajustan en función del error acumulado en cada paso de tiempo.\n",
    "2. **Ajuste de $W_{h2}$**: Los pesos de la segunda capa oculta se ajustan en función de los errores retropropagados desde la salida hasta los nodos de la segunda capa.\n",
    "3. **Ajuste de $W_{h1}$**: Los pesos de la primera capa oculta se ajustan en función de los errores retropropagados desde la segunda capa oculta.\n",
    "\n",
    "#### Resumen del proceso de retropropagación en la RNN\n",
    "\n",
    "En resumen:\n",
    "\n",
    "1. Se calcula el error en cada paso de tiempo, comparando la salida generada con la salida esperada.\n",
    "2. El error acumulado se retropropaga desde el último paso hasta el primer paso, calculando los gradientes de los pesos en cada nodo.\n",
    "3. Los pesos se actualizan en función de los gradientes acumulados, ajustando la red para reducir el error en futuras predicciones.\n",
    "\n",
    "Este proceso de retropropagación a través del tiempo es lo que permite a la RNN aprender patrones en secuencias de datos y ajustar sus pesos para mejorar la precisión en tareas como la predicción de palabras o traducción automática."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfcad14-820a-42f5-897e-032ace20371d",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "Para ilustrar la retropropagación en una RNN con ejemplos numéricos, usaremos dos capas ocultas y un nodo de salida por cada paso de tiempo. Para simplificar, utilizaremos valores numéricos pequeños y solo se considerará un paso de retropropagación, ya que la retropropagación a través del tiempo (BPTT) puede ser compleja de realizar manualmente para una secuencia completa.\n",
    "\n",
    "Supongamos los siguientes valores ficticios y configuraciones:\n",
    "\n",
    "#### Configuración del ejemplo\n",
    "\n",
    "1. **Dimensiones y valores iniciales**:\n",
    "   - Secuencia de entrada de 3 pasos de tiempo.\n",
    "   - Cada vector de entrada tiene un tamaño de característica de 2.\n",
    "   - Cada capa oculta tiene 2 neuronas (tamaño oculto de 2).\n",
    "   - Supongamos una salida esperada para cada paso de tiempo y una función de pérdida simple.\n",
    "\n",
    "2. **Pesos iniciales**:\n",
    "   - Pesos de la capa oculta 1 ($ W_{h1} $):\n",
    "     $$\n",
    "     W_{h1} = \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix}\n",
    "     $$\n",
    "   - Pesos de la capa oculta 2 ($ W_{h2} $):\n",
    "     $$\n",
    "     W_{h2} = \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix}\n",
    "     $$\n",
    "   - Pesos de salida ($ W_{o} $):\n",
    "     $$\n",
    "     W_{o} = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "3. **Valores de entrada**:\n",
    "   - Paso de tiempo 1 (t=1): Entrada = $[0.5, 0.1]$\n",
    "   - Paso de tiempo 2 (t=2): Entrada = $[0.4, 0.2]$\n",
    "   - Paso de tiempo 3 (t=3): Entrada = $[0.3, 0.3]$\n",
    "\n",
    "4. **Salidas esperadas**:\n",
    "   - Paso de tiempo 1 (t=1): Salida esperada = $0.4$\n",
    "   - Paso de tiempo 2 (t=2): Salida esperada = $0.5$\n",
    "   - Paso de tiempo 3 (t=3): Salida esperada = $0.6$\n",
    "\n",
    "#### Proceso paso a paso\n",
    "\n",
    "Voy a calcular los valores de las neuronas en cada capa oculta y en la salida, así como los errores en cada paso de tiempo y el proceso de retropropagación en uno de los pesos.\n",
    "\n",
    "**Paso 1: Cálculo de la salida en cada paso de tiempo (Forward Pass)**\n",
    "\n",
    "1. **Paso de tiempo 1 (t=1)**:\n",
    "   - **Entrada**: $[0.5, 0.1]$\n",
    "   \n",
    "   - **Cálculo en la capa oculta 1 (Nodo 1)**:\n",
    "     $$\n",
    "     \\text{Nodo 1 (t=1)} = W_{h1} \\cdot \\text{Entrada} = \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 \\\\ 0.1 \\end{bmatrix}\n",
    "     $$\n",
    "     $$\n",
    "     = \\begin{bmatrix} (0.1 \\cdot 0.5) + (0.2 \\cdot 0.1) \\\\ (0.3 \\cdot 0.5) + (0.4 \\cdot 0.1) \\end{bmatrix} = \\begin{bmatrix} 0.05 + 0.02 \\\\ 0.15 + 0.04 \\end{bmatrix} = \\begin{bmatrix} 0.07 \\\\ 0.19 \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "   - **Cálculo en la capa oculta 2 (Nodo 2)**:\n",
    "     $$\n",
    "     \\text{Nodo 2 (t=1)} = W_{h2} \\cdot \\text{Nodo 1 (t=1)} = \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.07 \\\\ 0.19 \\end{bmatrix}\n",
    "     $$\n",
    "     $$\n",
    "     = \\begin{bmatrix} (0.5 \\cdot 0.07) + (0.6 \\cdot 0.19) \\\\ (0.7 \\cdot 0.07) + (0.8 \\cdot 0.19) \\end{bmatrix} = \\begin{bmatrix} 0.035 + 0.114 \\\\ 0.049 + 0.152 \\end{bmatrix} = \\begin{bmatrix} 0.149 \\\\ 0.201 \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "   - **Cálculo de la salida**:\n",
    "     $$\n",
    "     \\text{Salida (t=1)} = W_{o} \\cdot \\text{Nodo 2 (t=1)} = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.149 \\\\ 0.201 \\end{bmatrix}\n",
    "     $$\n",
    "     $$\n",
    "     = (0.9 \\cdot 0.149) + (1.0 \\cdot 0.201) = 0.1341 + 0.201 = 0.3351\n",
    "     $$\n",
    "\n",
    "   - **Error en la salida (t=1)**:\n",
    "     $$\n",
    "     \\text{Error (t=1)} = \\text{Salida esperada} - \\text{Salida (t=1)} = 0.4 - 0.3351 = 0.0649\n",
    "     $$\n",
    "\n",
    "2. **Repetir el proceso para t=2 y t=3**\n",
    "\n",
    "   Para los pasos $ t=2 $ y $ t=3 $, repetiríamos el mismo procedimiento de cálculo para las capas ocultas y la salida. Aquí detallaré solo $ t=1 $ para enfocarnos en la retropropagación.\n",
    "\n",
    "---\n",
    "\n",
    "#### Paso 2: Retropropagación del error\n",
    "\n",
    "Ahora, retropropagaremos el error a través del tiempo para ajustar los pesos de la red, empezando desde el último paso de tiempo hacia el primero.\n",
    "\n",
    "##### Retropropagación en t=1\n",
    "\n",
    "1. **Gradiente de error en la salida**:\n",
    "   Supongamos una función de pérdida cuadrática, donde el gradiente de error en la salida es:\n",
    "   $$\n",
    "   \\delta_{\\text{salida}} = \\text{Error (t=1)} \\cdot W_{o} = 0.0649 \\cdot \\begin{bmatrix} 0.9 \\\\ 1.0 \\end{bmatrix} = \\begin{bmatrix} 0.05841 \\\\ 0.0649 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Retropropagación al nodo 2 (capa oculta 2)**:\n",
    "   El gradiente en el nodo 2 se calcula retropropagando el error desde la salida hacia la capa oculta 2, usando los pesos $ W_{o} $.\n",
    "\n",
    "3. **Gradiente de los pesos $ W_{h2} $**:\n",
    "   Calculamos el gradiente acumulado para ajustar $ W_{h2} $.\n",
    "\n",
    "4. **Ajuste de pesos**:\n",
    "   Usamos una tasa de aprendizaje (por ejemplo, $ \\alpha = 0.01 $) para actualizar cada peso:\n",
    "   $$\n",
    "   W_{h2} = W_{h2} - \\alpha \\cdot \\text{gradiente acumulado}\n",
    "   $$\n",
    "\n",
    "Repetiríamos este proceso para cada peso en cada capa, ajustándolos en función de los gradientes acumulados.\n",
    "\n",
    "\n",
    "Este procedimiento continúa hacia atrás en cada paso de tiempo, ajustando los pesos a través de la retropropagación en cada capa oculta para reducir el error en futuras predicciones. Este ejemplo numérico es una simplificación, pero debería dar una idea general del proceso de retropropagación en RNNs.\n",
    "\n",
    "**Pregunta:** ¿Puedes continuar este proceso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727635dc-322d-4660-b8ca-36a2f71c27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
