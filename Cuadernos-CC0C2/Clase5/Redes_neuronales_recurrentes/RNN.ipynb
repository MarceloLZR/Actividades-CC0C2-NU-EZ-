{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d300584",
   "metadata": {},
   "source": [
    "## Actividad:  Redes neuronales recurrentes como compartición de pesos\n",
    "\n",
    "Las redes neuronales recurrentes (RNNs) son una clase de redes neuronales que son especialmente adecuadas para procesar datos secuenciales o temporales, como texto, audio o series temporales. Una característica clave de las RNNs es la **compartición de pesos**, lo que permite a la red generalizar mejor en secuencias de diferentes longitudes y capturar dependencias temporales.\n",
    "\n",
    "#### Compartición de pesos en una red totalmente conectada\n",
    "\n",
    "En una red neuronal totalmente conectada tradicional, cada capa tiene su propio conjunto de pesos y sesgos. Si consideramos una red con $L$ capas, la salida de la capa $l$ se puede expresar como:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(l)} = \\sigma(\\mathbf{W}^{(l)} \\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)})\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\mathbf{W}^{(l)}$ es la matriz de pesos de la capa $l$.\n",
    "- $\\mathbf{b}^{(l)}$ es el vector de sesgos.\n",
    "- $\\mathbf{h}^{(l-1)}$ es la salida de la capa anterior.\n",
    "- $\\sigma$ es una función de activación no lineal.\n",
    "\n",
    "En este enfoque, cada capa aprende sus propios pesos, lo que puede conducir a un gran número de parámetros y potencialmente al sobreajuste, especialmente si el conjunto de datos es pequeño.\n",
    "\n",
    "#### Compartición de pesos en el tiempo\n",
    "\n",
    "En las RNNs, la idea clave es reutilizar el mismo conjunto de pesos en cada paso temporal. Esto significa que para cada paso de tiempo $t$, la actualización del estado oculto $\\mathbf{h}_t$ se realiza utilizando los mismos pesos $\\mathbf{W}$ y $\\mathbf{U}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_t = \\sigma(\\mathbf{W} \\mathbf{x}_t + \\mathbf{U} \\mathbf{h}_{t-1} + \\mathbf{b})\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\mathbf{x}_t$ es la entrada en el tiempo $t$.\n",
    "- $\\mathbf{h}_{t-1}$ es el estado oculto anterior.\n",
    "- $\\mathbf{W}$ es la matriz de pesos de entrada a oculto.\n",
    "- $\\mathbf{U}$ es la matriz de pesos oculto a oculto (recurrente).\n",
    "- $\\mathbf{b}$ es el vector de sesgos.\n",
    "\n",
    "La compartición de pesos permite a la red aplicar la misma transformación en cada paso temporal, lo que es esencial para capturar patrones secuenciales y reducir el número total de parámetros.\n",
    "\n",
    "### RNNs en PyTorch\n",
    "\n",
    "PyTorch es un framework popular para el aprendizaje profundo que ofrece una implementación eficiente y flexible de RNNs. A continuación, exploraremos cómo construir y entrenar RNNs en PyTorch para resolver un problema sencillo de clasificación de secuencias.\n",
    "\n",
    "#### Un problema simple de clasificación de secuencias\n",
    "\n",
    "Supongamos que queremos clasificar secuencias de números enteros en dos categorías:\n",
    "\n",
    "- **Clase 0**: La suma de los números es par.\n",
    "- **Clase 1**: La suma de los números es impar.\n",
    "\n",
    "El objetivo es construir una RNN que pueda procesar secuencias de longitud variable y predecir la categoría correcta.\n",
    "\n",
    "#### Generación de datos\n",
    "\n",
    "Para entrenar el modelo, necesitamos generar secuencias aleatorias y sus etiquetas correspondientes.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def generate_sequence(max_length=10):\n",
    "    length = random.randint(1, max_length)\n",
    "    sequence = [random.randint(0, 9) for _ in range(length)]\n",
    "    label = sum(sequence) % 2  # 0 si la suma es par, 1 si es impar\n",
    "    return sequence, label\n",
    "```\n",
    "\n",
    "#### Capas de embedding\n",
    "\n",
    "Antes de alimentar los números enteros a la RNN, es común transformarlos en vectores de embedding. Una capa de embedding aprende una representación densa de los índices enteros.\n",
    "\n",
    "En PyTorch, podemos definir una capa de embedding:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 16\n",
    "vocab_size = 10  # Los dígitos del 0 al 9\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "```\n",
    "\n",
    "Esto convierte cada entero en un vector de dimensión `embedding_dim`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5afd71",
   "metadata": {},
   "source": [
    "### Definición de la RNN\n",
    "\n",
    "Definimos una RNN simple utilizando `nn.RNN`:\n",
    "\n",
    "```python\n",
    "hidden_size = 32\n",
    "rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
    "```\n",
    "\n",
    "- `input_size` es la dimensión del embedding.\n",
    "- `hidden_size` es la dimensión del estado oculto.\n",
    "\n",
    "\n",
    "Después de procesar la secuencia completa con la RNN, utilizamos el estado oculto del último paso temporal para hacer la predicción:\n",
    "\n",
    "```python\n",
    "output_dim = 2  # Dos clases: par e impar\n",
    "output_layer = nn.Linear(hidden_size, output_dim)\n",
    "```\n",
    "\n",
    "El flujo completo es:\n",
    "\n",
    "1. Pasar la secuencia a través de la capa de embedding.\n",
    "2. Procesar los embeddings con la RNN.\n",
    "3. Extraer el estado oculto del último tiempo.\n",
    "4. Pasar el estado oculto por la capa lineal para obtener las predicciones.\n",
    "\n",
    "#### Ejemplo del paso hacía adelante\n",
    "\n",
    "```python\n",
    "def forward_pass(sequence):\n",
    "    embedded = embedding(sequence)  # [batch_size, seq_length, embedding_dim]\n",
    "    output, hidden = rnn(embedded)  # hidden: [1, batch_size, hidden_size]\n",
    "    logits = output_layer(hidden.squeeze(0))  # [batch_size, output_dim]\n",
    "    return logits\n",
    "```\n",
    "\n",
    "### Mejorando el tiempo de entrenamiento con packing\n",
    "\n",
    "Cuando trabajamos con secuencias de diferentes longitudes, es eficiente agruparlas en lotes y procesarlas simultáneamente. Sin embargo, necesitamos manejar el padding y packing de secuencias.\n",
    "\n",
    "#### Pad y  pack\n",
    "\n",
    "El padding implica rellenar las secuencias más cortas con tokens especiales (por ejemplo, ceros) para igualar la longitud de las secuencias en el lote.\n",
    "\n",
    "```python\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "sequences = [torch.tensor(seq) for seq, label in batch]\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "```\n",
    "\n",
    "Sin embargo, procesar estos tokens de padding puede ser ineficiente.\n",
    "\n",
    "El packing permite a PyTorch ignorar los pasos de padding durante el procesamiento de la RNN.\n",
    "\n",
    "```python\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "lengths = [len(seq) for seq in sequences]\n",
    "packed_input = pack_padded_sequence(embedded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "```\n",
    "\n",
    "\n",
    "La capa de embedding debe aplicarse antes de empaquetar las secuencias, ya que no puede manejar secuencias empaquetadas.\n",
    "\n",
    "```python\n",
    "embedded_sequences = embedding(padded_sequences)\n",
    "packed_input = pack_padded_sequence(embedded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "```\n",
    "\n",
    "\n",
    "Al entrenar la RNN en lotes, mejoramos significativamente la eficiencia computacional.\n",
    "\n",
    "```python\n",
    "output_packed, hidden = rnn(packed_input)\n",
    "```\n",
    "\n",
    "\n",
    "En algunos casos, es necesario manejar tanto secuencias empaquetadas como no empaquetadas. Por ejemplo, cuando combinamos datos que requieren diferentes formas de procesamiento. Es importante asegurar que las dimensiones y tipos de datos sean compatibles en todo el flujo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7684513",
   "metadata": {},
   "source": [
    "### RNNs más complejas\n",
    "\n",
    "Las RNNs básicas pueden tener limitaciones en su capacidad para capturar dependencias a largo plazo. Existen arquitecturas más complejas que abordan estas limitaciones.\n",
    "\n",
    "#### Múltiples capas\n",
    "\n",
    "Agregar múltiples capas a una RNN puede aumentar su capacidad para modelar patrones complejos. En PyTorch, podemos especificar el número de capas:\n",
    "\n",
    "```python\n",
    "num_layers = 2\n",
    "rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "```\n",
    "\n",
    "La salida de cada capa se utiliza como entrada para la siguiente capa.\n",
    "\n",
    "#### RNN bidireccionales\n",
    "\n",
    "Las RNNs bidireccionales procesan la secuencia en ambas direcciones: hacia adelante y hacia atrás. Esto permite a la red tener en cuenta tanto el contexto pasado como el futuro en cada punto temporal.\n",
    "\n",
    "En PyTorch:\n",
    "\n",
    "```python\n",
    "rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, bidirectional=True, batch_first=True)\n",
    "```\n",
    "\n",
    "La salida tendrá el doble de dimensión, ya que se concatenan los estados ocultos de ambas direcciones.\n",
    "\n",
    "#### Ecuaciones para una  RNN bidireccional\n",
    "\n",
    "Para la dirección hacia adelante:\n",
    "\n",
    "$$\n",
    "\\overrightarrow{\\mathbf{h}}_t = \\sigma(\\mathbf{W} \\mathbf{x}_t + \\mathbf{U} \\overrightarrow{\\mathbf{h}}_{t-1} + \\mathbf{b})\n",
    "$$\n",
    "\n",
    "Para la dirección hacia atrás:\n",
    "\n",
    "$$\n",
    "\\overleftarrow{\\mathbf{h}}_t = \\sigma(\\mathbf{W} \\mathbf{x}_t + \\mathbf{U} \\overleftarrow{\\mathbf{h}}_{t+1} + \\mathbf{b})\n",
    "$$\n",
    "\n",
    "La salida combinada es:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_t = [\\overrightarrow{\\mathbf{h}}_t; \\overleftarrow{\\mathbf{h}}_t]\n",
    "$$\n",
    "\n",
    "Donde $[;]$ denota la concatenación.\n",
    "\n",
    "### Long short-term memory (LSTM) y gated recurrent unit (GRU)\n",
    "\n",
    "Para abordar el problema de los gradientes desvanecientes en RNNs simples, se introdujeron las arquitecturas LSTM y GRU, que utilizan puertas para controlar el flujo de información.\n",
    "\n",
    "#### LSTM\n",
    "\n",
    "El LSTM introduce celdas de memoria y puertas de entrada, olvido y salida:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{i}_t &= \\sigma(\\mathbf{W}_i \\mathbf{x}_t + \\mathbf{U}_i \\mathbf{h}_{t-1} + \\mathbf{b}_i) \\\\\n",
    "\\mathbf{f}_t &= \\sigma(\\mathbf{W}_f \\mathbf{x}_t + \\mathbf{U}_f \\mathbf{h}_{t-1} + \\mathbf{b}_f) \\\\\n",
    "\\mathbf{o}_t &= \\sigma(\\mathbf{W}_o \\mathbf{x}_t + \\mathbf{U}_o \\mathbf{h}_{t-1} + \\mathbf{b}_o) \\\\\n",
    "\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tanh(\\mathbf{W}_c \\mathbf{x}_t + \\mathbf{U}_c \\mathbf{h}_{t-1} + \\mathbf{b}_c) \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\mathbf{i}_t$: puerta de entrada.\n",
    "- $\\mathbf{f}_t$: puerta de olvido.\n",
    "- $\\mathbf{o}_t$: puerta de salida.\n",
    "- $\\mathbf{c}_t$: estado de la celda.\n",
    "- $\\odot$: multiplicación elemento a elemento.\n",
    "\n",
    "#### GRU\n",
    "\n",
    "El GRU simplifica el LSTM combinando algunas puertas:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{z}_t &= \\sigma(\\mathbf{W}_z \\mathbf{x}_t + \\mathbf{U}_z \\mathbf{h}_{t-1} + \\mathbf{b}_z) \\\\\n",
    "\\mathbf{r}_t &= \\sigma(\\mathbf{W}_r \\mathbf{x}_t + \\mathbf{U}_r \\mathbf{h}_{t-1} + \\mathbf{b}_r) \\\\\n",
    "\\mathbf{n}_t &= \\tanh(\\mathbf{W}_n \\mathbf{x}_t + \\mathbf{U}_n (\\mathbf{r}_t \\odot \\mathbf{h}_{t-1}) + \\mathbf{b}_n) \\\\\n",
    "\\mathbf{h}_t &= (1 - \\mathbf{z}_t) \\odot \\mathbf{n}_t + \\mathbf{z}_t \\odot \\mathbf{h}_{t-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Implementación en PyTorch\n",
    "\n",
    "Para utilizar LSTM o GRU, simplemente reemplazamos `nn.RNN` con `nn.LSTM` o `nn.GRU`:\n",
    "\n",
    "```python\n",
    "lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5aed84",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b50cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época [1/20], Pérdida: 0.6989\n",
      "Época [2/20], Pérdida: 0.7008\n",
      "Época [3/20], Pérdida: 0.6834\n",
      "Época [4/20], Pérdida: 0.6908\n",
      "Época [5/20], Pérdida: 0.6922\n",
      "Época [6/20], Pérdida: 0.6888\n",
      "Época [7/20], Pérdida: 0.6977\n",
      "Época [8/20], Pérdida: 0.6992\n",
      "Época [9/20], Pérdida: 0.7038\n",
      "Época [10/20], Pérdida: 0.6992\n",
      "Época [11/20], Pérdida: 0.6904\n",
      "Época [12/20], Pérdida: 0.6927\n",
      "Época [13/20], Pérdida: 0.6917\n",
      "Época [14/20], Pérdida: 0.6947\n",
      "Época [15/20], Pérdida: 0.7001\n",
      "Época [16/20], Pérdida: 0.6924\n",
      "Época [17/20], Pérdida: 0.6946\n",
      "Época [18/20], Pérdida: 0.6975\n",
      "Época [19/20], Pérdida: 0.6914\n",
      "Época [20/20], Pérdida: 0.6899\n",
      "Precisión en prueba: 0.3750\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import random\n",
    "\n",
    "# Generación de datos\n",
    "def generar_secuencia(max_longitud=10):\n",
    "    longitud = random.randint(1, max_longitud)\n",
    "    secuencia = [random.randint(0, 9) for _ in range(longitud)]\n",
    "    etiqueta = sum(secuencia) % 2  # 0 si es par, 1 si es impar\n",
    "    return secuencia, etiqueta\n",
    "\n",
    "def generar_lote(tamano_lote, max_longitud=10):\n",
    "    secuencias = []\n",
    "    etiquetas = []\n",
    "    for _ in range(tamano_lote):\n",
    "        seq, label = generar_secuencia(max_longitud)\n",
    "        secuencias.append(torch.tensor(seq, dtype=torch.long))\n",
    "        etiquetas.append(label)\n",
    "    return secuencias, torch.tensor(etiquetas, dtype=torch.long)\n",
    "\n",
    "# Definición del modelo\n",
    "class ClasificadorSecuencia(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_dim, num_layers=2, bidirectional=True):\n",
    "        super(ClasificadorSecuencia, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size * self.num_directions, output_dim)\n",
    "\n",
    "    def forward(self, sequences, lengths):\n",
    "        embedded = self.embedding(sequences)\n",
    "        packed_input = pack_padded_sequence(\n",
    "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_input)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            hidden_forward = hidden[-2]\n",
    "            hidden_backward = hidden[-1]\n",
    "            hidden_combined = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
    "        else:\n",
    "            hidden_combined = hidden[-1]\n",
    "\n",
    "        logits = self.output_layer(hidden_combined)\n",
    "        return logits\n",
    "\n",
    "# Hiperparámetros\n",
    "vocab_size = 10  # Dígitos del 0 al 9\n",
    "embedding_dim = 16\n",
    "hidden_size = 32\n",
    "output_dim = 2  # Clases: par e impar\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Inicialización del modelo, función de pérdida y optimizador\n",
    "model = ClasificadorSecuencia(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    output_dim=output_dim,\n",
    "    num_layers=num_layers,\n",
    "    bidirectional=bidirectional\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    sequences, labels = generar_lote(batch_size)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = model(padded_sequences, lengths)\n",
    "\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Época [{epoch + 1}/{num_epochs}], Pérdida: {loss.item():.4f}')\n",
    "\n",
    "# Evaluación\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_sequences, test_labels = generar_lote(batch_size)\n",
    "    lengths = torch.tensor([len(seq) for seq in test_sequences], dtype=torch.long)\n",
    "    padded_sequences = pad_sequence(test_sequences, batch_first=True, padding_value=0)\n",
    "    logits = model(padded_sequences, lengths)\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    accuracy = (predictions == test_labels).float().mean()\n",
    "    print(f'Precisión en prueba: {accuracy.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f98e8",
   "metadata": {},
   "source": [
    "### Detalles adicionales y explicaciones\n",
    "\n",
    "\n",
    "En este ejemplo, no implementamos explícitamente la compartición de pesos en una red totalmente conectada, ya que PyTorch no soporta directamente esta funcionalidad en capas lineales estándar. Sin embargo, podemos ilustrarlo definiendo una capa personalizada que reutiliza los mismos pesos en múltiples capas.\n",
    "\n",
    "```python\n",
    "class CapaCompartida(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CapaCompartida, self).__init__()\n",
    "        self.pesos_compartidos = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplicamos la misma capa varias veces\n",
    "        x = self.pesos_compartidos(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pesos_compartidos(x)\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "**Manejo de secuencias con padding y packing**\n",
    "\n",
    "- **Padding:** Rellenamos las secuencias más cortas con un valor (0 en este caso) para que todas tengan la misma longitud.\n",
    "- **Packing:** Utilizamos `pack_padded_sequence` para indicar a la RNN las longitudes reales de las secuencias, evitando computaciones innecesarias en los pasos de padding.\n",
    "\n",
    "\n",
    "La capa de embedding transforma los índices enteros de los dígitos en vectores densos de dimensión `embedding_dim`.\n",
    "\n",
    "**Uso del último estado oculto para predicciones**\n",
    "\n",
    "Utilizamos el estado oculto final de la RNN (o la concatenación de los estados ocultos finales en ambas direcciones si es bidireccional) para hacer la predicción.\n",
    "\n",
    "\n",
    "**RNNs más complejas**\n",
    "\n",
    "- **Múltiples capas:** Especificado por `num_layers`. Cada capa toma la salida de la capa anterior como su entrada.\n",
    "- **Bidireccionalidad:** Si `bidirectional=True`, la RNN procesa la secuencia en ambas direcciones (hacia adelante y hacia atrás).\n",
    "\n",
    "**Entradas empaquetadas y desempaquetadas simultáneamente**\n",
    "\n",
    "Si necesitamos acceder a las salidas en cada paso temporal (por ejemplo, para tareas de secuencia a secuencia), podemos desempaquetar las secuencias después de la RNN.\n",
    "\n",
    "```python\n",
    "# Después de la RNN\n",
    "packed_output, (hidden, cell) = self.rnn(packed_input)\n",
    "\n",
    "# Desempaquetamos las secuencias\n",
    "output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "# Ahora output tiene forma [batch_size, max_length, hidden_size * num_directions]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d488281d",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "**Compartición de pesos en una red totalmente conectada**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Implementa en PyTorch una red neuronal totalmente conectada con múltiples capas ocultas que comparten el mismo conjunto de pesos y sesgos. La red debe:\n",
    "\n",
    "- Tener una capa de entrada, tres capas ocultas y una capa de salida.\n",
    "- Utilizar la función de activación ReLU en las capas ocultas.\n",
    "- Predecir la clase de entrada en una tarea de clasificación simple (por ejemplo, clasificación de dígitos escritos a mano).\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Define una clase de PyTorch `nn.Module` que reutilice los mismos pesos en todas las capas ocultas.\n",
    "- Explica cómo la compartición de pesos afecta el número total de parámetros y el entrenamiento del modelo.\n",
    "- Entrena y evalúa el modelo en un conjunto de datos, como MNIST.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Utiliza un solo `nn.Linear` para las capas ocultas y aplícalo varias veces en el método `forward`.\n",
    "  \n",
    "---\n",
    "\n",
    "**2. Compartición de pesos en el tiempo**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Implementa manualmente una red neuronal recurrente simple (RNN) sin utilizar las clases predefinidas de PyTorch como `nn.RNN`, `nn.LSTM` o `nn.GRU`. Tu implementación debe:\n",
    "\n",
    "- Procesar una secuencia de números (por ejemplo, una serie temporal).\n",
    "- Compartir los mismos pesos recurrentes a lo largo del tiempo.\n",
    "- Predecir el siguiente valor en la secuencia (tarea de predicción secuencial).\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Define los pesos de entrada y recurrentes manualmente.\n",
    "- Escribe el bucle temporal en el método `forward` de tu clase `nn.Module`.\n",
    "- Demuestra cómo se comparten los pesos a lo largo del tiempo.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Utiliza una función de activación como `torch.tanh` o `torch.relu` en cada paso temporal.\n",
    "  \n",
    "---\n",
    "\n",
    "**3. Clasificación de secuencias en PyTorch**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Desarrolla un modelo en PyTorch para clasificar secuencias de texto como positivas o negativas (análisis de sentimiento). El modelo debe:\n",
    "\n",
    "- Utilizar una capa de embedding para representar las palabras.\n",
    "- Emplear una RNN (LSTM o GRU) para procesar las secuencias.\n",
    "- Hacer predicciones basadas en el último estado oculto.\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Preprocesa el texto (tokenización, creación de vocabulario, secuenciación).\n",
    "- Maneja secuencias de diferentes longitudes con padding y packing.\n",
    "- Entrena y evalúa el modelo en un conjunto de datos, como IMDb.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Puedes utilizar `torchtext` para facilitar el manejo de datos de texto.\n",
    "  \n",
    "---\n",
    "\n",
    "**4. Implementación de capas de embedding**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Implementa una capa de embedding personalizada que:\n",
    "\n",
    "- Inicialice los embeddings con valores aleatorios o preentrenados (por ejemplo, Word2Vec).\n",
    "- Permita congelar (no entrenar) o descongelar (entrenar) los embeddings durante el entrenamiento.\n",
    "- Sea compatible con el uso de padding y packing.\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Explica cómo afecta al modelo el hecho de entrenar o no los embeddings.\n",
    "- Prueba ambas opciones y compara los resultados en términos de precisión y tiempo de entrenamiento.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Controla el parámetro `requires_grad` de los embeddings para congelarlos o descongelarlos.\n",
    "  \n",
    "---\n",
    "\n",
    "**5. Predicciones usando el último paso temporal**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Modifica el modelo de clasificación de secuencias para que, en lugar de usar solo el último estado oculto, combine los estados ocultos de todos los pasos temporales (por ejemplo, mediante una media o una suma ponderada). Evalúa cómo afecta esta modificación al rendimiento del modelo.\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Implementa una forma de agregar información de todos los estados ocultos.\n",
    "- Compara los resultados con el modelo que utiliza solo el último estado oculto.\n",
    "- Discute las ventajas y desventajas de cada enfoque.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Puedes utilizar `torch.mean` o `torch.sum` a lo largo de la dimensión temporal.\n",
    "  \n",
    "---\n",
    "\n",
    "**Mejora del tiempo de entrenamiento con packing**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Entrena una RNN en un conjunto de datos con secuencias de diferentes longitudes. Mide y compara el tiempo de entrenamiento y el rendimiento del modelo al usar:\n",
    "\n",
    "- Entrenamiento sin padding ni packing (procesamiento individual de secuencias).\n",
    "- Entrenamiento con padding pero sin packing.\n",
    "- Entrenamiento con padding y packing.\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Implementa el manejo de secuencias con padding y packing.\n",
    "- Explica cómo el packing mejora la eficiencia del entrenamiento.\n",
    "- Presenta los tiempos de entrenamiento y las métricas de rendimiento en cada caso.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Utiliza `time.time()` o la función `perf_counter` para medir el tiempo de entrenamiento.\n",
    "  \n",
    "---\n",
    "\n",
    "**7. Entrenamiento de una RNN en lotes**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Crea una RNN que pueda entrenarse en lotes con secuencias de diferentes longitudes para predecir la siguiente palabra en una secuencia de texto (modelo de lenguaje). Asegúrate de:\n",
    "\n",
    "- Manejar correctamente el padding y packing durante el entrenamiento en lotes.\n",
    "- Actualizar los estados ocultos entre lotes de forma adecuada.\n",
    "- Evaluar el modelo en términos de pérdida de entropía cruzada y precisión de predicción.\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Preprocesa el texto para crear un conjunto de datos adecuado.\n",
    "- Implementa un `DataLoader` personalizado si es necesario.\n",
    "- Discute cómo se maneja el estado oculto en el entrenamiento en lotes.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Al final de cada lote, reinicia el estado oculto o utiliza técnicas para mantener la continuidad si es necesario.\n",
    "  \n",
    "---\n",
    "\n",
    "**8. Entradas empaquetadas y desempaquetadas simultáneamente**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Implementa un modelo seq2seq (secuencia a secuencia) para traducción automática que:\n",
    "\n",
    "- Utilice una RNN como codificador (encoder) y otra como decodificador (decoder).\n",
    "- Maneje correctamente las secuencias empaquetadas y desempaquetadas en el codificador y decodificador.\n",
    "- Aplique un mecanismo de atención para mejorar la calidad de la traducción.\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Asegúrate de que el codificador procese secuencias empaquetadas.\n",
    "- Desempaqueta las salidas del codificador si es necesario para el decodificador.\n",
    "- Implementa y explica el mecanismo de atención utilizado.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- El decodificador generalmente procesa un paso a la vez, por lo que puede no requerir empaquetamiento.\n",
    "  \n",
    "---\n",
    "\n",
    "**9. RNNs más Complejas: múltiples capas**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Construye una RNN profunda con múltiples capas ocultas para realizar reconocimiento de voz básico. El modelo debe:\n",
    "\n",
    "- Procesar espectrogramas o características de audio (como MFCCs).\n",
    "- Utilizar una RNN con al menos tres capas ocultas.\n",
    "- Manejar secuencias de entrada de longitud variable.\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Preprocesa los datos de audio para extraer características relevantes.\n",
    "- Explica cómo las múltiples capas permiten capturar características más complejas.\n",
    "- Evalúa el modelo en términos de precisión y tasa de error.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Puedes utilizar conjuntos de datos como el Librispeech o el dataset de comandos de voz de Google.\n",
    "  \n",
    "---\n",
    "\n",
    "**RNNs bidireccionales**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Implementa una RNN bidireccional para realizar etiquetado de secuencias en texto (por ejemplo, reconocimiento de entidades nombradas o etiquetado POS). Compara su rendimiento con una RNN unidireccional y analiza las diferencias.\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Explica cómo la bidireccionalidad ayuda a capturar dependencias contextuales.\n",
    "- Implementa ambas versiones del modelo y entrena en un conjunto de datos etiquetado.\n",
    "- Presenta y compara los resultados obtenidos.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Utiliza conjuntos de datos etiquetados como CoNLL-2003 o Penn Treebank.\n",
    "  \n",
    "---\n",
    "\n",
    "**Desafío adicional: implementación de un modelo de lenguaje con LSTM**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Crea un modelo de lenguaje utilizando una LSTM en PyTorch que sea capaz de generar texto similar a una fuente dada (por ejemplo, obras de Shakespeare). El modelo debe:\n",
    "\n",
    "- Procesar secuencias de texto y predecir la siguiente palabra o carácter.\n",
    "- Manejar eficientemente el estado oculto entre secuencias.\n",
    "- Generar texto nuevo a partir de una semilla inicial.\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Preprocesa el texto para crear un vocabulario y secuencias de entrenamiento.\n",
    "- Implementa el entrenamiento del modelo y el proceso de generación de texto.\n",
    "- Ajusta los hiperparámetros para mejorar la calidad del texto generado.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Experimenta con diferentes tamaños de secuencia y dimensiones del estado oculto.\n",
    "  \n",
    "---\n",
    "\n",
    "####  **11. Atención en RNNs**\n",
    "\n",
    "**Ejercicio:**\n",
    "\n",
    "Añade un mecanismo de atención a un modelo seq2seq de traducción automática. Tu modelo debe:\n",
    "\n",
    "- Utilizar una RNN para el codificador y otra para el decodificador.\n",
    "- Implementar atención según los modelos de Bahdanau o Luong.\n",
    "- Mostrar cómo la atención mejora el rendimiento del modelo.\n",
    "\n",
    "**Puntos a abordar:**\n",
    "\n",
    "- Explica el concepto de atención y su importancia en modelos seq2seq.\n",
    "- Implementa el cálculo de los pesos de atención y cómo se aplican al decodificador.\n",
    "- Evalúa el modelo con y sin atención y compara los resultados.\n",
    "\n",
    "**Pista:**\n",
    "\n",
    "- Visualiza los pesos de atención para interpretar cómo el modelo se enfoca en diferentes partes de la entrada.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a17448",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
