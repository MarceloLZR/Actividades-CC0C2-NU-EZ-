{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9754276-8fb2-4e52-99f0-ea134cf627e1",
   "metadata": {},
   "source": [
    "### Preliminares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf301f-f696-4600-bee4-b8bcea8f8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab39538-6273-4f0f-b615-e58b66518063",
   "metadata": {},
   "source": [
    "#### Definimos las funciones de activación a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aace101-3333-4df1-bbfe-ff951dcc9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: ndarray):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x: ndarray):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def tanh(x: ndarray):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(x: ndarray):\n",
    "    return 1 - np.tanh(x) * np.tanh(x)\n",
    "\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
    "\n",
    "\n",
    "def batch_softmax(input_array: ndarray):\n",
    "    out = []\n",
    "    for row in input_array:\n",
    "        out.append(softmax(row, axis=1))\n",
    "    return np.stack(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d66ad-1ebe-45b8-b928-0d8f0f54b41f",
   "metadata": {},
   "source": [
    "##### Optimizador de redes neuronales recurrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc5b71-2bce-499b-8b38-b87711a3b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNOptimizer(object):\n",
    "    # Constructor de la clase optimizador para redes neuronales recurrentes.\n",
    "    # Args:\n",
    "    #   lr (float): Tasa de aprendizaje, con un valor por defecto de 0.01.\n",
    "    #   gradient_clipping (bool): Indica si se aplica clipping a los gradientes,\n",
    "    #                             activado por defecto.\n",
    "    def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -> None:\n",
    "        self.lr = lr  # Almacena la tasa de aprendizaje.\n",
    "        self.gradient_clipping = gradient_clipping  # Almacena la configuración de clipping.\n",
    "        self.first = True  # Variable auxiliar, posiblemente para controlar la primera actualización.\n",
    "\n",
    "    # Método que ejecuta un paso de optimización sobre todos los parámetros del modelo.\n",
    "    def step(self) -> None:\n",
    "        # Itera sobre cada capa del modelo.\n",
    "        for layer in self.model.layers:\n",
    "            # Itera sobre cada parámetro de la capa.\n",
    "            for key in layer.params.keys():\n",
    "                # Si el clipping de gradientes está activado, aplica clipping.\n",
    "                if self.gradient_clipping:\n",
    "                    # Los gradientes se limitan a estar entre -2 y 2 para evitar la explosión de gradientes.\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "\n",
    "                # Llama a la regla de actualización para ajustar el valor del parámetro\n",
    "                # usando la tasa de aprendizaje y el gradiente actual.\n",
    "                self._update_rule(param=layer.params[key]['value'],\n",
    "                                  grad=layer.params[key]['deriv'])\n",
    "\n",
    "    # Método abstracto para definir la regla de actualización de parámetros.\n",
    "    # Debe ser implementado por subclases.\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        raise NotImplementedError(\"Este método debe ser implementado por subclases.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff18f2-df0c-47e7-87ee-1a64478cc3df",
   "metadata": {},
   "source": [
    "#### Algunos optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da952c52-241f-4660-835b-ebd0569d56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(RNNOptimizer):\n",
    "    # Constructor de la clase SGD (Stochastic Gradient Descent).\n",
    "    # Args:\n",
    "    #   lr (float): Tasa de aprendizaje.\n",
    "    #   gradient_clipping (bool): Indica si se activa el clipping de gradientes.\n",
    "    def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -> None:\n",
    "        super().__init__(lr, gradient_clipping)  # Inicializa la clase base\n",
    "\n",
    "    # Método específico de SGD para actualizar los parámetros según el gradiente.\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        update = self.lr * kwargs['grad']  # Calcula la actualización del parámetro.\n",
    "        kwargs['param'] -= update  # Actualiza el parámetro restando la actualización.\n",
    "        \n",
    "class AdaGrad(RNNOptimizer):\n",
    "    # Constructor de la clase AdaGrad.\n",
    "    # Args:\n",
    "    #   lr (float): Tasa de aprendizaje.\n",
    "    #   gradient_clipping (bool): Indica si se activa el clipping de gradientes.\n",
    "    def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -> None:\n",
    "        super().__init__(lr, gradient_clipping)  # Inicializa la clase base\n",
    "        self.eps = 1e-7  # Un pequeño número para evitar división por cero en la actualización.\n",
    "\n",
    "    # Método que ejecuta un paso de optimización sobre todos los parámetros del modelo.\n",
    "    def step(self) -> None:\n",
    "        if self.first:\n",
    "            self.sum_squares = {}  # Diccionario para almacenar la suma acumulada de los cuadrados de los gradientes.\n",
    "            # Inicializa sum_squares para cada parámetro en cada capa.\n",
    "            for i, layer in enumerate(self.model.layers):\n",
    "                self.sum_squares[i] = {}\n",
    "                for key in layer.params.keys():\n",
    "                    self.sum_squares[i][key] = np.zeros_like(layer.params[key]['value'])\n",
    "            self.first = False  # Marca la inicialización como completa.\n",
    "\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            for key in layer.params.keys():\n",
    "                if self.gradient_clipping:\n",
    "                    # Si el clipping de gradientes está activado, aplica clipping.\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "\n",
    "                # Actualiza cada parámetro utilizando la regla específica de AdaGrad.\n",
    "                self._update_rule(param=layer.params[key]['value'],\n",
    "                                  grad=layer.params[key]['deriv'],\n",
    "                                  sum_square=self.sum_squares[i][key])\n",
    "\n",
    "    # Método específico de AdaGrad para actualizar los parámetros.\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        # Actualiza la suma acumulada de los cuadrados de los gradientes.\n",
    "        kwargs['sum_square'] += self.eps + np.power(kwargs['grad'], 2)\n",
    "        # Calcula la tasa de aprendizaje escalada.\n",
    "        lr_scaled = self.lr / np.sqrt(kwargs['sum_square'])\n",
    "        # Utiliza la tasa de aprendizaje escalada para actualizar los parámetros.\n",
    "        kwargs['param'] -= lr_scaled * kwargs['grad']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f9936-c0e7-4294-88b8-6659aee3d1ef",
   "metadata": {},
   "source": [
    "#### Funciones de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9e0d4-e4d9-46c3-b719-f0faadf2b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def assert_same_shape(output: np.ndarray, output_grad: np.ndarray):\n",
    "    assert output.shape == output_grad.shape, \\\n",
    "        '''\n",
    "        Dos tensores deben tener la misma forma;\n",
    "        en cambio, la forma del primer tensor es {0}\n",
    "        y la forma del segundo tensor es {1}.\n",
    "        '''.format(tuple(output_grad.shape), tuple(output.shape))\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68577fda-7a5b-4145-867d-d29bdc700294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    # Constructor de la clase base para las pérdidas.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Método forward para calcular la pérdida entre las predicciones y los objetivos.\n",
    "    def forward(self,\n",
    "                prediction: ndarray,\n",
    "                target: ndarray) -> float:\n",
    "        # Asegura que las predicciones y los objetivos tengan la misma forma.\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction  # Almacena las predicciones.\n",
    "        self.target = target  # Almacena los objetivos.\n",
    "\n",
    "        self.output = self._output()  # Calcula la pérdida utilizando una función interna.\n",
    "\n",
    "        return self.output  # Devuelve el valor de la pérdida.\n",
    "    \n",
    "    # Método backward para calcular el gradiente de la pérdida respecto a las entradas.\n",
    "    def backward(self) -> ndarray:\n",
    "        self.input_grad = self._input_grad()  # Calcula el gradiente utilizando una función interna.\n",
    "\n",
    "        # Asegura que las predicciones y el gradiente tengan la misma forma.\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad  # Devuelve el gradiente de la entrada.\n",
    "\n",
    "    # Método abstracto para calcular la pérdida; debe ser implementado por subclases.\n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Método abstracto para calcular el gradiente de la entrada; debe ser implementado por subclases.\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    # Constructor de la clase para la pérdida de entropía cruzada con softmax.\n",
    "    def __init__(self, eps: float=1e-9) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps  # Un pequeño número para evitar inestabilidad numérica.\n",
    "\n",
    "    # Método para calcular la salida de la pérdida de entropía cruzada con softmax.\n",
    "    def _output(self) -> float:\n",
    "        out = []\n",
    "        # Aplica softmax a cada fila de la predicción.\n",
    "        for row in self.prediction:\n",
    "            out.append(softmax(row, axis=1))\n",
    "        softmax_preds = np.stack(out)\n",
    "\n",
    "        # Recorta la salida de softmax para prevenir inestabilidad numérica.\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # Calcula la pérdida real de entropía cruzada.\n",
    "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "            (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss)  # Retorna la suma total de la pérdida.\n",
    "\n",
    "    # Método para calcular el gradiente de la entrada basado en la salida de softmax.\n",
    "    def _input_grad(self) -> np.ndarray:\n",
    "        return self.softmax_preds - self.target  # Gradiente de la pérdida respecto a la entrada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672d134-e2fd-48a3-8e73-1de5671ddaf8",
   "metadata": {},
   "source": [
    " ## Redes neuronales recurrentes\n",
    "Una red neuronal recurrente (RNN) es una clase de redes neuronales diseñadas para manejar secuencias de datos, como series temporales o secuencias lingüísticas. \n",
    "\n",
    "A diferencia de las redes neuronales tradicionales que asumen independencia entre las entradas, las RNNs tienen \"memoria\" sobre entradas anteriores. Esto les permite retener información a través del tiempo y utilizar esta información para influir en la salida actual, lo cual es crucial para tareas donde el contexto y el orden de los datos son importantes, como el procesamiento del lenguaje natural o el análisis de series temporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fac0e-6e07-4c7d-8489-58590dd6dac6",
   "metadata": {},
   "source": [
    "#### Clase RNNNode\n",
    "La clase RNNNode define un nodo dentro de una red neuronal recurrente (RNN), capaz de realizar cálculos hacia adelante y hacia atrás. El método forward calcula la salida y el nuevo estado oculto del nodo basado en la entrada actual y el estado oculto anterior, utilizando matrices de pesos y sesgos. \n",
    "\n",
    "El método backward se encarga de la retropropagación del error, calculando los gradientes respecto a las entradas y actualizando los parámetros del modelo basados en estos gradientes. Esto permite entrenar la red para ajustar sus pesos y mejorar la precisión de sus predicciones a lo largo del tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5009e-d6cc-401d-8167-944bfb994d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNode(object):\n",
    "    # Constructor del nodo RNN, inicialmente no realiza ninguna operación específica.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Método forward para calcular la salida del nodo RNN a partir de la entrada y el estado oculto anterior.\n",
    "    def forward(self,\n",
    "                x_in: ndarray, \n",
    "                H_in: ndarray,\n",
    "                params_dict: Dict[str, Dict[str, ndarray]]\n",
    "                ) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        Calcula la salida del nodo RNN y el nuevo estado oculto.\n",
    "        Args:\n",
    "        x_in: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        H_in: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        Retorna x_out: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        Retorna H_out: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        '''\n",
    "        self.X_in = x_in\n",
    "        self.H_in = H_in\n",
    "    \n",
    "        # Concatena la entrada x con el estado oculto anterior H.\n",
    "        self.Z = np.column_stack((x_in, H_in))\n",
    "        \n",
    "        # Calcula el nuevo estado oculto intermedio usando los pesos y biases.\n",
    "        self.H_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
    "        \n",
    "        # Aplica la función de activación tanh al estado oculto intermedio.\n",
    "        self.H_out = tanh(self.H_int)\n",
    "\n",
    "        # Calcula la salida del nodo RNN.\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out\n",
    "\n",
    "    # Método backward para la propagación hacia atrás del error a través del nodo RNN.\n",
    "    def backward(self, \n",
    "                 X_out_grad: ndarray, \n",
    "                 H_out_grad: ndarray,\n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        Retropropaga el gradiente a través del nodo RNN.\n",
    "        Args:\n",
    "        X_out_grad: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        H_out_grad: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        Retorna X_in_grad: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        Retorna H_in_grad: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        '''\n",
    "        \n",
    "        # Verifica que los gradientes y las salidas tengan la misma forma.\n",
    "        assert_same_shape(X_out_grad, self.X_out)\n",
    "        assert_same_shape(H_out_grad, self.H_out)\n",
    "\n",
    "        # Calcula los gradientes para los parámetros de salida y acumula en los derivados.\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        \n",
    "        # Propaga el gradiente hacia atrás a través de la red.\n",
    "        dh = np.dot(X_out_grad, params_dict['W_v']['value'].T)\n",
    "        dh += H_out_grad\n",
    "        \n",
    "        # Calcula el gradiente del estado oculto intermedio.\n",
    "        dH_int = dh * dtanh(self.H_int)\n",
    "        \n",
    "        # Acumula gradientes en los parámetros del estado oculto.\n",
    "        params_dict['B_f']['deriv'] += dH_int.sum(axis=0)\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, dH_int)     \n",
    "        \n",
    "        # Calcula los gradientes de entrada.\n",
    "        dz = np.dot(dH_int, params_dict['W_f']['value'].T)\n",
    "        X_in_grad = dz[:, :self.X_in.shape[1]]\n",
    "        H_in_grad = dz[:, self.X_in.shape[1]:]\n",
    "        \n",
    "        return X_in_grad, H_in_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1207c7e3-a638-4604-aa50-63ed3bbc89ff",
   "metadata": {},
   "source": [
    "#### RNNLayer\n",
    "\n",
    "La clase RNNLayer es una capa de red neuronal recurrente que maneja la propagación hacia adelante y hacia atrás de los datos a través de una secuencia de tiempo. En el método forward, la capa procesa secuencialmente la entrada utilizando nodos RNN internos, cada uno correspondiente a un paso de tiempo, manteniendo un estado oculto que pasa de un nodo a otro. Esta capa es capaz de ajustar sus pesos y sesgos para mejorar la predicción del siguiente carácter en una secuencia.\n",
    "\n",
    "Durante la retropropagación, calcula los gradientes para actualizar los parámetros con el fin de minimizar el error en las predicciones. Esta estructura es fundamental para tareas de procesamiento de secuencias como la generación de texto, donde la dependencia temporal entre los datos es crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46601f36-22ce-4830-b739-0156ec964868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(object):\n",
    "    # Constructor de la clase de la capa RNN.\n",
    "    # Args:\n",
    "    #   hidden_size: int - Número de neuronas ocultas en la capa RNN.\n",
    "    #   output_size: int - Número de caracteres en el vocabulario para predecir el siguiente carácter.\n",
    "    #   weight_scale: float - Escala para la inicialización de los pesos.\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = None):\n",
    "        self.hidden_size = hidden_size  # Almacena el tamaño del estado oculto.\n",
    "        self.output_size = output_size  # Almacena el tamaño de salida.\n",
    "        self.weight_scale = weight_scale  # Escala de inicialización de los pesos.\n",
    "        self.start_H = np.zeros((1, hidden_size))  # Estado oculto inicial.\n",
    "        self.first = True  # Bandera para inicialización en el primer paso forward.\n",
    "\n",
    "    # Método para inicializar los parámetros de la capa.\n",
    "    def _init_params(self, input_: ndarray):\n",
    "        self.vocab_size = input_.shape[2]  # Tamaño del vocabulario a partir de la entrada.\n",
    "        # Establece la escala de peso si no se proporcionó.\n",
    "        if not self.weight_scale:\n",
    "            self.weight_scale = 2 / (self.vocab_size + self.output_size)\n",
    "        \n",
    "        self.params = {'W_f': {}, 'B_f': {}, 'W_v': {}, 'B_v': {}}\n",
    "        # Inicializa pesos y sesgos con valores aleatorios normalizados.\n",
    "        self.params['W_f']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                       (self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(0.0, self.weight_scale, (1, self.hidden_size))\n",
    "        self.params['W_v']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                       (self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(0.0, self.weight_scale, (1, self.output_size))\n",
    "\n",
    "        self.params['W_f']['deriv'] = np.zeros_like(self.params['W_f']['value'])\n",
    "        self.params['B_f']['deriv'] = np.zeros_like(self.params['B_f']['value'])\n",
    "        self.params['W_v']['deriv'] = np.zeros_like(self.params['W_v']['value'])\n",
    "        self.params['B_v']['deriv'] = np.zeros_like(self.params['B_v']['value'])\n",
    "        \n",
    "        self.cells = [RNNNode() for _ in range(input_.shape[1])]  # Inicializa nodos RNN por cada paso de secuencia.\n",
    "\n",
    "    # Limpia los gradientes acumulados en los parámetros.\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "\n",
    "    # Procesa la entrada a través de la capa RNN y calcula la salida para cada paso de tiempo.\n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)  # Inicializa parámetros en el primer paso.\n",
    "            self.first = False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        H_in = np.repeat(self.start_H, batch_size, axis=0)\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)  # Actualiza el estado oculto inicial para la próxima ejecución.\n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "    # Retropropaga el error desde la salida hacia las entradas.\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        sequence_length = x_seq_out_grad.shape[1]\n",
    "        x_seq_in_grad = np.zeros((batch_size, sequence_length, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(sequence_length)):\n",
    "            x_out_grad = x_seq_out_grad[:, t, :]\n",
    "            grad_out, h_in_grad = self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        \n",
    "        return x_seq_in_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef54c0-a343-4280-b8f5-758e9d8ca1e3",
   "metadata": {},
   "source": [
    "#### RNNModel\n",
    "\n",
    "El código define un modelo de red neuronal recurrente (RNN) que es capaz de procesar secuencias de datos, como series temporales o texto. El modelo está compuesto por varias capas (RNNLayer), cada una procesando la entrada y pasándola a la siguiente. El proceso de entrenamiento ocurre en pasos, donde cada paso involucra:\n",
    "\n",
    "* Paso hacia adelante (forward): Cada entrada de la secuencia es procesada por todas las capas de la red, pasando de una a otra. Esta operación se utiliza para obtener la salida de la red que luego se compara con el objetivo real para calcular la pérdida.\n",
    "* Cálculo de la pérdida: Se usa un objeto de pérdida para evaluar qué tan bien la salida de la red coincide con los objetivos esperados.\n",
    "\n",
    "* Paso hacia atrás (backward): Una vez calculada la pérdida, se calcula el gradiente de la pérdida respecto a las salidas, y este gradiente se propaga hacia atrás a través de la red para actualizar los pesos de las neuronas en cada capa, lo que permite que la red aprenda.\n",
    "\n",
    "* Actualización de parámetros: Basándose en los gradientes obtenidos de la retropropagación, se actualizan los parámetros de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340dcf7-9330-4e5d-af6a-0067c0ef7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(object):\n",
    "    '''\n",
    "    Clase Modelo que recibe entradas y objetivos, entrena la red y calcula la pérdida.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[RNNLayer],\n",
    "                 sequence_length: int, \n",
    "                 vocab_size: int, \n",
    "                 loss: Loss):\n",
    "        '''\n",
    "        Inicializa el modelo de red neuronal recurrente.\n",
    "        Args:\n",
    "        layers: Lista de capas RNN en la red.\n",
    "        sequence_length: Longitud de la secuencia que pasa a través de la red.\n",
    "        vocab_size: Número de caracteres en el vocabulario.\n",
    "        loss: Objeto de pérdida utilizado para calcular la pérdida durante el entrenamiento.\n",
    "        '''\n",
    "        self.layers = layers  # Lista de capas RNN.\n",
    "        self.vocab_size = vocab_size  # Tamaño del vocabulario.\n",
    "        self.sequence_length = sequence_length  # Longitud de la secuencia.\n",
    "        self.loss = loss  # Objeto de pérdida.\n",
    "        # Establece la longitud de la secuencia para cada capa.\n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "    def forward(self, \n",
    "                x_batch: ndarray):\n",
    "        '''\n",
    "        Realiza la propagación hacia adelante a través de la red.\n",
    "        Args:\n",
    "        x_batch: Array de entrada con forma (batch_size, sequence_length, vocab_size)\n",
    "        Returns:\n",
    "        x_batch_in: Array de salida de la última capa.\n",
    "        '''       \n",
    "        for layer in self.layers:\n",
    "            x_batch = layer.forward(x_batch)  # Propaga la entrada a través de cada capa.\n",
    "        return x_batch\n",
    "        \n",
    "    def backward(self, \n",
    "                 loss_grad: ndarray):\n",
    "        '''\n",
    "        Realiza la retropropagación a través de la red utilizando el gradiente de la pérdida.\n",
    "        Args:\n",
    "        loss_grad: Gradiente de la pérdida con forma (batch_size, sequence_length, vocab_size)\n",
    "        Returns:\n",
    "        loss_grad: Propaga el gradiente a través de todas las capas.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(loss_grad)  # Retropropaga a través de cada capa en orden inverso.\n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, \n",
    "                    x_batch: ndarray, \n",
    "                    y_batch: ndarray):\n",
    "        '''\n",
    "        Ejecuta un único paso de entrenamiento completo:\n",
    "        1. Paso hacia adelante y aplicación de softmax.\n",
    "        2. Calcula la pérdida y su gradiente.\n",
    "        3. Paso hacia atrás.\n",
    "        4. Actualización de parámetros.\n",
    "        Args:\n",
    "        x_batch: Array de entrada con forma (batch_size, sequence_length, vocab_size)\n",
    "        y_batch: Array objetivo correspondiente.\n",
    "        Returns:\n",
    "        loss: Pérdida calculada para el lote actual.\n",
    "        '''\n",
    "        x_batch_out = self.forward(x_batch)  # Paso hacia adelante.\n",
    "        loss = self.loss.forward(x_batch_out, y_batch)  # Calcula la pérdida.\n",
    "        loss_grad = self.loss.backward()  # Calcula el gradiente de la pérdida.\n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()  # Limpia los gradientes en cada capa.\n",
    "        self.backward(loss_grad)  # Retropropaga el gradiente de la pérdida.\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3a356-02a5-4e5e-8375-8d8a1bd092d6",
   "metadata": {},
   "source": [
    "#### RNNTrainer\n",
    "\n",
    "Este código define una clase RNNTrainer que se utiliza para entrenar un modelo de red neuronal recurrente (RNN) para la generación de texto. Utiliza un archivo de texto como datos de entrada y realiza las siguientes tareas principales:\n",
    "\n",
    "* Inicialización: Prepara el modelo, el optimizador y los datos necesarios para el entrenamiento, incluyendo la creación de mapeos de caracteres a índices y viceversa.\n",
    "\n",
    "* Generación de entradas y objetivos: Crea los lotes de datos de entrada y los objetivos (targets) correspondientes que el modelo intentará predecir.\n",
    "\n",
    "* Entrenamiento: Ejecuta el proceso de entrenamiento en varias iteraciones, donde cada iteración incluye un paso hacia adelante (forward), el cálculo de la pérdida, un paso hacia atrás (backward) para la propagación del error, y la actualización de los parámetros del modelo.\n",
    "\n",
    "* Muestreo de salidas: Opcionalmente, genera texto basado en el modelo entrenado para visualizar cómo está aprendiendo el modelo durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14ab9c-a999-4705-90e7-0a3c22e17411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    '''\n",
    "    Clase que toma un archivo de texto y un modelo, y comienza a generar caracteres.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 text_file: str, \n",
    "                 model: RNNModel,\n",
    "                 optim: RNNOptimizer,\n",
    "                 batch_size: int = 32):\n",
    "        # Leer los datos del archivo de texto.\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = model  # Modelo de red neuronal recurrente.\n",
    "        self.chars = list(set(self.data))  # Lista de caracteres únicos en el texto.\n",
    "        self.vocab_size = len(self.chars)  # Tamaño del vocabulario.\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}  # Diccionario de caracteres a índices.\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}  # Diccionario inverso de índices a caracteres.\n",
    "        self.sequence_length = self.model.sequence_length  # Longitud de la secuencia usada en el modelo.\n",
    "        self.batch_size = batch_size  # Tamaño del lote para el entrenamiento.\n",
    "        self.optim = optim  # Optimizador para ajustar los parámetros del modelo.\n",
    "        setattr(self.optim, 'model', self.model)  # Establece el modelo en el optimizador.\n",
    "\n",
    "    def _generate_inputs_targets(self, start_pos: int):\n",
    "        # Genera índices para los lotes de entradas y objetivos desde una posición inicial.\n",
    "        inputs_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
    "        targets_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            inputs_indices[i, :] = np.array([self.char_to_idx[ch] \n",
    "                            for ch in self.data[start_pos + i: start_pos + self.sequence_length  + i]])\n",
    "            targets_indices[i, :] = np.array([self.char_to_idx[ch] \n",
    "                         for ch in self.data[start_pos + 1 + i: start_pos + self.sequence_length + 1 + i]])\n",
    "\n",
    "        return inputs_indices, targets_indices\n",
    "\n",
    "    def _generate_one_hot_array(self, indices: ndarray):\n",
    "        # Convierte los índices de caracteres a una representación one-hot.\n",
    "        batch = []\n",
    "        for seq in indices:\n",
    "            one_hot_sequence = np.zeros((self.sequence_length, self.vocab_size))\n",
    "            for i in range(self.sequence_length):\n",
    "                one_hot_sequence[i, seq[i]] = 1.0\n",
    "            batch.append(one_hot_sequence) \n",
    "        return np.stack(batch)\n",
    "\n",
    "    def sample_output(self, input_char: int, sample_length: int):\n",
    "        # Genera una muestra de salida del modelo actual, caracter por caracter.\n",
    "        indices = []\n",
    "        sample_model = deepcopy(self.model)  # Hace una copia del modelo para usar en muestreo.\n",
    "        for i in range(sample_length):\n",
    "            input_char_batch = np.zeros((1, 1, self.vocab_size))\n",
    "            input_char_batch[0, 0, input_char] = 1.0\n",
    "            x_batch_out = sample_model.forward(input_char_batch)\n",
    "            x_softmax = batch_softmax(x_batch_out)\n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
    "            indices.append(input_char)\n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "\n",
    "    def train(self, num_iterations: int, sample_every: int=100):\n",
    "        # Entrena el modelo para generar caracteres.\n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        num_iter = 0\n",
    "        start_pos = 0\n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            if start_pos + self.sequence_length + self.batch_size + 1 > len(self.data):\n",
    "                start_pos = 0\n",
    "            inputs_indices, targets_indices = self._generate_inputs_targets(start_pos)\n",
    "            inputs_batch, targets_batch = \\\n",
    "                self._generate_one_hot_array(inputs_indices), self._generate_one_hot_array(targets_indices)\n",
    "            loss = self.model.single_step(inputs_batch, targets_batch)\n",
    "            self.optim.step()\n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            start_pos += self.batch_size\n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            if num_iter % sample_every == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], 200)\n",
    "                print(sample_text)\n",
    "            num_iter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6df46d-d512-496a-a7d4-b48ce873c01a",
   "metadata": {},
   "source": [
    "#### Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4e3f6-a27a-4b51-8df5-c95ead849313",
   "metadata": {},
   "outputs": [],
   "source": [
    "capas = [RNNLayer(hidden_size=256, output_size=62)]\n",
    "mod = RNNModel(layers=capas,\n",
    "               vocab_size=62, sequence_length=10,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = SGD(lr=0.001, gradient_clipping=True)\n",
    "trainer = RNNTrainer('Ejemplo.txt', mod, optim)\n",
    "trainer.train(10000, sample_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc29180",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "#### **Ejercicio 1: Redes neuronales recurrentes y compartición de pesos**\n",
    "\n",
    "**Tema:** Redes neuronales recurrentes como compartición de pesos a lo largo del tiempo.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Conceptualización:**\n",
    "   - Explica cómo las redes neuronales recurrentes (RNN) implementan el concepto de **compartición de pesos** y por qué es beneficioso en problemas de secuencias.\n",
    "\n",
    "2. **Análisis del código:**\n",
    "   - Revisa el código proporcionado y señala dónde se implementa la compartición de pesos en la RNN.\n",
    "   - Describe cómo esta implementación afecta al entrenamiento y a la capacidad del modelo para generalizar en secuencias de diferentes longitudes.\n",
    "\n",
    "**Sugerencias:**\n",
    "\n",
    "1. **Conceptualización:**\n",
    "\n",
    "   Las RNNs implementan la compartición de pesos al utilizar los mismos pesos y biases en cada paso de tiempo de la secuencia. Esto significa que la transformación aplicada a la entrada en cada paso es la misma, independientemente de la posición en la secuencia.\n",
    "\n",
    "   **Beneficios:**\n",
    "   - **Reducción de parámetros:** Al compartir pesos, el número total de parámetros que el modelo necesita aprender se reduce significativamente.\n",
    "   - **Generalización en secuencias variables:** Permite que el modelo generalice mejor en secuencias de diferentes longitudes, ya que aplica la misma transformación en cada paso.\n",
    "   - **Captura de dependencias temporales:** Facilita el aprendizaje de patrones temporales y dependencias a largo plazo en los datos secuenciales.\n",
    "\n",
    "2. **Análisis del código:**\n",
    "\n",
    "   - **Implementación de compartición de pesos:**\n",
    "     - En el código, la clase `RNNNode` representa un nodo de la RNN que procesa un paso de tiempo en la secuencia.\n",
    "     - Los parámetros `W_f`, `B_f`, `W_v`, y `B_v` se almacenan en `params_dict` y son compartidos por todos los nodos en la secuencia.\n",
    "     - Al instanciar los nodos en `self.cells = [RNNNode() for _ in range(input_.shape[1])]`, se crean múltiples nodos que utilizarán los mismos parámetros durante el forward y backward.\n",
    "\n",
    "   - **Efecto en entrenamiento y generalización:**\n",
    "     - **Entrenamiento:** La compartición de pesos permite que el gradiente acumulado durante el backward pase a través de todos los pasos de tiempo, actualizando los mismos parámetros. Esto mejora la eficiencia del entrenamiento.\n",
    "     - **Generalización:** Al aplicar la misma transformación en cada paso, el modelo puede generalizar su aprendizaje a secuencias más largas o más cortas que las vistas durante el entrenamiento, siempre y cuando los patrones temporales sean consistentes.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 2: Implementación de RNNs en PyTorch**\n",
    "\n",
    "**Tema:** Implementación práctica de una RNN simple en PyTorch para un problema de clasificación de secuencias.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Problema a resolver:**\n",
    "   - Crea un modelo RNN en PyTorch que clasifique secuencias de números enteros en dos clases: secuencias cuya suma es par y secuencias cuya suma es impar.\n",
    "\n",
    "2. **Implementación:**\n",
    "   - Diseña la red neuronal con las siguientes especificaciones:\n",
    "     - Una capa de embedding para mapear los números enteros a vectores de dimensión fija.\n",
    "     - Una capa RNN (puedes usar `nn.RNN` o `nn.LSTM`).\n",
    "     - Una capa fully connected para la clasificación final.\n",
    "   - Utiliza la salida del **último paso de tiempo** para hacer la predicción.\n",
    "\n",
    "3. **Entrenamiento:**\n",
    "   - Genera un conjunto de datos sintético con secuencias de longitud fija.\n",
    "   - Entrena el modelo y reporta la precisión en un conjunto de prueba.\n",
    "\n",
    "**Sugerencia:**\n",
    "\n",
    "1. **Problema a resolver:**\n",
    "\n",
    "   Queremos clasificar secuencias de números enteros en dos clases:\n",
    "   - **Clase 0:** Si la suma de los elementos de la secuencia es par.\n",
    "   - **Clase 1:** Si la suma es impar.\n",
    "\n",
    "2. **Implementación:**\n",
    "\n",
    "   ```python\n",
    "   import torch\n",
    "   import torch.nn as nn\n",
    "   import torch.optim as optim\n",
    "   from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "   # Definición del modelo\n",
    "   class ParityRNN(nn.Module):\n",
    "       def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "           super(ParityRNN, self).__init__()\n",
    "           self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "           self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "           self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "       \n",
    "       def forward(self, x):\n",
    "           embedded = self.embedding(x)\n",
    "           output, hidden = self.rnn(embedded)\n",
    "           # Usamos la salida del último paso de tiempo\n",
    "           out = self.fc(hidden.squeeze(0))\n",
    "           return out\n",
    "\n",
    "   # Hiperparámetros\n",
    "   VOCAB_SIZE = 10  # Números del 0 al 9\n",
    "   EMBEDDING_DIM = 16\n",
    "   HIDDEN_DIM = 32\n",
    "   OUTPUT_DIM = 2  # Par o impar\n",
    "   SEQ_LENGTH = 5\n",
    "   BATCH_SIZE = 32\n",
    "   NUM_EPOCHS = 5\n",
    "\n",
    "   # Generación del conjunto de datos sintético\n",
    "   class ParityDataset(Dataset):\n",
    "       def __init__(self, num_samples):\n",
    "           self.data = []\n",
    "           self.labels = []\n",
    "           for _ in range(num_samples):\n",
    "               seq = torch.randint(0, VOCAB_SIZE, (SEQ_LENGTH,))\n",
    "               label = seq.sum().item() % 2  # 0 si es par, 1 si es impar\n",
    "               self.data.append(seq)\n",
    "               self.labels.append(label)\n",
    "       \n",
    "       def __len__(self):\n",
    "           return len(self.data)\n",
    "       \n",
    "       def __getitem__(self, idx):\n",
    "           return self.data[idx], self.labels[idx]\n",
    "\n",
    "   train_dataset = ParityDataset(1000)\n",
    "   test_dataset = ParityDataset(200)\n",
    "   train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "   test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "   # Instanciación del modelo, criterio y optimizador\n",
    "   model = ParityRNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "   criterion = nn.CrossEntropyLoss()\n",
    "   optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "   # Entrenamiento\n",
    "   for epoch in range(NUM_EPOCHS):\n",
    "       model.train()\n",
    "       total_loss = 0\n",
    "       for sequences, labels in train_loader:\n",
    "           optimizer.zero_grad()\n",
    "           outputs = model(sequences)\n",
    "           loss = criterion(outputs, labels)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "           total_loss += loss.item()\n",
    "       print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "   # Evaluación\n",
    "   model.eval()\n",
    "   correct = 0\n",
    "   total = 0\n",
    "   with torch.no_grad():\n",
    "       for sequences, labels in test_loader:\n",
    "           outputs = model(sequences)\n",
    "           _, predicted = torch.max(outputs.data, 1)\n",
    "           total += labels.size(0)\n",
    "           correct += (predicted == labels).sum().item()\n",
    "   print(f'Accuracy on test set: {100 * correct / total}%')\n",
    "   ```\n",
    "\n",
    "3. **Resultados:**\n",
    "\n",
    "   Después del entrenamiento, el modelo debería lograr una precisión razonablemente alta en el conjunto de prueba, demostrando su capacidad para aprender la tarea de clasificación basada en la suma de la secuencia.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 3: Uso de capas de embedding**\n",
    "\n",
    "**Tema:** Implementación y beneficios de las capas de embedding en modelos de RNN.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Conceptualización:**\n",
    "   - Explica qué es una capa de embedding y por qué es útil en el procesamiento de secuencias discretas como texto o números enteros.\n",
    "\n",
    "2. **Implementación práctica:**\n",
    "   - En el modelo del **Ejercicio 2**, reemplaza la capa de embedding por una representación one-hot de los números de entrada.\n",
    "   - Compara el rendimiento y el tiempo de entrenamiento del modelo con y sin la capa de embedding.\n",
    "\n",
    "**Sugerencia:**\n",
    "\n",
    "1. **Conceptualización:**\n",
    "\n",
    "   Una capa de embedding es una capa que convierte entradas discretas (como índices de palabras o símbolos) en vectores densos de dimensión fija. Estos vectores densos capturan relaciones semánticas entre los elementos discretos y reducen la dimensionalidad en comparación con las representaciones one-hot.\n",
    "\n",
    "   **Beneficios:**\n",
    "   - **Dimensionalidad reducida:** Las representaciones embedding son de menor dimensión que las one-hot, lo que reduce el número de parámetros y el consumo de memoria.\n",
    "   - **Captura de relaciones semánticas:** Los embeddings pueden aprender relaciones entre los elementos, mejorando el rendimiento del modelo.\n",
    "   - **Eficiencia computacional:** Menos operaciones y multiplicaciones de matrices más pequeñas.\n",
    "\n",
    "2. **Implementación práctica:**\n",
    "\n",
    "   **Reemplazo por representación One-Hot:**\n",
    "\n",
    "   ```python\n",
    "   class ParityRNNOneHot(nn.Module):\n",
    "       def __init__(self, vocab_size, hidden_dim, output_dim):\n",
    "           super(ParityRNNOneHot, self).__init__()\n",
    "           self.rnn = nn.RNN(vocab_size, hidden_dim, batch_first=True)\n",
    "           self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "       \n",
    "       def forward(self, x):\n",
    "           # Convertimos a one-hot\n",
    "           batch_size = x.size(0)\n",
    "           seq_length = x.size(1)\n",
    "           x_one_hot = torch.zeros(batch_size, seq_length, vocab_size)\n",
    "           x_one_hot.scatter_(2, x.unsqueeze(2), 1)\n",
    "           output, hidden = self.rnn(x_one_hot)\n",
    "           out = self.fc(hidden.squeeze(0))\n",
    "           return out\n",
    "\n",
    "   # Instanciamos y entrenamos el modelo como antes\n",
    "   model_one_hot = ParityRNNOneHot(VOCAB_SIZE, HIDDEN_DIM, OUTPUT_DIM)\n",
    "   # ... (entrenamiento similar al anterior)\n",
    "   ```\n",
    "\n",
    "   **Comparación:**\n",
    "\n",
    "   - **Rendimiento:** Es posible que el modelo con representación one-hot tenga un rendimiento similar o ligeramente inferior, ya que no captura relaciones entre los números.\n",
    "   - **Tiempo de entrenamiento:** El modelo con one-hot puede ser más lento debido a la mayor dimensionalidad de las entradas y las operaciones adicionales para crear las representaciones one-hot.\n",
    "\n",
    "---\n",
    "#### **Ejercicio 4: Predicciones utilizando el último paso de tiempo**\n",
    "\n",
    "**Tema:** Uso del último estado oculto en RNNs para hacer predicciones.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Explicación:**\n",
    "   - Justifica por qué en problemas de clasificación de secuencias es común utilizar la salida del último paso de tiempo para hacer predicciones.\n",
    "\n",
    "2. **Implementación alternativa:**\n",
    "   - Modifica el modelo del **Ejercicio 2** para utilizar la salida promedio de todos los pasos de tiempo en lugar del último estado oculto.\n",
    "   - Entrena el modelo con esta modificación y compara los resultados con la versión original.\n",
    "\n",
    "**Sugerencia:**\n",
    "\n",
    "1. **Explicación:**\n",
    "\n",
    "   En problemas donde la predicción depende de la secuencia completa, el último estado oculto de una RNN estándar (como `nn.RNN` o `nn.LSTM`) contiene información agregada de todos los pasos anteriores. Por lo tanto, es adecuado para capturar la representación de la secuencia entera.\n",
    "\n",
    "2. **Implementación alternativa:**\n",
    "\n",
    "   ```python\n",
    "   class ParityRNNAverage(nn.Module):\n",
    "       def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "           super(ParityRNNAverage, self).__init__()\n",
    "           self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "           self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "           self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "       \n",
    "       def forward(self, x):\n",
    "           embedded = self.embedding(x)\n",
    "           output, hidden = self.rnn(embedded)\n",
    "           # Promediamos las salidas de todos los pasos de tiempo\n",
    "           out = self.fc(output.mean(dim=1))\n",
    "           return out\n",
    "\n",
    "   # Entrenamiento similar al anterior\n",
    "   model_avg = ParityRNNAverage(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "   # ... (entrenamiento y evaluación)\n",
    "   ```\n",
    "\n",
    "   **Comparación de resultados:**\n",
    "\n",
    "   - **Rendimiento:** Dependiendo de la tarea, promediar las salidas puede mejorar o empeorar el rendimiento. En este caso, como la suma de la secuencia determina la clase, utilizar el último estado oculto (que acumula la información) suele ser más efectivo.\n",
    "   - **Interpretación:** Promediar puede diluir la información importante que se captura en los últimos pasos, especialmente en secuencias donde los elementos finales son más relevantes.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 5: Mejorando el tiempo de entrenamiento con packing**\n",
    "\n",
    "**Tema:** Uso de `pad_sequence` y `pack_padded_sequence` en PyTorch para manejar secuencias de longitud variable.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Preparación de datos:**\n",
    "   - Modifica el conjunto de datos para que las secuencias tengan longitudes variables entre 3 y 7.\n",
    "   - Asegúrate de que las secuencias estén correctamente etiquetadas según la suma de sus elementos.\n",
    "\n",
    "2. **Implementación:**\n",
    "   - Utiliza `pad_sequence` para crear lotes con secuencias de diferentes longitudes.\n",
    "   - Emplea `pack_padded_sequence` y `pad_packed_sequence` para que la RNN procese eficientemente las secuencias.\n",
    "\n",
    "3. **Entrenamiento y evaluación:**\n",
    "   - Entrena el modelo con las secuencias de longitud variable.\n",
    "   - Compara el tiempo de entrenamiento y el rendimiento con respecto al modelo que utiliza secuencias de longitud fija.\n",
    "\n",
    "**Sugerencia:**\n",
    "\n",
    "1. **Preparación de datos:**\n",
    "\n",
    "   ```python\n",
    "   from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "   class VariableLengthParityDataset(Dataset):\n",
    "       def __init__(self, num_samples):\n",
    "           self.data = []\n",
    "           self.labels = []\n",
    "           self.lengths = []\n",
    "           for _ in range(num_samples):\n",
    "               seq_length = torch.randint(3, 8, (1,)).item()\n",
    "               seq = torch.randint(0, VOCAB_SIZE, (seq_length,))\n",
    "               label = seq.sum().item() % 2\n",
    "               self.data.append(seq)\n",
    "               self.labels.append(label)\n",
    "               self.lengths.append(seq_length)\n",
    "       \n",
    "       def __len__(self):\n",
    "           return len(self.data)\n",
    "       \n",
    "       def __getitem__(self, idx):\n",
    "           return self.data[idx], self.labels[idx], self.lengths[idx]\n",
    "   ```\n",
    "\n",
    "2. **Implementación:**\n",
    "\n",
    "   **Función de collate para DataLoader:**\n",
    "\n",
    "   ```python\n",
    "   def collate_fn(batch):\n",
    "       batch.sort(key=lambda x: x[2], reverse=True)  # Ordenar por longitud\n",
    "       sequences, labels, lengths = zip(*batch)\n",
    "       sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "       labels = torch.tensor(labels)\n",
    "       lengths = torch.tensor(lengths)\n",
    "       return sequences_padded, labels, lengths\n",
    "   ```\n",
    "\n",
    "   **Modelo modificado:**\n",
    "\n",
    "   ```python\n",
    "   class ParityRNNPacked(nn.Module):\n",
    "       def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "           super(ParityRNNPacked, self).__init__()\n",
    "           self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "           self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "           self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "       \n",
    "       def forward(self, x, lengths):\n",
    "           embedded = self.embedding(x)\n",
    "           packed_input = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True)\n",
    "           packed_output, hidden = self.rnn(packed_input)\n",
    "           out = self.fc(hidden.squeeze(0))\n",
    "           return out\n",
    "   ```\n",
    "\n",
    "3. **Entrenamiento y evaluación:**\n",
    "\n",
    "   ```python\n",
    "   # Dataset y DataLoader\n",
    "   train_dataset_var = VariableLengthParityDataset(1000)\n",
    "   train_loader_var = DataLoader(train_dataset_var, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "   # Modelo y entrenamiento\n",
    "   model_packed = ParityRNNPacked(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "   # ... (entrenamiento similar al anterior, pasando 'lengths' al forward)\n",
    "\n",
    "   # Evaluación\n",
    "   # ... (ajustar el DataLoader y el bucle de evaluación)\n",
    "   ```\n",
    "\n",
    "   **Comparación:**\n",
    "\n",
    "   - **Tiempo de entrenamiento:** Usar `pack_padded_sequence` evita cómputos innecesarios en los pasos de tiempo que corresponden al padding, mejorando la eficiencia.\n",
    "   - **Rendimiento:** El modelo debería mantener o incluso mejorar su precisión, ya que ahora maneja secuencias de longitud variable de manera más adecuada.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 6: Simultáneo uso de inputs empaquetados y no empaquetados**\n",
    "\n",
    "**Tema:** Manejar capas que no soportan inputs empaquetados después de una RNN que utiliza `pack_padded_sequence`.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. **Contexto:**\n",
    "   - Después de la RNN, queremos agregar una capa que no soporta inputs empaquetados, como una capa de atención personalizada.\n",
    "\n",
    "2. **Implementación:**\n",
    "   - Modifica el modelo para desempaquetar la salida de la RNN antes de pasarla a la siguiente capa.\n",
    "   - Asegúrate de que el modelo sigue siendo eficiente y que maneja correctamente las secuencias de longitud variable.\n",
    "\n",
    "**Sugerencia:**\n",
    "\n",
    "```python\n",
    "class ParityRNNWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(ParityRNNWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_input = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True)\n",
    "        packed_output, _ = self.rnn(packed_input)\n",
    "        # Desempaquetamos la salida\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # Aplicamos atención\n",
    "        attn_weights = torch.softmax(self.attention(output), dim=1)\n",
    "        attn_output = torch.sum(output * attn_weights, dim=1)\n",
    "        out = self.fc(attn_output)\n",
    "        return out\n",
    "```\n",
    "\n",
    "**Notas:**\n",
    "\n",
    "- **Desempaquetado necesario:** Para aplicar la capa de atención, necesitamos la salida completa de la RNN sin empaquetar.\n",
    "- **Eficiencia mantener:** Aunque desempaquetamos la salida, el uso de `pack_padded_sequence` en la RNN sigue aportando beneficios en términos de eficiencia computacional.\n",
    "- **Manejo de longitudes:** Al desempaquetar, obtenemos tensores donde las secuencias más cortas tienen padding al final, lo cual manejamos correctamente en la capa de atención.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95dfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8b3e6-eaee-41f6-b7b4-be5ac29c497d",
   "metadata": {},
   "source": [
    "### Ejercicios adicionales\n",
    "\n",
    "#### Ejercicio 1: Extensión a LSTM y GRU\n",
    "\n",
    "1. Implementa LSTMNode y GRUNode: Basándote en la estructura de RNNNode, implementa dos nuevas clases, LSTMNode y GRUNode, que representen las operaciones específicas de las celdas LSTM y GRU, respectivamente.\n",
    "2. Actualiza RNNLayer: Modifica la clase RNNLayer para que pueda utilizar RNNNode, LSTMNode, o GRUNode según un parámetro de configuración. Esto podría implicar agregar un argumento adicional en el constructor que especifique el tipo de nodo a utilizar.\n",
    "\n",
    "3. Experimentación: Entrena modelos utilizando las diferentes configuraciones de nodos (RNN simple, LSTM, GRU) en un conjunto de datos de texto para comparar su rendimiento en términos de velocidad de convergencia y capacidad de generación de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc5505-d2ee-468d-8620-cb31794c66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054aa272-e031-4914-9e56-9916f9fb9942",
   "metadata": {},
   "source": [
    "#### Ejercicio 2: Análisis de sentimientos usando RNN\n",
    "\n",
    "\n",
    "1. Preprocesa un conjunto de datos de reseñas de películas o tweets para convertir el texto en secuencias de índices.\n",
    "\n",
    "2. Modifica el  RNNModel:Asegúrate de que RNNModel pueda manejar tareas de clasificación agregando una capa densa al final con una activación de softmax o sigmoide.\n",
    "\n",
    "3. Entrena y compara: Entrena el modelo usando RNN, LSTM, y GRU, y compara su efectividad en la clasificación de sentimientos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e97fa6-e5cc-4650-8a15-67a0fa5d3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c002bc-2139-4b3a-a35b-d9f0ab56374d",
   "metadata": {},
   "source": [
    "#### Ejercicio 3: Autoencoders recurrentes para la detección de anomalías\n",
    "\n",
    "1. Modifica la arquitectura actual para crear un autoencoder, donde la capa RNNLayer sirva como encoder y decoder. La entrada al decoder puede ser la representación codificada de la entrada más una secuencia de \"start tokens\" para la reconstrucción.\n",
    "\n",
    "2. Detección de anomalías: Entrena el autoencoder en datos normales y luego utiliza el error de reconstrucción para detectar anomalías en nuevos datos.\n",
    "\n",
    "3. Experimentación: Utiliza un conjunto de datos como el de series temporales de sensores o datos financieros para entrenar y evaluar el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9a057-84ec-4ca0-bfc8-e32acdf61e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e6165-c2ab-43af-9d76-ecaacac86b66",
   "metadata": {},
   "source": [
    "#### Ejercicio 4: Mejora y optimización del RNNTrainer\n",
    "\n",
    "1. Implementa métodos en RNNTrainer para guardar el estado del modelo en puntos específicos durante el entrenamiento y cargar modelos previamente entrenados.\n",
    "\n",
    "2. Early stopping: Añade una comprobación de early stopping para terminar el entrenamiento si el modelo no mejora después de un número determinado de épocas.\n",
    "\n",
    "3. Integra una programación de la tasa de aprendizaje que ajuste automáticamente el lr del optimizador basándose en el progreso del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7a8d8-4613-4da0-a2e0-432012bb9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
