{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9754276-8fb2-4e52-99f0-ea134cf627e1",
   "metadata": {},
   "source": [
    "### Preliminares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf301f-f696-4600-bee4-b8bcea8f8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab39538-6273-4f0f-b615-e58b66518063",
   "metadata": {},
   "source": [
    "#### Definimos las funciones de activación a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aace101-3333-4df1-bbfe-ff951dcc9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: ndarray):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x: ndarray):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def tanh(x: ndarray):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(x: ndarray):\n",
    "    return 1 - np.tanh(x) * np.tanh(x)\n",
    "\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
    "\n",
    "\n",
    "def batch_softmax(input_array: ndarray):\n",
    "    out = []\n",
    "    for row in input_array:\n",
    "        out.append(softmax(row, axis=1))\n",
    "    return np.stack(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d66ad-1ebe-45b8-b928-0d8f0f54b41f",
   "metadata": {},
   "source": [
    "##### Optimizador de redes neuronales recurrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc5b71-2bce-499b-8b38-b87711a3b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNOptimizer(object):\n",
    "    # Constructor de la clase optimizador para redes neuronales recurrentes.\n",
    "    # Args:\n",
    "    #   lr (float): Tasa de aprendizaje, con un valor por defecto de 0.01.\n",
    "    #   gradient_clipping (bool): Indica si se aplica clipping a los gradientes,\n",
    "    #                             activado por defecto.\n",
    "    def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -> None:\n",
    "        self.lr = lr  # Almacena la tasa de aprendizaje.\n",
    "        self.gradient_clipping = gradient_clipping  # Almacena la configuración de clipping.\n",
    "        self.first = True  # Variable auxiliar, posiblemente para controlar la primera actualización.\n",
    "\n",
    "    # Método que ejecuta un paso de optimización sobre todos los parámetros del modelo.\n",
    "    def step(self) -> None:\n",
    "        # Itera sobre cada capa del modelo.\n",
    "        for layer in self.model.layers:\n",
    "            # Itera sobre cada parámetro de la capa.\n",
    "            for key in layer.params.keys():\n",
    "                # Si el clipping de gradientes está activado, aplica clipping.\n",
    "                if self.gradient_clipping:\n",
    "                    # Los gradientes se limitan a estar entre -2 y 2 para evitar la explosión de gradientes.\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "\n",
    "                # Llama a la regla de actualización para ajustar el valor del parámetro\n",
    "                # usando la tasa de aprendizaje y el gradiente actual.\n",
    "                self._update_rule(param=layer.params[key]['value'],\n",
    "                                  grad=layer.params[key]['deriv'])\n",
    "\n",
    "    # Método abstracto para definir la regla de actualización de parámetros.\n",
    "    # Debe ser implementado por subclases.\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        raise NotImplementedError(\"Este método debe ser implementado por subclases.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff18f2-df0c-47e7-87ee-1a64478cc3df",
   "metadata": {},
   "source": [
    "#### Algunos optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da952c52-241f-4660-835b-ebd0569d56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(RNNOptimizer):\n",
    "    # Constructor de la clase SGD (Stochastic Gradient Descent).\n",
    "    # Args:\n",
    "    #   lr (float): Tasa de aprendizaje.\n",
    "    #   gradient_clipping (bool): Indica si se activa el clipping de gradientes.\n",
    "    def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -> None:\n",
    "        super().__init__(lr, gradient_clipping)  # Inicializa la clase base\n",
    "\n",
    "    # Método específico de SGD para actualizar los parámetros según el gradiente.\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        update = self.lr * kwargs['grad']  # Calcula la actualización del parámetro.\n",
    "        kwargs['param'] -= update  # Actualiza el parámetro restando la actualización.\n",
    "        \n",
    "class AdaGrad(RNNOptimizer):\n",
    "    # Constructor de la clase AdaGrad.\n",
    "    # Args:\n",
    "    #   lr (float): Tasa de aprendizaje.\n",
    "    #   gradient_clipping (bool): Indica si se activa el clipping de gradientes.\n",
    "    def __init__(self, lr: float = 0.01, gradient_clipping: bool = True) -> None:\n",
    "        super().__init__(lr, gradient_clipping)  # Inicializa la clase base\n",
    "        self.eps = 1e-7  # Un pequeño número para evitar división por cero en la actualización.\n",
    "\n",
    "    # Método que ejecuta un paso de optimización sobre todos los parámetros del modelo.\n",
    "    def step(self) -> None:\n",
    "        if self.first:\n",
    "            self.sum_squares = {}  # Diccionario para almacenar la suma acumulada de los cuadrados de los gradientes.\n",
    "            # Inicializa sum_squares para cada parámetro en cada capa.\n",
    "            for i, layer in enumerate(self.model.layers):\n",
    "                self.sum_squares[i] = {}\n",
    "                for key in layer.params.keys():\n",
    "                    self.sum_squares[i][key] = np.zeros_like(layer.params[key]['value'])\n",
    "            self.first = False  # Marca la inicialización como completa.\n",
    "\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            for key in layer.params.keys():\n",
    "                if self.gradient_clipping:\n",
    "                    # Si el clipping de gradientes está activado, aplica clipping.\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "\n",
    "                # Actualiza cada parámetro utilizando la regla específica de AdaGrad.\n",
    "                self._update_rule(param=layer.params[key]['value'],\n",
    "                                  grad=layer.params[key]['deriv'],\n",
    "                                  sum_square=self.sum_squares[i][key])\n",
    "\n",
    "    # Método específico de AdaGrad para actualizar los parámetros.\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        # Actualiza la suma acumulada de los cuadrados de los gradientes.\n",
    "        kwargs['sum_square'] += self.eps + np.power(kwargs['grad'], 2)\n",
    "        # Calcula la tasa de aprendizaje escalada.\n",
    "        lr_scaled = self.lr / np.sqrt(kwargs['sum_square'])\n",
    "        # Utiliza la tasa de aprendizaje escalada para actualizar los parámetros.\n",
    "        kwargs['param'] -= lr_scaled * kwargs['grad']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f9936-c0e7-4294-88b8-6659aee3d1ef",
   "metadata": {},
   "source": [
    "#### Funciones de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9e0d4-e4d9-46c3-b719-f0faadf2b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def assert_same_shape(output: np.ndarray, output_grad: np.ndarray):\n",
    "    assert output.shape == output_grad.shape, \\\n",
    "        '''\n",
    "        Dos tensores deben tener la misma forma;\n",
    "        en cambio, la forma del primer tensor es {0}\n",
    "        y la forma del segundo tensor es {1}.\n",
    "        '''.format(tuple(output_grad.shape), tuple(output.shape))\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68577fda-7a5b-4145-867d-d29bdc700294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    # Constructor de la clase base para las pérdidas.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Método forward para calcular la pérdida entre las predicciones y los objetivos.\n",
    "    def forward(self,\n",
    "                prediction: ndarray,\n",
    "                target: ndarray) -> float:\n",
    "        # Asegura que las predicciones y los objetivos tengan la misma forma.\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction  # Almacena las predicciones.\n",
    "        self.target = target  # Almacena los objetivos.\n",
    "\n",
    "        self.output = self._output()  # Calcula la pérdida utilizando una función interna.\n",
    "\n",
    "        return self.output  # Devuelve el valor de la pérdida.\n",
    "    \n",
    "    # Método backward para calcular el gradiente de la pérdida respecto a las entradas.\n",
    "    def backward(self) -> ndarray:\n",
    "        self.input_grad = self._input_grad()  # Calcula el gradiente utilizando una función interna.\n",
    "\n",
    "        # Asegura que las predicciones y el gradiente tengan la misma forma.\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad  # Devuelve el gradiente de la entrada.\n",
    "\n",
    "    # Método abstracto para calcular la pérdida; debe ser implementado por subclases.\n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Método abstracto para calcular el gradiente de la entrada; debe ser implementado por subclases.\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    # Constructor de la clase para la pérdida de entropía cruzada con softmax.\n",
    "    def __init__(self, eps: float=1e-9) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps  # Un pequeño número para evitar inestabilidad numérica.\n",
    "\n",
    "    # Método para calcular la salida de la pérdida de entropía cruzada con softmax.\n",
    "    def _output(self) -> float:\n",
    "        out = []\n",
    "        # Aplica softmax a cada fila de la predicción.\n",
    "        for row in self.prediction:\n",
    "            out.append(softmax(row, axis=1))\n",
    "        softmax_preds = np.stack(out)\n",
    "\n",
    "        # Recorta la salida de softmax para prevenir inestabilidad numérica.\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # Calcula la pérdida real de entropía cruzada.\n",
    "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "            (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss)  # Retorna la suma total de la pérdida.\n",
    "\n",
    "    # Método para calcular el gradiente de la entrada basado en la salida de softmax.\n",
    "    def _input_grad(self) -> np.ndarray:\n",
    "        return self.softmax_preds - self.target  # Gradiente de la pérdida respecto a la entrada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672d134-e2fd-48a3-8e73-1de5671ddaf8",
   "metadata": {},
   "source": [
    " ## Redes neuronales recurrentes\n",
    "Una red neuronal recurrente (RNN) es una clase de redes neuronales diseñadas para manejar secuencias de datos, como series temporales o secuencias lingüísticas. \n",
    "\n",
    "A diferencia de las redes neuronales tradicionales que asumen independencia entre las entradas, las RNNs tienen \"memoria\" sobre entradas anteriores. Esto les permite retener información a través del tiempo y utilizar esta información para influir en la salida actual, lo cual es crucial para tareas donde el contexto y el orden de los datos son importantes, como el procesamiento del lenguaje natural o el análisis de series temporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fac0e-6e07-4c7d-8489-58590dd6dac6",
   "metadata": {},
   "source": [
    "#### Clase RNNNode\n",
    "La clase RNNNode define un nodo dentro de una red neuronal recurrente (RNN), capaz de realizar cálculos hacia adelante y hacia atrás. El método forward calcula la salida y el nuevo estado oculto del nodo basado en la entrada actual y el estado oculto anterior, utilizando matrices de pesos y sesgos. El método backward se encarga de la retropropagación del error, calculando los gradientes respecto a las entradas y actualizando los parámetros del modelo basados en estos gradientes. Esto permite entrenar la red para ajustar sus pesos y mejorar la precisión de sus predicciones a lo largo del tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5009e-d6cc-401d-8167-944bfb994d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNode(object):\n",
    "    # Constructor del nodo RNN, inicialmente no realiza ninguna operación específica.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Método forward para calcular la salida del nodo RNN a partir de la entrada y el estado oculto anterior.\n",
    "    def forward(self,\n",
    "                x_in: ndarray, \n",
    "                H_in: ndarray,\n",
    "                params_dict: Dict[str, Dict[str, ndarray]]\n",
    "                ) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        Calcula la salida del nodo RNN y el nuevo estado oculto.\n",
    "        Args:\n",
    "        x_in: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        H_in: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        Retorna x_out: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        Retorna H_out: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        '''\n",
    "        self.X_in = x_in\n",
    "        self.H_in = H_in\n",
    "    \n",
    "        # Concatena la entrada x con el estado oculto anterior H.\n",
    "        self.Z = np.column_stack((x_in, H_in))\n",
    "        \n",
    "        # Calcula el nuevo estado oculto intermedio usando los pesos y biases.\n",
    "        self.H_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
    "        \n",
    "        # Aplica la función de activación tanh al estado oculto intermedio.\n",
    "        self.H_out = tanh(self.H_int)\n",
    "\n",
    "        # Calcula la salida del nodo RNN.\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out\n",
    "\n",
    "    # Método backward para la propagación hacia atrás del error a través del nodo RNN.\n",
    "    def backward(self, \n",
    "                 X_out_grad: ndarray, \n",
    "                 H_out_grad: ndarray,\n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        Retropropaga el gradiente a través del nodo RNN.\n",
    "        Args:\n",
    "        X_out_grad: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        H_out_grad: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        Retorna X_in_grad: Arreglo de NumPy con forma (tamaño_lote, tam_vocabulario)\n",
    "        Retorna H_in_grad: Arreglo de NumPy con forma (tamaño_lote, tam_oculto)\n",
    "        '''\n",
    "        \n",
    "        # Verifica que los gradientes y las salidas tengan la misma forma.\n",
    "        assert_same_shape(X_out_grad, self.X_out)\n",
    "        assert_same_shape(H_out_grad, self.H_out)\n",
    "\n",
    "        # Calcula los gradientes para los parámetros de salida y acumula en los derivados.\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        \n",
    "        # Propaga el gradiente hacia atrás a través de la red.\n",
    "        dh = np.dot(X_out_grad, params_dict['W_v']['value'].T)\n",
    "        dh += H_out_grad\n",
    "        \n",
    "        # Calcula el gradiente del estado oculto intermedio.\n",
    "        dH_int = dh * dtanh(self.H_int)\n",
    "        \n",
    "        # Acumula gradientes en los parámetros del estado oculto.\n",
    "        params_dict['B_f']['deriv'] += dH_int.sum(axis=0)\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, dH_int)     \n",
    "        \n",
    "        # Calcula los gradientes de entrada.\n",
    "        dz = np.dot(dH_int, params_dict['W_f']['value'].T)\n",
    "        X_in_grad = dz[:, :self.X_in.shape[1]]\n",
    "        H_in_grad = dz[:, self.X_in.shape[1]:]\n",
    "        \n",
    "        return X_in_grad, H_in_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1207c7e3-a638-4604-aa50-63ed3bbc89ff",
   "metadata": {},
   "source": [
    "#### RNNLayer\n",
    "\n",
    "La clase RNNLayer es una capa de red neuronal recurrente que maneja la propagación hacia adelante y hacia atrás de los datos a través de una secuencia de tiempo. En el método forward, la capa procesa secuencialmente la entrada utilizando nodos RNN internos, cada uno correspondiente a un paso de tiempo, manteniendo un estado oculto que pasa de un nodo a otro. Esta capa es capaz de ajustar sus pesos y sesgos para mejorar la predicción del siguiente carácter en una secuencia.\n",
    "Durante la retropropagación, calcula los gradientes para actualizar los parámetros con el fin de minimizar el error en las predicciones. Esta estructura es fundamental para tareas de procesamiento de secuencias como la generación de texto, donde la dependencia temporal entre los datos es crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46601f36-22ce-4830-b739-0156ec964868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(object):\n",
    "    # Constructor de la clase de la capa RNN.\n",
    "    # Args:\n",
    "    #   hidden_size: int - Número de neuronas ocultas en la capa RNN.\n",
    "    #   output_size: int - Número de caracteres en el vocabulario para predecir el siguiente carácter.\n",
    "    #   weight_scale: float - Escala para la inicialización de los pesos.\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = None):\n",
    "        self.hidden_size = hidden_size  # Almacena el tamaño del estado oculto.\n",
    "        self.output_size = output_size  # Almacena el tamaño de salida.\n",
    "        self.weight_scale = weight_scale  # Escala de inicialización de los pesos.\n",
    "        self.start_H = np.zeros((1, hidden_size))  # Estado oculto inicial.\n",
    "        self.first = True  # Bandera para inicialización en el primer paso forward.\n",
    "\n",
    "    # Método para inicializar los parámetros de la capa.\n",
    "    def _init_params(self, input_: ndarray):\n",
    "        self.vocab_size = input_.shape[2]  # Tamaño del vocabulario a partir de la entrada.\n",
    "        # Establece la escala de peso si no se proporcionó.\n",
    "        if not self.weight_scale:\n",
    "            self.weight_scale = 2 / (self.vocab_size + self.output_size)\n",
    "        \n",
    "        self.params = {'W_f': {}, 'B_f': {}, 'W_v': {}, 'B_v': {}}\n",
    "        # Inicializa pesos y sesgos con valores aleatorios normalizados.\n",
    "        self.params['W_f']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                       (self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(0.0, self.weight_scale, (1, self.hidden_size))\n",
    "        self.params['W_v']['value'] = np.random.normal(0.0, self.weight_scale,\n",
    "                                                       (self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(0.0, self.weight_scale, (1, self.output_size))\n",
    "\n",
    "        self.params['W_f']['deriv'] = np.zeros_like(self.params['W_f']['value'])\n",
    "        self.params['B_f']['deriv'] = np.zeros_like(self.params['B_f']['value'])\n",
    "        self.params['W_v']['deriv'] = np.zeros_like(self.params['W_v']['value'])\n",
    "        self.params['B_v']['deriv'] = np.zeros_like(self.params['B_v']['value'])\n",
    "        \n",
    "        self.cells = [RNNNode() for _ in range(input_.shape[1])]  # Inicializa nodos RNN por cada paso de secuencia.\n",
    "\n",
    "    # Limpia los gradientes acumulados en los parámetros.\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "\n",
    "    # Procesa la entrada a través de la capa RNN y calcula la salida para cada paso de tiempo.\n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)  # Inicializa parámetros en el primer paso.\n",
    "            self.first = False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        H_in = np.repeat(self.start_H, batch_size, axis=0)\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)  # Actualiza el estado oculto inicial para la próxima ejecución.\n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "    # Retropropaga el error desde la salida hacia las entradas.\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        sequence_length = x_seq_out_grad.shape[1]\n",
    "        x_seq_in_grad = np.zeros((batch_size, sequence_length, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(sequence_length)):\n",
    "            x_out_grad = x_seq_out_grad[:, t, :]\n",
    "            grad_out, h_in_grad = self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        \n",
    "        return x_seq_in_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef54c0-a343-4280-b8f5-758e9d8ca1e3",
   "metadata": {},
   "source": [
    "#### RNNModel\n",
    "\n",
    "El código define un modelo de red neuronal recurrente (RNN) que es capaz de procesar secuencias de datos, como series temporales o texto. El modelo está compuesto por varias capas (RNNLayer), cada una procesando la entrada y pasándola a la siguiente. El proceso de entrenamiento ocurre en pasos, donde cada paso involucra:\n",
    "\n",
    "* Paso hacia adelante (forward): Cada entrada de la secuencia es procesada por todas las capas de la red, pasando de una a otra. Esta operación se utiliza para obtener la salida de la red que luego se compara con el objetivo real para calcular la pérdida.\n",
    "* Cálculo de la pérdida: Se usa un objeto de pérdida para evaluar qué tan bien la salida de la red coincide con los objetivos esperados.\n",
    "\n",
    "* Paso hacia atrás (backward): Una vez calculada la pérdida, se calcula el gradiente de la pérdida respecto a las salidas, y este gradiente se propaga hacia atrás a través de la red para actualizar los pesos de las neuronas en cada capa, lo que permite que la red aprenda.\n",
    "\n",
    "* Actualización de parámetros: Basándose en los gradientes obtenidos de la retropropagación, se actualizan los parámetros de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340dcf7-9330-4e5d-af6a-0067c0ef7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(object):\n",
    "    '''\n",
    "    Clase Modelo que recibe entradas y objetivos, entrena la red y calcula la pérdida.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[RNNLayer],\n",
    "                 sequence_length: int, \n",
    "                 vocab_size: int, \n",
    "                 loss: Loss):\n",
    "        '''\n",
    "        Inicializa el modelo de red neuronal recurrente.\n",
    "        Args:\n",
    "        layers: Lista de capas RNN en la red.\n",
    "        sequence_length: Longitud de la secuencia que pasa a través de la red.\n",
    "        vocab_size: Número de caracteres en el vocabulario.\n",
    "        loss: Objeto de pérdida utilizado para calcular la pérdida durante el entrenamiento.\n",
    "        '''\n",
    "        self.layers = layers  # Lista de capas RNN.\n",
    "        self.vocab_size = vocab_size  # Tamaño del vocabulario.\n",
    "        self.sequence_length = sequence_length  # Longitud de la secuencia.\n",
    "        self.loss = loss  # Objeto de pérdida.\n",
    "        # Establece la longitud de la secuencia para cada capa.\n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "    def forward(self, \n",
    "                x_batch: ndarray):\n",
    "        '''\n",
    "        Realiza la propagación hacia adelante a través de la red.\n",
    "        Args:\n",
    "        x_batch: Array de entrada con forma (tamaño_lote, longitud_secuencia, tamaño_vocabulario)\n",
    "        Returns:\n",
    "        x_batch_in: Array de salida de la última capa.\n",
    "        '''       \n",
    "        for layer in self.layers:\n",
    "            x_batch = layer.forward(x_batch)  # Propaga la entrada a través de cada capa.\n",
    "        return x_batch\n",
    "        \n",
    "    def backward(self, \n",
    "                 loss_grad: ndarray):\n",
    "        '''\n",
    "        Realiza la retropropagación a través de la red utilizando el gradiente de la pérdida.\n",
    "        Args:\n",
    "        loss_grad: Gradiente de la pérdida con forma (tamaño_lote, longitud_secuencia, tamaño_vocabulario)\n",
    "        Returns:\n",
    "        loss_grad: Propaga el gradiente a través de todas las capas.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(loss_grad)  # Retropropaga a través de cada capa en orden inverso.\n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, \n",
    "                    x_batch: ndarray, \n",
    "                    y_batch: ndarray):\n",
    "        '''\n",
    "        Ejecuta un único paso de entrenamiento completo:\n",
    "        1. Paso hacia adelante y aplicación de softmax.\n",
    "        2. Calcula la pérdida y su gradiente.\n",
    "        3. Paso hacia atrás.\n",
    "        4. Actualización de parámetros.\n",
    "        Args:\n",
    "        x_batch: Array de entrada con forma (tamaño_lote, longitud_secuencia, tamaño_vocabulario)\n",
    "        y_batch: Array objetivo correspondiente.\n",
    "        Returns:\n",
    "        loss: Pérdida calculada para el lote actual.\n",
    "        '''\n",
    "        x_batch_out = self.forward(x_batch)  # Paso hacia adelante.\n",
    "        loss = self.loss.forward(x_batch_out, y_batch)  # Calcula la pérdida.\n",
    "        loss_grad = self.loss.backward()  # Calcula el gradiente de la pérdida.\n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()  # Limpia los gradientes en cada capa.\n",
    "        self.backward(loss_grad)  # Retropropaga el gradiente de la pérdida.\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3a356-02a5-4e5e-8375-8d8a1bd092d6",
   "metadata": {},
   "source": [
    "#### RNNTrainer\n",
    "\n",
    "Este código define una clase RNNTrainer que se utiliza para entrenar un modelo de red neuronal recurrente (RNN) para la generación de texto. Utiliza un archivo de texto como datos de entrada y realiza las siguientes tareas principales:\n",
    "\n",
    "* Inicialización: Prepara el modelo, el optimizador y los datos necesarios para el entrenamiento, incluyendo la creación de mapeos de caracteres a índices y viceversa.\n",
    "\n",
    "* Generación de entradas y objetivos: Crea los lotes de datos de entrada y los objetivos (targets) correspondientes que el modelo intentará predecir.\n",
    "\n",
    "* Entrenamiento: Ejecuta el proceso de entrenamiento en varias iteraciones, donde cada iteración incluye un paso hacia adelante (forward), el cálculo de la pérdida, un paso hacia atrás (backward) para la propagación del error, y la actualización de los parámetros del modelo.\n",
    "\n",
    "* Muestreo de salidas: Opcionalmente, genera texto basado en el modelo entrenado para visualizar cómo está aprendiendo el modelo durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14ab9c-a999-4705-90e7-0a3c22e17411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    '''\n",
    "    Clase que toma un archivo de texto y un modelo, y comienza a generar caracteres.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 text_file: str, \n",
    "                 model: RNNModel,\n",
    "                 optim: RNNOptimizer,\n",
    "                 batch_size: int = 32):\n",
    "        # Leer los datos del archivo de texto.\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = model  # Modelo de red neuronal recurrente.\n",
    "        self.chars = list(set(self.data))  # Lista de caracteres únicos en el texto.\n",
    "        self.vocab_size = len(self.chars)  # Tamaño del vocabulario.\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}  # Diccionario de caracteres a índices.\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}  # Diccionario inverso de índices a caracteres.\n",
    "        self.sequence_length = self.model.sequence_length  # Longitud de la secuencia usada en el modelo.\n",
    "        self.batch_size = batch_size  # Tamaño del lote para el entrenamiento.\n",
    "        self.optim = optim  # Optimizador para ajustar los parámetros del modelo.\n",
    "        setattr(self.optim, 'model', self.model)  # Establece el modelo en el optimizador.\n",
    "\n",
    "    def _generate_inputs_targets(self, start_pos: int):\n",
    "        # Genera índices para los lotes de entradas y objetivos desde una posición inicial.\n",
    "        inputs_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
    "        targets_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            inputs_indices[i, :] = np.array([self.char_to_idx[ch] \n",
    "                            for ch in self.data[start_pos + i: start_pos + self.sequence_length  + i]])\n",
    "            targets_indices[i, :] = np.array([self.char_to_idx[ch] \n",
    "                         for ch in self.data[start_pos + 1 + i: start_pos + self.sequence_length + 1 + i]])\n",
    "\n",
    "        return inputs_indices, targets_indices\n",
    "\n",
    "    def _generate_one_hot_array(self, indices: ndarray):\n",
    "        # Convierte los índices de caracteres a una representación one-hot.\n",
    "        batch = []\n",
    "        for seq in indices:\n",
    "            one_hot_sequence = np.zeros((self.sequence_length, self.vocab_size))\n",
    "            for i in range(self.sequence_length):\n",
    "                one_hot_sequence[i, seq[i]] = 1.0\n",
    "            batch.append(one_hot_sequence) \n",
    "        return np.stack(batch)\n",
    "\n",
    "    def sample_output(self, input_char: int, sample_length: int):\n",
    "        # Genera una muestra de salida del modelo actual, caracter por caracter.\n",
    "        indices = []\n",
    "        sample_model = deepcopy(self.model)  # Hace una copia del modelo para usar en muestreo.\n",
    "        for i in range(sample_length):\n",
    "            input_char_batch = np.zeros((1, 1, self.vocab_size))\n",
    "            input_char_batch[0, 0, input_char] = 1.0\n",
    "            x_batch_out = sample_model.forward(input_char_batch)\n",
    "            x_softmax = batch_softmax(x_batch_out)\n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
    "            indices.append(input_char)\n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "\n",
    "    def train(self, num_iterations: int, sample_every: int=100):\n",
    "        # Entrena el modelo para generar caracteres.\n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        num_iter = 0\n",
    "        start_pos = 0\n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            if start_pos + self.sequence_length + self.batch_size + 1 > len(self.data):\n",
    "                start_pos = 0\n",
    "            inputs_indices, targets_indices = self._generate_inputs_targets(start_pos)\n",
    "            inputs_batch, targets_batch = \\\n",
    "                self._generate_one_hot_array(inputs_indices), self._generate_one_hot_array(targets_indices)\n",
    "            loss = self.model.single_step(inputs_batch, targets_batch)\n",
    "            self.optim.step()\n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            start_pos += self.batch_size\n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            if num_iter % sample_every == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], 200)\n",
    "                print(sample_text)\n",
    "            num_iter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6df46d-d512-496a-a7d4-b48ce873c01a",
   "metadata": {},
   "source": [
    "#### Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4e3f6-a27a-4b51-8df5-c95ead849313",
   "metadata": {},
   "outputs": [],
   "source": [
    "capas = [RNNLayer(hidden_size=256, output_size=62)]\n",
    "mod = RNNModel(layers=capas,\n",
    "               vocab_size=62, sequence_length=10,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = SGD(lr=0.001, gradient_clipping=True)\n",
    "trainer = RNNTrainer('Ejemplo.txt', mod, optim)\n",
    "trainer.train(1000, sample_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8b3e6-eaee-41f6-b7b4-be5ac29c497d",
   "metadata": {},
   "source": [
    "#### Ejercicio 1: Extensión a LSTM y GRU\n",
    "\n",
    "1. Implementa LSTMNode y GRUNode: Basándote en la estructura de RNNNode, implementa dos nuevas clases, LSTMNode y GRUNode, que representen las operaciones específicas de las celdas LSTM y GRU, respectivamente.\n",
    "2. Actualiza RNNLayer: Modifica la clase RNNLayer para que pueda utilizar RNNNode, LSTMNode, o GRUNode según un parámetro de configuración. Esto podría implicar agregar un argumento adicional en el constructor que especifique el tipo de nodo a utilizar.\n",
    "\n",
    "3. Experimentación: Entrena modelos utilizando las diferentes configuraciones de nodos (RNN simple, LSTM, GRU) en un conjunto de datos de texto para comparar su rendimiento en términos de velocidad de convergencia y capacidad de generación de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc5505-d2ee-468d-8620-cb31794c66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054aa272-e031-4914-9e56-9916f9fb9942",
   "metadata": {},
   "source": [
    "#### Ejercicio 2: Análisis de sentimientos usando RNN\n",
    "\n",
    "\n",
    "1. Preprocesa un conjunto de datos de reseñas de películas o tweets para convertir el texto en secuencias de índices.\n",
    "\n",
    "2. Modifica el  RNNModel:Asegúrate de que RNNModel pueda manejar tareas de clasificación agregando una capa densa al final con una activación de softmax o sigmoide.\n",
    "\n",
    "3. Entrena y compara: Entrena el modelo usando RNN, LSTM, y GRU, y compara su efectividad en la clasificación de sentimientos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e97fa6-e5cc-4650-8a15-67a0fa5d3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c002bc-2139-4b3a-a35b-d9f0ab56374d",
   "metadata": {},
   "source": [
    "#### Ejercicio 3: Autoencoders recurrentes para la detección de anomalías\n",
    "\n",
    "1. Modifica la arquitectura actual para crear un autoencoder, donde la capa RNNLayer sirva como encoder y decoder. La entrada al decoder puede ser la representación codificada de la entrada más una secuencia de \"start tokens\" para la reconstrucción.\n",
    "\n",
    "2. Detección de anomalías: Entrena el autoencoder en datos normales y luego utiliza el error de reconstrucción para detectar anomalías en nuevos datos.\n",
    "\n",
    "3. Experimentación: Utiliza un conjunto de datos como el de series temporales de sensores o datos financieros para entrenar y evaluar el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9a057-84ec-4ca0-bfc8-e32acdf61e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e6165-c2ab-43af-9d76-ecaacac86b66",
   "metadata": {},
   "source": [
    "#### Ejercicio 4: Mejora y optimización del RNNTrainer\n",
    "\n",
    "1. Implementa métodos en RNNTrainer para guardar el estado del modelo en puntos específicos durante el entrenamiento y cargar modelos previamente entrenados.\n",
    "\n",
    "2. Early stopping: Añade una comprobación de early stopping para terminar el entrenamiento si el modelo no mejora después de un número determinado de épocas.\n",
    "\n",
    "3. Integra una programación de la tasa de aprendizaje que ajuste automáticamente el lr del optimizador basándose en el progreso del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7a8d8-4613-4da0-a2e0-432012bb9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
