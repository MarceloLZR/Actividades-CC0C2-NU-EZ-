{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ac8b33-dbb4-40cc-a197-6a69ba943fc6",
   "metadata": {},
   "source": [
    "### Word2vec  \n",
    "\n",
    "En las secciones previas se explicó cómo representar palabras mediante vectores dispersos y de alta dimensión. Ahora se introduce una representación más eficaz: los *embeddings*, que son vectores densos y de baja dimensión (entre 50 y 1000 dimensiones). A diferencia de los vectores dispersos, los *embeddings* son más eficientes, mejoran la generalización de los modelos y capturan mejor relaciones como la sinonimia.\n",
    "\n",
    "Se presenta el método **skip-gram** con **negative sampling** (SGNS), parte de **word2vec**, como una técnica para calcular *embeddings*. **word2vec** es rápido, eficiente para entrenar y ofrece *embeddings* preentrenados. Estos *embeddings* son estáticos, asignando un vector fijo a cada palabra, a diferencia de los *embeddings* contextuales dinámicos como los de **BERT**, que varían según el contexto.\n",
    "\n",
    "La idea clave de **word2vec** es entrenar un clasificador para predecir si una palabra aparece cerca de otra, utilizando texto en ejecución como datos de entrenamiento de forma auto-supervisada. Esto elimina la necesidad de etiquetas manuales. A diferencia de los modelos de lenguaje neuronal más complejos, **word2vec** simplifica tanto la tarea (clasificación binaria) como la arquitectura (regresión logística en lugar de redes neuronales multicapa).\n",
    "\n",
    "Por ejemplo la intuición de **skip-gram** es:\n",
    "\n",
    "1. Tratar la palabra objetivo y una palabra de contexto vecina como ejemplos positivos.\n",
    "2. Muestrear aleatoriamente otras palabras en el léxico para obtener ejemplos negativos.\n",
    "3. Usar regresión logística para entrenar un clasificador que distinga esos dos casos.\n",
    "4. Usar los pesos aprendidos como los *embeddings*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebd9cb-2be6-48cb-87d6-d13257f6a93d",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. Representación de palabras como vectores densos\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "Supongamos que tenemos un pequeño vocabulario compuesto por las siguientes palabras: *manzana*, *naranja*, *fruta*, *coche*, *automóvil*, *vehículo*. Utilizando **word2vec**, cada una de estas palabras se representaría mediante un vector denso de, por ejemplo, 100 dimensiones. Aunque cada vector tiene 100 valores, no hay una interpretación directa de cada dimensión; sin embargo, las relaciones semánticas entre palabras se capturan en la proximidad de sus vectores en el espacio.\n",
    "\n",
    "- **manzana**: `[0.25, -0.10, ..., 0.47]`\n",
    "- **naranja**: `[0.30, -0.08, ..., 0.50]`\n",
    "- **fruta**: `[0.28, -0.12, ..., 0.45]`\n",
    "- **coche**: `[0.60, 0.15, ..., -0.20]`\n",
    "- **automóvil**: `[0.62, 0.18, ..., -0.22]`\n",
    "- **vehículo**: `[0.59, 0.14, ..., -0.19]`\n",
    "\n",
    "#### 2. Captura de sinonimia y relaciones semánticas\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "Observemos que *coche*, *automóvil* y *vehículo* están relacionadas semánticamente. En el espacio de embeddings, estos vectores estarán cerca entre sí, reflejando su sinonimia y relación temática. Por otro lado, *manzana*, *naranja* y *fruta* también estarán agrupadas, pero separadas de las palabras relacionadas con vehículos.\n",
    "\n",
    "Además, **word2vec** puede capturar relaciones más complejas. Por ejemplo:\n",
    "\n",
    "- **Reina** - **Hombre** + **Mujer** ≈ **Reina**\n",
    "\n",
    "Esto significa que si restamos el vector de *hombre* del de *rey* y sumamos el vector de *mujer*, obtenemos un vector que está cerca del vector de *reina*.\n",
    "\n",
    "#### 3. Funcionamiento del modelo Skip-Gram con negative sampling (SGNS)\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "Supongamos que queremos entrenar un modelo **skip-gram** para la palabra objetivo *fruta*. El modelo intentará predecir las palabras de contexto que aparecen cerca de *fruta* en el texto.\n",
    "\n",
    "- **Contexto positivo:** Si en el corpus aparece la frase \"La **fruta** es saludable\", las palabras *la*, *es*, *saludable* serán ejemplos positivos.\n",
    "\n",
    "- **Contexto negativo:** El modelo también seleccionará aleatoriamente palabras que no aparecen cerca de *fruta* en ese contexto, por ejemplo, *coche*, *automóvil*, *vehículo*, para actuar como ejemplos negativos.\n",
    "\n",
    "El modelo entrenará un clasificador que aprende a distinguir entre las palabras que realmente aparecen en el contexto de *fruta* y las que no, ajustando los vectores de embeddings en consecuencia.\n",
    "\n",
    "#### 4. Auto-supervisión en el entrenamiento de embeddings\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "Consideremos una oración del corpus: \"El **coche** rojo acelera por la **carretera**\".\n",
    "\n",
    "Durante el entrenamiento con **word2vec**, el modelo utiliza esta oración para generar ejemplos de entrenamiento de forma automática:\n",
    "\n",
    "- **Palabra objetivo:** *coche*\n",
    "  - **Contexto:** *el*, *rojo*, *acelera*, *por*, *la*, *carretera*\n",
    "\n",
    "El modelo crea pares positivos (*coche*, *el*), (*coche*, *rojo*), etc., y genera pares negativos seleccionando palabras al azar del vocabulario que no aparecen en el contexto de *coche*. Así, utiliza el texto existente como señal de supervisión sin necesidad de etiquetas manuales.\n",
    "\n",
    "#### 5. Comparación entre embeddings estáticos y contextuales\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "- **Embeddings estáticos (word2vec):**\n",
    "  - La palabra *banco* tendrá un único vector de embedding, independientemente del contexto en el que aparezca.\n",
    "  - Por ejemplo:\n",
    "    - *banco* (institución financiera): `[0.45, -0.22, ..., 0.33]`\n",
    "    - *banco* (asiento): `[0.45, -0.22, ..., 0.33]`\n",
    "\n",
    "- **Embeddings contextuales (BERT):**\n",
    "  - La palabra *banco* tendrá diferentes vectores dependiendo de su contexto.\n",
    "  - Por ejemplo:\n",
    "    - En \"Voy al **banco** a sacar dinero\": *banco* podría tener un vector cercano al de *institución financiera*.\n",
    "    - En \"Me senté en el **banco** del parque\": *banco* podría tener un vector cercano al de *asiento*.\n",
    "\n",
    "#### 6. Ventajas de usar vectores densos sobre vectores dispersos\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "Imaginemos que tenemos un vocabulario de 50,000 palabras. Representar cada palabra como un vector disperso implicaría vectores de 50,000 dimensiones, la mayoría de los cuales serían ceros. Esto no solo consume mucha memoria, sino que también dificulta el aprendizaje de modelos debido a la alta dimensionalidad.\n",
    "\n",
    "En cambio, usando **word2vec** para representar cada palabra con un vector denso de 300 dimensiones:\n",
    "\n",
    "- **Espacio de parámetros reducido:** Menos dimensiones significan que el clasificador necesita aprender menos pesos, lo que facilita la generalización y reduce el riesgo de sobreajuste.\n",
    "  \n",
    "- **Captura de similitud semántica:** Los vectores densos permiten que palabras similares estén cerca en el espacio vectorial, mejorando tareas como la clasificación de texto, la traducción automática y la detección de sinónimos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced7b3f6-ef2e-460f-a9a1-4d7210f5f90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manzana: [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]...\n",
      "naranja: [-1.41537074 -0.42064532 -0.34271452 -0.80227727 -0.16128571]...\n",
      "fruta: [ 0.35778736  0.56078453  1.08305124  1.05380205 -1.37766937]...\n",
      "coche: [-0.82899501 -0.56018104  0.74729361  0.61037027 -0.02090159]...\n",
      "automóvil: [-1.59442766 -0.59937502  0.0052437   0.04698059 -0.45006547]...\n",
      "vehículo: [ 0.92617755  1.90941664 -1.39856757  0.56296924 -0.65064257]...\n"
     ]
    }
   ],
   "source": [
    "#  Representación de palabras como vectores densos\n",
    "import numpy as np\n",
    "\n",
    "# Definir un vocabulario pequeño\n",
    "vocabulario = ['manzana', 'naranja', 'fruta', 'coche', 'automóvil', 'vehículo']\n",
    "\n",
    "# Dimensionalidad de los embeddings\n",
    "d = 100\n",
    "\n",
    "# Inicializar embeddings aleatorios para cada palabra\n",
    "embeddings = {}\n",
    "np.random.seed(42)  # Para reproducibilidad\n",
    "for palabra in vocabulario:\n",
    "    embeddings[palabra] = np.random.randn(d)\n",
    "\n",
    "# Mostrar los embeddings\n",
    "for palabra, vector in embeddings.items():\n",
    "    print(f\"{palabra}: {vector[:5]}...\")  # Mostrar solo las primeras 5 dimensiones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da816550-843f-42b9-8692-c0702f031690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre 'coche' y 'automóvil': 0.2060\n",
      "Similitud entre 'coche' y 'vehículo': 0.0565\n",
      "Similitud entre 'manzana' y 'naranja': -0.1382\n"
     ]
    }
   ],
   "source": [
    "#  Captura de sinonimia y relaciones semánticas\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# Calcular similitudes\n",
    "sim_coche_automovil = cos_sim(embeddings['coche'], embeddings['automóvil'])\n",
    "sim_coche_vehiculo = cos_sim(embeddings['coche'], embeddings['vehículo'])\n",
    "sim_manzana_naranja = cos_sim(embeddings['manzana'], embeddings['naranja'])\n",
    "sim_reina_hombre = 0  # Placeholder\n",
    "sim_rey_mujer = 0    # Placeholder\n",
    "\n",
    "print(f\"Similitud entre 'coche' y 'automóvil': {sim_coche_automovil:.4f}\")\n",
    "print(f\"Similitud entre 'coche' y 'vehículo': {sim_coche_vehiculo:.4f}\")\n",
    "print(f\"Similitud entre 'manzana' y 'naranja': {sim_manzana_naranja:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25344a5a-b4dc-4907-b288-a0d5abb42de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario (18 palabras): ['saludable', 'el', 'jugosa', 'fruta', 'deliciosa', 'avanza', 'es', 'acelera', 'automóvil', 'naranja', 'vehículo', 'rojo', 'por', 'rápido', 'carretera', 'manzana', 'coche', 'la']\n",
      "Epoca 1 completada\n",
      "Epoca 200 completada\n",
      "Epoca 400 completada\n",
      "Epoca 600 completada\n",
      "Epoca 800 completada\n",
      "Epoca 1000 completada\n",
      "Embedding de 'coche': [ 0.12239252  0.36296517 -0.17998622  0.63964944 -0.51743025]...\n"
     ]
    }
   ],
   "source": [
    "# Funcionamiento del modelo Skip-Gram con negative sampling (SGNS)\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Parámetros\n",
    "d = 50  # Dimensionalidad de los embeddings\n",
    "window_size = 2 \n",
    "negative_samples = 2\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Corpus de ejemplo\n",
    "corpus = [\n",
    "    \"la manzana es saludable\",\n",
    "    \"el coche rojo acelera\",\n",
    "    \"la naranja es jugosa\",\n",
    "    \"el automóvil es rápido\",\n",
    "    \"la fruta es deliciosa\",\n",
    "    \"el vehículo avanza por la carretera\"\n",
    "]\n",
    "\n",
    "# Preprocesar el corpus: convertir a minúsculas y tokenizar\n",
    "preprocesado = [line.lower().split() for line in corpus]\n",
    "\n",
    "# Construir el vocabulario dinámicamente a partir del corpus\n",
    "vocabulario = list(set([palabra for frase in preprocesado for palabra in frase]))\n",
    "print(f\"Vocabulario ({len(vocabulario)} palabras): {vocabulario}\")\n",
    "\n",
    "# Crear índices para las palabras\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulario)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Inicializar matrices de embeddings (input y output) con valores pequeños\n",
    "np.random.seed(42)  # Para reproducibilidad\n",
    "W_in = np.random.randn(len(vocabulario), d) * 0.01\n",
    "W_out = np.random.randn(len(vocabulario), d) * 0.01\n",
    "\n",
    "# Función de entrenamiento simplificada\n",
    "def train_skipgram(corpus, W_in, W_out, epochs=1000, learning_rate=0.01):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for sentence in corpus:\n",
    "            for i, palabra in enumerate(sentence):\n",
    "                target = word_to_idx[palabra]\n",
    "                # Definir el contexto dentro del window_size\n",
    "                context_indices = list(range(max(0, i - window_size), min(len(sentence), i + window_size + 1)))\n",
    "                context_indices.remove(i)  # Remover el target del contexto\n",
    "                for j in context_indices:\n",
    "                    context = word_to_idx[sentence[j]]\n",
    "                    \n",
    "                    # Positivo\n",
    "                    z = np.dot(W_out[context], W_in[target])\n",
    "                    sigmoid = 1 / (1 + np.exp(-z))\n",
    "                    error = 1 - sigmoid\n",
    "                    # Actualizar W_out y W_in para el ejemplo positivo\n",
    "                    W_out[context] += learning_rate * error * W_in[target]\n",
    "                    W_in[target] += learning_rate * error * W_out[context]\n",
    "                    \n",
    "                    # Negativos\n",
    "                    for _ in range(negative_samples):\n",
    "                        neg = random.randint(0, len(vocabulario)-1)\n",
    "                        while neg == context:\n",
    "                            neg = random.randint(0, len(vocabulario)-1)\n",
    "                        z_neg = np.dot(W_out[neg], W_in[target])\n",
    "                        sigmoid_neg = 1 / (1 + np.exp(-z_neg))\n",
    "                        error_neg = 0 - sigmoid_neg\n",
    "                        # Actualizar W_out y W_in para el ejemplo negativo\n",
    "                        W_out[neg] += learning_rate * error_neg * W_in[target]\n",
    "                        W_in[target] += learning_rate * error_neg * W_out[neg]\n",
    "        \n",
    "        # Mostrar progreso cada 200 epochs\n",
    "        if epoch % 200 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"Epoca {epoch} completada\")\n",
    "    \n",
    "    return W_in, W_out\n",
    "\n",
    "# Entrenar el modelo\n",
    "W_in_trained, W_out_trained = train_skipgram(preprocesado, W_in, W_out, epochs, learning_rate)\n",
    "\n",
    "# Obtener el embedding de una palabra\n",
    "def get_embedding(word):\n",
    "    if word in word_to_idx:\n",
    "        return W_in_trained[word_to_idx[word]]\n",
    "    else:\n",
    "        raise ValueError(f\"La palabra '{word}' no está en el vocabulario.\")\n",
    "\n",
    "# Mostrar el embedding de 'coche'\n",
    "embedding_coche = get_embedding('coche')\n",
    "print(f\"Embedding de 'coche': {embedding_coche[:5]}...\")  # Mostrar solo las primeras 5 dimensiones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41119175-c783-44e6-94f1-656ca1bf1793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 0 completada\n",
      "Epoca 200 completada\n",
      "Epoca 400 completada\n",
      "Epoca 600 completada\n",
      "Epoca 800 completada\n",
      "Embedding de 'coche': [-1.1343032   0.15760294 -0.67531012 -0.33490473  1.87541599]...\n"
     ]
    }
   ],
   "source": [
    "# Auto-supervisión en el entrenamiento de embedding\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Definir un corpus de ejemplo\n",
    "corpus = [\n",
    "    \"El coche rojo acelera por la carretera\",\n",
    "    \"La manzana es saludable y la naranja es jugosa\",\n",
    "    \"El automóvil rápido pasa al vehículo lento\",\n",
    "    \"La fruta deliciosa se vende en el mercado\",\n",
    "    \"El banco está cerrado hoy\",\n",
    "    \"Me senté en el banco del parque\"\n",
    "]\n",
    "\n",
    "# Preprocesar el corpus\n",
    "preprocesado = [line.lower().split() for line in corpus]\n",
    "\n",
    "# Construir el vocabulario\n",
    "vocabulario = list(set([palabra for frase in preprocesado for palabra in frase]))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulario)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Parámetros\n",
    "window_size = 2\n",
    "negative_samples = 2\n",
    "d = 50  # Dimensionalidad\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Inicializar embeddings\n",
    "W_in = np.random.randn(len(vocabulario), d)\n",
    "W_out = np.random.randn(len(vocabulario), d)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_auto_supervision(corpus, W_in, W_out, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        for sentence in corpus:\n",
    "            for i, palabra in enumerate(sentence):\n",
    "                target = word_to_idx[palabra]\n",
    "                context_indices = list(range(max(0, i - window_size), min(len(sentence), i + window_size + 1)))\n",
    "                context_indices.remove(i)\n",
    "                for j in context_indices:\n",
    "                    context = word_to_idx[sentence[j]]\n",
    "                    \n",
    "                    # Positivo\n",
    "                    z = np.dot(W_out[context], W_in[target])\n",
    "                    sigmoid = 1 / (1 + np.exp(-z))\n",
    "                    error = 1 - sigmoid\n",
    "                    W_out[context] += learning_rate * error * W_in[target]\n",
    "                    W_in[target] += learning_rate * error * W_out[context]\n",
    "                    \n",
    "                    # Negativos\n",
    "                    for _ in range(negative_samples):\n",
    "                        neg = random.randint(0, len(vocabulario)-1)\n",
    "                        while neg == context:\n",
    "                            neg = random.randint(0, len(vocabulario)-1)\n",
    "                        z_neg = np.dot(W_out[neg], W_in[target])\n",
    "                        sigmoid_neg = 1 / (1 + np.exp(-z_neg))\n",
    "                        error_neg = 0 - sigmoid_neg\n",
    "                        W_out[neg] += learning_rate * error_neg * W_in[target]\n",
    "                        W_in[target] += learning_rate * error_neg * W_out[neg]\n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoca {epoch} completada\")\n",
    "    return W_in, W_out\n",
    "\n",
    "# Entrenar el modelo\n",
    "W_in_trained, W_out_trained = train_auto_supervision(preprocesado, W_in, W_out, epochs, learning_rate)\n",
    "\n",
    "# Obtener el embedding de 'coche'\n",
    "def get_embedding(word):\n",
    "    return W_in_trained[word_to_idx[word]]\n",
    "\n",
    "print(f\"Embedding de 'coche': {get_embedding('coche')[:5]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee1bc7c-9e81-4f2f-9f74-4bf764c57402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre 'banco' en contexto financiero y de asiento: 0.1388\n"
     ]
    }
   ],
   "source": [
    "# Comparación entre embeddings estáticos y contextuales\n",
    "import numpy as np\n",
    "\n",
    "# Embeddings estáticos: un único vector por palabra\n",
    "embeddings_estaticos = {\n",
    "    'banco': np.random.randn(50)\n",
    "}\n",
    "\n",
    "# Embeddings contextuales: diferentes vectores según el contexto\n",
    "# Por simplicidad, usaremos dos contextos diferentes\n",
    "contexto_financiero = \"Voy al banco a sacar dinero\".lower().split()\n",
    "contexto_asiento = \"Me senté en el banco del parque\".lower().split()\n",
    "\n",
    "# Función para obtener embeddings contextuales\n",
    "def obtener_embedding_contextual(palabra, contexto, embeddings):\n",
    "    # Simplemente agregamos ruido basado en el contexto para ilustrar\n",
    "    context_factor = 1 if 'dinero' in contexto else -1\n",
    "    return embeddings_estaticos.get(palabra, np.random.randn(50)) + context_factor * 1.0\n",
    "\n",
    "# Obtener embeddings\n",
    "embedding_banco_fin = obtener_embedding_contextual('banco', contexto_financiero, embeddings_estaticos)\n",
    "embedding_banco_asiento = obtener_embedding_contextual('banco', contexto_asiento, embeddings_estaticos)\n",
    "\n",
    "# Comparar\n",
    "similitud = np.dot(embedding_banco_fin, embedding_banco_asiento) / (norm(embedding_banco_fin) * norm(embedding_banco_asiento))\n",
    "print(f\"Similitud entre 'banco' en contexto financiero y de asiento: {similitud:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa0f7490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5152a4e7e99437cbe0b6ffffef00d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b22ab8d54b4fcd9172a6e65251bb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780fcfa955b14a148b3aa7f3e9c84c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e21fc291cd452abb971308bb0a1872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre 'banco' en contexto financiero y de asiento: 0.6015\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el tokenizador y el modelo preentrenado multilingüe de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Función para obtener el embedding contextual de una palabra en un contexto específico\n",
    "def obtener_embedding_contextual(frase, palabra, tokenizer, model):\n",
    "    # Tokenizar la frase\n",
    "    inputs = tokenizer(frase, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Convertir la salida en embeddings (última capa de BERT)\n",
    "    embeddings = outputs.last_hidden_state.squeeze(0).detach().numpy()\n",
    "    \n",
    "    # Tokenizar la frase para obtener la lista de tokens\n",
    "    tokens = tokenizer.tokenize(frase)\n",
    "    \n",
    "    # Buscar todas las ocurrencias de la palabra en los tokens\n",
    "    # En modelos multilingües, es posible que la palabra se divida en subpalabras\n",
    "    word_tokens = tokenizer.tokenize(palabra)\n",
    "    token_len = len(word_tokens)\n",
    "    \n",
    "    # Encontrar la posición de la palabra en los tokens\n",
    "    posiciones = []\n",
    "    for i in range(len(tokens) - token_len + 1):\n",
    "        if tokens[i:i+token_len] == word_tokens:\n",
    "            posiciones.append(i)\n",
    "    \n",
    "    if not posiciones:\n",
    "        raise ValueError(f\"La palabra '{palabra}' no fue encontrada en la frase.\")\n",
    "    \n",
    "    # Asumimos que la palabra aparece una vez; para múltiples apariciones, puedes adaptar el código\n",
    "    palabra_idx = posiciones[0]\n",
    "    \n",
    "    if token_len == 1:\n",
    "        # Si la palabra no se dividió en subpalabras\n",
    "        embedding = embeddings[palabra_idx]\n",
    "    else:\n",
    "        # Si la palabra se dividió en subpalabras, promediar sus embeddings\n",
    "        embedding = np.mean(embeddings[palabra_idx:palabra_idx+token_len], axis=0)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Contextos diferentes\n",
    "contexto_financiero = \"Voy al banco a sacar dinero\"\n",
    "contexto_asiento = \"Me senté en el banco del parque\"\n",
    "\n",
    "# Obtener los embeddings contextuales de la palabra 'banco'\n",
    "embedding_banco_fin = obtener_embedding_contextual(contexto_financiero, 'banco', tokenizer, model)\n",
    "embedding_banco_asiento = obtener_embedding_contextual(contexto_asiento, 'banco', tokenizer, model)\n",
    "\n",
    "# Calcular la similitud coseno entre los dos embeddings\n",
    "def similitud_coseno(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "similitud = similitud_coseno(embedding_banco_fin, embedding_banco_asiento)\n",
    "print(f\"Similitud entre 'banco' en contexto financiero y de asiento: {similitud:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d2393-b76c-4eb4-b74d-2e818ec09886",
   "metadata": {},
   "source": [
    "En un modelo real de embeddings contextuales como BERT, la similitud sería menor ya que los vectores reflejarían diferentes significados. En este ejemplo simplificado, hemos añadido un pequeño cambio para ilustrar la diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f45690-0ad1-4ec2-8f79-e0728326b030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de vector disperso (one-hot) para una palabra: 400104 bytes\n",
      "Tamaño de vector denso para una palabra: 2504 bytes\n"
     ]
    }
   ],
   "source": [
    "# Ventajas de usar vectores densos sobre vectores dispersos\n",
    "import numpy as np\n",
    "\n",
    "# Supongamos un vocabulario grande\n",
    "vocabulario_grande = [f\"palabra_{i}\" for i in range(50000)]\n",
    "d = 300  # Dimensionalidad de los embeddings densos\n",
    "\n",
    "# Representación dispersa: vectores de 50,000 dimensiones con mayoría de ceros\n",
    "# Ejemplo: Representación one-hot para una palabra\n",
    "def one_hot(word, vocab):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    vector[vocab.index(word)] = 1\n",
    "    return vector\n",
    "\n",
    "# Representación densa: vectores de 300 dimensiones\n",
    "def dense_embedding(word, embeddings):\n",
    "    return embeddings[word]\n",
    "\n",
    "# Inicializar embeddings densos\n",
    "embeddings_densos = {word: np.random.randn(d) for word in vocabulario_grande}\n",
    "\n",
    "# Comparar el tamaño en memoria\n",
    "import sys\n",
    "\n",
    "# Tamaño de una representación dispersa para una palabra\n",
    "palabra = 'palabra_12345'\n",
    "vector_disperso = one_hot(palabra, vocabulario_grande)\n",
    "tamaño_disperso = sys.getsizeof(vector_disperso)\n",
    "\n",
    "# Tamaño de una representación densa para una palabra\n",
    "vector_denso = dense_embedding(palabra, embeddings_densos)\n",
    "tamaño_denso = sys.getsizeof(vector_denso)\n",
    "\n",
    "print(f\"Tamaño de vector disperso (one-hot) para una palabra: {tamaño_disperso} bytes\")\n",
    "print(f\"Tamaño de vector denso para una palabra: {tamaño_denso} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195023ce-dd47-46cc-a42f-71a5be5031f4",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "**Ejercicio 1: Comprensión de vectores densos**\n",
    "\n",
    "**Descripción:**  \n",
    "\n",
    "Explica la diferencia entre vectores dispersos y vectores densos en el contexto de la representación de palabras. ¿Por qué los vectores densos son preferidos en tareas de procesamiento del lenguaje natural?\n",
    "\n",
    "**Puntos a considerar:**\n",
    "- Dimensionalidad\n",
    "- Interpretación de las dimensiones\n",
    "- Eficiencia en el almacenamiento y procesamiento\n",
    "- Capacidad para capturar relaciones semánticas\n",
    "\n",
    "**Ejercicio 2: Captura de sinonimia y relaciones semánticas**\n",
    "\n",
    "**Descripción:**  \n",
    "Imagina que has entrenado un modelo **word2vec** con un corpus específico. Observas que las palabras *\"gato\"*, *\"felino\"* y *\"minino\"* tienen vectores muy similares entre sí, mientras que *\"perro\"* está algo separado pero aún cercano.  \n",
    "- ¿Qué indica esta distribución sobre las relaciones semánticas entre estas palabras?\n",
    "- ¿Cómo afectaría esto a tareas como la detección de sinónimos o la clasificación de texto?\n",
    "\n",
    "\n",
    "**Ejercicio 3: Funcionamiento del modelo Skip-Gram con negative sampling (SGNS)**\n",
    "\n",
    "**Descripción:**  \n",
    "Describe paso a paso cómo el modelo **skip-gram** con **negative sampling** actualiza los vectores de embeddings durante el entrenamiento.  \n",
    "- ¿Cuál es el propósito de los ejemplos negativos?\n",
    "- ¿Cómo contribuyen los ejemplos positivos y negativos a la optimización de los embeddings?\n",
    "\n",
    "---\n",
    "\n",
    "**Ejercicio 4: Auto-Supervisión en el entrenamiento de embeddings**\n",
    "\n",
    "**Descripción:**  \n",
    "Explica el concepto de **auto-supervisión** en el contexto de **word2vec**.  \n",
    "- ¿Cómo se generan las señales de supervisión de manera implícita a partir del texto?\n",
    "- ¿Cuáles son las ventajas de utilizar auto-supervisión en comparación con métodos que requieren etiquetas manuales?\n",
    "\n",
    "**Ejercicio 5: Embeddings estáticos vs. contextuales**\n",
    "\n",
    "**Descripción:**  \n",
    "Comparar y contrastar los **embeddings estáticos** (como los generados por **word2vec**) con los **embeddings contextuales** (como los de **BERT**).  \n",
    "- ¿Cuáles son las principales diferencias en cómo representan las palabras?\n",
    "- ¿En qué escenarios sería más beneficioso utilizar embeddings contextuales en lugar de estáticos?\n",
    "\n",
    "**Ejercicio 6: Ventajas de usar vectores densos sobre vectores dispersos**\n",
    "\n",
    "**Descripción:**  \n",
    "Analiza las ventajas de utilizar representaciones densas de palabras frente a representaciones dispersas, especialmente en vocabularios grandes.  \n",
    "- Considera aspectos como el uso de memoria, la eficiencia computacional y la capacidad de generalización.\n",
    "- ¿Cómo impacta la dimensionalidad de los vectores en el rendimiento de los modelos de NLP?\n",
    "\n",
    "**Ejercicio 7: Construcción del vocabulario dinámico**\n",
    "\n",
    "**Descripción:**  \n",
    "En el ejemplo corregido, se construyó el vocabulario dinámicamente a partir del corpus.  \n",
    "- ¿Por qué es importante asegurarse de que todas las palabras del corpus estén incluidas en el vocabulario?\n",
    "- ¿Qué problemas pueden surgir si se utilizan vocabularios estáticos que no abarcan todas las palabras del corpus?\n",
    "\n",
    "\n",
    "**Ejercicio 8: Similitud coseno entre vectores de palabras**\n",
    "\n",
    "**Descripción:**  \n",
    "Después de entrenar un modelo **word2vec**, calculas la similitud coseno entre diferentes pares de palabras y observas que *\"coche\"* y *\"automóvil\"* tienen una alta similitud, mientras que *\"coche\"* y *\"manzana\"* tienen una baja similitud.  \n",
    "- ¿Qué interpretaciones puedes hacer sobre las relaciones semánticas entre estas palabras?\n",
    "- ¿Cómo podrías utilizar estas similitudes en aplicaciones prácticas de NLP?\n",
    "\n",
    "\n",
    "**Ejercicio 9: Impacto de los hiperparámetros en el entrenamiento**\n",
    "\n",
    "**Descripción:**  \n",
    "Considera los hiperparámetros utilizados en el entrenamiento de **word2vec**, como `window_size`, `negative_samples`, `learning_rate` y `epochs`.  \n",
    "- Explica cómo cada uno de estos hiperparámetros puede afectar la calidad de los embeddings resultantes.\n",
    "- ¿Qué estrategias podrías emplear para seleccionar valores óptimos para estos hiperparámetros?\n",
    "\n",
    "\n",
    "**Ejercicio 10: Evaluación de la calidad de los embeddings**\n",
    "\n",
    "**Descripción:**  \n",
    "Después de entrenar tus propios embeddings con **word2vec**, quieres evaluar su calidad.  \n",
    "- Describe al menos dos métodos o tareas que podrías utilizar para evaluar la efectividad de tus embeddings.\n",
    "- ¿Qué métricas específicas utilizarías para medir el rendimiento en estas tareas?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6243a43-9456-4278-bf3e-7496c5b5b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f09ae6-7f54-4cda-8c83-e814faf5516c",
   "metadata": {},
   "source": [
    "### El clasificador\n",
    "\n",
    "En esta sección se explica cómo **skip-gram** utiliza un clasificador probabilístico para aprender *embeddings*. Consideremos una oración con una palabra objetivo, por ejemplo *apricot*, y una ventana de contexto de ±2 palabras. El objetivo es entrenar un clasificador que, dado un par $(w, c)$ donde $w$ es la palabra objetivo y $c$ una palabra candidata del contexto, calcule la probabilidad de que $c$ sea un contexto real de $w$, es decir, $P(+|w, c)$.\n",
    "\n",
    "El clasificador basa esta probabilidad en la similitud de los *embeddings* de $w$ y $c$, calculada mediante el producto punto de sus vectores. Este valor se transforma en una probabilidad utilizando la función sigmoide $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$. \n",
    "\n",
    "Así, $P(+|w, c) = \\sigma(\\mathbf{c} \\cdot \\mathbf{w})$ y $P(-|w, c) = 1 - P(+|w, c) = \\sigma(-\\mathbf{c} \\cdot \\mathbf{w})$.\n",
    "\n",
    "**Skip-gram** asume que las palabras de contexto son independientes, permitiendo calcular la probabilidad conjunta como el producto de las probabilidades individuales: $P(+|w, c_1:L) = \\prod_{i=1}^{L} \\sigma(\\mathbf{c_i} \\cdot \\mathbf{w})$. Al entrenar, se maximiza la suma de los logaritmos de estas probabilidades.\n",
    "\n",
    "En resumen, **skip-gram** entrena un clasificador que asigna probabilidades basadas en la similitud de *embeddings* entre una palabra objetivo y sus contextos, utilizando la función sigmoide aplicada al producto punto de sus vectores. Este enfoque permite aprender representaciones densas y efectivas para cada palabra en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810b23c-a120-4435-8708-b7eb01a77b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos la función sigmoide σ(x) = 1 / (1 + exp(-x))\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Tamaño de los embeddings (dimensión de los vectores)\n",
    "D = 5  # Puedes cambiar este valor\n",
    "\n",
    "# Establecemos una semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Creamos un embedding aleatorio para la palabra objetivo w (por ejemplo, 'apricot')\n",
    "w_embedding = np.random.randn(D)\n",
    "\n",
    "# Creamos embeddings aleatorios para las palabras de contexto c1, c2, c3, c4\n",
    "c1_embedding = np.random.randn(D)\n",
    "c2_embedding = np.random.randn(D)\n",
    "c3_embedding = np.random.randn(D)\n",
    "c4_embedding = np.random.randn(D)\n",
    "\n",
    "# Agrupamos los embeddings de las palabras de contexto en una lista\n",
    "context_embeddings = [c1_embedding, c2_embedding, c3_embedding, c4_embedding]\n",
    "\n",
    "# Calculamos los productos punto c_i ⋅ w para cada palabra de contexto\n",
    "# Esto corresponde a la ecuación: Similaridad(w, c) ≈ c ⋅ w\n",
    "dot_products = [np.dot(c_embedding, w_embedding) for c_embedding in context_embeddings]\n",
    "\n",
    "# Calculamos P(+|w, c_i) = σ(c_i ⋅ w) para cada palabra de contexto\n",
    "# Esto corresponde a la ecuación: P(+|w, c) = σ(c ⋅ w)\n",
    "P_positive = [sigmoid(dot_product) for dot_product in dot_products]\n",
    "\n",
    "# Calculamos P(-|w, c_i) = σ(-c_i ⋅ w) para cada palabra de contexto\n",
    "# Esto corresponde a la ecuación: P(-|w, c) = σ(-c ⋅ w)\n",
    "P_negative = [sigmoid(-dot_product) for dot_product in dot_products]\n",
    "\n",
    "# Alternativamente, podemos calcular P(-|w, c_i) = 1 - P(+|w, c_i)\n",
    "# P(-|w, c_i) = 1 - P(+|w, c_i)\n",
    "P_negative_alternative = [1 - p for p in P_positive]\n",
    "\n",
    "# Verificamos que ambas formas de calcular P(-|w, c_i) son equivalentes\n",
    "for i in range(len(P_negative)):\n",
    "    assert np.isclose(P_negative[i], P_negative_alternative[i]), \"Las probabilidades no coinciden\"\n",
    "\n",
    "# Suponiendo independencia, calculamos la probabilidad total P(+|w, c_1:L)\n",
    "# Esto corresponde a la ecuación: P(+|w, c_1:L) = ∏_{i=1}^{L} σ(c_i ⋅ w)\n",
    "P_positive_total = np.prod(P_positive)\n",
    "\n",
    "# Calculamos el logaritmo de la probabilidad total\n",
    "# Esto corresponde a la ecuación: log P(+|w, c_1:L) = ∑_{i=1}^{L} log σ(c_i ⋅ w)\n",
    "log_P_positive_total = np.sum([np.log(p) for p in P_positive])\n",
    "\n",
    "# Imprimimos los resultados\n",
    "print(\"Embedding de la palabra objetivo w:\")\n",
    "print(w_embedding)\n",
    "print(\"\\nEmbeddings de las palabras de contexto c_i:\")\n",
    "for i, c_embedding in enumerate(context_embeddings):\n",
    "    print(f\"c_{i+1}:\", c_embedding)\n",
    "\n",
    "print(\"\\nProductos punto c_i ⋅ w:\")\n",
    "for i, dot_product in enumerate(dot_products):\n",
    "    print(f\"c_{i+1} ⋅ w:\", dot_product)\n",
    "\n",
    "print(\"\\nProbabilidades P(+|w, c_i) = σ(c_i ⋅ w):\")\n",
    "for i, p in enumerate(P_positive):\n",
    "    print(f\"P(+|w, c_{i+1}):\", p)\n",
    "\n",
    "print(\"\\nProbabilidades P(-|w, c_i) = σ(-c_i ⋅ w):\")\n",
    "for i, p in enumerate(P_negative):\n",
    "    print(f\"P(-|w, c_{i+1}):\", p)\n",
    "\n",
    "print(\"\\nProbabilidad total P(+|w, c_1:L) = producto de P(+|w, c_i):\")\n",
    "print(\"P(+|w, c_1:L):\", P_positive_total)\n",
    "\n",
    "print(\"\\nLogaritmo de la probabilidad total log P(+|w, c_1:L):\")\n",
    "print(\"log P(+|w, c_1:L):\", log_P_positive_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79589a4-1d75-4063-9699-a4952df2821f",
   "metadata": {},
   "source": [
    "### El algoritmo de aprendizaje de **Skip-gram**\n",
    "\n",
    "El algoritmo de **skip-gram** para aprender *embeddings* comienza con un corpus de texto y un vocabulario de tamaño $N$. Inicialmente, asigna vectores de *embedding* aleatorios a cada una de las $N$ palabras. Luego, ajusta iterativamente estos *embeddings* para que las palabras objetivo sean más similares a sus palabras de contexto reales y menos similares a palabras negativas (no relacionadas).\n",
    "\n",
    "#### Proceso de entrenamiento\n",
    "\n",
    "1. **Ejemplos de entrenamiento:**\n",
    "   - **Positivos (+):** Pares $(w, c_{\\text{pos}})$ donde $w$ es la palabra objetivo y $c_{\\text{pos}}$ son palabras de contexto reales dentro de una ventana de contexto $L = \\pm2$.\n",
    "   - **Negativos (-):** Para cada ejemplo positivo, se generan $k$ pares negativos $(w, c_{\\text{neg}})$ usando palabras de ruido seleccionadas aleatoriamente del vocabulario, excluyendo $w$. La probabilidad de selección de una palabra de ruido está ponderada por $p_{\\alpha}(w)$ con $\\alpha = 0.75$, lo que incrementa ligeramente la probabilidad de palabras raras.\n",
    "\n",
    "2. **Función de pérdida:**\n",
    "   - Se define una función de pérdida $L$ que busca maximizar la similitud entre $w$ y $c_{\\text{pos}}$, mientras minimiza la similitud entre $w$ y cada $c_{\\text{neg}}$.\n",
    "   - La pérdida se expresa como:\n",
    "     $$\n",
    "     L = - \\left[ \\log \\sigma(\\mathbf{c}_{\\text{pos}} \\cdot \\mathbf{w}) + \\sum_{i=1}^{k} \\log \\sigma(-\\mathbf{c}_{\\text{neg}_i} \\cdot \\mathbf{w}) \\right]\n",
    "     $$\n",
    "   - Aquí, $\\sigma(x)$ es la función sigmoide que convierte el producto punto en una probabilidad.\n",
    "\n",
    "3. **Actualización de *embeddings*:**\n",
    "   - Utilizando descenso de gradiente estocástico, los vectores de *embedding* se actualizan para minimizar la función de pérdida.\n",
    "   - Las actualizaciones ajustan tanto los *embeddings* de las palabras objetivo como los de las palabras de contexto y negativas.\n",
    "\n",
    "4. **Representación final:**\n",
    "   - Cada palabra tiene dos *embeddings*: uno objetivo ($\\mathbf{w}_i$) y uno de contexto ($\\mathbf{c}_i$).\n",
    "   - Comúnmente, se suman estos vectores para obtener la representación final de la palabra: $\\mathbf{w}_i + \\mathbf{c}_i$. Alternativamente, se puede usar solo $\\mathbf{w}_i$.\n",
    "\n",
    "#### Consideraciones adicionales\n",
    "\n",
    "- **Tamaño de la ventana de contexto ($L$):** Influye en la calidad de los *embeddings* y se suele ajustar experimentalmente.\n",
    "- **Eficiencia:** **Skip-gram** con *negative sampling* es eficiente y permite entrenar *embeddings* densos y efectivos que capturan relaciones semánticas entre palabras.\n",
    "\n",
    "En resumen, el algoritmo de **skip-gram** optimiza los *embeddings* de palabras mediante la maximización de la similitud entre palabras objetivo y sus contextos reales, y la minimización con palabras de ruido, utilizando un enfoque de auto-supervisión y descenso de gradiente para aprender representaciones vectoriales densas y útiles para tareas de procesamiento del lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9aeec7-b3cb-4e2e-a402-d4b6aba4e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos la función sigmoide σ(x) = 1 / (1 + exp(-x))\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Función de pérdida L para un ejemplo de entrenamiento\n",
    "def loss_function(w, c_pos, c_negs):\n",
    "    # Producto punto c_pos ⋅ w\n",
    "    dot_product_pos = np.dot(c_pos, w)\n",
    "    # Calculamos σ(c_pos ⋅ w)\n",
    "    sigma_pos = sigmoid(dot_product_pos)\n",
    "    # Pérdida para el ejemplo positivo\n",
    "    loss_pos = -np.log(sigma_pos + 1e-7)  # Añadimos un pequeño valor para evitar log(0)\n",
    "    \n",
    "    # Pérdida para los ejemplos negativos\n",
    "    loss_negs = 0\n",
    "    for c_neg in c_negs:\n",
    "        dot_product_neg = np.dot(c_neg, w)\n",
    "        sigma_neg = sigmoid(-dot_product_neg)\n",
    "        loss_neg = -np.log(sigma_neg + 1e-7)\n",
    "        loss_negs += loss_neg\n",
    "    \n",
    "    # Pérdida total\n",
    "    total_loss = loss_pos + loss_negs\n",
    "    return total_loss\n",
    "\n",
    "# Funciones para calcular los gradientes\n",
    "def gradients(w, c_pos, c_negs):\n",
    "    # Gradiente respecto a c_pos\n",
    "    dot_product_pos = np.dot(c_pos, w)\n",
    "    sigma_pos = sigmoid(dot_product_pos)\n",
    "    grad_c_pos = (sigma_pos - 1) * w  # ∂L/∂c_pos\n",
    "    \n",
    "    # Gradiente respecto a c_negs\n",
    "    grad_c_negs = []\n",
    "    for c_neg in c_negs:\n",
    "        dot_product_neg = np.dot(c_neg, w)\n",
    "        sigma_neg = sigmoid(np.dot(c_neg, w))\n",
    "        grad_c_neg = sigma_neg * w  # ∂L/∂c_neg\n",
    "        grad_c_negs.append(grad_c_neg)\n",
    "    \n",
    "    # Gradiente respecto a w\n",
    "    grad_w = (sigma_pos - 1) * c_pos\n",
    "    for i, c_neg in enumerate(c_negs):\n",
    "        dot_product_neg = np.dot(c_neg, w)\n",
    "        sigma_neg = sigmoid(dot_product_neg)\n",
    "        grad_w += sigma_neg * c_neg  # Sumatoria para cada c_neg_i\n",
    "        \n",
    "    return grad_w, grad_c_pos, grad_c_negs\n",
    "\n",
    "# Parámetros iniciales\n",
    "np.random.seed(42)\n",
    "embedding_size = 5  # Dimensión de los embeddings\n",
    "learning_rate = 0.01  # Tasa de aprendizaje\n",
    "k = 2  # Número de muestras negativas por ejemplo positivo\n",
    "\n",
    "# Palabra objetivo y palabras de contexto positivas\n",
    "w_word = 'apricot'\n",
    "context_positive_words = ['tablespoon', 'of', 'jam', 'a']\n",
    "\n",
    "# Vocabulario simulado (incluyendo palabras de ruido)\n",
    "vocab = ['apricot', 'tablespoon', 'of', 'jam', 'a', 'aardvark', 'my', 'where', 'coaxial', 'seven', 'forever', 'dear', 'if']\n",
    "\n",
    "# Inicializamos los embeddings para cada palabra en el vocabulario\n",
    "embeddings = {word: np.random.randn(embedding_size) for word in vocab}\n",
    "\n",
    "# Función para muestrear palabras negativas según Pα(w)\n",
    "def sample_negative_words(vocab, positive_words, w_word, alpha=0.75, k=2):\n",
    "    # Contamos la frecuencia simulada de cada palabra (para este ejemplo, asignamos valores arbitrarios)\n",
    "    word_counts = {'apricot': 5, 'tablespoon': 10, 'of': 50, 'jam': 15, 'a': 40,\n",
    "                   'aardvark': 1, 'my': 30, 'where': 20, 'coaxial': 2, 'seven': 7,\n",
    "                   'forever': 3, 'dear': 4, 'if': 25}\n",
    "    \n",
    "    total_count = sum(count ** alpha for count in word_counts.values())\n",
    "    word_probs = {word: (count ** alpha) / total_count for word, count in word_counts.items()}\n",
    "    \n",
    "    # Excluimos las palabras positivas y la palabra objetivo\n",
    "    negative_vocab = [word for word in vocab if word not in positive_words and word != w_word]\n",
    "    negative_probs = [word_probs[word] for word in negative_vocab]\n",
    "    \n",
    "    # Normalizamos las probabilidades para que sumen 1\n",
    "    prob_sum = sum(negative_probs)\n",
    "    normalized_probs = [prob / prob_sum for prob in negative_probs]\n",
    "    \n",
    "    # Muestreamos palabras negativas\n",
    "    negative_words = np.random.choice(negative_vocab, size=k, p=normalized_probs)\n",
    "    return negative_words\n",
    "\n",
    "# Iteramos sobre los ejemplos de entrenamiento\n",
    "for epoch in range(1):  # Para simplicidad, solo una época\n",
    "    print(f\"\\nEpoca {epoch+1}\")\n",
    "    total_loss = 0\n",
    "    for c_pos_word in context_positive_words:\n",
    "        w = embeddings[w_word]\n",
    "        c_pos = embeddings[c_pos_word]\n",
    "        \n",
    "        # Muestreamos k palabras negativas\n",
    "        negative_words = sample_negative_words(vocab, context_positive_words, w_word, k=k)\n",
    "        c_negs = [embeddings[neg_word] for neg_word in negative_words]\n",
    "        \n",
    "        # Calculamos la pérdida\n",
    "        loss = loss_function(w, c_pos, c_negs)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Calculamos los gradientes\n",
    "        grad_w, grad_c_pos, grad_c_negs = gradients(w, c_pos, c_negs)\n",
    "        \n",
    "        # Actualizamos los embeddings\n",
    "        embeddings[w_word] -= learning_rate * grad_w\n",
    "        embeddings[c_pos_word] -= learning_rate * grad_c_pos\n",
    "        for i, neg_word in enumerate(negative_words):\n",
    "            embeddings[neg_word] -= learning_rate * grad_c_negs[i]\n",
    "        \n",
    "        # Imprimimos información del entrenamiento\n",
    "        print(f\"\\nPalabra objetivo: '{w_word}', Palabra de contexto positiva: '{c_pos_word}'\")\n",
    "        print(f\"Palabras negativas: {negative_words}\")\n",
    "        print(f\"Pérdida: {loss}\")\n",
    "        print(f\"Gradiente w: {grad_w}\")\n",
    "        print(f\"Gradiente c_pos ('{c_pos_word}'): {grad_c_pos}\")\n",
    "        for i, neg_word in enumerate(negative_words):\n",
    "            print(f\"Gradiente c_neg_{i+1} ('{neg_word}'): {grad_c_negs[i]}\")\n",
    "    \n",
    "    print(f\"\\nPérdida total en la época {epoch+1}: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcdcdac-fc36-4657-9800-4c1cec30618c",
   "metadata": {},
   "source": [
    "### Otros tipos de *embeddings* estáticos\n",
    "\n",
    "Existen muchos tipos de *embeddings* estáticos. Una extensión de **word2vec**, **fasttext** , aborda un problema con **word2vec** tal como lo hemos presentado hasta ahora: no tiene una buena manera de manejar palabras desconocidas, es decir, palabras que aparecen en un corpus de prueba pero no se vieron en el corpus de entrenamiento. Un problema relacionado es la dispersión de palabras, como en lenguas con morfología rica, donde algunas de las muchas formas para cada sustantivo y verbo pueden aparecer solo raramente. \n",
    "\n",
    "**Fasttext** aborda estos problemas utilizando modelos de subpalabras, representando cada palabra como sí misma más un conjunto de n-gramas constituyentes, con símbolos especiales de frontera `<` y `>` añadidos a cada palabra. Por ejemplo, con $n = 3$, la palabra *where* se representaría por la secuencia `<where>` más los n-gramas de caracteres:\n",
    "\n",
    "$$\n",
    "\\text{<wh, whe, her, ere, re>}\n",
    "$$\n",
    "\n",
    "Luego se aprende un *embedding* de **skipgram** para cada n-grama constituyente, y la palabra *where* se representa por la suma de todos los *embeddings* de sus n-gramas constituyentes. Las palabras desconocidas pueden representarse solo por la suma de los n-gramas constituyentes. Una biblioteca de código abierto de **fasttext**, que incluye *embeddings* preentrenados para 157 idiomas, está disponible en [https://fasttext.cc](https://fasttext.cc).\n",
    "\n",
    "Otro modelo de *embedding* estático muy utilizado es **GloVe**, abreviatura de *Global Vectors*, ya que el modelo se basa en capturar estadísticas globales del corpus. **GloVe** se basa en proporciones de probabilidades de la matriz de coocurrencia palabra-palabra, combinando las intuiciones de los modelos basados en conteo como **PPMI** y las estructuras lineales utilizadas por métodos como **word2vec**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80cfe9c-43bd-42f6-9cfd-7eef9eb814a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a964a-9180-4a09-b5d8-10419e3e5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Definir rutas\n",
    "base_dir = Path.cwd()  # Directorio actual del notebook\n",
    "fasttext_dir = base_dir / 'models' / 'fasttext'\n",
    "fasttext_dir.mkdir(parents=True, exist_ok=True)  # Crear directorios si no existen\n",
    "\n",
    "# URL del modelo preentrenado para español\n",
    "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.bin.gz'\n",
    "\n",
    "# Rutas de los archivos\n",
    "gz_path = fasttext_dir / 'cc.es.300.bin.gz'\n",
    "bin_path = fasttext_dir / 'cc.es.300.bin'\n",
    "\n",
    "# Función para descargar el archivo\n",
    "def descargar_modelo(url, destino):\n",
    "    if not destino.exists():\n",
    "        print(f\"Descargando {url}...\")\n",
    "        urllib.request.urlretrieve(url, destino)\n",
    "        print(\"Descarga completada.\")\n",
    "    else:\n",
    "        print(f\"El archivo {destino} ya existe. Se omitirá la descarga.\")\n",
    "\n",
    "# Función para descomprimir el archivo .gz\n",
    "def descomprimir_gz(origen, destino):\n",
    "    if not destino.exists():\n",
    "        print(f\"Descomprimiendo {origen}...\")\n",
    "        with gzip.open(origen, 'rb') as f_in:\n",
    "            with open(destino, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(\"Descompresión completada.\")\n",
    "    else:\n",
    "        print(f\"El archivo descomprimido {destino} ya existe. Se omitirá la descompresión.\")\n",
    "\n",
    "# Descargar el modelo preentrenado\n",
    "descargar_modelo(url, gz_path)\n",
    "\n",
    "# Descomprimir el modelo\n",
    "descomprimir_gz(gz_path, bin_path)\n",
    "\n",
    "# Verificar la existencia del archivo descomprimido\n",
    "if bin_path.exists():\n",
    "    print(f\"El modelo fastText está listo en: {bin_path}\")\n",
    "else:\n",
    "    print(f\"Hubo un problema al descomprimir el modelo {gz_path}.\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea7098-38bf-49f4-8e65-5148de592e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import fasttext\n",
    "\n",
    "# Ruta al modelo preentrenado descargado y descomprimido\n",
    "modelo_fasttext_path = 'models/fasttext/cc.es.300.bin'\n",
    "\n",
    "# Cargar el modelo preentrenado\n",
    "modelo = fasttext.load_model(modelo_fasttext_path)\n",
    "\n",
    "# Obtener el vector para una palabra conocida\n",
    "vector_conocida = modelo.get_word_vector('donde')\n",
    "print(f\"Vector para 'donde':\\n{vector_conocida}\\n\")\n",
    "\n",
    "# Obtener el vector para una palabra desconocida\n",
    "vector_desconocida = modelo.get_word_vector('dondeque')\n",
    "print(f\"Vector para 'dondeque' (desconocida):\\n{vector_desconocida}\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475598a-58ed-4f41-9bd7-f9f8a7ee3040",
   "metadata": {},
   "source": [
    "### Embeddings densos como word2vec y embeddings dispersos como PPMI\n",
    "\n",
    "\n",
    "En el procesamiento del lenguaje natural (NLP), una de las tareas fundamentales es representar palabras de manera que las máquinas puedan entender y procesar el lenguaje humano. Las palabras, en su forma textual, no son directamente procesables por algoritmos matemáticos o modelos estadísticos. Por lo tanto, es esencial convertir estas palabras en representaciones numéricas, conocidas como *embeddings*, que capturan relaciones semánticas y sintácticas entre ellas.\n",
    "\n",
    "Existen dos enfoques principales para crear estos embeddings: los *embeddings densos*, como word2vec, y los *embeddings dispersos*, como los basados en PPMI (Positive Pointwise Mutual Information). Este informe explorará en detalle ambos métodos, incluyendo las ecuaciones fundamentales, la implementación de matrices de co-ocurrencia y el cálculo de los valores de PPMI.\n",
    "\n",
    "\n",
    "**Embeddings Densos: word2vec**\n",
    "\n",
    "Los embeddings densos son vectores de baja dimensionalidad donde cada dimensión es un número real que captura características latentes de las palabras. Word2vec, desarrollado por Mikolov et al. en 2013, es uno de los métodos más populares para generar embeddings densos.\n",
    "\n",
    "*Arquitecturas de word2vec*\n",
    "\n",
    "Word2vec tiene dos arquitecturas principales:\n",
    "\n",
    "1. **Continuous Bag-of-Words (CBOW):** Predice la palabra objetivo basada en su contexto.\n",
    "\n",
    "2. **Skip-Gram:** Predice las palabras de contexto basadas en la palabra objetivo.\n",
    "\n",
    "Ambas arquitecturas utilizan una red neuronal simple para aprender los embeddings.\n",
    "\n",
    "*Modelo Skip-Gram*\n",
    "\n",
    "El objetivo del modelo Skip-Gram es maximizar la probabilidad de que, dada una palabra central $w_t$, se puedan predecir las palabras de contexto $w_{t+j}$ dentro de una ventana de contexto definida.\n",
    "\n",
    "La función de probabilidad para el modelo Skip-Gram es:\n",
    "\n",
    "$$\n",
    "\\max \\prod_{t=1}^{T} \\prod_{-c \\leq j \\leq c, j \\neq 0} P(w_{t+j} | w_t)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $T$ es el número total de palabras en el corpus.\n",
    "- $c$ es el tamaño de la ventana de contexto.\n",
    "\n",
    "*Función de Probabilidad Condicional*\n",
    "\n",
    "La probabilidad condicional $P(w_{O} | w_{I})$ se modela utilizando la función softmax:\n",
    "\n",
    "\n",
    "$$\n",
    "P(w_{O} | w_{I}) = \\frac{\\exp(\\mathbf{v}_{w_{O}}^\\top \\mathbf{v}_{w_{I}})}{\\sum_{w=1}^{|V|} \\exp(\\mathbf{v}_{w}^\\top \\mathbf{v}_{w_{I}})}\n",
    "$$\n",
    "\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\mathbf{v}_{w_{I}}$ es el vector de entrada de la palabra objetivo.\n",
    "- $\\mathbf{v}'_{w_{O}}$es el vector de salida de la palabra de contexto.\n",
    "- $|V|$ es el tamaño del vocabulario.\n",
    "\n",
    "*Optimización*\n",
    "\n",
    "El cálculo directo de la función softmax es computacionalmente costoso para grandes vocabularios. Para abordar esto, se utilizan métodos como *negative sampling* y *hierarchical softmax*.\n",
    "\n",
    "- **Negative sampling:** En lugar de actualizar todo el vocabulario, se actualiza un subconjunto de palabras negativas.\n",
    "  \n",
    "- **Hierarchical softmax:** Utiliza una estructura de árbol para reducir la complejidad computacional.\n",
    "\n",
    "*Ecuaciones de actualización*\n",
    "\n",
    "Durante el entrenamiento, los vectores de las palabras se actualizan utilizando gradientes derivados de la función de pérdida. Por ejemplo, utilizando *Negative Sampling*, la función de pérdida para una pareja $(w_{I}, w_{O})$ es:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "L = -\\log \\sigma(\\mathbf{v}_{w_{O}}^\\top \\mathbf{v}_{w_{I}}) - \\sum_{i=1}^{k} \\log \\sigma(-\\mathbf{v}_{w_{i}}^\\top \\mathbf{v}_{w_{I}})\n",
    "$$\n",
    "\n",
    "Esta es la forma corregida de la ecuación, adecuada para el contexto de *skip-gram* con *negative sampling*.\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\sigma(x)$ es la función sigmoide.\n",
    "- $k$ es el número de muestras negativas.\n",
    "- $w_{i}$ son las palabras negativas.\n",
    "\n",
    "**Embeddings dispersos: PPMI**\n",
    "\n",
    "Los embeddings dispersos representan palabras en vectores de alta dimensionalidad donde la mayoría de las entradas son cero. Una técnica común para crear estos embeddings es utilizar la Información Mutua Puntual Positiva (PPMI).\n",
    "\n",
    "*Información Mutua Puntual (PMI)*\n",
    "\n",
    "La PMI mide la asociación entre dos eventos, en este caso, dos palabras. Se define como:\n",
    "\n",
    "$$\n",
    "\\text{PMI}(w, c) = \\log \\left( \\frac{P(w, c)}{P(w)P(c)} \\right)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $P(w, c)$ es la probabilidad conjunta de las palabras $w$ y $c$.\n",
    "- $P(w)$ y $P(c)$ son las probabilidades marginales.\n",
    "\n",
    "Un valor de PMI alto indica una asociación fuerte entre $w$ y $c$.\n",
    "\n",
    "*PPMI (Positive PMI)*\n",
    "\n",
    "La PMI puede producir valores negativos cuando $P(w, c) < P(w)P(c)$. Para enfocarse en asociaciones positivas, se utiliza PPMI:\n",
    "\n",
    "$$\n",
    "\\text{PPMI}(w, c) = \\max( \\text{PMI}(w, c), 0 )\n",
    "$$\n",
    "\n",
    "\n",
    "**Implementación de la matriz de co-ocurrencia**\n",
    "\n",
    "La matriz de co-ocurrencia es esencial para calcular los valores de PPMI. Cada entrada en la matriz representa la frecuencia con la que dos palabras aparecen juntas en un contexto definido.\n",
    "\n",
    "*Pasos para Construir la Matriz*\n",
    "\n",
    "1. **Definir el Vocabulario $V$:** Extraer todas las palabras únicas del corpus.\n",
    "\n",
    "2. **Inicializar la Matriz $M$:** Crear una matriz de $|V| \\times |V|$ inicializada en cero.\n",
    "\n",
    "3. **Recorrer el Corpus:**\n",
    "\n",
    "   - Para cada palabra $w_i$, identificar su contexto $C(w_i)$ dentro de una ventana de tamaño $k$.\n",
    "   - Incrementar $M_{w_i, w_j}$ en uno para cada $w_j \\in C(w_i)$.\n",
    "\n",
    "*Ejemplo:*\n",
    "\n",
    "Si la oración es \"el perro ladra fuerte\", y la ventana de contexto es de tamaño 2, las co-ocurrencias serían:\n",
    "\n",
    "- \"el\" con \"perro\" y \"ladra\".\n",
    "- \"perro\" con \"el\", \"ladra\" y \"fuerte\".\n",
    "- \"ladra\" con \"el\", \"perro\", \"fuerte\".\n",
    "- \"fuerte\" con \"perro\" y \"ladra\".\n",
    "\n",
    "*Calculando probabilidades*\n",
    "\n",
    "- **Frecuencia total de co-ocurrencias $N$:**\n",
    "\n",
    "$$\n",
    "N = \\sum_{w \\in V} \\sum_{c \\in V} M_{w,c}\n",
    "$$\n",
    "\n",
    "- **Probabilidad conjunta $P(w, c)$:**\n",
    "\n",
    "$$\n",
    "P(w, c) = \\frac{M_{w,c}}{N}\n",
    "$$\n",
    "\n",
    "- **Probabilidades marginales:**\n",
    "\n",
    "$$\n",
    "P(w) = \\frac{\\sum_{c \\in V} M_{w,c}}{N}\n",
    "$$\n",
    "$$\n",
    "P(c) = \\frac{\\sum_{w \\in V} M_{w,c}}{N}\n",
    "$$\n",
    "\n",
    "\n",
    "**Cálculo de valores de PPMI**\n",
    "\n",
    "Con las probabilidades calculadas, se procede a calcular los valores de PMI y luego aplicar PPMI.\n",
    "\n",
    "*Paso 1: Calcular PMI para cada par $(w, c)$*\n",
    "\n",
    "$$\n",
    "\\text{PMI}(w, c) = \\log_2 \\left( \\frac{P(w, c)}{P(w)P(c)} \\right)\n",
    "$$\n",
    "\n",
    "*Paso 2: Aplicar PPMI*\n",
    "\n",
    "$$\n",
    "\\text{PPMI}(w, c) = \\max( \\text{PMI}(w, c), 0 )\n",
    "$$\n",
    "\n",
    "*Interpretación de PPMI*\n",
    "\n",
    "Un valor de PPMI alto indica que las palabras $w$ y $c$ co-ocurren más frecuentemente de lo esperado si fueran independientes.\n",
    "\n",
    "\n",
    "**Comparación entre embeddings densos y dispersos**\n",
    "\n",
    "*Embeddings densos (word2vec)*\n",
    "\n",
    "- **Ventajas:**\n",
    "  - Capturan relaciones semánticas y sintácticas complejas.\n",
    "  - Dimensionalidad reducida (típicamente entre 100 y 300 dimensiones).\n",
    "  - Eficientes en términos de almacenamiento y cómputo.\n",
    "  \n",
    "- **Desventajas:**\n",
    "  - Menos interpretables; las dimensiones no tienen un significado específico.\n",
    "  - Requieren grandes cantidades de datos para entrenar eficazmente.\n",
    "  - Dependientes del corpus utilizado para el entrenamiento.\n",
    "\n",
    "*Embeddings dispersos (PPMI)*\n",
    "\n",
    "- **Ventajas:**\n",
    "  - Más interpretables; cada dimensión corresponde a una palabra del vocabulario.\n",
    "  - Basados en estadísticas directas del corpus.\n",
    "  - No requieren entrenamiento complejo.\n",
    "\n",
    "- **Desventajas:**\n",
    "  - Alta dimensionalidad (igual al tamaño del vocabulario), lo que aumenta los requerimientos de almacenamiento.\n",
    "  - Pueden ser menos efectivos para capturar relaciones semánticas complejas.\n",
    "  - Menos eficientes computacionalmente para tareas de aprendizaje automático.\n",
    "\n",
    "\n",
    "**Consideraciones de implementación**\n",
    "\n",
    "*Eficiencia y escalabilidad*\n",
    "\n",
    "- **Almacenamiento disperso:** Utilizar estructuras de datos dispersas (como matrices dispersas) para almacenar la matriz de co-ocurrencia y los embeddings PPMI, lo que reduce significativamente el uso de memoria.\n",
    "\n",
    "- **Reducción de dimensionalidad:** Aplicar técnicas como la Descomposición en Valores Singulares (SVD) para reducir la dimensionalidad de los embeddings dispersos, manteniendo la mayor parte de la información relevante.\n",
    "\n",
    "*Optimización del cálculo*\n",
    "\n",
    "- **Filtrado de palabras frecuentes/infrecuentes:** Ignorar palabras demasiado frecuentes (como \"el\", \"la\", \"de\") y palabras muy infrecuentes para reducir el ruido y el tamaño de la matriz.\n",
    "\n",
    "- **Limitación del contexto:** Ajustar el tamaño de la ventana de contexto para equilibrar la cantidad de información capturada y la complejidad computacional.\n",
    "\n",
    "\n",
    "**Ejemplo práctico: Cálculo de PPMI**\n",
    "\n",
    "Supongamos un vocabulario reducido y una matriz de co-ocurrencia simplificada.\n",
    "\n",
    "*Vocabulario:* $V = \\{ \\text{perro}, \\text{gato}, \\text{animal} \\}$\n",
    "\n",
    "*Matriz de Co-ocurrencia $M$:*\n",
    "\n",
    "|       | perro | gato | animal |\n",
    "|-------|-------|------|--------|\n",
    "| perro |   0   |  3   |   2    |\n",
    "| gato  |   3   |  0   |   1    |\n",
    "| animal|   2   |  1   |   0    |\n",
    "\n",
    "*Paso 1: Calcular el total de co-ocurrencias $N$*\n",
    "\n",
    "$$\n",
    "N = \\sum_{w \\in V} \\sum_{c \\in V} M_{w,c} = (0+3+2)+(3+0+1)+(2+1+0) = 12\n",
    "$$\n",
    "\n",
    "*Paso 2: Calcular probabilidades marginales*\n",
    "\n",
    "- $P(\\text{perro}) = \\frac{0+3+2}{12} = \\frac{5}{12}$\n",
    "- $P(\\text{gato}) = \\frac{3+0+1}{12} = \\frac{4}{12}$\n",
    "- $P(\\text{animal}) = \\frac{2+1+0}{12} = \\frac{3}{12}$\n",
    "\n",
    "*Paso 3: Calcular probabilidades conjuntas*\n",
    "\n",
    "- $P(\\text{perro}, \\text{gato}) = \\frac{M_{\\text{perro}, \\text{gato}}}{N} = \\frac{3}{12} = \\frac{1}{4}$\n",
    "- $P(\\text{perro}, \\text{animal}) = \\frac{2}{12} = \\frac{1}{6}$\n",
    "\n",
    "*Paso 4: Calcular PMI*\n",
    "\n",
    "- Para $(\\text{perro}, \\text{gato})$:\n",
    "\n",
    "$$\n",
    "\\text{PMI}(\\text{perro}, \\text{gato}) = \\log_2 \\left( \\frac{P(\\text{perro}, \\text{gato})}{P(\\text{perro})P(\\text{gato})} \\right) = \\log_2 \\left( \\frac{\\frac{1}{4}}{\\frac{5}{12} \\times \\frac{4}{12}} \\right) = \\log_2 \\left( \\frac{\\frac{1}{4}}{\\frac{20}{144}} \\right) = \\log_2 \\left( \\frac{36}{20} \\right) \\approx \\log_2(1.8) \\approx 0.85\n",
    "$$\n",
    "\n",
    "- Para $(\\text{perro}, \\text{animal})$:\n",
    "\n",
    "$$\n",
    "\\text{PMI}(\\text{perro}, \\text{animal}) = \\log_2 \\left( \\frac{\\frac{1}{6}}{\\frac{5}{12} \\times \\frac{3}{12}} \\right) = \\log_2 \\left( \\frac{\\frac{1}{6}}{\\frac{15}{144}} \\right) = \\log_2 \\left( \\frac{24}{15} \\right) \\approx \\log_2(1.6) \\approx 0.68\n",
    "$$\n",
    "\n",
    "*Paso 5: Calcular PPMI*\n",
    "\n",
    "Como ambos valores de PMI son positivos, los valores de PPMI serán los mismos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272db99-880d-4952-bb16-f20b6a0d9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Importar las bibliotecas necesarias\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Definir el corpus\n",
    "corpus = [\n",
    "    \"el perro ladra fuerte\",\n",
    "    \"el gato maúlla suavemente\",\n",
    "    \"el perro y el gato son amigos\",\n",
    "    \"el animal ladra y maúlla\"\n",
    "]\n",
    "\n",
    "# Preprocesamiento básico\n",
    "def preprocess_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Procesar el corpus\n",
    "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# Construcción del vocabulario\n",
    "word_counts = Counter()\n",
    "for doc in processed_corpus:\n",
    "    word_counts.update(doc)\n",
    "\n",
    "# Crear un índice para cada palabra\n",
    "vocab = {word: idx for idx, word in enumerate(word_counts.keys())}\n",
    "index_to_word = {idx: word for word, idx in vocab.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulario:\", vocab)\n",
    "\n",
    "# Construcción de la matriz de co-ocurrencia\n",
    "def build_cooccurrence_matrix(corpus, vocab, window_size=2):\n",
    "    cooccurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for doc in corpus:\n",
    "        doc_length = len(doc)\n",
    "        for idx, word in enumerate(doc):\n",
    "            word_id = vocab[word]\n",
    "            context_start = max(0, idx - window_size)\n",
    "            context_end = min(doc_length, idx + window_size + 1)\n",
    "            \n",
    "            for context_idx in range(context_start, context_end):\n",
    "                if context_idx != idx:\n",
    "                    context_word = doc[context_idx]\n",
    "                    context_word_id = vocab[context_word]\n",
    "                    cooccurrence_matrix[word_id][context_word_id] += 1\n",
    "    return cooccurrence_matrix\n",
    "\n",
    "# Construir la matriz de co-ocurrencia\n",
    "cooccurrence_matrix = build_cooccurrence_matrix(processed_corpus, vocab)\n",
    "\n",
    "print(\"\\nMatriz de Co-ocurrencia:\")\n",
    "print(cooccurrence_matrix)\n",
    "\n",
    "# Cálculo de probabilidades marginales y conjuntas\n",
    "N = np.sum(cooccurrence_matrix)\n",
    "word_probabilities = np.sum(cooccurrence_matrix, axis=1) / N\n",
    "context_probabilities = np.sum(cooccurrence_matrix, axis=0) / N\n",
    "joint_probabilities = cooccurrence_matrix / N\n",
    "\n",
    "# Cálculo de PMI y PPMI\n",
    "with np.errstate(divide='ignore'):\n",
    "    pmi_matrix = np.log2(joint_probabilities / (word_probabilities[:, None] * context_probabilities[None, :]))\n",
    "    pmi_matrix[np.isinf(pmi_matrix)] = 0.0  # Reemplazar infinito por cero\n",
    "    pmi_matrix[np.isnan(pmi_matrix)] = 0.0  # Reemplazar NaN por cero\n",
    "\n",
    "ppmi_matrix = np.maximum(pmi_matrix, 0)\n",
    "\n",
    "print(\"\\nMatriz PPMI:\")\n",
    "print(ppmi_matrix)\n",
    "\n",
    "# Función para imprimir la matriz con etiquetas\n",
    "def print_matrix(matrix, vocab):\n",
    "    print(\" \", end=\"\\t\")\n",
    "    for idx in range(vocab_size):\n",
    "        print(index_to_word[idx], end=\"\\t\")\n",
    "    print()\n",
    "    for i in range(vocab_size):\n",
    "        print(index_to_word[i], end=\"\\t\")\n",
    "        for j in range(vocab_size):\n",
    "            print(f\"{matrix[i][j]:.2f}\", end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "print(\"\\nMatriz de Co-ocurrencia (con etiquetas):\")\n",
    "print_matrix(cooccurrence_matrix, vocab)\n",
    "\n",
    "print(\"\\nMatriz PPMI (con etiquetas):\")\n",
    "print_matrix(ppmi_matrix, vocab)\n",
    "\n",
    "# Reducción de dimensionalidad con SVD\n",
    "n_components = 2  # Reducimos a 2 dimensiones para visualización\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "ppmi_reduced = svd.fit_transform(ppmi_matrix)\n",
    "\n",
    "print(\"\\nEmbeddings PPMI Reducidos:\")\n",
    "for i in range(len(ppmi_reduced)):\n",
    "    print(f\"{index_to_word[i]}: {ppmi_reduced[i]}\")\n",
    "\n",
    "# Visualización de los embeddings PPMI reducidos\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(ppmi_reduced)):\n",
    "    plt.scatter(ppmi_reduced[i, 0], ppmi_reduced[i, 1])\n",
    "    plt.text(ppmi_reduced[i, 0]+0.01, ppmi_reduced[i, 1]+0.01, index_to_word[i])\n",
    "\n",
    "plt.title(\"Visualización de Embeddings PPMI Reducidos\")\n",
    "plt.xlabel(\"Componente 1\")\n",
    "plt.ylabel(\"Componente 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Implementación de word2vec con gensim\n",
    "w2v_model = Word2Vec(sentences=processed_corpus, vector_size=2, window=2, min_count=1, workers=1, sg=1, epochs=100)\n",
    "\n",
    "# Obtener los vectores de word2vec\n",
    "word_vectors = w2v_model.wv\n",
    "w2v_embeddings = np.array([word_vectors[word] for word in vocab.keys()])\n",
    "\n",
    "print(\"\\nEmbeddings word2vec:\")\n",
    "for i, word in enumerate(vocab.keys()):\n",
    "    print(f\"{word}: {w2v_embeddings[i]}\")\n",
    "\n",
    "# Visualización de los embeddings word2vec\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, word in enumerate(vocab.keys()):\n",
    "    vector = w2v_embeddings[i]\n",
    "    plt.scatter(vector[0], vector[1])\n",
    "    plt.text(vector[0]+0.01, vector[1]+0.01, word)\n",
    "\n",
    "plt.title(\"Visualización de Embeddings word2vec\")\n",
    "plt.xlabel(\"Dimensión 1\")\n",
    "plt.ylabel(\"Dimensión 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66309df3-f240-4686-94e0-1b42201cef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir el corpus\n",
    "corpus = [\n",
    "    \"el perro ladra fuerte\",\n",
    "    \"el gato maúlla suavemente\",\n",
    "    \"el perro y el gato son amigos\",\n",
    "    \"el animal ladra y maúlla\"\n",
    "]\n",
    "\n",
    "# Preprocesamiento básico\n",
    "def preprocess_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Procesar el corpus\n",
    "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# Construcción del vocabulario\n",
    "word_counts = Counter()\n",
    "for doc in processed_corpus:\n",
    "    word_counts.update(doc)\n",
    "\n",
    "# Crear un índice para cada palabra\n",
    "vocab = {word: idx for idx, word in enumerate(word_counts.keys())}\n",
    "index_to_word = {idx: word for word, idx in vocab.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulario:\", vocab)\n",
    "\n",
    "# Construcción de la matriz de co-ocurrencia\n",
    "def build_cooccurrence_matrix(corpus, vocab, window_size=2):\n",
    "    cooccurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for doc in corpus:\n",
    "        doc_length = len(doc)\n",
    "        for idx, word in enumerate(doc):\n",
    "            word_id = vocab[word]\n",
    "            context_start = max(0, idx - window_size)\n",
    "            context_end = min(doc_length, idx + window_size + 1)\n",
    "            \n",
    "            for context_idx in range(context_start, context_end):\n",
    "                if context_idx != idx:\n",
    "                    context_word = doc[context_idx]\n",
    "                    context_word_id = vocab[context_word]\n",
    "                    cooccurrence_matrix[word_id][context_word_id] += 1\n",
    "    return cooccurrence_matrix\n",
    "\n",
    "# Construir la matriz de co-ocurrencia\n",
    "cooccurrence_matrix = build_cooccurrence_matrix(processed_corpus, vocab)\n",
    "\n",
    "print(\"\\nMatriz de Co-ocurrencia:\")\n",
    "print(cooccurrence_matrix)\n",
    "\n",
    "# Cálculo de probabilidades marginales y conjuntas\n",
    "N = np.sum(cooccurrence_matrix)\n",
    "word_probabilities = np.sum(cooccurrence_matrix, axis=1) / N\n",
    "context_probabilities = np.sum(cooccurrence_matrix, axis=0) / N\n",
    "joint_probabilities = cooccurrence_matrix / N\n",
    "\n",
    "# Cálculo de PMI y PPMI\n",
    "with np.errstate(divide='ignore'):\n",
    "    pmi_matrix = np.log2(joint_probabilities / (word_probabilities[:, None] * context_probabilities[None, :]))\n",
    "    pmi_matrix[np.isinf(pmi_matrix)] = 0.0  # Reemplazar infinito por cero\n",
    "    pmi_matrix[np.isnan(pmi_matrix)] = 0.0  # Reemplazar NaN por cero\n",
    "\n",
    "ppmi_matrix = np.maximum(pmi_matrix, 0)\n",
    "\n",
    "print(\"\\nMatriz PPMI:\")\n",
    "print(ppmi_matrix)\n",
    "\n",
    "# Función para imprimir la matriz con etiquetas\n",
    "def print_matrix(matrix, vocab):\n",
    "    print(\" \", end=\"\\t\")\n",
    "    for idx in range(vocab_size):\n",
    "        print(index_to_word[idx], end=\"\\t\")\n",
    "    print()\n",
    "    for i in range(vocab_size):\n",
    "        print(index_to_word[i], end=\"\\t\")\n",
    "        for j in range(vocab_size):\n",
    "            print(f\"{matrix[i][j]:.2f}\", end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "print(\"\\nMatriz de Co-ocurrencia (con etiquetas):\")\n",
    "print_matrix(cooccurrence_matrix, vocab)\n",
    "\n",
    "print(\"\\nMatriz PPMI (con etiquetas):\")\n",
    "print_matrix(ppmi_matrix, vocab)\n",
    "\n",
    "# Reducción de dimensionalidad con SVD\n",
    "n_components = 2  # Reducimos a 2 dimensiones para visualización\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "ppmi_reduced = svd.fit_transform(ppmi_matrix)\n",
    "\n",
    "print(\"\\nEmbeddings PPMI Reducidos:\")\n",
    "for i in range(len(ppmi_reduced)):\n",
    "    print(f\"{index_to_word[i]}: {ppmi_reduced[i]}\")\n",
    "\n",
    "# Visualización de los embeddings PPMI reducidos\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(ppmi_reduced)):\n",
    "    plt.scatter(ppmi_reduced[i, 0], ppmi_reduced[i, 1])\n",
    "    plt.text(ppmi_reduced[i, 0]+0.01, ppmi_reduced[i, 1]+0.01, index_to_word[i])\n",
    "\n",
    "plt.title(\"Visualización de Embeddings PPMI Reducidos\")\n",
    "plt.xlabel(\"Componente 1\")\n",
    "plt.ylabel(\"Componente 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ---- Implementación de Word2Vec sin usar gensim ----\n",
    "\n",
    "# Parámetros del modelo\n",
    "embedding_dim = 2  # Dimensionalidad de los embeddings\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "window_size = 2\n",
    "\n",
    "# Generar pares (objetivo, contexto)\n",
    "def generate_training_data(corpus, window_size):\n",
    "    training_data = []\n",
    "    for sentence in corpus:\n",
    "        sentence_length = len(sentence)\n",
    "        for idx, target_word in enumerate(sentence):\n",
    "            context_start = max(0, idx - window_size)\n",
    "            context_end = min(sentence_length, idx + window_size + 1)\n",
    "            for context_idx in range(context_start, context_end):\n",
    "                if context_idx != idx:\n",
    "                    context_word = sentence[context_idx]\n",
    "                    training_data.append((vocab[target_word], vocab[context_word]))\n",
    "    return training_data\n",
    "\n",
    "training_data = generate_training_data(processed_corpus, window_size)\n",
    "print(\"\\nPares de entrenamiento (objetivo, contexto):\")\n",
    "print(training_data)\n",
    "\n",
    "# Inicialización de matrices de pesos\n",
    "np.random.seed(0)  # Para reproducibilidad\n",
    "W_in = np.random.uniform(-0.8, 0.8, (vocab_size, embedding_dim))\n",
    "W_out = np.random.uniform(-0.8, 0.8, (vocab_size, embedding_dim))\n",
    "\n",
    "# Funciones auxiliares\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Estabilidad numérica\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def train_skipgram(training_data, W_in, W_out, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for target, context in training_data:\n",
    "            # Forward pass\n",
    "            z = np.dot(W_out, W_in[target])\n",
    "            y_pred = softmax(z)\n",
    "            \n",
    "            # Cálculo de la pérdida (cross-entropy)\n",
    "            loss -= np.log(y_pred[context] + 1e-7)  # Evitar log(0)\n",
    "            \n",
    "            # Backward pass\n",
    "            y_pred[context] -= 1  # Derivada de la pérdida respecto a z\n",
    "            \n",
    "            # Gradientes\n",
    "            dW_out = np.outer(y_pred, W_in[target])\n",
    "            dW_in = np.dot(W_out.T, y_pred)\n",
    "            \n",
    "            # Actualización de pesos\n",
    "            W_out -= learning_rate * dW_out\n",
    "            W_in[target] -= learning_rate * dW_in\n",
    "        if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Pérdida: {loss:.4f}\")\n",
    "    return W_in, W_out\n",
    "\n",
    "# Entrenar el modelo Skip-Gram\n",
    "W_in, W_out = train_skipgram(training_data, W_in, W_out, epochs, learning_rate)\n",
    "\n",
    "# Extraer embeddings de palabras\n",
    "word_embeddings = W_in\n",
    "\n",
    "print(\"\\nEmbeddings Word2Vec personalizados:\")\n",
    "for word, idx in vocab.items():\n",
    "    print(f\"{word}: {word_embeddings[idx]}\")\n",
    "\n",
    "# Visualización de los embeddings Word2Vec personalizados\n",
    "plt.figure(figsize=(8, 6))\n",
    "for word, idx in vocab.items():\n",
    "    vector = word_embeddings[idx]\n",
    "    plt.scatter(vector[0], vector[1])\n",
    "    plt.text(vector[0]+0.01, vector[1]+0.01, word)\n",
    "\n",
    "plt.title(\"Visualización de Embeddings Word2Vec Personalizados\")\n",
    "plt.xlabel(\"Dimensión 1\")\n",
    "plt.ylabel(\"Dimensión 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81b077-f41d-4496-879b-bef418fbb746",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "**Ejercicio 1: Comprensión del preprocesamiento del corpus**\n",
    "\n",
    "**a.** Explica por qué es importante el preprocesamiento del texto antes de construir el vocabulario y la matriz de co-ocurrencia. ¿Qué técnicas de preprocesamiento adicionales podrían implementarse para mejorar la calidad de los embeddings?\n",
    "\n",
    "**b.** Dado el corpus proporcionado, identifica manualmente el vocabulario y asigne un índice a cada palabra. ¿Coinciden tus resultados con los obtenidos en el código?\n",
    "\n",
    "\n",
    "**Ejercicio 2: Construcción manual de la matriz de co-ocurrencia**\n",
    "\n",
    "**a.** Utilizando una ventana de contexto de tamaño 2, construya manualmente la matriz de co-ocurrencia para el corpus dado. Asegúrate de considerar todas las oraciones y de contar correctamente las co-ocurrencias.\n",
    "\n",
    "**b.** Compara tu matriz con la generada en el código. Identifica y explica cualquier discrepancia.\n",
    "\n",
    "\n",
    "**Ejercicio 3: Cálculo de probabilidades y PMI**\n",
    "\n",
    "**a.** Calcula el total de co-ocurrencias ($N$) basado en la matriz de co-ocurrencia que construyó en el ejercicio anterior.\n",
    "\n",
    "**b.** Calcula las probabilidades marginales $P(w)$ y $P(c)$ para cada palabra del vocabulario.\n",
    "\n",
    "**c.** Calcula las probabilidades conjuntas $P(w, c)$ para cada par de palabras que co-ocurren.\n",
    "\n",
    "**d.** Utilizando las probabilidades calculadas, determina los valores de PMI para al menos cinco pares de palabras con co-ocurrencias significativas.\n",
    "\n",
    "\n",
    "**Ejercicio 4: Aplicación de PPMI**\n",
    "\n",
    "**a.** A partir de los valores de PMI obtenidos, aplique la función PPMI para obtener los valores positivos. ¿Qué efecto tiene esta transformación en los valores negativos de PMI?\n",
    "\n",
    "**b.** Analiza los valores de PPMI obtenidos. ¿Cuáles son las palabras que presentan la mayor asociación? Explica por qué, considerando el contexto del corpus.\n",
    "\n",
    "\n",
    "**Ejercicio 5: Interpretación de la matriz PPMI**\n",
    "\n",
    "**a.** Examina la matriz PPMI resultante y explique qué representa cada dimensión en este espacio vectorial.\n",
    "\n",
    "**b.** Discute las ventajas y desventajas de utilizar embeddings dispersos de alta dimensionalidad en aplicaciones de PLN.\n",
    "\n",
    "\n",
    "**Ejercicio 6: Reducción de dimensionalidad con SVD**\n",
    "\n",
    "**a.** Explica el propósito de aplicar la Descomposición en Valores Singulares (SVD) a la matriz PPMI.\n",
    "\n",
    "**b.** ¿Qué significan los componentes resultantes después de la reducción de dimensionalidad? ¿Se mantiene la interpretabilidad de las dimensiones?\n",
    "\n",
    "**c.** Discute cómo la elección del número de componentes afecta la calidad y utilidad de los embeddings reducidos.\n",
    "\n",
    "\n",
    "**Ejercicio 7: Visualización y análisis de embeddings PPMI reducidos**\n",
    "\n",
    "**a.** Basándote en la visualización de los embeddings PPMI reducidos a dos dimensiones, identifica grupos de palabras que estén cercanas en el espacio vectorial. ¿Qué relaciones semánticas o sintácticas comparten estas palabras?\n",
    "\n",
    "**b.** Propone una interpretación para la posición relativa de las palabras \"perro\", \"gato\", \"animal\", \"ladra\" y \"maúlla\" en el espacio reducido.\n",
    "\n",
    "\n",
    "**Ejercicio 8: Implementación y comprensión de word2vec**\n",
    "\n",
    "**a.** Explica brevemente cómo funciona el modelo skip-gram con negative sampling utilizado en word2vec.\n",
    "\n",
    "**b.** Discute cómo los parámetros del modelo (por ejemplo, tamaño de ventana, dimensiones del vector, número de épocas) pueden influir en los embeddings resultantes.\n",
    "\n",
    "**c.** Sin ejecutar código, describa los pasos necesarios para entrenar un modelo word2vec con el corpus dado.\n",
    "\n",
    "**Ejercicio 9: Comparación entre embeddings PPMI y word2vec**\n",
    "\n",
    "**a.** Compara las visualizaciones de los embeddings PPMI reducidos y los embeddings word2vec. ¿Qué similitudes y diferencias observa en la distribución de las palabras en ambos espacios?\n",
    "\n",
    "**b.** Analiza cómo cada método captura las relaciones entre palabras. Proporciona ejemplos específicos del corpus para respaldar su análisis.\n",
    "\n",
    "\n",
    "**Ejercicio 10: Aplicación práctica de los embeddings**\n",
    "\n",
    "**a.** Proponiendo una tarea sencilla de NLP (por ejemplo, clasificación de textos o detección de sinónimos), explica cómo utilizarías los embeddings PPMI y word2vec para resolverla.\n",
    "\n",
    "**b.** Discute las posibles ventajas y limitaciones de cada tipo de embedding en el contexto de la tarea propuesta.\n",
    "\n",
    "**Ejercicio 11: Impacto del tamaño del corpus**\n",
    "\n",
    "**a.** Reflexiona sobre cómo el tamaño y la diversidad del corpus afectan la calidad de los embeddings generados tanto por PPMI como por word2vec.\n",
    "\n",
    "**b.** Si tuvieras la oportunidad de ampliar el corpus, ¿qué tipo de datos adicionales incluiría y por qué?\n",
    "\n",
    "\n",
    "**Ejercicio 12: Ajuste de parámetros y optimización**\n",
    "\n",
    "**a.** Supongamos que experimentas con diferentes tamaños de ventana en la construcción de la matriz de co-ocurrencia. ¿Cómo esperas que esto afecte los valores de PPMI y, en consecuencia, los embeddings?\n",
    "\n",
    "**b.** Analiza cómo el parámetro `min_count` en word2vec influye en el vocabulario y en los embeddings generados.\n",
    "\n",
    "\n",
    "**Ejercicio 13: Exploración de palabras raras y rrecuentes**\n",
    "\n",
    "**a.** Considera las palabras muy frecuentes y muy infrecuentes en el corpus. ¿Cómo afectan estas palabras a la matriz de co-ocurrencia y al cálculo de PPMI?\n",
    "\n",
    "**b.** Discute estrategias para manejar palabras de alta frecuencia (como \"el\", \"y\") al construir embeddings. ¿Por qué podría ser útil filtrarlas o asignarles un peso diferente?\n",
    "\n",
    "\n",
    "**Ejercicio 14: Interpretación de resultados y conclusiones**\n",
    "\n",
    "**a.** Resume las principales diferencias entre los embeddings densos y dispersos en términos de representación y uso práctico.\n",
    "\n",
    "**b.** Basándose en los ejercicios anteriores, ¿cuál método consideras más adecuado para aplicaciones específicas de PLN y por qué?\n",
    "\n",
    "**Ejercicio 15: Investigación adicional**\n",
    "\n",
    "**a.** Investiga otras técnicas de embeddings dispersos y densos no mencionadas en el texto (por ejemplo, GloVe, FastText). Compara sus enfoques y características principales con PPMI y word2vec.\n",
    "\n",
    "**b.** Explora cómo incorporarías información contextual adicional (como etiquetado gramatical o entidades nombradas) en la generación de embeddings. ¿Qué modificaciones al código o al proceso serían necesarias?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f31a09-3a22-4801-9d9c-fffa5e3551d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af009c8-94c7-4998-af94-cd5cfcf362b2",
   "metadata": {},
   "source": [
    "### Visualización de *embeddings*\n",
    "\n",
    "Visualizar *embeddings* es un objetivo importante para ayudar a comprender, aplicar y mejorar estos modelos de significado de palabras. Pero ¿cómo podemos visualizar un vector de (por ejemplo) 100 dimensiones?\n",
    "\n",
    "La forma más sencilla de visualizar el significado de una palabra $w$ incrustada en un espacio es listar las palabras más similares a $w$ ordenando los vectores de todas las palabras en el vocabulario según su coseno con el vector de $w$. Por ejemplo, las 7 palabras más cercanas a *frog* usando un conjunto de *embeddings* calculados con el algoritmo **GloVe** son: *frogs*, *toad*, *litoria*, *leptodactylidae*, *rana*, *lizard*, y *eleutherodactylus*.\n",
    "\n",
    "Otro método de visualización es usar un algoritmo de agrupamiento para mostrar una representación jerárquica de qué palabras son similares a otras en el espacio de *embeddings*.\n",
    "\n",
    "Sin embargo, el método de visualización probablemente más común es proyectar las 100 dimensiones de una palabra a 2 dimensiones  utilizando un método de proyección llamado **t-SNE**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810f8a2-bd8a-4d9f-af58-f9d118468da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import time  # Para medir el tiempo de computación\n",
    "\n",
    "# Paso 1: Cargar los embeddings de GloVe\n",
    "def cargar_glove(ruta_archivo):\n",
    "    embeddings = {}\n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "        for linea in f:\n",
    "            valores = linea.strip().split()\n",
    "            palabra = valores[0]\n",
    "            vector = np.array(valores[1:], dtype='float32')\n",
    "            embeddings[palabra] = vector\n",
    "    return embeddings\n",
    "\n",
    "#https://github.com/rohanrao619/Twitter_Sentiment_Analysis/blob/master/glove.6B.100d.txt\n",
    "ruta_glove = 'glove.6B.100d.txt'  # Asegúrate de tener este archivo en tu directorio de trabajo\n",
    "embeddings = cargar_glove(ruta_glove)\n",
    "\n",
    "# Paso 2: Seleccionar un subconjunto de palabras\n",
    "# Puedes experimentar con diferentes conjuntos de palabras\n",
    "palabras_interes = input(\"Ingresa las palabras que deseas visualizar, separadas por comas: \")\n",
    "palabras_interes = [palabra.strip() for palabra in palabras_interes.split(',')]\n",
    "\n",
    "# Filtrar las palabras que existen en los embeddings\n",
    "palabras_existentes = [palabra for palabra in palabras_interes if palabra in embeddings]\n",
    "\n",
    "# Verificar si hay palabras que no están en los embeddings\n",
    "palabras_no_encontradas = set(palabras_interes) - set(palabras_existentes)\n",
    "if palabras_no_encontradas:\n",
    "    print(f\"Advertencia: Las siguientes palabras no se encontraron en los embeddings y serán omitidas: {', '.join(palabras_no_encontradas)}\")\n",
    "\n",
    "# Obtener los vectores correspondientes\n",
    "vectores = np.array([embeddings[palabra] for palabra in palabras_existentes])\n",
    "\n",
    "# Paso 3: Ajustar parámetros de t-SNE\n",
    "perplexity = int(input(\"Ingresa el valor de perplexity para t-SNE (recomendado entre 5 y 50): \"))\n",
    "n_iter = int(input(\"Ingresa el número de iteraciones para t-SNE (recomendado al menos 1000): \"))\n",
    "\n",
    "# Medir el tiempo de computación\n",
    "inicio_tiempo = time.time()\n",
    "\n",
    "# Aplicar t-SNE para reducir a 2 dimensiones\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, random_state=42)\n",
    "vectores_tsne = tsne.fit_transform(vectores)\n",
    "\n",
    "fin_tiempo = time.time()\n",
    "tiempo_total = fin_tiempo - inicio_tiempo\n",
    "print(f\"Tiempo de computación de t-SNE: {tiempo_total:.2f} segundos\")\n",
    "\n",
    "# Paso 4: Graficar los resultados\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, palabra in enumerate(palabras_existentes):\n",
    "    x, y = vectores_tsne[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(palabra, (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.title('Visualización de Embeddings con t-SNE')\n",
    "plt.xlabel('Componente 1')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da9c1c-e2f7-4151-8bf1-02e96c0d8805",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Extiende el código para guardar la visualización como una imagen o explorar otros métodos de reducción de dimensionalidad, como PCA o UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212d412-8e12-461f-b7e2-d262aecbd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
