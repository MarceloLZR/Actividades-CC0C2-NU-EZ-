{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64fd7bb",
   "metadata": {},
   "source": [
    "## Modelos seq2seq + atención\n",
    "\n",
    "\n",
    "\n",
    "Los modelos de atención han transformado la forma en que abordamos las tareas de traducción automática y otras aplicaciones de procesamiento de lenguaje natural (PLN). Estos modelos permiten que el decodificador se centre en diferentes partes de la secuencia de entrada mientras genera cada palabra de la secuencia de salida. Dos mecanismos importantes en este contexto son la atención global y la atención local. Este informe detalla estos mecanismos, sus implementaciones, y sus aplicaciones prácticas.\n",
    "\n",
    "#### Mecanismo de atención global\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de atención global, introducido por Bahdanau también conocido como atención suave, permite que el decodificador considere todas las posiciones de la secuencia de entrada al generar cada palabra de la secuencia de salida. Este enfoque asegura que el modelo tenga acceso a toda la información de la entrada en cada paso del proceso de decodificación, mejorando la calidad de la traducción, especialmente en secuencias largas y complejas.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El mecanismo de atención global se basa en los siguientes pasos y ecuaciones:\n",
    "\n",
    "1. **Cálculo de los puntajes de atención**:\n",
    "   Para cada paso de tiempo del decodificador, se calcula un puntaje de atención $ e_{ij}$ que mide la afinidad entre el estado oculto del decodificador en el paso de tiempo $ j$, denotado como $ s_{j-1}$, y el estado oculto del codificador en el paso de tiempo $ i$, denotado como $ h_i$. Esto se puede calcular usando una red neuronal feedforward con una sola capa oculta (o cualquier otra función de afinidad):\n",
    "\n",
    "   $$\n",
    "   e_{ij} = v^T \\tanh(W_1 h_i + W_2 s_{j-1})\n",
    "   $$\n",
    "\n",
    "   donde $W_1$ y $W_2$ son matrices de peso aprendibles y $v$ es un vector de peso aprendible.\n",
    "\n",
    "2. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $ \\alpha_{ij}$, que son distribuciones de probabilidad sobre las posiciones de la secuencia de entrada:\n",
    "\n",
    " $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}\n",
    "  $$\n",
    "\n",
    "3. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $ c_j$ para cada paso de tiempo del decodificador se calcula como una combinación ponderada de los estados ocultos del codificador:\n",
    "\n",
    " $$\n",
    "   c_j = \\sum_{i=1}^{T_x} \\alpha_{ij} h_i\n",
    " $$\n",
    "\n",
    "4. **Generación de la salida del decodificador**:\n",
    "   Finalmente, el vector de contexto $ c_j$ se combina con el estado oculto del decodificador $ s_j$ para generar la salida $ y_j$:\n",
    "\n",
    " $$\n",
    "   y_j = g(c_j, s_j)\n",
    " $$\n",
    "\n",
    "  donde $ g$ puede ser una función no lineal como una red neuronal.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3787c5",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención global dentro de un modelo seq2seq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfaeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        H = hidden.repeat(max_len, 1, 1).transpose(0, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat([H, encoder_outputs], 2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        energy = torch.bmm(v, energy)\n",
    "        attn_weights = F.softmax(energy.squeeze(1), dim=1)\n",
    "        return attn_weights\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.GRU(output_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        decoder_outputs, hidden = self.decoder(trg, hidden)\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)\n",
    "        output = torch.cat([hidden.squeeze(0), context.squeeze(1)], 1)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d178228",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención local\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de atención local, propuesto por Luong et al. (2015), reduce la complejidad computacional al limitar el alcance de la atención a una ventana local alrededor de cada posición de la secuencia de entrada. Este enfoque es particularmente útil en secuencias largas, donde la atención global puede ser computacionalmente costosa.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El mecanismo de atención local se define a través de los siguientes pasos:\n",
    "\n",
    "1. **Predicción de la posición de atención**:\n",
    "   Primero, se predice una posición de atención $p_j$ para cada paso de tiempo $j$ del decodificador. Esto puede hacerse mediante una simple función lineal o una red neuronal:\n",
    "   \n",
    "\n",
    "   $$\n",
    "   p_j = S \\cdot \\sigma(W_p s_{j-1})\n",
    "   $$ \n",
    "\n",
    "   donde $S$ es la longitud de la secuencia de entrada, $\\sigma$ es la función sigmoide, $W_p$ es una matriz de peso aprendible, y $s_{j-1}$ es el estado oculto del decodificador en el paso $j-1$.\n",
    "\n",
    "\n",
    "2. **Definición de la ventana local**:\n",
    "   Se define una ventana local de tamaño $2D + 1$ centrada en $p_j$. Los límites de la ventana se calculan como:\n",
    "\n",
    "   $$\n",
    "   [p_j - D, p_j + D]\n",
    "  $$ \n",
    "\n",
    "\n",
    "3. **Cálculo de puntajes de atención dentro de la ventana**:\n",
    "   Los puntajes de atención $e_{ij}$ se calculan solo para las posiciones dentro de la ventana local:\n",
    "\n",
    "\n",
    "   $$\n",
    "   e_{ij} = v^T \\tanh(W_1 h_i + W_2 s_{j-1})\n",
    "  $$ \n",
    "\n",
    "\n",
    "4. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $\\alpha_{ij}$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in [p_j - D, p_j + D]} \\exp(e_{ik})}\n",
    "  $$ \n",
    "\n",
    "5. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $c_j$ se calcula como una combinación ponderada de los estados ocultos del codificador dentro de la ventana local:\n",
    "\n",
    "   $$\n",
    "   c_j = \\sum_{i \\in [p_j - D, p_j + D]} \\alpha_{ij} h_i\n",
    "  $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f80f8a",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención local dentro de un modelo seq2seq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6146062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, window_size):\n",
    "        super(LocalAttention, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        pos = torch.arange(max_len).unsqueeze(0).repeat(batch_size, 1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, max_len, 1)\n",
    "        attn_energies = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
    "        attn_energies = torch.sum(self.v * attn_energies, dim=2)\n",
    "        attn_weights = F.softmax(attn_energies, dim=1)\n",
    "\n",
    "        return attn_weights\n",
    "\n",
    "class Seq2SeqWithLocalAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, window_size):\n",
    "        super(Seq2SeqWithLocalAttention, self).__init__()\n",
    "        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.GRU(output_dim, hidden_dim, batch_first=True)\n",
    "        self.local_attention = LocalAttention(hidden_dim, window_size)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        decoder_outputs, hidden = self.decoder(trg, hidden)\n",
    "        attn_weights = self.local_attention(hidden, encoder_outputs)\n",
    "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)\n",
    "        output = torch.cat([hidden.squeeze(0), context.squeeze(1)], 1)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a0b29",
   "metadata": {},
   "source": [
    "Los mecanismos de atención global y local ofrecen diferentes enfoques para mejorar la generación de secuencias en modelos seq2seq. La atención global proporciona una visión completa de la secuencia de entrada, capturando dependencias a largo plazo, mientras que la atención local mejora la eficiencia computacional al enfocarse en ventanas locales. La elección entre estos mecanismos depende de la naturaleza de la tarea y las prioridades del modelo en términos de precisión y eficiencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141bb915",
   "metadata": {},
   "source": [
    "### Ejemplo \n",
    "\n",
    "Se implementa un modelo de red neuronal con mecanismo de atención para procesar y traducir secuencias de texto, típicamente utilizado en tareas de traducción automática, como del alemán al inglés. Este tipo de modelo pertenece a la categoría de modelos secuencia a secuencia (seq2seq) con atención, donde la \"atención\" permite al modelo enfocarse en diferentes partes de la entrada para cada paso de la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Importar las librerías necesarias\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Definición de símbolos especiales para el proceso de codificación/decodificación\n",
    "# S: Símbolo que indica el inicio de la decodificación de la entrada\n",
    "# E: Símbolo que indica el inicio de la salida de la decodificación\n",
    "# P: Símbolo que se utiliza para rellenar la secuencia si el tamaño del lote actual es menor que los pasos de tiempo\n",
    "\n",
    "def make_batch():\n",
    "    # Creación de lotes de datos usando one-hot encoding para las palabras en las frases\n",
    "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
    "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
    "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
    "\n",
    "    # Convertir los lotes a tensores de PyTorch\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        # Celdas RNN para el codificador y decodificador con dropout\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "\n",
    "        # Capas lineales para calcular la atención y la salida\n",
    "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
    "\n",
    "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
    "        # Transposición de las entradas para obtener la dimensión correcta\n",
    "        enc_inputs = enc_inputs.transpose(0, 1)\n",
    "        dec_inputs = dec_inputs.transpose(0, 1)\n",
    "\n",
    "        # Obtención de las salidas y estados ocultos del codificador\n",
    "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
    "\n",
    "        trained_attn = []\n",
    "        hidden = enc_hidden\n",
    "        n_step = len(dec_inputs)\n",
    "        model = torch.empty([n_step, 1, n_class])\n",
    "\n",
    "        for i in range(n_step):  # Proceso para cada paso de tiempo\n",
    "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
    "            attn_weights = self.get_att_weight(dec_output, enc_outputs)\n",
    "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
    "\n",
    "            # Producto de matrices para obtener el contexto y combinarlo con la salida del decodificador\n",
    "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
    "            dec_output = dec_output.squeeze(0)\n",
    "            context = context.squeeze(1)\n",
    "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
    "\n",
    "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
    "\n",
    "    def get_att_weight(self, dec_output, enc_outputs):\n",
    "        n_step = len(enc_outputs)\n",
    "        attn_scores = torch.zeros(n_step)\n",
    "\n",
    "        for i in range(n_step):\n",
    "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
    "\n",
    "        # Normalizar los puntajes para obtener pesos en el rango de 0 a 1\n",
    "        return F.softmax(attn_scores, dim=0).view(1, 1, -1)\n",
    "\n",
    "    def get_att_score(self, dec_output, enc_output):\n",
    "        score = self.attn(enc_output)\n",
    "        return torch.dot(dec_output.view(-1), score.view(-1))  # Producto punto para obtener un valor escalar\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_step = 5  # Número de celdas (número de pasos)\n",
    "    n_hidden = 128  # Número de unidades ocultas en una celda\n",
    "\n",
    "    # Frases de ejemplo para la traducción\n",
    "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "\n",
    "    # Crear listas de palabras únicas y diccionarios para el mapeo de palabras a índices\n",
    "    word_list = \" \".join(sentences).split()\n",
    "    word_list = list(set(word_list))\n",
    "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "    n_class = len(word_dict)  # Tamaño del vocabulario\n",
    "\n",
    "    hidden = torch.zeros(1, 1, n_hidden)  # Estado oculto inicial\n",
    "\n",
    "    model = Attention()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "    # Proceso de entrenamiento\n",
    "    for epoch in range(2000):\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(input_batch, hidden, output_batch)\n",
    "\n",
    "        loss = criterion(output, target_batch.squeeze(0))\n",
    "        if (epoch + 1) % 400 == 0:\n",
    "            print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Prueba del modelo con nuevas entradas\n",
    "    test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
    "    test_batch = torch.FloatTensor(test_batch)\n",
    "    predict, trained_attn = model(input_batch, hidden, test_batch)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "    # Mostrar la matriz de atención\n",
    "    # Suponiendo que 'trained_attn' es tu matriz de atención y quieres visualizarla\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    cax = ax.matshow(trained_attn, cmap='viridis')\n",
    "\n",
    "    # Supongamos que 'sentences' son tus oraciones de entrada y salida\n",
    "    input_sentence = sentences[0].split()  # Oración de entrada\n",
    "    output_sentence = sentences[2].split()  # Oración de salida\n",
    "\n",
    "    # Configurando los ticks del eje X\n",
    "    ax.set_xticks(range(len(input_sentence)))\n",
    "    ax.set_xticklabels(input_sentence)\n",
    "    ax.xaxis.set_major_locator(ticker.FixedLocator(range(len(input_sentence))))\n",
    "\n",
    "    # Configurando los ticks del eje Y\n",
    "    ax.set_yticks(range(len(output_sentence)))\n",
    "    ax.set_yticklabels(output_sentence)\n",
    "    ax.yaxis.set_major_locator(ticker.FixedLocator(range(len(output_sentence))))\n",
    "\n",
    "    # Rotar las etiquetas del eje X para mejorar la visibilidad\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Añadiendo etiquetas y título si es necesario\n",
    "    ax.set_xlabel('Oracion entrada ')\n",
    "    ax.set_ylabel('Oracion de salida')\n",
    "    ax.set_title('Matriz de atencion')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8be7b7",
   "metadata": {},
   "source": [
    "La matriz de atención visualiza cómo el modelo de traducción con mecanismo de atención distribuye su \"foco\" o \"atención\" mientras traduce una secuencia de entrada a una secuencia de salida. En este caso específico, parece que la traducción va del alemán al inglés, de la frase \"ich mochte ein bier P\" a \"i want a beer E\". Vamos a analizar la matriz:\n",
    "\n",
    "\n",
    "* Ejes: El eje horizontal representa las palabras de la secuencia de entrada (en alemán), mientras que el eje vertical representa las palabras de la secuencia de salida (en inglés).\n",
    "* Colores: Las áreas más claras (amarillas) muestran mayor atención, es decir, cuando el modelo se centra más en esa palabra específica de la entrada al generar la palabra correspondiente de la salida. Las áreas más oscuras indican menos atención.\n",
    "\n",
    "**Análisis detallado**\n",
    "\n",
    "Palabra por palabra:\n",
    "\n",
    "- \"i\" (output) presta atención principalmente a \"ich\" (input), lo cual tiene sentido ya que ambos son pronombres personales.\n",
    "- \"want\" (output) presta atención a \"mochte\" (input), reflejando la relación entre el deseo expresado en ambas lenguas.\n",
    "- \"a\" (output) y \"beer\" (output) ambos enfocan principalmente en \"bier\" (input), lo que indica que el modelo reconoce correctamente que \"bier\" es el objeto principal de la oración.\n",
    "- \"beer\" (output) también muestra atención a \"ein\" (input), lo que es lógico dado que \"ein\" es el artículo indefinido para \"bier\".\n",
    "\n",
    "La palabra \"E\" en la salida, que probablemente sea un símbolo especial para indicar el final de la frase, presta atención a la palabra \"P\" en la entrada. Esto podría sugerir que \"P\" se usa también como un marcador de posición o un símbolo especial en el modelo (posiblemente para marcar el fin de una frase o rellenar espacio si la entrada es más corta que el número máximo de pasos de tiempo esperados por el modelo).\n",
    "\n",
    "La matriz sugiere que el mecanismo de atención está funcionando eficazmente, al concentrarse en las partes relevantes de la entrada para cada palabra de la salida. Esto es crucial para la calidad de la traducción, especialmente en oraciones más largas y complejas donde la correspondencia directa entre las palabras de entrada y salida no es tan lineal.\n",
    "\n",
    "Observar la atención pagada a los símbolos especiales como \"E\" y \"P\" puede ofrecer conocimiento sobre cómo el modelo maneja los finales de frases y el relleno, aspectos que podrían ajustarse durante la optimización del modelo para mejorar su rendimiento o hacerlo más adaptativo a diferentes longitudes de entrada.\n",
    "Esta visualización de la matriz de atención no solo ayuda a verificar la correcta funcionalidad del modelo, sino que también proporciona una herramienta valiosa para ajustar y mejorar el modelo, asegurando que la atención se distribuye de manera que refleje las estructuras lingüísticas y semánticas de ambos idiomas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9c79e",
   "metadata": {},
   "source": [
    "A partir de este ejemplo resuelve los siguientes ejercicios:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a9248",
   "metadata": {},
   "source": [
    "1 .  Implementa beam search en tu modelo de atención. Comienza con un factor de beam pequeño (por ejemplo, k=3) y experimenta con diferentes valores para ver cómo afecta la calidad de las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(dec_outputs, k):\n",
    "    sequences = [[list(), 1.0]]\n",
    "    # Recorrer cada paso en la salida del decodificador\n",
    "    for row in dec_outputs:\n",
    "        all_candidates = list()\n",
    "        # Expandir cada secuencia actual\n",
    "        for seq, score in sequences:\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score * -np.log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # Ordenar todos los candidatos por puntuación\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        # Seleccionar k mejores\n",
    "        sequences = ordered[:k]\n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1ce71",
   "metadata": {},
   "source": [
    "2 . Modifica la decodificación para incorporar sampling con temperatura. La temperatura modifica la distribución de probabilidad de la salida del modelo, permitiendo controlar el grado de aleatoriedad en la elección de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e469b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sampling(logits, temperature=1.0):\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "    return torch.multinomial(probs, 1).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c7d2a",
   "metadata": {},
   "source": [
    "3 . Experimenta con diferentes tasas de dropout en las celdas RNN y añade regularización L2 al optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6428a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifica la tasa de dropout en las celdas RNN\n",
    "self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.3)  # menor dropout\n",
    "self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.3)\n",
    "\n",
    "# Añadir regularización L2 al optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # añadir weight_decay para L2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf62422",
   "metadata": {},
   "source": [
    "4 . Varía el número de unidades ocultas, la tasa de aprendizaje, y el número de épocas de entrenamiento. Observa cómo cada cambio afecta el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referencia : https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n",
    "        self.out = nn.Linear(n_hidden * 2, num_classes)\n",
    "\n",
    "    # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.view(-1, n_hidden * 2, 1)   # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "\n",
    "    def forward(self, X):\n",
    "        input = self.embedding(X) # input : [batch_size, len_seq, embedding_dim]\n",
    "        input = input.permute(1, 0, 2) # input : [len_seq, batch_size, embedding_dim]\n",
    "\n",
    "        hidden_state = torch.zeros(1*2, len(X), n_hidden) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        cell_state = torch.zeros(1*2, len(X), n_hidden) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "\n",
    "        # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "        output = output.permute(1, 0, 2) # output : [batch_size, len_seq, n_hidden]\n",
    "        attn_output, attention = self.attention_net(output, final_hidden_state)\n",
    "        return self.out(attn_output), attention # model : [batch_size, num_classes], attention : [batch_size, n_step]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    embedding_dim = 2 # tamaño del embedding\n",
    "    n_hidden = 5  # número de unidades ocultas en una celda\n",
    "    num_classes = 2  # 0 o 1\n",
    "\n",
    "    # Sentencias de 3 palabras (=la longitud de la secuencia es 3)\n",
    "    sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "    labels = [1, 1, 1, 0, 0, 0]  # 1 es bueno, 0 no es bueno.\n",
    "\n",
    "    word_list = \" \".join(sentences).split()\n",
    "    word_list = list(set(word_list))\n",
    "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "    vocab_size = len(word_dict)\n",
    "\n",
    "    model = BiLSTM_Attention()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    inputs = np.array([np.asarray([word_dict[n] for n in sen.split()]) for sen in sentences])\n",
    "    inputs = torch.LongTensor(inputs)\n",
    "    targets = torch.LongTensor([out for out in labels])  # Para usar la función de pérdida Softmax de Torch\n",
    "\n",
    "    # Entrenamiento\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        output, attention = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Prueba\n",
    "    test_text = 'sorry hate you'\n",
    "    tests = np.array([np.asarray([word_dict[n] for n in test_text.split()])])\n",
    "    test_batch = torch.LongTensor(tests)\n",
    "\n",
    "    # Predicción\n",
    "    predict, _ = model(test_batch)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    if predict[0][0] == 0:\n",
    "        print(test_text,\"is Bad Mean...\")\n",
    "    else:\n",
    "        print(test_text,\"is Good Mean!!\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 3)) # [batch_size, n_step]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    ax.set_xticks(range(len(['first_word', 'second_word', 'third_word'])))\n",
    "    ax.set_xticklabels(['first_word', 'second_word', 'third_word'], fontdict={'fontsize': 14}, rotation=90)\n",
    "    \n",
    "    ax.set_yticks(range(len(['batch_1', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6'])))\n",
    "    ax.set_yticklabels(['batch_1', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6'], fontdict={'fontsize': 14})\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6d8b9",
   "metadata": {},
   "source": [
    "El resultado representa una matriz de atención de un modelo BiLSTM con mecanismo de atención. En esta matriz:\n",
    "\n",
    "* Las etiquetas en el eje X representan las palabras en las sentencias (\"first_word\", \"second_word\", \"third_word\").\n",
    "* Las etiquetas en el eje Y representan los diferentes lotes (\"batch_1\", \"batch_2\", \"batch_3\", \"batch_4\", \"batch_5\", \"batch_6\").\n",
    "\n",
    "Cada celda en la matriz muestra el peso de atención que el modelo asigna a cada palabra en las oraciones de entrada. Los colores indican la intensidad de estos pesos, donde los colores más claros (amarillo) representan mayores pesos de atención y los colores más oscuros (morado) representan menores pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3364a5b6",
   "metadata": {},
   "source": [
    "* Incrementar el tamaño de la capa de embedding y las unidades ocultas de LSTM para mejorar la capacidad del modelo de capturar características más complejas.\n",
    "* Ajusta la tasa de aprendizaje y probar diferentes optimizadores como AdamW o RMSprop.\n",
    "* Incluye más oraciones en el conjunto de datos para entrenar el modelo de manera más robusta.\n",
    "* Mejora la visualización de la matriz de atención utilizando técnicas de matplotlib avanzadas para una interpretación más clara.\n",
    "* Meustra las palabras reales en los ejes en lugar de \"first_word\", \"second_word\", etc.\n",
    "* Agrega capas de Dropout después de la capa de LSTM para evitar el sobreajuste.\n",
    "* Implementa técnicas de regularización como L2 regularization en los parámetros del modelo.\n",
    "\n",
    "Presenta una versión mejorada del código que incorpora algunas de esas mejoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bd137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81333cd4",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención jerárquica\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "La atención jerárquica se utiliza para manejar estructuras de datos complejas y de múltiples niveles, como documentos largos divididos en párrafos, párrafos divididos en oraciones y oraciones divididas en palabras. Este mecanismo aplica la atención en dos niveles: a nivel de palabra dentro de cada oración y a nivel de oración dentro del documento. Esta estructura permite capturar dependencias tanto locales como globales de manera eficiente.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El proceso de atención jerárquica se puede dividir en dos fases principales:\n",
    "\n",
    "1. **Atención a nivel de palabra**:\n",
    "   Primero, se aplica la atención para cada palabra dentro de cada oración. Supongamos que una oración $ \\text{sentence}_i $ contiene $ T_i $ palabras y sus representaciones de palabra son $ \\{h_{i1}, h_{i2}, \\ldots, h_{iT_i}\\} $. La atención a nivel de palabra se calcula de la siguiente manera:\n",
    "\n",
    "   $$\n",
    "   e_{ij} = v_1^T \\tanh(W_1 h_{ij} + b_1)\n",
    "   $$\n",
    "\n",
    "   donde $ W_1 $ y $ v_1 $ son parámetros aprendibles, y $ b_1 $ es un vector de sesgo.\n",
    "\n",
    "   Los pesos de atención se obtienen aplicando la función softmax a los puntajes $ e_{ij} $:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_i} \\exp(e_{ik})}\n",
    "   $$\n",
    "\n",
    "   El vector de contexto para la oración $ i $ se calcula como una combinación ponderada de las representaciones de palabra:\n",
    "\n",
    "   $$\n",
    "   c_i = \\sum_{j=1}^{T_i} \\alpha_{ij} h_{ij}\n",
    "   $$\n",
    "\n",
    "2. **Atención a nivel de oración**:\n",
    "   Una vez obtenidos los vectores de contexto $ \\{c_1, c_2, \\ldots, c_N\\} $ para todas las oraciones en un documento (donde $ N $ es el número de oraciones en el documento), se aplica la atención a nivel de oración:\n",
    "\n",
    "   $$\n",
    "   e_i = v_2^T \\tanh(W_2 c_i + b_2)\n",
    "   $$\n",
    "\n",
    "   Los pesos de atención a nivel de oración se obtienen aplicando la función softmax a los puntajes $ e_i $:\n",
    "\n",
    "   $$\n",
    "   \\beta_i = \\frac{\\exp(e_i)}{\\sum_{k=1}^{N} \\exp(e_k)}\n",
    "   $$\n",
    "\n",
    "   El vector de contexto para el documento se calcula como una combinación ponderada de los vectores de contexto de las oraciones:\n",
    "\n",
    "   $$\n",
    "   d = \\sum_{i=1}^{N} \\beta_i c_i\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb7f8b",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención jerárquica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd757ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalAttention(nn.Module):\n",
    "    def __init__(self, word_hidden_size, sentence_hidden_size, vocab_size, word_embedding_dim):\n",
    "        super(HierarchicalAttention, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        self.word_encoder = nn.GRU(word_embedding_dim, word_hidden_size, batch_first=True)\n",
    "        self.word_attention = nn.Linear(word_hidden_size, word_hidden_size)\n",
    "        self.sentence_encoder = nn.GRU(word_hidden_size, sentence_hidden_size, batch_first=True)\n",
    "        self.sentence_attention = nn.Linear(sentence_hidden_size, sentence_hidden_size)\n",
    "        self.fc = nn.Linear(sentence_hidden_size, num_classes)  # num_classes depende de la tarea específica\n",
    "\n",
    "    def forward(self, documents):\n",
    "        sentence_vectors = []\n",
    "        for sentences in documents:  # Iterar sobre documentos\n",
    "            word_vectors = []\n",
    "            for sentence in sentences:  # Iterar sobre oraciones\n",
    "                embedded_words = self.word_embedding(sentence)\n",
    "                word_enc_outputs, _ = self.word_encoder(embedded_words)\n",
    "                word_att_weights = F.softmax(self.word_attention(word_enc_outputs), dim=1)\n",
    "                word_vector = torch.sum(word_att_weights * word_enc_outputs, dim=1)\n",
    "                word_vectors.append(word_vector)\n",
    "            sentence_vectors.append(torch.stack(word_vectors))\n",
    "        sentence_enc_outputs, _ = self.sentence_encoder(torch.stack(sentence_vectors))\n",
    "        sentence_att_weights = F.softmax(self.sentence_attention(sentence_enc_outputs), dim=1)\n",
    "        doc_vector = torch.sum(sentence_att_weights * sentence_enc_outputs, dim=1)\n",
    "        output = self.fc(doc_vector)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e7277",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención basada en consultas\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "La atención basada en consultas, utilizada en modelos como el Transformer, utiliza tres componentes principales: consultas (queries), claves (keys) y valores (values). Este mecanismo permite calcular la atención como una función de similitud entre las consultas y las claves, aplicándola a los valores para obtener una representación ponderada. Este enfoque es altamente eficiente y escalable.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El mecanismo de atención basada en consultas se define a través de los siguientes pasos:\n",
    "\n",
    "1. **Cálculo de consultas, claves y valores**:\n",
    "   Las consultas $ Q$, las claves $ K$ y los valores $ V$ se obtienen mediante proyecciones lineales de la entrada:\n",
    "\n",
    "   $$\n",
    "   Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "   $$\n",
    "\n",
    "   donde $ W_Q$, $ W_K$ y $ W_V$ son matrices de peso aprendibles.\n",
    "\n",
    "2. **Cálculo de puntajes de atención**:\n",
    "   Los puntajes de atención se calculan como el producto punto escalado entre las consultas y las claves:\n",
    "\n",
    "   $$\n",
    "   e_{ij} = \\frac{Q_i K_j^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "   donde $ d_k$ es la dimensión de las claves.\n",
    "\n",
    "3. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $ \\alpha_{ij}$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})}\n",
    "   $$\n",
    "\n",
    "4. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $ c_i$ se calcula como una combinación ponderada de los valores:\n",
    "\n",
    "   $$\n",
    "   c_i = \\sum_{j} \\alpha_{ij} V_j\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14003c",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención basada en consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryBasedAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(QueryBasedAttention, self).__init__()\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        Q = self.linear_q(query)\n",
    "        K = self.linear_k(key)\n",
    "        V = self.linear_v(value)\n",
    "        \n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        output = self.fc_out(attn_output)\n",
    "        return output\n",
    "\n",
    "class Seq2SeqWithQueryBasedAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Seq2SeqWithQueryBasedAttention, self).__init__()\n",
    "        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.GRU(output_dim, hidden_dim, batch_first=True)\n",
    "        self.query_attn = QueryBasedAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        enc_output, hidden = self.encoder(src)\n",
    "        dec_output, _ = self.decoder(trg, hidden)\n",
    "        attn_output = self.query_attn(dec_output, enc_output, enc_output)\n",
    "        output = self.fc(attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41960b",
   "metadata": {},
   "source": [
    "Los mecanismos de atención jerárquica y basada en consultas ofrecen enfoques avanzados y efectivos para mejorar el rendimiento de los modelos de secuencia a secuencia en diversas tareas de procesamiento de lenguaje natural. La atención jerárquica es ideal para manejar datos estructurados jerárquicamente y capturar dependencias a múltiples niveles, mientras que la atención basada en consultas es altamente eficiente y escalable, lo que la hace fundamental para modelos modernos como el Transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865bfb10",
   "metadata": {},
   "source": [
    "#### Mecanismo de auto-atención (Self-Attention)\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de auto-atención, o self-attention, permite que cada elemento de la secuencia preste atención a todos los demás elementos de la misma secuencia. Esto es fundamental para capturar las dependencias a largo plazo en las secuencias y es un componente clave en los modelos Transformer.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El proceso de auto-atención se puede describir mediante los siguientes pasos:\n",
    "\n",
    "1. **Proyección lineal**:\n",
    "   Al igual que en la atención multi-cabecera, se proyectan las consultas $Q$, las claves $K$ y los valores $V$:\n",
    "\n",
    "   $$\n",
    "   Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "   $$\n",
    "\n",
    "   donde $W_Q$, $W_K$ y $W_V$ son matrices de peso aprendibles.\n",
    "\n",
    "2. **Cálculo de puntajes de atención**:\n",
    "   Los puntajes de atención se calculan utilizando el producto punto escalado:\n",
    "\n",
    "   $$\n",
    "   e_{ij} = \\frac{Q_i K_j^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "3. **Normalización de puntajes de atención**:\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención  $\\alpha_{ij}$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})}\n",
    "   $$\n",
    "\n",
    "4. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto $c_i$ se calcula como una combinación ponderada de los valores:\n",
    "\n",
    "   $$\n",
    "   c_i = \\sum_{j} \\alpha_{ij} V_j\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2fc8f",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de auto-atención:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.linear_q(x)\n",
    "        K = self.linear_k(x)\n",
    "        V = self.linear_v(x)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = self.fc_out(attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d22d58",
   "metadata": {},
   "source": [
    "#### Mecanismo de atención multi-cabecera (Multi-Head Attention)\n",
    "\n",
    "**Descripción general**\n",
    "\n",
    "El mecanismo de atención multi-cabecera, introducido por Vaswani en el modelo Transformer, extiende la idea de la auto-atención al permitir que el modelo se concentre en diferentes partes de la secuencia de entrada de manera simultánea y desde múltiples perspectivas. Esto se logra al tener múltiples \"cabeceras\" de atención, cada una de las cuales realiza una operación de atención independiente.\n",
    "\n",
    "**Ecuaciones y cálculos**\n",
    "\n",
    "El proceso de atención multi-cabecera se puede dividir en varios pasos:\n",
    "\n",
    "1. **Proyección lineal**:\n",
    "   Se proyectan las consultas  $Q$, las claves $K$ y los valores $V$ en subespacios diferentes para cada cabecera de atención. Supongamos que tenemos $h$ cabecera de atención y una dimensión de modelo $d_{\\text{model}}$. La proyección se realiza de la siguiente manera:\n",
    "\n",
    "   $$\n",
    "   Q_h = X W_Q^h, \\quad K_h = X W_K^h, \\quad V_h = X W_V^h\n",
    "   $$\n",
    "\n",
    "   donde $W_Q^h$, $W_K^h$ y $W_V^h$ son matrices de peso específicas para la cabecera  $h$.\n",
    "\n",
    "2. **Cálculo de puntajes de atención**:\n",
    "   Para cada cabecera de atención, se calcula el puntaje de atención utilizando el producto punto escalado:\n",
    "\n",
    "   $$\n",
    "   e_{ij}^h = \\frac{Q_i^h (K_j^h)^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "   donde $d_k$ es la dimensión de las claves.\n",
    "\n",
    "3. **Normalización de puntajes de atención**:\n",
    "\n",
    "   Los puntajes de atención se normalizan usando la función softmax para obtener los pesos de atención $\\alpha_{ij}^h$:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij}^h = \\frac{\\exp(e_{ij}^h)}{\\sum_{k} \\exp(e_{ik}^h)}\n",
    "   $$\n",
    "\n",
    "4. **Cálculo del vector de contexto**:\n",
    "   El vector de contexto para cada cabecera de atención se calcula como una combinación ponderada de los valores:\n",
    "\n",
    "   $$\n",
    "   c_i^h = \\sum_{j} \\alpha_{ij}^h V_j^h\n",
    "   $$\n",
    "\n",
    "5. **Concatenación y proyección final**:\n",
    "   Los vectores de contexto de todas las cabeceras se concatenan y se proyectan de nuevo en el espacio original:\n",
    "\n",
    "   $$\n",
    "   \\text{MultiHead}(Q, K, V) = \\text{Concat}(c_i^1, c_i^2, \\ldots, c_i^h) W_O\n",
    "   $$\n",
    "\n",
    "   donde  $W_O$ es la matriz de peso de proyección final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d27616",
   "metadata": {},
   "source": [
    "La siguiente implementación en PyTorch muestra cómo se puede construir un mecanismo de atención multi-cabecera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046437fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Proyección lineal\n",
    "        Q = self.linear_q(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.linear_k(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.linear_v(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Cálculo de puntajes de atención\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.head_dim)\n",
    "\n",
    "        # Normalización de puntajes de atención\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Cálculo del vector de contexto\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "\n",
    "        # Proyección final\n",
    "        output = self.fc_out(attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63bbf6",
   "metadata": {},
   "source": [
    "Estos mecanismos son fundamentales para el funcionamiento de los modelos Transformer, permitiendo capturar dependencias a largo plazo y manejar grandes cantidades de datos de manera eficiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384f534",
   "metadata": {},
   "source": [
    "### Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d6ef2",
   "metadata": {},
   "source": [
    "1 .Implementa un modelo de atención jerárquica donde primero se aplique atención a nivel de palabra y luego a nivel de frase para tareas de resumen de texto o clasificación de documentos.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Divide un documento en oraciones y cada oración en palabras.\n",
    "- Implementa atención a nivel de palabra para obtener una representación de cada oración.\n",
    "- Implementa atención a nivel de frase para obtener una representación del documento.\n",
    "- Usa la representación del documento para realizar una tarea específica (por ejemplo, clasificación de documentos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d80f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be317e",
   "metadata": {},
   "source": [
    "2 . Implementa un mecanismo de atención multi-cabecera similar al utilizado en los modelos de transformadores, para permitir que el modelo enfoque en diferentes partes de la entrada de manera simultánea.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Implementa un bloque de atención con múltiples cabezas.\n",
    "- Integra este bloque en un modelo seq2seq.\n",
    "- Evalua el rendimiento del modelo en una tarea de traducción automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55639b",
   "metadata": {},
   "source": [
    "3 . Implementa un modelo de atención local que solo considere una ventana alrededor de la posición actual en lugar de toda la secuencia de entrada, reduciendo la complejidad computacional.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "* Define una ventana de atención fija.\n",
    "* Implementa el cálculo de los pesos de atención solo dentro de esta ventana.\n",
    "* Integra este mecanismo en un modelo seq2seq y evaluar su rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1112631",
   "metadata": {},
   "source": [
    "4 . Implementa un modelo de atención basada en consultas similar al mecanismo utilizado en el Transformador, donde las consultas, las claves y los valores provienen de proyecciones de la entrada.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Implementa las proyecciones lineales para consultas, claves y valores.\n",
    "- Calcula los pesos de atención usando productos escalares entre las consultas y las claves.\n",
    "- Aplica estos pesos a los valores para obtener la representación de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617dc25",
   "metadata": {},
   "source": [
    "5 . Implementa la auto-atención donde cada elemento de la secuencia presta atención a todos los demás elementos de la misma secuencia. Esto es útil para tareas de traducción automática y clasificación de secuencias.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Implementa el cálculo de auto-atención utilizando proyecciones lineales para claves, consultas y valores.\n",
    "- Integra el mecanismo de auto-atención en un modelo seq2seq.\n",
    "- Evalua el rendimiento del modelo en una tarea de traducción automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
