{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a1d661",
   "metadata": {},
   "source": [
    "### Modelo secuencia a secuencia\n",
    "\n",
    "Un modelo secuencia a secuencia (Seq2Seq) es una arquitectura de red neuronal diseñada para transformar una secuencia de entrada en una secuencia de salida, y es especialmente útil para tareas donde la longitud de las secuencias de entrada y salida puede diferir. Los modelos Seq2Seq son comúnmente utilizados en aplicaciones como traducción automática, resumen de texto, y generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c74e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# S: Símbolo que muestra el inicio de la entrada de decodificación\n",
    "# E: Símbolo que muestra el inicio de la salida de decodificación\n",
    "# P: Símbolo que llenará la secuencia en blanco si el tamaño de los datos del lote actual es menor que los pasos de tiempo\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input_seq = [num_dic[n] for n in seq[0]]\n",
    "        output_seq = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input_seq])\n",
    "        output_batch.append(np.eye(n_class)[output_seq])\n",
    "        target_batch.append(target) # no es one-hot\n",
    "\n",
    "    # Convertir listas a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    # crear tensor\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# crear lote de prueba\n",
    "def make_testbatch(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    "\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))\n",
    "    input_seq = [num_dic[n] for n in input_w]\n",
    "    output_seq = [num_dic[n] for n in 'S' + 'P' * n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input_seq]\n",
    "    output_batch = np.eye(n_class)[output_seq]\n",
    "\n",
    "    # Convertir a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Modelo\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, pasos de tiempo), tamaño_lote, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, pasos de tiempo), tamaño_lote, n_class]\n",
    "\n",
    "        # enc_states : [num_layers * num_directions(=1), tamaño_lote, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        # outputs : [max_len+1(=6), tamaño_lote, num_directions(=1) * n_hidden(=128)]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        model = self.fc(outputs) # model : [max_len+1(=6), tamaño_lote, n_class]\n",
    "        return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_step = 5\n",
    "    n_hidden = 128\n",
    "\n",
    "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "    num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "    seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "    n_class = len(num_dic)\n",
    "    tamaño_lote = len(seq_data)\n",
    "\n",
    "    model = Seq2Seq()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        # crear forma oculta [num_layers * num_directions, tamaño_lote, n_hidden]\n",
    "        hidden = torch.zeros(2, tamaño_lote, n_hidden) # cambiar a 2 capas\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # input_batch : [tamaño_lote, max_len(=n_step, pasos de tiempo), n_class]\n",
    "        # output_batch : [tamaño_lote, max_len+1(=n_step, pasos de tiempo) (por 'S' o 'E'), n_class]\n",
    "        # target_batch : [tamaño_lote, max_len+1(=n_step, pasos de tiempo)], no es one-hot\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1, tamaño_lote, n_class]\n",
    "        output = output.transpose(0, 1) # [tamaño_lote, max_len+1(=6), n_class]\n",
    "        loss = 0\n",
    "        for i in range(0, len(target_batch)):\n",
    "            # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1]\n",
    "            loss += criterion(output[i], target_batch[i])\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Prueba\n",
    "    def translate(word):\n",
    "        input_batch, output_batch = make_testbatch(word)\n",
    "\n",
    "        # crear forma oculta [num_layers * num_directions, tamaño_lote, n_hidden]\n",
    "        hidden = torch.zeros(2, 1, n_hidden) # cambiar a 2 capas\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1(=6), tamaño_lote(=1), n_class]\n",
    "\n",
    "        predict = output.data.max(2, keepdim=True)[1] # seleccionar dimensión n_class\n",
    "        decoded = [char_arr[i] for i in predict]\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "\n",
    "        return translated.replace('P', '')\n",
    "\n",
    "    print('test')\n",
    "    print('man ->', translate('man'))\n",
    "    print('mans ->', translate('mans'))\n",
    "    print('king ->', translate('king'))\n",
    "    print('black ->', translate('black'))\n",
    "    print('upp ->', translate('upp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ae88d",
   "metadata": {},
   "source": [
    "### Greedy, beam search y selección aleatoria con temperatura\n",
    "\n",
    "Podemos implementar las estrategias de decodificación greedy, beam search y selección aleatoria con temperatura, podemos modificar el código dado agregando funciones específicas para cada método de decodificación.\n",
    "\n",
    "**Estrategia greedy**\n",
    "\n",
    "La estrategia Greedy selecciona el token con la mayor probabilidad en cada paso de decodificación.\n",
    "\n",
    "**Estrategia beam search**\n",
    "\n",
    "Beam Search mantiene las mejores k secuencias en cada paso, lo que permite explorar múltiples caminos en la decodificación y seleccionar la secuencia más probable.\n",
    "\n",
    "**Selección aleatoria con temperatura**\n",
    "\n",
    "La selección aleatoria con temperatura ajusta las probabilidades de los tokens antes de muestrear de la distribución, permitiendo más exploración en la generación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# S: Símbolo que muestra el inicio de la entrada de decodificación\n",
    "# E: Símbolo que muestra el inicio de la salida de decodificación\n",
    "# P: Símbolo que llenará la secuencia en blanco si el tamaño de los datos del lote actual es menor que los pasos de tiempo\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input_seq = [num_dic[n] for n in seq[0]]\n",
    "        output_seq = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input_seq])\n",
    "        output_batch.append(np.eye(n_class)[output_seq])\n",
    "        target_batch.append(target) # no es one-hot\n",
    "\n",
    "    # Convertir listas a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    # crear tensor\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# crear lote de prueba\n",
    "def make_testbatch(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    "\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))\n",
    "    input_seq = [num_dic[n] for n in input_w]\n",
    "    output_seq = [num_dic[n] for n in 'S' + 'P' * n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input_seq]\n",
    "    output_batch = np.eye(n_class)[output_seq]\n",
    "\n",
    "    # Convertir a numpy arrays antes de convertir a tensores\n",
    "    input_batch = np.array(input_batch)\n",
    "    output_batch = np.array(output_batch)\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Modelo\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, num_layers=2, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, pasos de tiempo), tamaño_lote, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, pasos de tiempo), tamaño_lote, n_class]\n",
    "\n",
    "        # enc_states : [num_layers * num_directions(=1), tamaño_lote, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        # outputs : [max_len+1(=6), tamaño_lote, num_directions(=1) * n_hidden(=128)]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        model = self.fc(outputs) # model : [max_len+1(=6), tamaño_lote, n_class]\n",
    "        return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_step = 5\n",
    "    n_hidden = 128\n",
    "\n",
    "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "    num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "    seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "    n_class = len(num_dic)\n",
    "    tamaño_lote = len(seq_data)\n",
    "\n",
    "    model = Seq2Seq()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        # crear forma oculta [num_layers * num_directions, tamaño_lote, n_hidden]\n",
    "        hidden = torch.zeros(2, tamaño_lote, n_hidden) # cambiar a 2 capas\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # input_batch : [tamaño_lote, max_len(=n_step, pasos de tiempo), n_class]\n",
    "        # output_batch : [tamaño_lote, max_len+1(=n_step, pasos de tiempo) (por 'S' o 'E'), n_class]\n",
    "        # target_batch : [tamaño_lote, max_len+1(=n_step, pasos de tiempo)], no es one-hot\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1, tamaño_lote, n_class]\n",
    "        output = output.transpose(0, 1) # [tamaño_lote, max_len+1(=6), n_class]\n",
    "        loss = 0\n",
    "        for i in range(0, len(target_batch)):\n",
    "            # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1]\n",
    "            loss += criterion(output[i], target_batch[i])\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoca:', '%04d' % (epoch + 1), 'costo =', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Estrategias de Decodificación\n",
    "    def greedy_decode(input_batch, hidden, output_batch):\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        predict = output.data.max(2, keepdim=True)[1] # seleccionar dimensión n_class\n",
    "        return predict\n",
    "\n",
    "    def beam_search_decode(input_batch, hidden, output_batch, beam_width=3):\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        output = F.softmax(output, dim=2)  # Aplicar softmax para obtener probabilidades\n",
    "        sequences = [[list(), 1.0]]  # (sequence, score)\n",
    "\n",
    "        for row in output.squeeze(1):  # Asegurar que se procesa la salida correcta\n",
    "            all_candidates = list()\n",
    "            for seq, score in sequences:\n",
    "                for j in range(len(row)):\n",
    "                    candidate = [seq + [j], score * row[j].item()]  # Multiplicar las probabilidades, no sumar logaritmos negativos\n",
    "                    all_candidates.append(candidate)\n",
    "            # Ordenar todos los candidatos por puntuación de manera descendente (probabilidad más alta primero)\n",
    "            ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
    "            sequences = ordered[:beam_width]\n",
    "\n",
    "        best_sequence = sequences[0][0]\n",
    "        return torch.tensor(best_sequence, dtype=torch.long).view(1, -1, 1)\n",
    "\n",
    "\n",
    "    def random_sample_with_temperature(output, temperature=1.0):\n",
    "        output = output.div(temperature).exp()\n",
    "        probs = F.softmax(output, dim=-1)\n",
    "        return torch.multinomial(probs.view(-1, probs.size(-1)), 1).view(output.size(0), output.size(1), -1)\n",
    "\n",
    "    def decode_with_temperature(input_batch, hidden, output_batch, temperature=1.0):\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        sampled_output = random_sample_with_temperature(output, temperature)\n",
    "        return sampled_output\n",
    "\n",
    "    def translate(word, strategy='greedy', beam_width=3, temperature=1.0):\n",
    "        input_batch, output_batch = make_testbatch(word)\n",
    "        hidden = torch.zeros(2, 1, n_hidden) # cambiar a 2 capas\n",
    "    \n",
    "        if strategy == 'greedy':\n",
    "            predict = greedy_decode(input_batch, hidden, output_batch)\n",
    "        elif strategy == 'beam_search':\n",
    "            predict = beam_search_decode(input_batch, hidden, output_batch, beam_width)\n",
    "        elif strategy == 'temperature':\n",
    "            predict = decode_with_temperature(input_batch, hidden, output_batch, temperature)\n",
    "    \n",
    "        decoded = [char_arr[i] for i in predict.squeeze()]\n",
    "        if 'E' in decoded:\n",
    "            end = decoded.index('E')\n",
    "            translated = ''.join(decoded[:end])\n",
    "        else:\n",
    "            translated = ''.join(decoded)\n",
    "\n",
    "        return translated.replace('P', '')\n",
    "\n",
    "    words = ['man', 'mans', 'king', 'black', 'upp']\n",
    "    \n",
    "    print('Comparativa de decodificación:\\n')\n",
    "    \n",
    "    for word in words:\n",
    "        print(f'Palabra: {word}')\n",
    "        print(f'Greedy: {translate(word, strategy=\"greedy\")}')\n",
    "        print(f'Beam Search: {translate(word, strategy=\"beam_search\", beam_width=3)}')\n",
    "        print(f'Temperatura (T=1.0): {translate(word, strategy=\"temperature\", temperature=1.0)}')\n",
    "        print('---------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d8228",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 .Implementa un mecanismo de atención en el modelo Seq2Seq para mejorar la traducción.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Añade una capa de atención al modelo Seq2Seq.\n",
    "- Modifica el método forward para incorporar la atención.\n",
    "- Entrena el modelo y compara el rendimiento con el modelo original.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Utiliza la clase nn.Linear para calcular los pesos de atención.\n",
    "- Multiplica los pesos de atención con los estados ocultos del codificador para obtener el contexto.\n",
    "- Concatena el contexto con la entrada del decodificador en cada paso de tiempo.\n",
    "\n",
    "2 . Compara el rendimiento de las estrategias de decodificación Greedy, Beam Search y Temperatura en diferentes configuraciones de entrenamiento.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Entrena el modelo con diferentes tamaños de conjunto de datos y configuraciones de hiperparámetros (por ejemplo, diferentes tamaños de hidden_size y num_layers).\n",
    "- Aplica las tres estrategias de decodificación a cada modelo entrenado.\n",
    "- Evalúa y compara la calidad de las traducciones utilizando métricas como la precisión y el BLEU score.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa conjuntos de datos grandes y pequeños para ver cómo cambia el rendimiento.\n",
    "- Experimenta con diferentes beam_width y temperatures.\n",
    "\n",
    "3 . Implementa una variante de Beam Search que penalice secuencias más largas para evitar repeticiones innecesarias.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Modifica la función beam_search_decode para incluir una penalización de longitud.\n",
    "- Ajusta el cálculo de las puntuaciones de las secuencias para penalizar las secuencias más largas.\n",
    "- Compara los resultados con el Beam Search estándar.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Multiplica la puntuación de cada secuencia por una función de penalización basada en su longitud.\n",
    "- Puedes usar una función de penalización lineal o exponencial.\n",
    "\n",
    "4 . Implementa una estrategia de decodificación por temperatura que ajuste dinámicamente la temperatura en cada paso de tiempo.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Modifica la función decode_with_temperature para ajustar la temperatura en cada paso de tiempo.\n",
    "- Implementa una función que disminuya la temperatura a medida que avanza la decodificación, incentivando exploración al principio y explotación al final.\n",
    "- Evalúa el impacto de la temperatura dinámica en la calidad de las traducciones.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa una función de decremento lineal o exponencial para la temperatura.\n",
    "- Compara los resultados con una temperatura fija.\n",
    "\n",
    "5 . Analiza la complejidad computacional y el rendimiento de las diferentes estrategias de decodificación.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Mide el tiempo de ejecución de las estrategias Greedy, Beam Search y Temperatura para diferentes tamaños de vocabulario y longitudes de secuencia.\n",
    "- Analiza cómo cambia la complejidad computacional con respecto a beam_width y temperature.\n",
    "- Discute los trade-offs entre la calidad de la traducción y el tiempo de ejecución.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa la biblioteca time para medir el tiempo de ejecución.\n",
    "- Realiza pruebas con diferentes configuraciones y grafica los resultados.\n",
    "\n",
    "\n",
    "6 .Mejora la generalización del modelo incorporando técnicas de regularización y dropout.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Añade capas de dropout adicionales al modelo Seq2Seq.\n",
    "- Implementa técnicas de regularización como L2.\n",
    "- Entrena el modelo con estas técnicas y compara el rendimiento con el modelo original.\n",
    "\n",
    "Pistas:\n",
    "\n",
    "- Usa nn.Dropout en las capas RNN y totalmente conectadas.\n",
    "- Ajusta los hiperparámetros de regularización y dropout para encontrar la configuración óptima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7cb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
