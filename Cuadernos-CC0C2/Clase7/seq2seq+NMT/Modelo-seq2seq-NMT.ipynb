{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7361330",
   "metadata": {},
   "source": [
    "## ¿Qué es un modelo Seq2Seq?\n",
    "\n",
    "Seq2Seq (Sequence to Sequence) es un método de traducción automática basado en una arquitectura codificador-decodificador (encoder-decoder). Mapea una secuencia de entrada a una secuencia de salida. La arquitectura consiste en dos redes neuronales recurrentes (RNNs): el codificador (encoder) y el decodificador (decoder). El codificador procesa la secuencia de entrada y genera un estado oculto (hidden state), que captura la información de la entrada. Este estado oculto se pasa al decodificador, que utiliza esta información para generar la secuencia de salida, prediciendo un token a la vez. A menudo, se utiliza un mecanismo de atención (attention) para permitir que el decodificador se centre en diferentes partes de la secuencia de entrada en cada paso de tiempo, mejorando así la precisión de las predicciones. Se usan tokens especiales como SOS (start of sequence) y EOS (end of sequence) para indicar el inicio y el final de las secuencias.\n",
    "\n",
    "**Componentes clave:**\n",
    "\n",
    "- Codificador (Encoder): Procesa la secuencia de entrada y genera un estado oculto que resume la información de la secuencia.\n",
    "- Decodificador (Decoder): Utiliza el estado oculto del codificador para generar la secuencia de salida, prediciendo un token a la vez.\n",
    "- Mecanismo de atención (Attention): Permite que el decodificador se enfoque en diferentes partes de la secuencia de entrada durante la generación de la secuencia de salida.\n",
    "- Tokens especiales: Incluyen SOS (start of sequence) y EOS (end of sequence) para indicar el inicio y el final de las secuencias.\n",
    "\n",
    "Puedes revisar ![](https://www.guru99.com/images/1/111318_0848_seq2seqSequ1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc5fdb",
   "metadata": {},
   "source": [
    "El gráfico anterior muestra una arquitectura típica de un modelo Seq2Seq (Sequence to Sequence) con un codificador (encoder) y un decodificador (decoder) para tareas de traducción automática. Vamos a desglosar cada componente del gráfico y su funcionamiento:\n",
    "\n",
    "* Embed (Embedding Layer): La capa de embedding toma las palabras de la secuencia de entrada (\"He\", \"loved\", \"to\", \"eat\", \".\") y las convierte en vectores de alta dimensión. Esta representación densa y continua facilita que el modelo capture las relaciones semánticas entre las palabras.\n",
    "\n",
    "* Encoder: El codificador (encoder) es una red neuronal recurrente (RNN) que procesa la secuencia de entrada. En cada paso de tiempo, la RNN toma una palabra (representada como un vector de embedding) y actualiza su estado oculto. La secuencia de estados ocultos generada por la RNN del encoder contiene la información codificada de la secuencia de entrada completa.\n",
    "\n",
    "* S (estado oculto final del codificador): El último estado oculto del codificador (S) actúa como un resumen de toda la secuencia de entrada y se pasa al decodificador como el estado inicial. Este vector contiene la información necesaria para generar la secuencia de salida correspondiente.\n",
    "\n",
    "* Decoder: El decodificador (decoder) es otra RNN que genera la secuencia de salida (\"Er\", \"liebte\", \"zu\", \"essen\", \".\"). En cada paso de tiempo, el decodificador toma el estado oculto anterior y el token generado previamente (o el token de inicio en el primer paso) para predecir el siguiente token en la secuencia de salida. El decodificador continúa este proceso hasta que se genera un token de fin de secuencia (no mostrado explícitamente en el gráfico).\n",
    "\n",
    "* NULL token: El gráfico muestra un token \"NULL\" como entrada inicial al decodificador, que a menudo es un token de inicio especial (SOS - Start of Sequence) para indicar el comienzo de la secuencia de salida.\n",
    "\n",
    "* Softmax layer: La capa softmax se aplica a la salida del decodificador en cada paso de tiempo para generar una distribución de probabilidad sobre el vocabulario de salida. La palabra con la mayor probabilidad se selecciona como la predicción del siguiente token en la secuencia de salida.\n",
    "\n",
    "* Secuencia de salida: El decodificador produce la secuencia de salida palabra por palabra, generando tokens como \"Er\", \"liebte\", \"zu\", \"essen\", \".\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd77b58",
   "metadata": {},
   "source": [
    "### 1. Cargando los datos\n",
    "\n",
    "See utilizará un conjunto de datos desde [ Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/) de inglés a indonesio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cdd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8acf8b",
   "metadata": {},
   "source": [
    "### 2.Preparación de datos\n",
    "\n",
    "No se puedes usar el conjunto de datos directamente. Se necesita dividir las oraciones en palabras y convertirlas en vectores one-hot. Cada palabra se indexará de forma única en la clase `Lang` para hacer un diccionario. `Lang Class` almacenará cada oración y realizará una división palabra por palabra con `addSentence`. Luego se crea un diccionario indexando cada palabra desconocida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "# Inicializamos la clase Lang\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        #inicializamos contenedores para almacenar las palabras y el correspondiente indice\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word ={0:\"SOS\",1: \"EOS\"}\n",
    "        self.n_words = 2 # Cuenta SOS y EOS\n",
    "    \n",
    "    #Dividimos una oración en palabras y lo agregamos al contenedor\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    # Si la palabra no está en el contenedor, la palabra debe ser agregada\n",
    "    # sino se actualiza el contador de palabras\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d594d",
   "metadata": {},
   "source": [
    "La clase `Lang` es una clase que nos ayudará a hacer un diccionario. Para cada lenguaje, cada oración se dividirá en palabras y luego se agregará al contenedor. Cada contenedor almacenará las palabras en el índice apropiado, contará la palabra y agregará el índice de la palabra para que podamos usarlo para encontrar el índice de una palabra o encontrar una palabra de su índice.\n",
    "\n",
    "Por cada oración que se tenga,\n",
    "\n",
    "* Se normalizará a minúsculas,\n",
    "* Se eliminará todo lo que no sea de carácter\n",
    "* Se convertirá a ASCII desde Unicode\n",
    "* Se dividirá las oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza cada oración\n",
    "\n",
    "def normalize_sentence(df, lang):\n",
    "    sentence = df[lang].str.lower()\n",
    "    sentence = sentence.str.replace('[^A-Za-z\\s]+', '', regex=True)\n",
    "    sentence = sentence.str.normalize('NFD')\n",
    "    sentence = sentence.str.encode('ascii', errors ='ignore').str.decode('utf-8')\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def read_sentence(df, lang1, lang2):\n",
    "    sentence1 = normalize_sentence(df, lang1)\n",
    "    sentence2 = normalize_sentence(df, lang2)\n",
    "    return sentence1, sentence2\n",
    "\n",
    "def read_file(loc, lang1, lang2):\n",
    "    df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2])\n",
    "    return df\n",
    "\n",
    "def process_data(lang1, lang2):\n",
    "    df = read_file('%s-%s.txt' %(lang1, lang2), lang1, lang2)\n",
    "    print(\"Leer %s oraciones pares\" %len(df))\n",
    "    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
    "    \n",
    "    source = Lang()\n",
    "    target = Lang()\n",
    "    pairs = []\n",
    "    for i in range(len(df)):\n",
    "        if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n",
    "            full = [sentence1[i], sentence2[i]]\n",
    "            source.addSentence(sentence1[i])\n",
    "            target.addSentence(sentence2[i])\n",
    "            pairs.append(full)\n",
    "            \n",
    "    return source, target, pairs\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b30c0a",
   "metadata": {},
   "source": [
    "Otra función útil que utilizará es la conversión de pares en Tensor. Esto es muy importante por que la red solo lee datos de tipo tensorial. También es importante porque esta es la parte que en cada extremo de la oración habrá un token para indicar a la red que la entrada ha finalizado. Para cada palabra en la oración, obtendrá el índice de la palabra apropiada en el diccionario y agregará un token al final de la oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f43798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentences(lang, sentences):\n",
    "    return [lang.word2index[word] for word in sentences.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentences(lang,sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device =device).view(-1,1)\n",
    "\n",
    "def tensorsFromPair(input_lang, output_lang, pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa616",
   "metadata": {},
   "source": [
    "- Conversión a Índices: La función indexesFromSentences(lang, sentence) convierte una oración en una lista de índices basados en el diccionario (word2index) del objeto Lang. Cada palabra en la oración se convierte a su índice correspondiente.\n",
    "- Conversión a Tensor: La función tensorFromSentence(lang, sentence) toma la lista de índices generada por indexesFromSentences y añade un token de fin de secuencia (EOS_token) al final de la lista. Luego, convierte esta lista en un tensor de PyTorch, asegurándose de que cada índice esté en una nueva fila, formando un vector columna.\n",
    "- Pares de Tensores: La función tensorsFromPair(input_lang, output_lang, pair) toma un par de oraciones (oración de entrada y oración de salida) y las convierte en un par de tensores utilizando tensorFromSentence para cada oración. Devuelve una tupla de tensores listos para ser procesados por la red.\n",
    "\n",
    "(EOS): Este token es crucial para indicar a la red cuándo termina una oración. Sin este token, la red no tendría una señal clara de cuándo dejar de procesar la entrada o la salida, lo que podría llevar a errores en la predicción y la generación de secuencias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0751750",
   "metadata": {},
   "source": [
    "### Modelo Seq2seq \n",
    "\n",
    "![](https://www.guru99.com/images/1/111318_0848_seq2seqSequ2.png)\n",
    "\n",
    "1 .Codificador (encoder):\n",
    "\n",
    "- Las primeras tres cajas representan el codificador.\n",
    "- Cada caja simboliza una celda de una red neuronal recurrente (RNN), como LSTM o GRU.\n",
    "- Las letras A, B y C representan los tokens de la secuencia de entrada.\n",
    "\n",
    "2 . Estado de contexto (context state):\n",
    "\n",
    "- La caja al final de la secuencia del codificador (después de C) almacena el estado oculto final del codificador.\n",
    "- Este estado contiene la información resumida de toda la secuencia de entrada y se pasa al decodificador.\n",
    "\n",
    "3 . Decodificador (decoder):\n",
    "\n",
    "- Las últimas cinco cajas representan el decodificador.\n",
    "- Similar al codificador, cada caja es una celda de RNN.\n",
    "- El decodificador toma el estado oculto final del codificador como su estado inicial.\n",
    "\n",
    "4 .Tokens de entrada y salida:\n",
    "\n",
    "- <go>: Token especial que indica el comienzo de la secuencia de salida.\n",
    "- W, X, Y, Z: Tokens generados por el decodificador durante el proceso de decodificación.\n",
    "- <eos>: Token especial que indica el fin de la secuencia de salida.\n",
    "\n",
    "**Flujo del proceso**\n",
    "\n",
    "1 . Codificación de la secuencia de entrada:\n",
    "\n",
    "- Entrada A: El primer token de la secuencia de entrada (A) se pasa al codificador.\n",
    "- Entrada B: El segundo token de la secuencia de entrada (B) se pasa a la siguiente celda del codificador.\n",
    "- Entrada C: El tercer token de la secuencia de entrada (C) se pasa a la última celda del codificador.\n",
    "- Estado de contexto: El estado oculto final del codificador se obtiene después de procesar C. Este estado contiene la información acumulada de A, B y C.\n",
    "\n",
    "2 . Inicio de la decodificación:\n",
    "\n",
    "- Token <go>: El decodificador toma el estado oculto final del codificador y comienza el proceso de generación de la secuencia de salida. El token <go> se usa para iniciar la decodificación.\n",
    "\n",
    "3 . Generación de la secuencia de salida:\n",
    "\n",
    "- Salida W: El decodificador genera el primer token de la secuencia de salida (W) basado en el estado oculto inicial.\n",
    "- Salida X: El decodificador toma W y su estado oculto actualizado para generar el siguiente token (X).\n",
    "- Salida Y: El decodificador toma X y su estado oculto actualizado para generar el siguiente token (Y).\n",
    "- Salida Z: El decodificador toma Y y su estado oculto actualizado para generar el siguiente token (Z).\n",
    "Token <eos>: Finalmente, el decodificador genera el token <eos> para indicar el fin de la secuencia de salida.\n",
    "\n",
    "**Detalles adicionales**\n",
    "\n",
    "- RNN Celdas: Cada caja en el codificador y el decodificador puede ser una celda LSTM o GRU que procesa secuencias de manera recurrente.\n",
    "- Estado Oculto: El estado oculto se actualiza en cada paso de tiempo y contiene la información relevante de la secuencia hasta ese punto.\n",
    "- Atención: Aunque no se muestra en esta figura, en arquitecturas más avanzadas, se puede agregar un mecanismo de atención para permitir que el decodificador enfoque diferentes partes de la secuencia de entrada en cada paso de tiempo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f233dc",
   "metadata": {},
   "source": [
    "![](https://www.guru99.com/images/1/111318_0848_seq2seqSequ3.png)\n",
    "\n",
    "El modelo Seq2Seq es un tipo de modelo de aprendizaje profundo que utiliza un codificador (encoder) y un decodificador (decoder) para transformar una secuencia de entrada en una secuencia de salida. La figura muestra el flujo básico del modelo:\n",
    "\n",
    "- El codificador toma la secuencia de entrada palabra por palabra y la convierte en una representación interna (estado oculto). Cada palabra se convierte en un índice correspondiente en el vocabulario.\n",
    "- Este estado oculto resume la información de toda la secuencia de entrada y se pasa al decodificador.\n",
    "- El estado oculto final del codificador contiene la información codificada de la secuencia de entrada. Este estado se transfiere al decodificador y actúa como el estado inicial del decodificador.\n",
    "- El decodificador toma el estado oculto del codificador y genera la secuencia de salida, decodificando la información recibida.\n",
    "- En cada paso, el decodificador puede utilizar el token generado en el paso anterior como entrada para el siguiente paso, un proceso conocido como \"teacher forcing\" si se usa la salida correcta en lugar de la generada.\n",
    "- A cada secuencia se le asigna un token especial al final para marcar el fin de la secuencia (token <eos>).\n",
    "- Un token especial al final de la secuencia de entrada indica el fin de la entrada, y un token especial al final de la salida. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c0ac57",
   "metadata": {},
   "source": [
    "![](https://www.guru99.com/images/1/111318_0848_seq2seqSequ4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c764df",
   "metadata": {},
   "source": [
    "El codificador procesa la oración de entrada palabra por palabra en secuencia. Al final de la secuencia de entrada, el codificador produce un estado oculto que resume toda la información de la oración. El codificador consiste en una capa de embedding y capas de GRU. La capa de embedding es una tabla de búsqueda que almacena los embeddings de las palabras de entrada en un diccionario de palabras de tamaño fijo. Estos embeddings se pasan a una capa de GRU. La capa de GRU es una unidad recurrente que calcula las representaciones ocultas de las palabras en secuencia, actualizando su estado oculto a medida que procesa cada palabra.\n",
    "\n",
    "El flujo es:\n",
    "\n",
    "1. Entrada secuencial: La secuencia de entrada, \"how are you ?\", se procesa palabra por palabra.\n",
    "\n",
    "2. Capa embedding:\n",
    "    - Función: Convierte cada palabra en un vector denso de dimensiones fijas utilizando una tabla de búsqueda (embedding matrix). Esto transforma las palabras en índices (word2id) que luego se convierten en embeddings.\n",
    "    - Proceso: Cada palabra se convierte a su índice correspondiente y luego a su vector de embedding.\n",
    "3. Capa GRU (Gated Recurrent Unit):\n",
    "    - Función: Procesa la secuencia de embeddings. Es un tipo de RNN (Red Neuronal Recurrente) que maneja dependencias a largo plazo en la secuencia.\n",
    "    - Proceso: Toma cada embedding y calcula el estado oculto. La GRU actualiza sus estados utilizando las puertas de actualización y reinicio para controlar el flujo de información.\n",
    "\n",
    "4. Estado final: El estado oculto final del codificador resume la información de toda la secuencia de entrada y se pasa al decodificador.\n",
    "\n",
    "Token Especial: Token de Inicio (<GO>): Este token especial indica el comienzo de la secuencia de salida y es la primera entrada para el decodificador.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0f566",
   "metadata": {},
   "source": [
    "El decodificador toma el estado oculto final del codificador y genera la secuencia de salida palabra por palabra. Intentará predecir la próxima palabra de salida y utilizará esta predicción como la siguiente entrada si es posible. El decodificador consiste en una capa de embedding, una capa de GRU y una capa lineal. La capa de embedding crea una tabla de búsqueda para convertir las palabras de salida en vectores de embedding. Estos embeddings se pasan a una capa de GRU, que calcula los estados ocultos para predecir las siguientes palabras en la secuencia. Finalmente, una capa lineal toma los estados ocultos y los transforma en probabilidades sobre el vocabulario de salida, utilizando una función de activación como softmax para determinar la palabra más probable.\n",
    "\n",
    "\n",
    "El flujo es:\n",
    "\n",
    "\n",
    "5. Capa Embedding:\n",
    "    - Función: Similar a la capa de embedding del codificador, convierte los índices de las palabras en vectores de embedding. Esta vez, para las palabras de la secuencia de salida.\n",
    "    - Proceso: Convierte el token `<GO>` y las palabras previas generadas en vectores de embedding.\n",
    "\n",
    "6.  Capa GRU:\n",
    "    - Función: Procesa las secuencias de embeddings de salida, utilizando el estado oculto final del codificador como su estado inicial.\n",
    "    - Proceso: Genera un nuevo estado oculto y predice el siguiente token en la secuencia de salida.\n",
    "\n",
    "7. Capa lineal:\n",
    "    - Función: Transforma el estado oculto generado por la GRU en una distribución de probabilidad sobre el vocabulario de salida.\n",
    "    - Proceso: Utiliza una función de activación (como softmax) para determinar la palabra más probable como salida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab95227",
   "metadata": {},
   "source": [
    "El  código define un modelo Seq2Seq utilizando PyTorch, compuesto por un codificador (Encoder), un decodificador (Decoder) y una clase Seq2Seq que los une para realizar tareas de traducción de secuencias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers =self.num_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src).view(-1, 1, self.embbed_dim)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden): # input es (1, batch_size)\n",
    "        input = input.view(1, -1)\n",
    "        embedded = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded, hidden)       \n",
    "        prediction = self.softmax(self.out(output[0]))\n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        \n",
    "      # inicionalimos el encoder y decoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "     \n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        input_length = source.size(0) \n",
    "        batch_size = target.shape[1] \n",
    "        target_length = target.shape[0]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "      \n",
    "    #inicializamos una variable para las salidas predecidas\n",
    "        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
    "\n",
    "    #codificamos cada palabra en una oracion\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.encoder(source[i])\n",
    "\n",
    "    # utilizar la capa oculta del codificador como el decodificador oculto\n",
    "        decoder_hidden = encoder_hidden.to(device)\n",
    "      \n",
    "    # agregamos un token antes de la primera palabra predicha\n",
    "        decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
    "\n",
    "    # Se usa para obtener el valor de K en una lista \n",
    "        for t in range(target_length):   \n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            input = (target[t] if teacher_force else topi)\n",
    "            if(teacher_force == False and input.item() == EOS_token):\n",
    "                break\n",
    "                \n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c2155",
   "metadata": {},
   "source": [
    "El codificador toma una secuencia de entrada (palabras) y las convierte en una representación interna (estado oculto).\n",
    "\n",
    "Funcionamiento:\n",
    "\n",
    "- Se configura las dimensiones de entrada, ocultas y de embedding, y el número de capas GRU.\n",
    "- La capa de embedding convierte las palabras de entrada en vectores densos (embeddings).\n",
    "- La capa GRU procesa los embeddings secuencialmente y produce un estado oculto que contiene la información de la secuencia de entrada.\n",
    "- El método forward: Toma una secuencia de entrada, la convierte en embeddings, la procesa con la GRU y devuelve las salidas y el estado oculto final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8448174",
   "metadata": {},
   "source": [
    "El decodificador toma el estado oculto del codificador y genera una secuencia de salida palabra por palabra.\n",
    "\n",
    "Funcionamiento:\n",
    "\n",
    "-  Se configura las dimensiones de salida, ocultas y de embedding, y el número de capas GRU.\n",
    "-  La capa de embedding convierte las palabras de salida en vectores densos (embeddings).\n",
    "- La capa GRU procesa los embeddings secuencialmente, utilizando el estado oculto del codificador como su estado inicial.\n",
    "- La capa lineal transforma el estado oculto generado por la GRU en una distribución de probabilidad sobre el vocabulario de salida.\n",
    "- El método forward toma una palabra de entrada y el estado oculto, los convierte en embeddings, los procesa con la GRU y predice la siguiente palabra en la secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5719a717",
   "metadata": {},
   "source": [
    "La clase seq2seq coordina el funcionamiento del codificador y el decodificador para transformar una secuencia de entrada en una secuencia de salida.\n",
    "\n",
    "Funcionamiento:\n",
    "\n",
    "- Toma el codificador y el decodificador, y establece el dispositivo para las operaciones tensoriales.\n",
    " - El método forward procesa cada palabra de la secuencia de entrada utilizando el codificador para generar un estado oculto final.\n",
    "- La inicialización del decodificador configura el decodificador para empezar con el estado oculto final del codificador y un token de inicio (SOS_token).\n",
    "-  En la decodificación se genera la secuencia de salida palabra por palabra. Se utiliza \"teacher forcing\" con una probabilidad (teacher_forcing_ratio), lo que significa que a veces usará la palabra real de la secuencia de salida como entrada para el siguiente paso, y a veces usará la palabra predicha.\n",
    "\n",
    "- En la salida se devuelve las predicciones de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad067df",
   "metadata": {},
   "source": [
    "### 3. Entrenamiento del modelo\n",
    "\n",
    "El proceso de entrenamiento se inicia al convertir cada par de oraciones en tensores utilizando los índices del vocabulario de la clase Lang. El modelo utilizará el optimizador SGD (Stochastic Gradient Descent) y la función de pérdida NLLLoss (Negative Log Likelihood Loss) para calcular las pérdidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def clacModel(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
    "    model_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    loss = 0\n",
    "    epoch_loss =0\n",
    "    \n",
    "    output = model(input_tensor, target_tensor)\n",
    "    \n",
    "    num_iter = output.size(0)\n",
    "    print(num_iter)\n",
    "    \n",
    "    # Calculamos la perdida desde una sentencia predicha con un resultado esperado\n",
    "    for ot in range(num_iter):\n",
    "        loss += criterion(output[ot], target_tensor[ot])\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        model_optimizer.step()\n",
    "        epoch_loss = loss.item()/num_iter\n",
    "        \n",
    "        return epoch_loss\n",
    "        \n",
    "def train_Model(model, source, target, pairs, num_iteration = 20000):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr =0.01)\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss_iterations = 0\n",
    "        \n",
    "    training_pairs = [tensorsFromPair(source, target, random.choice(pairs)) for i in range(num_iteration)]\n",
    "        \n",
    "    for iter in range(1, num_iteration +1):\n",
    "        training_pair = training_pairs[iter -1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "            \n",
    "        loss =clacModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
    "            \n",
    "        total_loss_iterations += loss\n",
    "        \n",
    "        if iter % 5000 == 0:\n",
    "            avarage_loss = total_loss_iterations / 5000\n",
    "            total_loss_iterations = 0\n",
    "            print('%d %.4f' % (iter, avarage_loss))\n",
    "            \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bec98",
   "metadata": {},
   "source": [
    "### 4. Prueba del modelo\n",
    "\n",
    "El proceso de evaluación consiste en verificar el resultado del modelo. Cada par de oraciones se incorporará al modelo y generará las palabras pronosticadas. Después de eso, buscará el valor más alto en cada salida para encontrar el índice correcto y al final, comparará para ver este modelo de predicción con la oración verdadera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c96071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, input_lang, output_lang, sentences, max_length = MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
    "        output_tensor = tensorFromSentence(output_lang, sentences[1])\n",
    "        \n",
    "        decoded_words = []\n",
    "        \n",
    "        output = model(input_tensor, output_tensor)\n",
    "        \n",
    "        for ot in range(output.size(0)):\n",
    "            topv, topi = output[ot].topk(1)\n",
    "            \n",
    "            if topi[0].item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi[0].item()])\n",
    "                \n",
    "    return decoded_words\n",
    "\n",
    "def evaluateRandomly(model, source, target, pairs, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('fuente {}'.format(pair[0]))\n",
    "        print('objetivo {}'.format(pair[1]))\n",
    "        output_words = evaluate(model, source, target, pair)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('predicho {}'.format(output_sentence))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53690fc7",
   "metadata": {},
   "source": [
    "### Usando BLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    smooth = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference.split()], candidate.split(), smoothing_function=smooth)\n",
    "\n",
    "def evaluate_bleu_score(model, source, target, pairs, n=10):\n",
    "    bleu_scores = []\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        fuente = pair[0]\n",
    "        objetivo = pair[1]\n",
    "        output_words = evaluate(model, source, target, pair)\n",
    "        predicho = ' '.join(output_words)\n",
    "        bleu_score = calculate_bleu(objetivo, predicho)\n",
    "        bleu_scores.append(bleu_score)\n",
    "        print(f'Fuente: {fuente}')\n",
    "        print(f'Objetivo: {objetivo}')\n",
    "        print(f'Predicho: {predicho}')\n",
    "        print(f'Puntuacion BLUE: {bleu_score:.4f}')\n",
    "        print('')\n",
    "    return bleu_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98249b",
   "metadata": {},
   "source": [
    "Ahora, comencemos el entrenamiento, con un número de iteraciones de 75000, el número capas RNN de 1 y con el tamaño oculto de 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Definimos los lenguajes\n",
    "lang1 = 'eng'\n",
    "lang2 = 'ind'\n",
    "\n",
    "# Procesamos los datos\n",
    "source, target, pairs = process_data(lang1, lang2)\n",
    "\n",
    "# Seleccionamos un par de oraciones aleatorio\n",
    "randomize = random.choice(pairs)\n",
    "print('Oración aleatoria {}'.format(randomize))\n",
    "\n",
    "# Imprimimos el número de palabras\n",
    "input_size = source.n_words\n",
    "output_size = target.n_words\n",
    "\n",
    "print('Entrada: {} Salida: {}'.format(input_size, output_size))\n",
    "\n",
    "# Definimos los parámetros del modelo\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_iteration = 100\n",
    "\n",
    "# Creamos el modelo encoder-decoder\n",
    "encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
    "decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model = train_Model(model, source, target, pairs, num_iteration)\n",
    "\n",
    "# Evaluamos aleatoriamente y almacenamos resultados\n",
    "def evaluateAndStoreResults(model, source, target, pairs, n=10):\n",
    "    data = {\n",
    "        'Fuente': [],\n",
    "        'Objetivo': [],\n",
    "        'Predicho': [],\n",
    "        'Longitud de la Predicción': []\n",
    "    }\n",
    "    \n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        fuente = pair[0]\n",
    "        objetivo = pair[1]\n",
    "        \n",
    "        output_words = evaluate(model, source, target, pair)\n",
    "        predicho = ' '.join(output_words)\n",
    "        \n",
    "        data['Fuente'].append(fuente)\n",
    "        data['Objetivo'].append(objetivo)\n",
    "        data['Predicho'].append(predicho)\n",
    "        data['Longitud de la Predicción'].append(len(output_words))\n",
    "        \n",
    "        # Solo imprimir las oraciones, no las longitudes\n",
    "        print('Fuente: {}'.format(fuente))\n",
    "        print('Objetivo: {}'.format(objetivo))\n",
    "        print('Predicho: {}'.format(predicho))\n",
    "        print('')\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generamos y mostramos los resultados\n",
    "results_df = evaluateAndStoreResults(model, source, target, pairs, n=10)\n",
    "\n",
    "# Guardamos los resultados en un archivo CSV para análisis posterior\n",
    "results_df.to_csv('seq2seq_results.csv', index=False)\n",
    "print(\"Resultados guardados en 'seq2seq_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd7111",
   "metadata": {},
   "source": [
    "Los números que ves en la salida del modelo representan la longitud de las secuencias generadas durante el proceso de decodificación. Cada número corresponde a la longitud de una oración predicha en términos de tokens generados.\n",
    "\n",
    "Los resultados de traducción incluyen la oración de entrada (fuente), la oración objetivo (objetivo) y la oración predicha (predicho). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34361046",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(model, source, target, pairs)\n",
    "\n",
    "# Calcular y mostrar las puntuaciones BLEU\n",
    "bleu_scores = evaluate_bleu_score(model, source, target, pairs, n=10)\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f'Puntuacion BLEU promedio: {average_bleu:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de880f",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "- Experimenta con diferentes tamaños para los embeddings y las capas ocultas de tu modelo. Por ejemplo, puedes probar con un tamaño de embedding de 512 y una capa oculta de 1024.\n",
    "\n",
    "- Añade más capas puede ayudar al modelo a capturar características más complejas del lenguaje. Prueba con 2 o 3 capas en lugar de solo una.\n",
    "\n",
    "- Si es posible, utiliza un dataset más grande para entrenar tu modelo. Más datos pueden ayudar a que el modelo aprenda mejor las características del lenguaje.\n",
    "\n",
    "- Asegúrate de que el preprocesamiento de tus datos sea adecuado. Considera la posibilidad de eliminar o reducir el ruido en las oraciones, normalizar contracciones y expandir abreviaturas.\n",
    "\n",
    "- Aumenta el número de iteraciones de entrenamiento. Es posible que tu modelo aún no haya convergido adecuadamente con solo 100 iteraciones.\n",
    "\n",
    "- Ajusta el ratio de teacher forcing. Un valor demasiado alto puede hacer que el modelo dependa en exceso del input verdadero en lugar de aprender a predecir correctamente. Experimenta con diferentes valores.\n",
    "\n",
    "Después de implementar las mejoras, evalúa nuevamente el modelo y compara las puntuaciones BLEU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24043d35",
   "metadata": {},
   "source": [
    "1 . Beam Search es una técnica de búsqueda que mantiene las k mejores secuencias en cada paso del decodificador. Esto puede mejorar la calidad de las traducciones. Implementa Beam Search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1089f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(decoder, decoder_hidden, encoder_outputs, beam_width=3, max_length=MAX_LENGTH):\n",
    "    # Inicio del token\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # Iniciar con un solo paso de predicción\n",
    "    sequences = [[list(), 1.0]]\n",
    "    completed_sequences = []\n",
    "\n",
    "    # Paso de beam search\n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            if seq and seq[-1] == EOS_token:\n",
    "                completed_sequences.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topk = decoder_output.topk(beam_width)\n",
    "\n",
    "            for i in range(beam_width):\n",
    "                candidate = [seq + [topk[1][0][i].item()], score * -topk[0][0][i].item()]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        sequences = ordered[:beam_width]\n",
    "\n",
    "    completed_sequences.extend(sequences)\n",
    "    ordered = sorted(completed_sequences, key=lambda tup: tup[1])\n",
    "\n",
    "    return ordered[0][0]\n",
    "\n",
    "# Modificar la clase Decoder para aceptar encoder_outputs\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers, attention):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(self.hidden_dim * 2, self.output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.view(1, -1)\n",
    "        embedded = F.relu(self.embedding(input))\n",
    "        gru_output, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "\n",
    "        gru_output = gru_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        output = self.softmax(self.out(torch.cat([gru_output, context], 1)))\n",
    "\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e40626",
   "metadata": {},
   "source": [
    " Modifica la función de evaluación para usar Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_beam_search(model, input_lang, output_lang, sentences, beam_width=3, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_outputs, encoder_hidden = model.encoder(input_tensor)\n",
    "\n",
    "        decoder_hidden = encoder_hidden.to(device)\n",
    "        beam_output = beam_search(model.decoder, decoder_hidden, encoder_outputs, beam_width, max_length)\n",
    "\n",
    "        decoded_words = [output_lang.index2word[token] for token in beam_output if token != EOS_token]\n",
    "\n",
    "    return decoded_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5726b5c",
   "metadata": {},
   "source": [
    "2 . Utiliza embeddings preentrenados, como GloVe, puede mejorar el rendimiento del modelo al proporcionar representaciones de palabras más ricas. Descarga e inicializa los embeddings preentrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.vocab as vocab\n",
    "\n",
    "glove = vocab.GloVe(name='6B', dim=100)\n",
    "\n",
    "def load_embeddings(vocab, glove):\n",
    "    matrix_len = len(vocab)\n",
    "    weights_matrix = np.zeros((matrix_len, 100))\n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(vocab):\n",
    "        try:\n",
    "            weights_matrix[i] = glove[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(100,))\n",
    "\n",
    "    return torch.tensor(weights_matrix).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27145c7",
   "metadata": {},
   "source": [
    "Modificar el modelo para usar los embeddings preentrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderWithPretrainedEmbeddings(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers, weights_matrix):\n",
    "        super(EncoderWithPretrainedEmbeddings, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
    "        self.embedding.load_state_dict({'weight': weights_matrix})\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src).view(-1, 1, self.embbed_dim)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "weights_matrix = load_embeddings(source.word2index, glove)\n",
    "\n",
    "encoder = EncoderWithPretrainedEmbeddings(input_size, hidden_size, embed_size, num_layers, weights_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc64a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722799c",
   "metadata": {},
   "source": [
    "3 . Agrega técnicas de regularización como Dropout puede ayudar a evitar el sobreajuste y mejorar el rendimiento general del modelo. Añade Dropout en el Encoder y Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9dd135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderWithDropout(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers, dropout=0.5):\n",
    "        super(EncoderWithDropout, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src).view(-1, 1, self.embbed_dim)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class DecoderWithDropout(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers, dropout=0.5):\n",
    "        super(DecoderWithDropout, self).__init__()\n",
    "        \n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden): # input es (1, batch_size)\n",
    "        input = input.view(1, -1)\n",
    "        embedded = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded, hidden)       \n",
    "        prediction = self.softmax(self.out(output[0]))\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6dbd7",
   "metadata": {},
   "source": [
    "4 . Experimenta con diferentes optimizadores y tasas de aprendizaje y después de realizar las modificaciones, vuelve a entrenar y evaluar el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
