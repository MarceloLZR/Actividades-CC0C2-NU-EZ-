{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8a4125",
   "metadata": {},
   "source": [
    "**Nota:** Este cuaderno acompaña a lo discutido en clases.\n",
    "\n",
    "### RNN simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46404b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Una celda RNN simple con una capa totalmente conectada para la salida.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de la celda RNN y la capa totalmente conectada.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.rnn_cell.weight_ih, nonlinearity='tanh')\n",
    "        nn.init.kaiming_uniform_(self.rnn_cell.weight_hh, nonlinearity='tanh')\n",
    "        nn.init.zeros_(self.rnn_cell.bias_ih)\n",
    "        nn.init.zeros_(self.rnn_cell.bias_hh)\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.fc.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Paso hacia adelante de la celda RNN.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Tensor de entrada en el paso de tiempo actual, forma (batch_size, input_size).\n",
    "            hidden (Tensor, opcional): Estado oculto del paso de tiempo anterior, forma (batch_size, hidden_size).\n",
    "                                       Si es None, se inicializa a ceros.\n",
    "        \n",
    "        Returns:\n",
    "            output (Tensor): Tensor de salida en el paso de tiempo actual, forma (batch_size, output_size).\n",
    "            hidden (Tensor): Estado oculto actualizado, forma (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(x.size(0), self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        hidden = self.rnn_cell(x, hidden)\n",
    "        output = self.fc(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros\n",
    "    input_size = 10    # Dimensión de entrada de ejemplo\n",
    "    hidden_size = 20   # Tamaño del estado oculto\n",
    "    output_size = 5    # Dimensión de salida de ejemplo\n",
    "    batch_size = 32    # Tamaño del lote de ejemplo\n",
    "    \n",
    "    # Configuración del dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Inicializar la celda RNN\n",
    "    rnn_cell = SimpleRNNCell(input_size, hidden_size, output_size).to(device)\n",
    "    \n",
    "    # Ejemplo de entrada y estado oculto\n",
    "    x_t = torch.randn(batch_size, input_size, device=device)       # Entrada en el paso de tiempo t\n",
    "    hidden_t = None  # Permitir que la celda RNN inicialice el estado oculto\n",
    "    \n",
    "    # Paso hacia adelante a través de la celda RNN\n",
    "    y_t, hidden_t = rnn_cell(x_t, hidden_t)\n",
    "    \n",
    "    print(\"Salida en el paso de tiempo t:\", y_t)\n",
    "    print(\"Estado oculto en el paso de tiempo t:\", hidden_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee13132",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "- Para entrenar este modelo, necesitarás definir una función de pérdida, un optimizador y un bucle de entrenamiento adecuado. Asegúrate de manejar correctamente la retropropagación a través del tiempo (BPTT) si trabajas con secuencias largas.\n",
    "\n",
    "- Si tu objetivo es construir una red RNN completa para tareas como clasificación de secuencias o generación de texto, podrías considerar usar módulos más avanzados como nn.RNN, nn.LSTM o nn.GRU, que están optimizados y ofrecen funcionalidades adicionales.\n",
    "\n",
    "- Para evitar el sobreajuste, podrías implementar técnicas de regularización como el dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee12f16",
   "metadata": {},
   "source": [
    "### RNN vista como una red neuronal feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592934a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FeedforwardRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Una Red Neuronal Recurrente (RNN) de tipo feedforward con una capa oculta.\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Dimensionalidad de la entrada.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto.\n",
    "        output_size (int): Dimensionalidad de la salida.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedforwardRNN, self).__init__()\n",
    "        # Definir matrices de pesos como capas lineales\n",
    "        self.W = nn.Linear(input_size, hidden_size)    # Peso para x_t\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)   # Peso para h_{t-1}\n",
    "        self.V = nn.Linear(hidden_size, output_size)   # Peso para h_t a y_t\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales usando inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        nn.init.zeros_(self.W.bias)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.U.weight)\n",
    "        nn.init.zeros_(self.U.bias)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.V.weight)\n",
    "        nn.init.zeros_(self.V.bias)\n",
    "\n",
    "    def forward(self, x_t, h_t_minus_1=None):\n",
    "        \"\"\"\n",
    "        Realiza un pase hacia adelante de la RNN.\n",
    "        \n",
    "        Args:\n",
    "            x_t (Tensor): Entrada en el paso de tiempo actual, forma (batch_size, input_size).\n",
    "            h_t_minus_1 (Tensor, opcional): Estado oculto del paso de tiempo anterior, forma (batch_size, hidden_size).\n",
    "                                             Si es None, se inicializa a ceros.\n",
    "        \n",
    "        Returns:\n",
    "            y_t (Tensor): Salida en el paso de tiempo actual, forma (batch_size, output_size).\n",
    "            h_t (Tensor): Estado oculto actualizado, forma (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        if h_t_minus_1 is None:\n",
    "            # Inicializar estado oculto a ceros si no se proporciona\n",
    "            h_t_minus_1 = torch.zeros(x_t.size(0), self.U.out_features, device=x_t.device, dtype=x_t.dtype)\n",
    "        \n",
    "        # Calcular el nuevo estado oculto\n",
    "        h_t = torch.tanh(self.W(x_t) + self.U(h_t_minus_1))\n",
    "        \n",
    "        # Calcular la salida\n",
    "        y_t = self.V(h_t)\n",
    "        \n",
    "        return y_t, h_t\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros\n",
    "    input_size = 10    # Dimensión de entrada de ejemplo\n",
    "    hidden_size = 20   # Tamaño del estado oculto\n",
    "    output_size = 5    # Dimensión de salida de ejemplo\n",
    "    batch_size = 32    # Tamaño del lote de ejemplo\n",
    "    \n",
    "    # Configuración del dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "    \n",
    "    # Inicializar la RNN\n",
    "    rnn = FeedforwardRNN(input_size, hidden_size, output_size).to(device)\n",
    "    \n",
    "    # Ejemplo de entrada y estado oculto\n",
    "    x_t = torch.randn(batch_size, input_size, device=device)         # Entrada en el paso de tiempo t\n",
    "    h_t_minus_1 = None  # Permitir que la RNN inicialice el estado oculto\n",
    "    \n",
    "    # Realizar un pase hacia adelante a través de la RNN\n",
    "    y_t, h_t = rnn(x_t, h_t_minus_1)\n",
    "    \n",
    "    print(\"Salida y_t:\", y_t)\n",
    "    print(\"Estado oculto h_t:\", h_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1556cc8",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "* Actualmente, la implementación maneja un solo paso de tiempo. Para trabajar con secuencias completas, necesitarás iterar sobre los pasos de tiempo y mantener el estado oculto a lo largo de la secuencia.\n",
    "Considera encapsular el procesamiento de secuencias dentro de la clase o manejarlo externamente en el bucle de entrenamiento.\n",
    "* Aunque estás implementando tu propia RNN, PyTorch ofrece módulos optimizados como nn.RNN, nn.LSTM y nn.GRU, que pueden ser más eficientes y ofrecer funcionalidades adicionales.\n",
    "Estos módulos también manejan automáticamente el procesamiento de secuencias y la gestión de estados ocultos.\n",
    "Regularización:\n",
    "* Para evitar el sobreajuste, puedes implementar técnicas de regularización como dropout. PyTorch proporciona capas de dropout que pueden integrarse fácilmente en tu modelo.\n",
    "Guardado y Carga del Modelo:\n",
    "* Para guardar el estado del modelo y reanudar el entrenamiento posteriormente, puedes utilizar torch.save y torch.load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b5468",
   "metadata": {},
   "source": [
    "### Desenrrollando una RNN en el tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UnrolledRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Una Red Neuronal Recurrente (RNN) desarrollada en el tiempo.\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Dimensionalidad de la entrada.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto.\n",
    "        output_size (int): Dimensionalidad de la salida.\n",
    "        activation (callable, opcional): Función de activación para el estado oculto. Por defecto es tanh.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation=torch.tanh):\n",
    "        super(UnrolledRNN, self).__init__()\n",
    "        # Definir matrices de pesos como capas lineales\n",
    "        self.W = nn.Linear(input_size, hidden_size)    # Peso para x_t\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)   # Peso para h_{t-1}\n",
    "        self.V = nn.Linear(hidden_size, output_size)   # Peso para h_t a y_t\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales usando inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        nn.init.zeros_(self.W.bias)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.U.weight)\n",
    "        nn.init.zeros_(self.U.bias)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.V.weight)\n",
    "        nn.init.zeros_(self.V.bias)\n",
    "\n",
    "    def forward(self, inputs, h_0=None):\n",
    "        \"\"\"\n",
    "        Realiza un pase hacia adelante de la RNN desarrollada en el tiempo.\n",
    "        \n",
    "        Args:\n",
    "            inputs (Tensor): Secuencia de entradas, forma (seq_length, batch_size, input_size).\n",
    "            h_0 (Tensor, opcional): Estado oculto inicial, forma (batch_size, hidden_size).\n",
    "                                     Si es None, se inicializa a ceros.\n",
    "        \n",
    "        Returns:\n",
    "            outputs (Tensor): Secuencia de salidas, forma (seq_length, batch_size, output_size).\n",
    "            h_n (Tensor): Estado oculto final, forma (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        seq_length, batch_size, _ = inputs.size()\n",
    "        \n",
    "        if h_0 is None:\n",
    "            # Inicializar estado oculto a ceros si no se proporciona\n",
    "            h_t = torch.zeros(batch_size, self.U.out_features, device=inputs.device, dtype=inputs.dtype)\n",
    "        else:\n",
    "            h_t = h_0\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        # Desenrollar la RNN a través del tiempo\n",
    "        for t in range(seq_length):\n",
    "            x_t = inputs[t]\n",
    "            # Calcular el nuevo estado oculto\n",
    "            h_t = self.activation(self.W(x_t) + self.U(h_t))\n",
    "            # Calcular la salida\n",
    "            y_t = self.V(h_t)\n",
    "            outputs.append(y_t.unsqueeze(0))  # Añadir una dimensión para el tiempo\n",
    "        \n",
    "        outputs = torch.cat(outputs, dim=0)  # Concatenar a lo largo del tiempo\n",
    "        return outputs, h_t\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros\n",
    "    input_size = 10      # Dimensión de entrada de ejemplo\n",
    "    hidden_size = 20     # Tamaño del estado oculto\n",
    "    output_size = 5      # Dimensión de salida de ejemplo\n",
    "    sequence_length = 3  # Longitud de la secuencia de entrada\n",
    "    batch_size = 4       # Tamaño del lote de ejemplo\n",
    "    \n",
    "    # Configuración del dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "    \n",
    "    # Inicializar la RNN\n",
    "    rnn = UnrolledRNN(input_size, hidden_size, output_size).to(device)\n",
    "    \n",
    "    # Ejemplo de entrada y estado oculto inicial\n",
    "    # Crear una secuencia de entradas: (seq_length, batch_size, input_size)\n",
    "    inputs = torch.randn(sequence_length, batch_size, input_size, device=device)\n",
    "    h_0 = torch.zeros(batch_size, hidden_size, device=device)  # Estado oculto inicial\n",
    "    \n",
    "    # Pase hacia adelante a través de la RNN\n",
    "    outputs, h_n = rnn(inputs, h_0)\n",
    "    \n",
    "    # Imprimir salidas para cada paso de tiempo\n",
    "    for t in range(sequence_length):\n",
    "        print(f\"Salida en el paso de tiempo {t+1}: {outputs[t]}\")\n",
    "    print(\"Estado oculto final:\", h_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008041c4",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "- Para entrenar este modelo, necesitarás definir una función de pérdida, un optimizador y un bucle de entrenamiento adecuado.  Si trabajas con secuencias más largas, considera optimizar el manejo de memoria y utilizar técnicas como truncated backpropagation through time (BPTT) para evitar problemas de memoria.\n",
    "- Para evitar el sobreajuste, puedes implementar técnicas de regularización como dropout. PyTorch proporciona capas de dropout que pueden integrarse fácilmente en tu modelo.\n",
    "- Para guardar el estado del modelo y reanudar el entrenamiento posteriormente, puedes utilizar torch.save y torch.load.\n",
    "- Aunque estás implementando tu propia RNN, PyTorch ofrece módulos optimizados como nn.RNN, nn.LSTM y nn.GRU, que pueden ser más eficientes y ofrecer funcionalidades adicionales.\n",
    "- Estos módulos también manejan automáticamente el procesamiento de secuencias y la gestión de estados ocultos.\n",
    "- Para una comprensión más profunda, puedes visualizar cómo se actualizan los estados ocultos y las salidas a lo largo de los pasos de tiempo utilizando herramientas de visualización como TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03c946",
   "metadata": {},
   "source": [
    "### Modelo de lenguaje FFN y modelo de lenguaje RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FFNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de Lenguaje basado en una Red Neuronal Feedforward (FFN).\n",
    "\n",
    "    Este modelo toma tres entradas consecutivas (x_{t-2}, x_{t-1}, x_t}),\n",
    "    las concatena y las procesa a través de una capa oculta para generar una\n",
    "    salida y un estado oculto.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Dimensionalidad de cada entrada x_t.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto.\n",
    "        output_size (int): Dimensionalidad de la salida y_t.\n",
    "        activation (callable, opcional): Función de activación para la capa oculta. Por defecto es tanh.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation=torch.tanh):\n",
    "        super(FFNLanguageModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        # Definir capas lineales\n",
    "        self.W = nn.Linear(input_size * 3, hidden_size)  # Para x_{t-2}, x_{t-1}, x_t\n",
    "        self.U = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales usando inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        nn.init.zeros_(self.W.bias)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.U.weight)\n",
    "        nn.init.zeros_(self.U.bias)\n",
    "\n",
    "    def forward(self, x_t_minus_2, x_t_minus_1, x_t):\n",
    "        \"\"\"\n",
    "        Realiza un pase hacia adelante del modelo FFN.\n",
    "\n",
    "        Args:\n",
    "            x_t_minus_2 (Tensor): Entrada en el tiempo t-2, forma (batch_size, input_size).\n",
    "            x_t_minus_1 (Tensor): Entrada en el tiempo t-1, forma (batch_size, input_size).\n",
    "            x_t (Tensor): Entrada en el tiempo t, forma (batch_size, input_size).\n",
    "\n",
    "        Returns:\n",
    "            y_t (Tensor): Salida en el tiempo t, forma (batch_size, output_size).\n",
    "            h_t (Tensor): Estado oculto en el tiempo t, forma (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        # Concatenar las entradas\n",
    "        x_concat = torch.cat((x_t_minus_2, x_t_minus_1, x_t), dim=1)\n",
    "        # Calcular el estado oculto\n",
    "        h_t = self.activation(self.W(x_concat))\n",
    "        # Calcular la salida\n",
    "        y_t = self.U(h_t)\n",
    "        return y_t, h_t\n",
    "\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de Lenguaje basado en una Red Neuronal Recurrente (RNN).\n",
    "\n",
    "    Este modelo toma una entrada actual x_t y el estado oculto anterior h_{t-1},\n",
    "    y genera una salida y_t y un nuevo estado oculto h_t.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Dimensionalidad de cada entrada x_t.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto.\n",
    "        output_size (int): Dimensionalidad de la salida y_t.\n",
    "        activation (callable, opcional): Función de activación para el estado oculto. Por defecto es tanh.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation=torch.tanh):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        # Definir capas lineales\n",
    "        self.W = nn.Linear(input_size, hidden_size)   # Para x_t\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)  # Para h_{t-1}\n",
    "        self.V = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales usando inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        nn.init.zeros_(self.W.bias)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.U.weight)\n",
    "        nn.init.zeros_(self.U.bias)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.V.weight)\n",
    "        nn.init.zeros_(self.V.bias)\n",
    "\n",
    "    def forward(self, x_t, h_t_minus_1=None):\n",
    "        \"\"\"\n",
    "        Realiza un pase hacia adelante del modelo RNN.\n",
    "\n",
    "        Args:\n",
    "            x_t (Tensor): Entrada en el tiempo t, forma (batch_size, input_size).\n",
    "            h_t_minus_1 (Tensor, opcional): Estado oculto anterior h_{t-1}, forma (batch_size, hidden_size).\n",
    "                                           Si es None, se inicializa a ceros.\n",
    "\n",
    "        Returns:\n",
    "            y_t (Tensor): Salida en el tiempo t, forma (batch_size, output_size).\n",
    "            h_t (Tensor): Nuevo estado oculto h_t, forma (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        if h_t_minus_1 is None:\n",
    "            # Inicializar el estado oculto a ceros si no se proporciona\n",
    "            h_t_minus_1 = torch.zeros(x_t.size(0), self.hidden_size, device=x_t.device, dtype=x_t.dtype)\n",
    "        \n",
    "        # Calcular el nuevo estado oculto\n",
    "        h_t = self.activation(self.W(x_t) + self.U(h_t_minus_1))\n",
    "        # Calcular la salida\n",
    "        y_t = self.V(h_t)\n",
    "        return y_t, h_t\n",
    "\n",
    "\n",
    "def initialize_models(input_size, hidden_size, output_size, device):\n",
    "    \"\"\"\n",
    "    Inicializa los modelos FFN y RNN y los mueve al dispositivo especificado.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Dimensionalidad de cada entrada x_t.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto.\n",
    "        output_size (int): Dimensionalidad de la salida y_t.\n",
    "        device (torch.device): Dispositivo (CPU o GPU) donde se alojarán los modelos.\n",
    "\n",
    "    Returns:\n",
    "        ffn_model (FFNLanguageModel): Modelo de lenguaje FFN inicializado.\n",
    "        rnn_model (RNNLanguageModel): Modelo de lenguaje RNN inicializado.\n",
    "    \"\"\"\n",
    "    ffn_model = FFNLanguageModel(input_size, hidden_size, output_size).to(device)\n",
    "    rnn_model = RNNLanguageModel(input_size, hidden_size, output_size).to(device)\n",
    "    return ffn_model, rnn_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parámetros\n",
    "    input_size = 5    # Dimensión de cada x_t\n",
    "    hidden_size = 10  # Dimensión del estado oculto h_t\n",
    "    output_size = 3   # Dimensión de la salida y_t\n",
    "    batch_size = 4    # Tamaño del lote\n",
    "\n",
    "    # Configuración del dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Inicializar los modelos\n",
    "    ffn_model, rnn_model = initialize_models(input_size, hidden_size, output_size, device)\n",
    "\n",
    "    # Ejemplo de entradas para el modelo FFN\n",
    "    x_t_minus_2 = torch.randn(batch_size, input_size, device=device)\n",
    "    x_t_minus_1 = torch.randn(batch_size, input_size, device=device)\n",
    "    x_t = torch.randn(batch_size, input_size, device=device)\n",
    "\n",
    "    # Estado oculto inicial para el modelo RNN\n",
    "    h_t_minus_1 = torch.zeros(batch_size, hidden_size, device=device)\n",
    "\n",
    "    # Pase hacia adelante a través del modelo FFN\n",
    "    y_t_ffn, h_t_ffn = ffn_model(x_t_minus_2, x_t_minus_1, x_t)\n",
    "    print(\"FFN salida y_t:\\n\", y_t_ffn)\n",
    "    print(\"FFN estado oculto h_t:\\n\", h_t_ffn)\n",
    "\n",
    "    # Pase hacia adelante a través del modelo RNN\n",
    "    y_t_rnn, h_t_rnn = rnn_model(x_t, h_t_minus_1)\n",
    "    print(\"RNN salida y_t:\\n\", y_t_rnn)\n",
    "    print(\"RNN estado oculta h_t:\\n\", h_t_rnn)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86796108",
   "metadata": {},
   "source": [
    "La salida que estás viendo es el resultado de pasar datos a través de los modelos de lenguaje FFN y RNN que definimos anteriormente en PyTorch. \n",
    "\n",
    "1. **Usando dispositivo: cpu**: Este mensaje indica que el modelo está utilizando la CPU para la computación. Si tuvieras acceso a una GPU y configuraras el modelo para usarla, aquí verías \"cuda\" en lugar de \"cpu\".\n",
    "\n",
    "2. **FFN salida $ y_t $**: Este tensor representa la salida $ y_t $ generada por el modelo de red neuronal feedforward (FFN) en cada uno de los pasos de tiempo. En este caso, tienes una salida para cada ejemplo en el lote (4 en total), con una dimensión de salida de 3 (especificada por el parámetro `output_size`).\n",
    "\n",
    "   Ejemplo de salida:\n",
    "   ```plaintext\n",
    "   tensor([[ 0.6602,  0.4772, -0.5175],\n",
    "           [-0.8437,  1.0741,  0.7102],\n",
    "           [-0.4538,  0.2009,  0.1363],\n",
    "           [-0.1967,  0.1131, -0.1583]], grad_fn=<AddmmBackward0>)\n",
    "   ```\n",
    "   Cada fila representa la salida $ y_t $ para una entrada diferente en el lote.\n",
    "\n",
    "3. **FFN estado oculto $ h_t $**: Este tensor es el estado oculto $ h_t $ generado por el modelo FFN después de procesar las entradas concatenadas $ x_{t-2}, x_{t-1}, x_t $. Cada fila representa el estado oculto correspondiente a una entrada en el lote, y tiene una dimensión de 10 (especificada por `hidden_size`).\n",
    "\n",
    "   Ejemplo de estado oculto:\n",
    "   ```plaintext\n",
    "   tensor([[ 0.5509,  0.8237,  0.3393,  0.3819, -0.9719,  0.9283, -0.0932, -0.4907,\n",
    "             0.9283, -0.9370],\n",
    "            ...], grad_fn=<TanhBackward0>)\n",
    "   ```\n",
    "\n",
    "4. **RNN salida $ y_t $**: Este tensor representa la salida $ y_t $ generada por el modelo RNN en cada uno de los pasos de tiempo. Similar a la salida del FFN, contiene una fila para cada entrada en el lote, con 3 valores de salida (especificado por `output_size`).\n",
    "\n",
    "   Ejemplo de salida:\n",
    "   ```plaintext\n",
    "   tensor([[-0.2385,  0.0706,  0.4726],\n",
    "           [ 0.3947, -0.2080,  1.6990],\n",
    "           [-0.2079, -0.6942,  1.2398],\n",
    "           [ 0.7105,  0.2177,  0.9162]], grad_fn=<AddmmBackward0>)\n",
    "   ```\n",
    "\n",
    "5. **RNN estado oculto $ h_t $**: Este tensor representa el estado oculto $ h_t $ actualizado en cada paso de tiempo del modelo RNN. Este estado se calcula usando tanto el estado oculto anterior $ h_{t-1} $ como la entrada actual $ x_t $, lo cual es característico de las redes RNN.\n",
    "\n",
    "   Ejemplo de estado oculto:\n",
    "   ```plaintext\n",
    "   tensor([[ 0.5921,  0.8117,  0.4427, -0.1933,  0.9189,  0.0968,  0.1448,  0.2683,\n",
    "             0.6134, -0.2545],\n",
    "            ...], grad_fn=<TanhBackward0>)\n",
    "   ```\n",
    "\n",
    "\n",
    "- Cada salida $ y_t $ representa el valor de predicción del modelo para una determinada entrada. En el contexto de un modelo de lenguaje, estos podrían representar las probabilidades de las siguientes palabras en una secuencia.\n",
    "- El estado oculto $ h_t $ es el \"estado de memoria\" de la red que ayuda a capturar información de las entradas anteriores (en el caso de la RNN) o de la concatenación de las entradas (en el caso del FFN).\n",
    "- `grad_fn=<AddmmBackward0>` y `grad_fn=<TanhBackward0>` son funciones de gradiente generadas por PyTorch para facilitar la retropropagación durante el entrenamiento del modelo. Estas indican que los tensores son parte de un gráfico computacional que PyTorch utilizará para calcular gradientes y actualizar los parámetros del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7478accc",
   "metadata": {},
   "source": [
    "### Entrenando RNN como modelos de lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # Barra de progreso\n",
    "import os\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de Lenguaje basado en una Red Neuronal Recurrente (RNN).\n",
    "\n",
    "    Este modelo utiliza una capa de embeddings para convertir tokens de entrada en vectores densos,\n",
    "    una capa RNN para procesar secuencias de embeddings y una capa completamente conectada para\n",
    "    predecir la siguiente palabra en la secuencia.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Tamaño del vocabulario (número de tokens únicos).\n",
    "        embed_size (int): Dimensionalidad de los embeddings.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto de la RNN.\n",
    "        num_layers (int): Número de capas de la RNN.\n",
    "        dropout (float, opcional): Tasa de dropout entre capas de la RNN. Por defecto es 0.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.0):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)  # Capa de embeddings\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers=num_layers,\n",
    "                          batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)  # Capa completamente conectada\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales usando la inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Realiza un pase hacia adelante del modelo RNN.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Secuencia de entrada, forma (batch_size, seq_length).\n",
    "            hidden (Tensor, opcional): Estado oculto inicial, forma (num_layers, batch_size, hidden_size).\n",
    "                                       Si es None, se inicializa a ceros.\n",
    "\n",
    "        Returns:\n",
    "            logits (Tensor): Logits para cada posición de la secuencia, forma (batch_size, seq_length, vocab_size).\n",
    "            hidden (Tensor): Estado oculto final, forma (num_layers, batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_length, embed_size)\n",
    "        output, hidden = self.rnn(embeddings, hidden)  # output: (batch_size, seq_length, hidden_size)\n",
    "        logits = self.fc(output)  # (batch_size, seq_length, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "class DummyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset de ejemplo que genera secuencias aleatorias de tokens.\n",
    "\n",
    "    Este dataset genera pares de secuencias de entrada y objetivo de manera aleatoria para fines de demostración.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, seq_length, vocab_size):\n",
    "        super(DummyDataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Secuencia de entrada\n",
    "        x = torch.randint(0, self.vocab_size, (self.seq_length,))\n",
    "        # Secuencia objetivo (desplazada una posición hacia la derecha)\n",
    "        y = torch.randint(0, self.vocab_size, (self.seq_length,))\n",
    "        return x, y\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, total_epochs, clip=5.0):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento para una época.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de entrenamiento.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "        epoch (int): Número de la época actual.\n",
    "        total_epochs (int): Número total de épocas.\n",
    "        clip (float, opcional): Valor máximo para el recorte de gradientes. Por defecto es 5.0.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "\n",
    "    for batch_idx, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Mover datos al dispositivo\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(inputs)  # Forward pass\n",
    "\n",
    "        # Reorganizar los logits y targets para calcular la pérdida\n",
    "        logits = logits.view(-1, logits.size(-1))  # (batch_size * seq_length, vocab_size)\n",
    "        targets = targets.view(-1)  # (batch_size * seq_length)\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Recorte de gradientes para evitar explosiones\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs} - Loss: {average_loss:.4f}\")\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Función de evaluación para calcular la pérdida en el conjunto de validación.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a evaluar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de validación.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "\n",
    "    Returns:\n",
    "        float: Pérdida promedio en el conjunto de validación.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            logits, _ = model(inputs)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = criterion(logits, targets)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    return average_loss\n",
    "\n",
    "def save_model(model, optimizer, epoch, loss, path):\n",
    "    \"\"\"\n",
    "    Guarda el estado del modelo y del optimizador.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a guardar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a guardar.\n",
    "        epoch (int): Número de la época actual.\n",
    "        loss (float): Pérdida actual.\n",
    "        path (str): Ruta donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "def load_model(model, optimizer, path, device):\n",
    "    \"\"\"\n",
    "    Carga el estado del modelo y del optimizador desde un archivo.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a cargar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a cargar.\n",
    "        path (str): Ruta desde donde se cargará el modelo.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "\n",
    "    Returns:\n",
    "        int: Época desde la cual se reanudó el entrenamiento.\n",
    "        float: Pérdida en la última época registrada.\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Modelo cargado desde {path} (Época {epoch}, Pérdida {loss})\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo {path}. Se iniciará el entrenamiento desde cero.\")\n",
    "        return 0, None\n",
    "\n",
    "def main():\n",
    "    # Parámetros\n",
    "    vocab_size = 10000    # Tamaño del vocabulario\n",
    "    embed_size = 128      # Dimensionalidad de los embeddings\n",
    "    hidden_size = 256     # Dimensionalidad del estado oculto\n",
    "    num_layers = 2        # Número de capas de la RNN\n",
    "    dropout = 0.5         # Tasa de dropout\n",
    "    sequence_length = 5   # Longitud de la secuencia\n",
    "    batch_size = 32       # Tamaño del lote\n",
    "    num_epochs = 10       # Número de épocas\n",
    "    learning_rate = 0.001 # Tasa de aprendizaje\n",
    "    save_path = 'rnn_language_model.pth'  # Ruta para guardar el modelo\n",
    "\n",
    "    # Configuración del dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = RNNLanguageModel(vocab_size, embed_size, hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "    # Definir la función de pérdida y el optimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Crear datasets y dataloaders\n",
    "    train_dataset = DummyDataset(num_samples=10000, seq_length=sequence_length, vocab_size=vocab_size)\n",
    "    val_dataset = DummyDataset(num_samples=2000, seq_length=sequence_length, vocab_size=vocab_size)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Opcional: Cargar un modelo previamente guardado\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(save_path):\n",
    "        start_epoch, _ = load_model(model, optimizer, save_path, device)\n",
    "\n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train(model, train_loader, criterion, optimizer, device, epoch, num_epochs)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoca {epoch+1}/{num_epochs} - Validacion cruzada: {val_loss:.4f}\")\n",
    "\n",
    "        # Guardar el modelo después de cada época\n",
    "        save_model(model, optimizer, epoch + 1, val_loss, save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ab3ff",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "\n",
    "- La clase DummyDataset genera datos aleatorios para propósitos de demostración. En un escenario real, deberías reemplazarla con un dataset que contenga secuencias de texto tokenizadas.\n",
    "Puedes utilizar datasets como Penn Treebank o cualquier otro corpus de texto.\n",
    "- Para trabajar con secuencias más largas, considera ajustar el sequence_length y manejar eficientemente la memoria.\n",
    "Técnicas como truncated backpropagation through time (BPTT) pueden ser útiles para evitar problemas de memoria y mejorar el entrenamiento en secuencias largas.\n",
    "- Se ha añadido una capa de dropout en la RNN para prevenir el sobreajuste. Puedes ajustar la tasa de dropout según sea necesario.\n",
    "- Además de la pérdida, puedes implementar métricas como accuracy para monitorear el rendimiento del modelo. Esto proporciona una visión más completa del comportamiento del modelo durante el entrenamiento y la evaluación.\n",
    "- Experimenta con diferentes tasas de aprendizaje, tamaños de lote, dimensiones de embeddings y tamaños ocultos para encontrar la mejor configuración para tu tarea específica.\n",
    "- Considera utilizar variantes más avanzadas de RNN, como LSTM o GRU, que manejan mejor el problema del desvanecimiento de gradientes y capturan dependencias a largo plazo de manera más efectiva.\n",
    "- PyTorch facilita la implementación de estos modelos utilizando nn.LSTM o nn.GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd150dc",
   "metadata": {},
   "source": [
    "Este ejemplo utiliza el dataset Penn Treebank, pero puedes adaptarlo!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c60afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para datos de texto que maneja la tokenización y la creación del vocabulario.\n",
    "    \"\"\"\n",
    "    def __init__(self, split, tokenizer, vocab, seq_length):\n",
    "        self.data = []\n",
    "        for item in split:\n",
    "            tokens = tokenizer(item)\n",
    "            token_ids = vocab(tokens)\n",
    "            self.data.extend(token_ids)\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - 1) // self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_length\n",
    "        end = start + self.seq_length\n",
    "        x = torch.tensor(self.data[start:end], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[start+1:end+1], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def build_vocab():\n",
    "    \"\"\"\n",
    "    Construye el vocabulario a partir del dataset de entrenamiento.\n",
    "    \"\"\"\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    train_iter = PennTreebank(split='train')\n",
    "    vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "def main_real_data():\n",
    "    # Parámetros\n",
    "    vocab_size = 10000    # Tamaño del vocabulario\n",
    "    embed_size = 128      # Dimensionalidad de los embeddings\n",
    "    hidden_size = 256     # Dimensionalidad del estado oculto\n",
    "    num_layers = 2        # Número de capas de la RNN\n",
    "    dropout = 0.5         # Tasa de dropout\n",
    "    sequence_length = 30  # Longitud de la secuencia\n",
    "    batch_size = 64       # Tamaño del lote\n",
    "    num_epochs = 20       # Número de épocas\n",
    "    learning_rate = 0.001 # Tasa de aprendizaje\n",
    "    save_path = 'rnn_language_model_real_data.pth'  # Ruta para guardar el modelo\n",
    "\n",
    "    # Configuración del dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Construir el vocabulario\n",
    "    vocab = build_vocab()\n",
    "    actual_vocab_size = len(vocab)\n",
    "    print(f\"Tamaño del vocabulario: {actual_vocab_size}\")\n",
    "\n",
    "    # Cargar los datos de texto\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    train_iter = PennTreebank(split='train')\n",
    "    val_iter = PennTreebank(split='valid')\n",
    "\n",
    "    train_dataset = TextDataset(train_iter, tokenizer, vocab, sequence_length)\n",
    "    val_dataset = TextDataset(val_iter, tokenizer, vocab, sequence_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = RNNLanguageModel(actual_vocab_size, embed_size, hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "    # Definir la función de pérdida y el optimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Opcional: Cargar un modelo previamente guardado\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(save_path):\n",
    "        start_epoch, _ = load_model(model, optimizer, save_path, device)\n",
    "\n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train(model, train_loader, criterion, optimizer, device, epoch, num_epochs)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Validación Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Guardar el modelo después de cada época\n",
    "        save_model(model, optimizer, epoch + 1, val_loss, save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # Para usar datos reales, comenta la línea anterior y descomenta la siguiente:\n",
    "    # main_real_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748317d",
   "metadata": {},
   "source": [
    "### POS como etiquetado de secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0119608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # Barra de progreso\n",
    "import os\n",
    "\n",
    "class POSTaggerRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de Etiquetado POS basado en una Red Neuronal Recurrente (RNN).\n",
    "\n",
    "    Este modelo utiliza una capa de embeddings para convertir tokens de entrada en vectores densos,\n",
    "    una capa LSTM para procesar secuencias de embeddings y una capa completamente conectada para\n",
    "    predecir la etiqueta POS de cada palabra en la secuencia.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Tamaño del vocabulario (número de tokens únicos).\n",
    "        embed_size (int): Dimensionalidad de los embeddings.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto de la LSTM.\n",
    "        num_tags (int): Número de etiquetas POS.\n",
    "        num_layers (int, opcional): Número de capas de la LSTM. Por defecto es 1.\n",
    "        dropout (float, opcional): Tasa de dropout entre capas de la LSTM. Por defecto es 0.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_tags, num_layers=1, dropout=0.0):\n",
    "        super(POSTaggerRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)  # Capa de embeddings\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0.0)  # Capa LSTM\n",
    "        self.fc = nn.Linear(hidden_size, num_tags)  # Capa completamente conectada\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales y de embeddings usando la inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Realiza un pase hacia adelante del modelo RNN para etiquetado POS.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Secuencia de entrada, forma (batch_size, seq_length).\n",
    "\n",
    "        Returns:\n",
    "            tag_scores (Tensor): Puntuaciones para cada etiqueta POS en cada posición, forma (batch_size, seq_length, num_tags).\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_length, embed_size)\n",
    "        lstm_out, _ = self.lstm(embeddings)  # (batch_size, seq_length, hidden_size)\n",
    "        tag_scores = self.fc(lstm_out)  # (batch_size, seq_length, num_tags)\n",
    "        return tag_scores\n",
    "\n",
    "class DummyPOSTaggingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset de ejemplo para Etiquetado POS que genera secuencias aleatorias de tokens y etiquetas.\n",
    "\n",
    "    Este dataset genera pares de secuencias de entrada y etiquetas de manera aleatoria para fines de demostración.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, seq_length, vocab_size, num_tags):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset.\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): Número de muestras en el dataset.\n",
    "            seq_length (int): Longitud de cada secuencia.\n",
    "            vocab_size (int): Tamaño del vocabulario.\n",
    "            num_tags (int): Número de etiquetas POS.\n",
    "        \"\"\"\n",
    "        super(DummyPOSTaggingDataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_tags = num_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Secuencia de entrada (tokens)\n",
    "        x = torch.randint(0, self.vocab_size, (self.seq_length,))\n",
    "        # Secuencia de etiquetas POS (etiquetas)\n",
    "        y = torch.randint(0, self.num_tags, (self.seq_length,))\n",
    "        return x, y\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, total_epochs, clip=5.0):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento para una época.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de entrenamiento.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "        epoch (int): Número de la época actual.\n",
    "        total_epochs (int): Número total de épocas.\n",
    "        clip (float, opcional): Valor máximo para el recorte de gradientes. Por defecto es 5.0.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "\n",
    "    for batch_idx, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Mover datos al dispositivo\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        tag_scores = model(inputs)  # Forward pass\n",
    "\n",
    "        # Reorganizar los tag_scores y targets para calcular la pérdida\n",
    "        # tag_scores: (batch_size, seq_length, num_tags) -> (batch_size * seq_length, num_tags)\n",
    "        # targets: (batch_size, seq_length) -> (batch_size * seq_length)\n",
    "        tag_scores = tag_scores.view(-1, tag_scores.size(-1))\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = criterion(tag_scores, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Recorte de gradientes para evitar explosiones\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs} - Loss: {average_loss:.4f}\")\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Función de evaluación para calcular la pérdida en el conjunto de validación.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a evaluar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de validación.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "\n",
    "    Returns:\n",
    "        float: Pérdida promedio en el conjunto de validación.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            tag_scores = model(inputs)\n",
    "            tag_scores = tag_scores.view(-1, tag_scores.size(-1))\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = criterion(tag_scores, targets)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    return average_loss\n",
    "\n",
    "def save_model(model, optimizer, epoch, loss, path):\n",
    "    \"\"\"\n",
    "    Guarda el estado del modelo y del optimizador.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a guardar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a guardar.\n",
    "        epoch (int): Número de la época actual.\n",
    "        loss (float): Pérdida actual.\n",
    "        path (str): Ruta donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "def load_model(model, optimizer, path, device):\n",
    "    \"\"\"\n",
    "    Carga el estado del modelo y del optimizador desde un archivo.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a cargar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a cargar.\n",
    "        path (str): Ruta desde donde se cargará el modelo.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "\n",
    "    Returns:\n",
    "        int: Época desde la cual se reanudó el entrenamiento.\n",
    "        float: Pérdida en la última época registrada.\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Modelo cargado desde {path} (Época {epoch}, Pérdida {loss})\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo {path}. Se iniciará el entrenamiento desde cero.\")\n",
    "        return 0, None\n",
    "\n",
    "def main():\n",
    "    # Parámetros\n",
    "    vocab_size = 10000    # Tamaño del vocabulario\n",
    "    embed_size = 128      # Dimensionalidad de los embeddings\n",
    "    hidden_size = 256     # Dimensionalidad del estado oculto\n",
    "    num_tags = 10         # Número de etiquetas POS\n",
    "    num_layers = 2        # Número de capas de la LSTM\n",
    "    dropout = 0.5         # Tasa de dropout\n",
    "    sequence_length = 5   # Longitud de la secuencia\n",
    "    batch_size = 32       # Tamaño del lote\n",
    "    num_epochs = 5        # Número de épocas\n",
    "    learning_rate = 0.001 # Tasa de aprendizaje\n",
    "    save_path = 'pos_tagger_rnn.pth'  # Ruta para guardar el modelo\n",
    "\n",
    "    # Configuración del dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = POSTaggerRNN(vocab_size, embed_size, hidden_size, num_tags, num_layers, dropout).to(device)\n",
    "\n",
    "    # Definir la función de pérdida y el optimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Crear datasets y dataloaders\n",
    "    train_dataset = DummyPOSTaggingDataset(num_samples=10000, seq_length=sequence_length, \n",
    "                                          vocab_size=vocab_size, num_tags=num_tags)\n",
    "    val_dataset = DummyPOSTaggingDataset(num_samples=2000, seq_length=sequence_length, \n",
    "                                        vocab_size=vocab_size, num_tags=num_tags)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Opcional: Cargar un modelo previamente guardado\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(save_path):\n",
    "        start_epoch, _ = load_model(model, optimizer, save_path, device)\n",
    "\n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train(model, train_loader, criterion, optimizer, device, epoch, num_epochs)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoca {epoch+1}/{num_epochs} - Validacion cruzada: {val_loss:.4f}\")\n",
    "\n",
    "        # Guardar el modelo después de cada época\n",
    "        save_model(model, optimizer, epoch + 1, val_loss, save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2993ec46",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "- Actualmente, el código utiliza datos generados aleatoriamente para fines de demostración. Para una aplicación real, deberías reemplazar DummyPOSTaggingDataset con un dataset que contenga secuencias de texto tokenizadas y sus correspondientes etiquetas POS.Puedes utilizar bibliotecas como torchtext para manejar datasets de texto reales.\n",
    "- Si trabajas con secuencias más largas, considera ajustar el sequence_length y manejar eficientemente la memoria. Técnicas como truncated backpropagation through time (BPTT) pueden ser útiles para evitar problemas de memoria y mejorar el entrenamiento en secuencias largas.\n",
    "- La capa de dropout en la LSTM ayuda a prevenir el sobreajuste. Puedes ajustar la tasa de dropout según las necesidades. Además del dropout, puedes implementar otras técnicas de regularización si es necesario.\n",
    "- Experimenta con diferentes tasas de aprendizaje, tamaños de lote, dimensiones de embeddings y tamaños ocultos para encontrar la mejor configuración para tu tarea específica.\n",
    "- Considera utilizar variantes más avanzadas de RNN, como GRU (nn.GRU), que también manejan bien las dependencias a largo plazo y pueden ser más eficientes computacionalmente que LSTM en ciertos casos.\n",
    "- Además de la pérdida, puedes implementar métricas como accuracy para monitorear el rendimiento del modelo durante el entrenamiento y la evaluación. Esto proporciona una visión más completa del comportamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7fe15b",
   "metadata": {},
   "source": [
    "### Clasificación de secuencias utilizando una RNN + FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee933dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # Barra de progreso\n",
    "import os\n",
    "\n",
    "class RNNSequenceClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de Clasificación de Secuencias basado en una Red Neuronal Recurrente (RNN) y una Red Neuronal Feedforward (FFN).\n",
    "\n",
    "    Este modelo utiliza una capa de embeddings para convertir tokens de entrada en vectores densos,\n",
    "    una capa LSTM para procesar secuencias de embeddings y una capa completamente conectada para\n",
    "    predecir la clase de la secuencia.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Tamaño del vocabulario (número de tokens únicos).\n",
    "        embed_size (int): Dimensionalidad de los embeddings.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto de la LSTM.\n",
    "        num_classes (int): Número de clases de salida para la clasificación.\n",
    "        num_layers (int, opcional): Número de capas de la LSTM. Por defecto es 1.\n",
    "        dropout (float, opcional): Tasa de dropout entre capas de la LSTM. Por defecto es 0.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes, num_layers=1, dropout=0.0):\n",
    "        super(RNNSequenceClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)  # Capa de embeddings\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0.0)  # Capa LSTM\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Capa completamente conectada\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales y de embeddings usando la inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Realiza un pase hacia adelante del modelo RNN + FFN para clasificación de secuencias.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Secuencia de entrada, forma (batch_size, seq_length).\n",
    "\n",
    "        Returns:\n",
    "            logits (Tensor): Puntuaciones para cada clase, forma (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_length, embed_size)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embeddings)  # lstm_out: (batch_size, seq_length, hidden_size)\n",
    "        # Usar el último estado oculto para la clasificación\n",
    "        final_hidden_state = h_n[-1]  # (batch_size, hidden_size)\n",
    "        logits = self.fc(final_hidden_state)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "class DummySequenceClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset de ejemplo para Clasificación de Secuencias que genera secuencias aleatorias de tokens y etiquetas.\n",
    "\n",
    "    Este dataset genera pares de secuencias de entrada y etiquetas de manera aleatoria para fines de demostración.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, seq_length, vocab_size, num_classes):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset.\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): Número de muestras en el dataset.\n",
    "            seq_length (int): Longitud de cada secuencia.\n",
    "            vocab_size (int): Tamaño del vocabulario.\n",
    "            num_classes (int): Número de clases de salida.\n",
    "        \"\"\"\n",
    "        super(DummySequenceClassificationDataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Secuencia de entrada (tokens)\n",
    "        x = torch.randint(0, self.vocab_size, (self.seq_length,))\n",
    "        # Etiqueta de clase (un valor por secuencia)\n",
    "        y = torch.randint(0, self.num_classes, (1,)).squeeze()  # Scalar\n",
    "        return x, y\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, total_epochs, clip=5.0):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento para una época.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de entrenamiento.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "        epoch (int): Número de la época actual.\n",
    "        total_epochs (int): Número total de épocas.\n",
    "        clip (float, opcional): Valor máximo para el recorte de gradientes. Por defecto es 5.0.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "\n",
    "    for batch_idx, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Mover datos al dispositivo\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)  # Forward pass\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Recorte de gradientes para evitar explosiones\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Cálculo de precisión\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        average_loss = epoch_loss / (batch_idx + 1)\n",
    "        accuracy = 100 * correct / total\n",
    "        progress_bar.set_postfix(loss=average_loss, accuracy=accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs} - Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Función de evaluación para calcular la pérdida y precisión en el conjunto de validación.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a evaluar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de validación.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pérdida promedio, precisión promedio)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return average_loss, accuracy\n",
    "\n",
    "def save_model(model, optimizer, epoch, loss, path):\n",
    "    \"\"\"\n",
    "    Guarda el estado del modelo y del optimizador.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a guardar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a guardar.\n",
    "        epoch (int): Número de la época actual.\n",
    "        loss (float): Pérdida actual.\n",
    "        path (str): Ruta donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "def load_model(model, optimizer, path, device):\n",
    "    \"\"\"\n",
    "    Carga el estado del modelo y del optimizador desde un archivo.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a cargar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a cargar.\n",
    "        path (str): Ruta desde donde se cargará el modelo.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (época desde la cual se reanudó el entrenamiento, pérdida en la última época registrada)\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Modelo cargado desde {path} (Época {epoch}, Pérdida {loss})\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo {path}. Se iniciará el entrenamiento desde cero.\")\n",
    "        return 0, None\n",
    "\n",
    "def main():\n",
    "    # Parámetros\n",
    "    vocab_size = 10000    # Tamaño del vocabulario\n",
    "    embed_size = 128      # Dimensionalidad de los embeddings\n",
    "    hidden_size = 256     # Dimensionalidad del estado oculto\n",
    "    num_classes = 5       # Número de clases de salida para la clasificación\n",
    "    num_layers = 2        # Número de capas de la LSTM\n",
    "    dropout = 0.5         # Tasa de dropout\n",
    "    sequence_length = 10  # Longitud de la secuencia\n",
    "    batch_size = 32       # Tamaño del lote\n",
    "    num_epochs = 5        # Número de épocas\n",
    "    learning_rate = 0.001 # Tasa de aprendizaje\n",
    "    save_path = 'rnn_sequence_classifier.pth'  # Ruta para guardar el modelo\n",
    "\n",
    "    # Configuración del dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = RNNSequenceClassifier(vocab_size, embed_size, hidden_size, num_classes, num_layers, dropout).to(device)\n",
    "\n",
    "    # Definir la función de pérdida y el optimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Crear datasets y dataloaders\n",
    "    train_dataset = DummySequenceClassificationDataset(num_samples=10000, seq_length=sequence_length, \n",
    "                                                      vocab_size=vocab_size, num_classes=num_classes)\n",
    "    val_dataset = DummySequenceClassificationDataset(num_samples=2000, seq_length=sequence_length, \n",
    "                                                    vocab_size=vocab_size, num_classes=num_classes)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Opcional: Cargar un modelo previamente guardado\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(save_path):\n",
    "        start_epoch, _ = load_model(model, optimizer, save_path, device)\n",
    "\n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train(model, train_loader, criterion, optimizer, device, epoch, num_epochs)\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoca {epoch+1}/{num_epochs} - Validación Loss: {val_loss:.4f}, Validación Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Guardar el modelo después de cada época\n",
    "        save_model(model, optimizer, epoch + 1, val_loss, save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52daf273",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "- Actualmente, el código utiliza datos generados aleatoriamente para fines de demostración. Para una aplicación real, deberías reemplazar DummySequenceClassificationDataset con un dataset que contenga secuencias de texto tokenizadas y sus correspondientes etiquetas de clase.\n",
    "- Puedes utilizar bibliotecas como torchtext para manejar datasets de texto reales.\n",
    "- Si trabajas con secuencias más largas, considera ajustar el sequence_length y manejar eficientemente la memoria. Técnicas como truncated backpropagation through time (BPTT) pueden ser útiles para evitar problemas de memoria y mejorar el entrenamiento en secuencias largas.\n",
    "- La capa de dropout en la LSTM ayuda a prevenir el sobreajuste. Puedes ajustar la tasa de dropout según las necesidades. Además del dropout, puedes implementar otras técnicas de regularización si es necesario.\n",
    "- Experimenta con diferentes tasas de aprendizaje, tamaños de lote, dimensiones de embeddings y tamaños ocultos para encontrar la mejor configuración para tu tarea específica.\n",
    "- Considera utilizar variantes más avanzadas de RNN, como GRU (nn.GRU), que también manejan bien las dependencias a largo plazo y pueden ser más eficientes computacionalmente que LSTM en ciertos casos.\n",
    "- Además de la precisión, puedes implementar otras métricas como F1-score, precision y recall para una evaluación más completa del rendimiento del modelo.\n",
    "- Puedes adaptar el código para usar un dataset real. Este ejemplo utiliza el dataset AG News con la ayuda de torchtext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9521f7",
   "metadata": {},
   "source": [
    "### Generación autorregresiva con RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # Barra de progreso\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class RNNTextGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo Autoregresivo RNN para Generación de Texto.\n",
    "\n",
    "    Este modelo utiliza una capa de embeddings para convertir tokens de entrada en vectores densos,\n",
    "    una capa LSTM para procesar secuencias de embeddings y una capa completamente conectada para\n",
    "    predecir la siguiente palabra en la secuencia.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Tamaño del vocabulario (número de tokens únicos).\n",
    "        embed_size (int): Dimensionalidad de los embeddings.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto de la LSTM.\n",
    "        num_layers (int, opcional): Número de capas de la LSTM. Por defecto es 1.\n",
    "        dropout (float, opcional): Tasa de dropout entre capas de la LSTM. Por defecto es 0.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.0):\n",
    "        super(RNNTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)  # Capa de embeddings\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0.0)  # Capa LSTM\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)  # Capa completamente conectada para predecir la siguiente palabra\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales y de embeddings usando la inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Realiza un pase hacia adelante del modelo RNN para generación de texto.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Secuencia de entrada, forma (batch_size, seq_length).\n",
    "            hidden (tuple, opcional): Estados ocultos iniciales (h_0, c_0). Si es None, se inicializan a cero.\n",
    "\n",
    "        Returns:\n",
    "            logits (Tensor): Puntuaciones para cada palabra en el vocabulario, forma (batch_size, seq_length, vocab_size).\n",
    "            hidden (tuple): Estados ocultos finales (h_n, c_n).\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_length, embed_size)\n",
    "        lstm_out, hidden = self.lstm(embeddings, hidden)  # lstm_out: (batch_size, seq_length, hidden_size)\n",
    "        logits = self.fc(lstm_out)  # (batch_size, seq_length, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para Generación de Texto que maneja la tokenización y la creación de secuencias.\n",
    "\n",
    "    Este dataset toma un texto, lo tokeniza y crea secuencias de longitud fija para el entrenamiento del modelo.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, seq_length, word_to_id, unk_token='<unk>'):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset.\n",
    "\n",
    "        Args:\n",
    "            text (str): Texto completo para entrenar el modelo.\n",
    "            seq_length (int): Longitud de cada secuencia de entrada.\n",
    "            word_to_id (dict): Mapeo de palabras a IDs.\n",
    "            unk_token (str, opcional): Token para palabras desconocidas. Por defecto es '<unk>'.\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length\n",
    "        self.word_to_id = word_to_id\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        # Tokenizar el texto\n",
    "        self.tokens = text.lower().split()\n",
    "        self.data = self.create_sequences()\n",
    "\n",
    "    def create_sequences(self):\n",
    "        \"\"\"\n",
    "        Crea secuencias de entrada y salida a partir de los tokens.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de tuplas (input_sequence, target_word).\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        for i in range(len(self.tokens) - self.seq_length):\n",
    "            input_seq = self.tokens[i:i + self.seq_length]\n",
    "            target = self.tokens[i + self.seq_length]\n",
    "            input_ids = [self.word_to_id.get(word, self.word_to_id[self.unk_token]) for word in input_seq]\n",
    "            target_id = self.word_to_id.get(target, self.word_to_id[self.unk_token])\n",
    "            sequences.append((input_ids, target_id))\n",
    "        return sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target = self.data[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, total_epochs, clip=5.0):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento para una época.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de entrenamiento.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "        epoch (int): Número de la época actual.\n",
    "        total_epochs (int): Número total de épocas.\n",
    "        clip (float, opcional): Valor máximo para el recorte de gradientes. Por defecto es 5.0.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "\n",
    "    for batch_idx, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Mover datos al dispositivo\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, hidden = model(inputs)  # Forward pass\n",
    "\n",
    "        # Extraer solo las predicciones del último token en la secuencia\n",
    "        # logits: (batch_size, seq_length, vocab_size) -> (batch_size, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Recorte de gradientes para evitar explosiones\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        average_loss = epoch_loss / (batch_idx + 1)\n",
    "        progress_bar.set_postfix(loss=average_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs} - Loss: {average_loss:.4f}\")\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Función de evaluación para calcular la pérdida en el conjunto de validación.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a evaluar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de validación.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "\n",
    "    Returns:\n",
    "        float: Pérdida promedio en el conjunto de validación.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            logits, hidden = model(inputs)\n",
    "            logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "\n",
    "            loss = criterion(logits, targets)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    return average_loss\n",
    "\n",
    "def save_model(model, optimizer, epoch, loss, path):\n",
    "    \"\"\"\n",
    "    Guarda el estado del modelo y del optimizador.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a guardar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a guardar.\n",
    "        epoch (int): Número de la época actual.\n",
    "        loss (float): Pérdida actual.\n",
    "        path (str): Ruta donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "def load_model(model, optimizer, path, device):\n",
    "    \"\"\"\n",
    "    Carga el estado del modelo y del optimizador desde un archivo.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a cargar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a cargar.\n",
    "        path (str): Ruta desde donde se cargará el modelo.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (época desde la cual se reanudó el entrenamiento, pérdida en la última época registrada)\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Modelo cargado desde {path} (Época {epoch}, Pérdida {loss})\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo {path}. Se iniciará el entrenamiento desde cero.\")\n",
    "        return 0, None\n",
    "\n",
    "def generate_text(model, start_token, id_to_word, word_to_id, length=5, device='cpu', temperature=1.0):\n",
    "    \"\"\"\n",
    "    Genera una secuencia de texto de forma autoregresiva.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo entrenado para generación de texto.\n",
    "        start_token (int): ID del token inicial (por ejemplo, <s>).\n",
    "        id_to_word (dict): Mapeo de IDs a palabras.\n",
    "        word_to_id (dict): Mapeo de palabras a IDs.\n",
    "        length (int, opcional): Longitud de la secuencia a generar. Por defecto es 5.\n",
    "        device (str, opcional): Dispositivo para generar el texto. Por defecto es 'cpu'.\n",
    "        temperature (float, opcional): Controla la aleatoriedad de la predicción. Por defecto es 1.0.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de palabras generadas.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_word = torch.LongTensor([[start_token]]).to(device)  # Palabra inicial (<s>)\n",
    "    hidden = None  # Estado oculto inicial\n",
    "    generated_words = [id_to_word[start_token]]  # Inicializamos con el token de inicio\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            logits, hidden = model(input_word, hidden)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_word_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated_words.append(id_to_word.get(next_word_id, '<unk>'))\n",
    "            input_word = torch.LongTensor([[next_word_id]]).to(device)\n",
    "\n",
    "    return generated_words\n",
    "\n",
    "def main():\n",
    "    # Parámetros\n",
    "    vocab_size = 10000    # Tamaño del vocabulario\n",
    "    embed_size = 128      # Dimensionalidad de los embeddings\n",
    "    hidden_size = 256     # Dimensionalidad del estado oculto\n",
    "    num_layers = 2        # Número de capas de la LSTM\n",
    "    dropout = 0.5         # Tasa de dropout\n",
    "    sequence_length = 5   # Longitud de la secuencia\n",
    "    batch_size = 64       # Tamaño del lote\n",
    "    num_epochs = 10       # Número de épocas\n",
    "    learning_rate = 0.001 # Tasa de aprendizaje\n",
    "    save_path = 'rnn_text_generator.pth'  # Ruta para guardar el modelo\n",
    "\n",
    "    # Configuración del dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Ejemplo de texto (reemplaza esto con un dataset real para mejores resultados)\n",
    "    sample_text = \"So long and thanks for all the fish. So long and thanks for all the fish.\"\n",
    "\n",
    "    # Crear mappings de palabras a IDs y viceversa\n",
    "    words = sample_text.lower().split()\n",
    "    unique_words = set(words)\n",
    "    word_to_id = {word: idx for idx, word in enumerate(unique_words, start=1)}  # Empezar en 1\n",
    "    word_to_id['<s>'] = 0  # Token de inicio\n",
    "    word_to_id['<unk>'] = len(word_to_id)  # Token desconocido\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "\n",
    "    vocab_size = len(word_to_id)\n",
    "\n",
    "    # Crear el dataset y el dataloader\n",
    "    dataset = TextDataset(text=sample_text, seq_length=sequence_length, word_to_id=word_to_id)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = RNNTextGenerator(vocab_size, embed_size, hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "    # Definir la función de pérdida y el optimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Opcional: Cargar un modelo previamente guardado\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(save_path):\n",
    "        start_epoch, _ = load_model(model, optimizer, save_path, device)\n",
    "\n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train(model, dataloader, criterion, optimizer, device, epoch, num_epochs)\n",
    "        val_loss = evaluate(model, dataloader, criterion, device)\n",
    "        print(f\"Epoca {epoch+1}/{num_epochs} - Validación Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Guardar el modelo después de cada época\n",
    "        save_model(model, optimizer, epoch + 1, val_loss, save_path)\n",
    "\n",
    "    # Generar una secuencia de texto\n",
    "    start_token = word_to_id['<s>']\n",
    "    generated_sequence = generate_text(model, start_token, id_to_word, word_to_id, length=10, device=device, temperature=1.0)\n",
    "    print(\"Secuencia generada:\", \" \".join(generated_sequence))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb422d",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "- El sample_text utilizado en el ejemplo es muy pequeño, lo que limita la capacidad del modelo para aprender patrones significativos. Para obtener mejores resultados, considera entrenar el modelo en un corpus de texto más grande y diverso, como los proporcionados por WikiText, BooksCorpus, o datasets de Hugging Face.\n",
    "- Actualmente, el modelo trunca las secuencias que exceden seq_length. Para manejar secuencias de longitud variable de manera más efectiva, podrías considerar el padding y el uso de pack_padded_sequence y pad_packed_sequence de PyTorch. Esto es especialmente útil si deseas que el modelo pueda manejar entradas de diferentes longitudes sin perder información.\n",
    "- Además del dropout ya implementado, considera añadir técnicas de regularización adicionales, como la normalización de pesos (Weight Normalization) o la utilización de capas adicionales, para mejorar la capacidad de generalización del modelo y prevenir el sobreajuste.\n",
    "- Experimenta con diferentes tasas de aprendizaje, tamaños de lote, dimensiones de embeddings y tamaños ocultos para encontrar la mejor configuración para tu tarea específica. Puedes utilizar herramientas como Optuna o Ray Tune para automatizar la búsqueda de hiperparámetros.\n",
    "- Considera utilizar variantes más avanzadas de RNN, como GRU (nn.GRU), que pueden ser más eficientes computacionalmente que LSTM en ciertos casos. Además, podrías explorar arquitecturas más modernas como Transformers, que han demostrado un rendimiento superior en tareas de generación de texto.\n",
    "- Además de la pérdida (loss), considera evaluar el modelo utilizando métricas adicionales como la perplejidad (perplexity), que es una medida comúnmente utilizada en modelos de lenguaje para evaluar la capacidad de predicción.\n",
    "- Para mejorar la calidad del texto generado, puedes implementar técnicas de muestreo más avanzadas, como top-k sampling o nucleus sampling (top-p sampling), que permiten un mejor control sobre la diversidad y coherencia del texto generado.\n",
    "- Se puede adaptar del código para entrenar el modelo utilizando el dataset WikiText-2 con la ayuda de torchtext. Este ejemplo reemplazaria el texto de muestra con un corpus más grande y maneja la construcción del vocabulario de manera más robusta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3197d",
   "metadata": {},
   "source": [
    "### Redes recurrentes apiladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db5fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # Barra de progreso\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Establecer semillas para reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Verificar dispositivo (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo LSTM apilado para generación de texto.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): Tamaño del vocabulario.\n",
    "        embed_size (int): Dimensionalidad de los embeddings.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto de la LSTM.\n",
    "        num_layers (int): Número de capas de LSTM apiladas.\n",
    "        dropout (float): Tasa de dropout entre capas de LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout if num_layers >1 else 0.0)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas de embeddings, LSTM y fully connected usando la inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Pase hacia adelante del modelo LSTM apilado para generación de texto.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Secuencia de entrada, forma (batch_size, seq_length).\n",
    "            hidden (tuple, opcional): Estados ocultos iniciales (h_0, c_0). Si es None, se inicializan a cero.\n",
    "        \n",
    "        Returns:\n",
    "            logits (Tensor): Puntuaciones para cada palabra en el vocabulario, forma (batch_size, seq_length, vocab_size).\n",
    "            hidden (tuple): Estados ocultos finales (h_n, c_n).\n",
    "        \"\"\"\n",
    "        embed = self.embedding(x)  # (batch_size, seq_length, embed_size)\n",
    "        lstm_out, hidden = self.lstm(embed, hidden)  # lstm_out: (batch_size, seq_length, hidden_size)\n",
    "        logits = self.fc(lstm_out)  # (batch_size, seq_length, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "class LanguageModelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para modelado de lenguaje que maneja la tokenización y creación de secuencias.\n",
    "    \n",
    "    Cada muestra consiste en una secuencia de tokens de entrada y el token objetivo que sigue a la secuencia.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto completo para entrenar el modelo.\n",
    "        seq_length (int): Longitud de las secuencias de entrada.\n",
    "        word_to_id (dict): Mapeo de palabras a IDs.\n",
    "        unk_token (str): Token para palabras desconocidas.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, seq_length, word_to_id, unk_token='<unk>'):\n",
    "        self.seq_length = seq_length\n",
    "        self.word_to_id = word_to_id\n",
    "        self.unk_token = unk_token\n",
    "        self.tokens = text.lower().split()\n",
    "        self.data = self.create_sequences()\n",
    "    \n",
    "    def create_sequences(self):\n",
    "        \"\"\"\n",
    "        Crea secuencias de entrada y salida a partir de los tokens.\n",
    "        \n",
    "        Returns:\n",
    "            list: Lista de tuplas (input_sequence, target_word).\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        for i in range(len(self.tokens) - self.seq_length):\n",
    "            seq = self.tokens[i:i + self.seq_length]\n",
    "            target = self.tokens[i + self.seq_length]\n",
    "            seq_ids = [self.word_to_id.get(word, self.word_to_id[self.unk_token]) for word in seq]\n",
    "            target_id = self.word_to_id.get(target, self.word_to_id[self.unk_token])\n",
    "            sequences.append((seq_ids, target_id))\n",
    "        return sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq, target = self.data[idx]\n",
    "        return torch.tensor(seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, clip=5.0):\n",
    "    \"\"\"\n",
    "    Entrena el modelo durante una época.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de entrenamiento.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "        clip (float): Valor máximo para el recorte de gradientes.\n",
    "    \n",
    "    Returns:\n",
    "        float: Pérdida promedio para la época.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    progress = tqdm(dataloader, desc=\"Entrenando\", leave=False)\n",
    "    \n",
    "    for inputs, targets in progress:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(inputs)  # logits: (batch_size, seq_length, vocab_size)\n",
    "        # Nos interesan las predicciones del último token de la secuencia\n",
    "        logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Recorte de gradientes\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss\n",
    "\n",
    "def evaluate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo durante una época.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Modelo a evaluar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de evaluación.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "    \n",
    "    Returns:\n",
    "        float: Pérdida promedio para la evaluación.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        progress = tqdm(dataloader, desc=\"Evaluando\", leave=False)\n",
    "        for inputs, targets in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits, _ = model(inputs)\n",
    "            logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "            loss = criterion(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "            progress.set_postfix(loss=loss.item())\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    \"\"\"\n",
    "    Guarda el estado del modelo y del optimizador.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Modelo a guardar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a guardar.\n",
    "        epoch (int): Época actual.\n",
    "        loss (float): Pérdida actual.\n",
    "        path (str): Ruta donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"Modelo guardado en {path} (Época {epoch}, Pérdida {loss:.4f})\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, path, device):\n",
    "    \"\"\"\n",
    "    Carga el estado del modelo y del optimizador desde un archivo.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Modelo a cargar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a cargar.\n",
    "        path (str): Ruta desde donde se cargará el modelo.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (época, pérdida) cargados desde el checkpoint.\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Modelo cargado desde {path} (Época {epoch}, Pérdida {loss:.4f})\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo {path}. Se iniciará el entrenamiento desde cero.\")\n",
    "        return 0, None\n",
    "\n",
    "def generate_text(model, start_token, id_to_word, word_to_id, length=20, device='cpu', temperature=1.0):\n",
    "    \"\"\"\n",
    "    Genera una secuencia de texto de forma autoregresiva.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Modelo entrenado para generación de texto.\n",
    "        start_token (int): ID del token inicial (por ejemplo, <s>).\n",
    "        id_to_word (dict): Mapeo de IDs a palabras.\n",
    "        word_to_id (dict): Mapeo de palabras a IDs.\n",
    "        length (int, opcional): Longitud de la secuencia a generar. Por defecto es 20.\n",
    "        device (torch.device, opcional): Dispositivo para generar el texto. Por defecto es 'cpu'.\n",
    "        temperature (float, opcional): Controla la aleatoriedad de la predicción. Por defecto es 1.0.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de palabras generadas.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = [id_to_word[start_token]]\n",
    "    input_seq = torch.tensor([[start_token]], dtype=torch.long).to(device)\n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            logits, hidden = model(input_seq, hidden)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(id_to_word.get(next_token, '<unk>'))\n",
    "            input_seq = torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "def main():\n",
    "    # Parámetros\n",
    "    vocab_size = 10000    # Tamaño del vocabulario (se ajustará basado en los datos)\n",
    "    embed_size = 128      # Dimensionalidad de los embeddings\n",
    "    hidden_size = 256     # Dimensionalidad del estado oculto\n",
    "    num_layers = 3        # Número de capas de la LSTM\n",
    "    dropout = 0.5         # Tasa de dropout\n",
    "    sequence_length = 10  # Longitud de la secuencia\n",
    "    batch_size = 32       # Tamaño del lote\n",
    "    num_epochs = 5        # Número de épocas\n",
    "    learning_rate = 0.001 # Tasa de aprendizaje\n",
    "    checkpoint_path = 'stacked_lstm_checkpoint.pth'  # Ruta para guardar el modelo\n",
    "    \n",
    "    # Ejemplo de texto (reemplaza esto con un dataset real para mejores resultados)\n",
    "    sample_text = (\n",
    "        \"So long and thanks for all the fish. \"\n",
    "        \"So long and thanks for all the fish. \"\n",
    "        \"So long and thanks for all the fish.\"\n",
    "    )\n",
    "    \n",
    "    # Crear mappings de palabras a IDs y viceversa\n",
    "    words = sample_text.lower().split()\n",
    "    unique_words = set(words)\n",
    "    word_to_id = {word: idx for idx, word in enumerate(unique_words, start=1)}  # Empezar en 1\n",
    "    word_to_id['<s>'] = 0  # Token de inicio\n",
    "    word_to_id['<unk>'] = len(word_to_id)  # Token desconocido\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "    \n",
    "    vocab_size = len(word_to_id)\n",
    "    \n",
    "    # Crear el dataset y el dataloader\n",
    "    dataset = LanguageModelDataset(text=sample_text, seq_length=sequence_length, word_to_id=word_to_id)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Inicializar el modelo, función de pérdida y optimizador\n",
    "    model = StackedLSTM(vocab_size, embed_size, hidden_size, num_layers, dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Opcional: Cargar un modelo previamente guardado\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        start_epoch, _ = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "    \n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"\\nEpoca {epoch+1}/{num_epochs}\")\n",
    "        train_loss = train_epoch(model, dataloader, criterion, optimizer, device)\n",
    "        print(f\"Entrenamiento - Pérdida: {train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluar en el mismo conjunto de datos (para demostración; usa un conjunto de validación separado en la práctica)\n",
    "        val_loss = evaluate_epoch(model, dataloader, criterion, device)\n",
    "        print(f\"Validación - Pérdida: {val_loss:.4f}\")\n",
    "        \n",
    "        # Guardar el checkpoint\n",
    "        save_checkpoint(model, optimizer, epoch+1, val_loss, checkpoint_path)\n",
    "    \n",
    "    # Generar una secuencia de texto\n",
    "    start_word = '<s>'\n",
    "    start_token = word_to_id.get(start_word, word_to_id['<unk>'])\n",
    "    generated = generate_text(model, start_token, id_to_word, word_to_id, length=10, device=device, temperature=1.0)\n",
    "    print(\"\\nSecuencia generada:\", \" \".join(generated))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e5cdd",
   "metadata": {},
   "source": [
    "**Observación**\n",
    "\n",
    "Para obtener resultados más significativos, es recomendable entrenar el modelo en un corpus de texto más grande y variado. Puedes utilizar datasets como WikiText-2, Penn Treebank, o cualquier otro corpus de texto que prefieras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ea0d5",
   "metadata": {},
   "source": [
    "### Redes recurrentes bidireccionales para clasificación de secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8bca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # Barra de progreso\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Establecer semillas para reproducibilidad\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Verificar dispositivo (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "class BidirectionalLSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de clasificación de secuencias utilizando LSTM bidireccional.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): Tamaño del vocabulario.\n",
    "        embed_size (int): Dimensionalidad de los embeddings.\n",
    "        hidden_size (int): Dimensionalidad del estado oculto de la LSTM.\n",
    "        num_layers (int): Número de capas de LSTM apiladas.\n",
    "        output_size (int): Número de clases para la clasificación.\n",
    "        dropout (float): Tasa de dropout entre capas de LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(BidirectionalLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, \n",
    "                            bidirectional=True, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # *2 por bidireccionalidad\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas de embeddings, LSTM y fully connected usando la inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pase hacia adelante del modelo LSTM bidireccional para clasificación de secuencias.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Secuencia de entrada, forma (batch_size, seq_length).\n",
    "        \n",
    "        Returns:\n",
    "            logits (Tensor): Puntuaciones para cada clase, forma (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        embed = self.embedding(x)  # (batch_size, seq_length, embed_size)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embed)  # lstm_out: (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # Concatenar los estados ocultos finales de ambas direcciones\n",
    "        hidden_forward = hidden[-2, :, :]  # Última capa, dirección forward\n",
    "        hidden_backward = hidden[-1, :, :]  # Última capa, dirección backward\n",
    "        hidden_cat = torch.cat((hidden_forward, hidden_backward), dim=1)  # (batch_size, hidden_size*2)\n",
    "        \n",
    "        out = self.dropout(hidden_cat)\n",
    "        logits = self.fc(out)  # (batch_size, output_size)\n",
    "        return logits\n",
    "\n",
    "class SequenceClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para clasificación de secuencias que maneja la creación de secuencias de entrada y etiquetas.\n",
    "    \n",
    "    Cada muestra consiste en una secuencia de tokens de entrada y una etiqueta de clase.\n",
    "    \n",
    "    Args:\n",
    "        sequences (list of list of int): Lista de secuencias de tokens.\n",
    "        labels (list of int): Lista de etiquetas de clase correspondientes a cada secuencia.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, labels):\n",
    "        assert len(sequences) == len(labels), \"Las secuencias y las etiquetas deben tener la misma longitud.\"\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, clip=5.0):\n",
    "    \"\"\"\n",
    "    Entrena el modelo durante una época.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de entrenamiento.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "        clip (float): Valor máximo para el recorte de gradientes.\n",
    "    \n",
    "    Returns:\n",
    "        float: Pérdida promedio para la época.\n",
    "        float: Exactitud promedio para la época.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress = tqdm(dataloader, desc=\"Entrenando\", leave=False)\n",
    "    \n",
    "    for inputs, targets in progress:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)  # (batch_size, output_size)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Recorte de gradientes\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Cálculo de exactitud\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / (len(dataloader))\n",
    "        accuracy = 100.0 * correct / total\n",
    "        progress.set_postfix(loss=avg_loss, accuracy=f\"{accuracy:.2f}%\")\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    average_accuracy = 100.0 * correct / total\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "def evaluate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo durante una época.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Modelo a evaluar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de evaluación.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "    \n",
    "    Returns:\n",
    "        float: Pérdida promedio para la evaluación.\n",
    "        float: Exactitud promedio para la evaluación.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        progress = tqdm(dataloader, desc=\"Evaluando\", leave=False)\n",
    "        for inputs, targets in progress:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = model(inputs)  # (batch_size, output_size)\n",
    "            loss = criterion(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Cálculo de exactitud\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            accuracy = 100.0 * correct / total\n",
    "            progress.set_postfix(loss=avg_loss, accuracy=f\"{accuracy:.2f}%\")\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    average_accuracy = 100.0 * correct / total\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    \"\"\"\n",
    "    Guarda el estado del modelo y del optimizador.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Modelo a guardar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a guardar.\n",
    "        epoch (int): Época actual.\n",
    "        loss (float): Pérdida actual.\n",
    "        path (str): Ruta donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"Modelo guardado en {path} (Época {epoch}, Pérdida {loss:.4f})\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, path, device):\n",
    "    \"\"\"\n",
    "    Carga el estado del modelo y del optimizador desde un archivo.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Modelo a cargar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a cargar.\n",
    "        path (str): Ruta desde donde se cargará el modelo.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (época, pérdida) cargados desde el checkpoint.\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Modelo cargado desde {path} (Época {epoch}, Pérdida {loss:.4f})\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo {path}. Se iniciará el entrenamiento desde cero.\")\n",
    "        return 0, None\n",
    "\n",
    "def generate_text(model, start_token, id_to_word, word_to_id, length=20, device='cpu', temperature=1.0):\n",
    "    \"\"\"\n",
    "    Genera una secuencia de texto de forma autoregresiva.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Modelo entrenado para generación de texto.\n",
    "        start_token (int): ID del token inicial (por ejemplo, <s>).\n",
    "        id_to_word (dict): Mapeo de IDs a palabras.\n",
    "        word_to_id (dict): Mapeo de palabras a IDs.\n",
    "        length (int, opcional): Longitud de la secuencia a generar. Por defecto es 20.\n",
    "        device (torch.device, opcional): Dispositivo para generar el texto. Por defecto es 'cpu'.\n",
    "        temperature (float, opcional): Controla la aleatoriedad de la predicción. Por defecto es 1.0.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de palabras generadas.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = [id_to_word[start_token]]\n",
    "    input_seq = torch.tensor([[start_token]], dtype=torch.long).to(device)\n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            logits = model(input_seq)  # (batch_size=1, output_size)\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(id_to_word.get(next_token, '<unk>'))\n",
    "            input_seq = torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "def main():\n",
    "    # Parámetros\n",
    "    vocab_size = 10000    # Tamaño del vocabulario\n",
    "    embed_size = 128      # Dimensionalidad de los embeddings\n",
    "    hidden_size = 256     # Dimensionalidad del estado oculto\n",
    "    num_layers = 3        # Número de capas de LSTM apiladas\n",
    "    output_size = 5       # Número de clases para clasificación\n",
    "    dropout = 0.5         # Tasa de dropout\n",
    "    sequence_length = 10  # Longitud de la secuencia\n",
    "    batch_size = 32       # Tamaño del lote\n",
    "    num_epochs = 10       # Número de épocas\n",
    "    learning_rate = 0.001 # Tasa de aprendizaje\n",
    "    checkpoint_path = 'bidirectional_lstm_classifier.pth'  # Ruta para guardar el modelo\n",
    "    \n",
    "    # Ejemplo de texto (reemplaza esto con un dataset real para mejores resultados)\n",
    "    sample_text = (\n",
    "        \"Este es un ejemplo de texto para la clasificación de secuencias. \"\n",
    "        \"Las redes neuronales recurrentes bidireccionales pueden capturar información contextual \"\n",
    "        \"desde ambas direcciones de una secuencia. Esto es útil para tareas como la clasificación \"\n",
    "        \"de texto, donde el significado de una palabra puede depender de las palabras que la preceden \"\n",
    "        \"y la siguen.\"\n",
    "    )\n",
    "    \n",
    "    # Preprocesamiento: Tokenización y creación de vocabulario\n",
    "    words = sample_text.lower().split()\n",
    "    unique_words = set(words)\n",
    "    word_to_id = {word: idx for idx, word in enumerate(unique_words, start=1)}  # Empezar en 1\n",
    "    word_to_id['<s>'] = 0  # Token de inicio\n",
    "    word_to_id['<unk>'] = len(word_to_id)  # Token desconocido\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "    \n",
    "    vocab_size = len(word_to_id)\n",
    "    \n",
    "    # Generar datos de ejemplo (para demostración; usa datos reales en práctica)\n",
    "    num_samples = 1000\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for _ in range(num_samples):\n",
    "        seq = random.randint(0, len(words) - sequence_length - 1)\n",
    "        input_seq = words[seq:seq + sequence_length]\n",
    "        target = random.randint(0, output_size - 1)  # Etiquetas aleatorias\n",
    "        input_ids = [word_to_id.get(word, word_to_id['<unk>']) for word in input_seq]\n",
    "        sequences.append(input_ids)\n",
    "        labels.append(target)\n",
    "    \n",
    "    # Crear el dataset y el dataloader\n",
    "    dataset = SequenceClassificationDataset(sequences, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Inicializar el modelo, función de pérdida y optimizador\n",
    "    model = BidirectionalLSTMClassifier(vocab_size, embed_size, hidden_size, num_layers, output_size, dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Opcional: Cargar un modelo previamente guardado\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        start_epoch, _ = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "    \n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"\\nEpoca {epoch+1}/{num_epochs}\")\n",
    "        train_loss, train_acc = train_epoch(model, dataloader, criterion, optimizer, device)\n",
    "        print(f\"Entrenamiento - Pérdida: {train_loss:.4f}, Exactitud: {train_acc:.2f}%\")\n",
    "        \n",
    "        # Evaluar en el mismo conjunto de datos (para demostración; usa un conjunto de validación separado en la práctica)\n",
    "        val_loss, val_acc = evaluate_epoch(model, dataloader, criterion, device)\n",
    "        print(f\"Validación - Pérdida: {val_loss:.4f}, Exactitud: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Guardar el checkpoint\n",
    "        save_checkpoint(model, optimizer, epoch+1, val_loss, checkpoint_path)\n",
    "    \n",
    "    # Generar una secuencia de texto (opcional; depende de la tarea)\n",
    "    # Aquí, la generación de texto no es directamente aplicable ya que el modelo está diseñado para clasificación.\n",
    "    # Sin embargo, si adaptas el modelo para generación de texto, puedes usar una función similar a la anterior.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8304aee",
   "metadata": {},
   "source": [
    "**Consideraciones adicionales**\n",
    "\n",
    "- En el ejemplo proporcionado, se generan datos aleatorios para la demostración. Para una aplicación real, reemplaza esta parte con un dataset real de clasificación de secuencias. Puedes usar datasets como IMDB para análisis de sentimientos, AG News o cualquier otro dataset relevante.\n",
    "- Actualmente, tanto el entrenamiento como la evaluación se realizan en el mismo conjunto de datos generado aleatoriamente. En una práctica real, deberías separar tus datos en conjuntos de entrenamiento y validación para evaluar el rendimiento del modelo de manera más precisa.\n",
    "- Si trabajas con secuencias de longitud variable, considera usar técnicas de padding y pack_padded_sequence y pad_packed_sequence de PyTorch para manejar eficientemente las secuencias de diferentes longitudes.\n",
    "- Además del dropout, puedes explorar otras técnicas de regularización como la normalización de pesos (Weight Normalization) o el uso de capas adicionales para mejorar la capacidad de generalización del modelo.\n",
    "- Experimenta con diferentes tasas de aprendizaje, tamaños de lote, dimensiones de embeddings y tamaños ocultos para encontrar la mejor configuración para tu tarea específica. Herramientas como Optuna o Ray Tune pueden ayudarte a automatizar la búsqueda de hiperparámetros.\n",
    "- Si adaptas el modelo para tareas generativas, considera técnicas como top-k sampling o nucleus sampling (top-p sampling) para controlar mejor la diversidad y coherencia del texto generado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1768d4b9",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e814b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # Barra de progreso\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Establecer semillas para reproducibilidad\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Verificar dispositivo (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementación personalizada de una celda LSTM.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Tamaño del vector de entrada (dimensionalidad de x_t).\n",
    "        hidden_size (int): Tamaño del estado oculto (dimensionalidad de h_t).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Capa lineal para los gates y el candidato de la celda\n",
    "        self.fc = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de la capa lineal usando la inicialización Xavier uniforme\n",
    "        y los sesgos a cero.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x_t, h_t_minus_1, c_t_minus_1):\n",
    "        \"\"\"\n",
    "        Realiza un pase hacia adelante de la celda LSTM.\n",
    "\n",
    "        Args:\n",
    "            x_t (Tensor): Entrada en el tiempo t, forma (batch_size, input_size).\n",
    "            h_t_minus_1 (Tensor): Estado oculto anterior, forma (batch_size, hidden_size).\n",
    "            c_t_minus_1 (Tensor): Estado de la celda anterior, forma (batch_size, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            h_t (Tensor): Nuevo estado oculto, forma (batch_size, hidden_size).\n",
    "            c_t (Tensor): Nuevo estado de la celda, forma (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        # Concatenar la entrada y el estado oculto anterior\n",
    "        combined = torch.cat((x_t, h_t_minus_1), dim=1)  # (batch_size, input_size + hidden_size)\n",
    "\n",
    "        # Aplicar la capa lineal\n",
    "        gates = self.fc(combined)  # (batch_size, 4 * hidden_size)\n",
    "        i_gate, f_gate, g_gate, o_gate = gates.chunk(4, dim=1)\n",
    "\n",
    "        # Aplicar funciones de activación\n",
    "        i_t = torch.sigmoid(i_gate)\n",
    "        f_t = torch.sigmoid(f_gate)\n",
    "        g_t = torch.tanh(g_gate)\n",
    "        o_t = torch.sigmoid(o_gate)\n",
    "\n",
    "        # Actualizar el estado de la celda\n",
    "        c_t = f_t * c_t_minus_1 + i_t * g_t\n",
    "\n",
    "        # Actualizar el estado oculto\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para manejar secuencias y sus etiquetas.\n",
    "\n",
    "    Args:\n",
    "        sequences (list of list of float): Lista de secuencias de vectores de entrada.\n",
    "        labels (list of int): Lista de etiquetas correspondientes a cada secuencia.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, labels):\n",
    "        assert len(sequences) == len(labels), \"Las secuencias y las etiquetas deben tener la misma longitud.\"\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Secuencia: (seq_length, input_size) -> torch.float32\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo completo que utiliza una celda LSTM personalizada para procesar secuencias y realizar clasificación.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Tamaño del vector de entrada.\n",
    "        hidden_size (int): Tamaño del estado oculto de la LSTM.\n",
    "        output_size (int): Tamaño de la salida (número de clases).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_cell = LSTMCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Mapea el estado oculto a logits de clases\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de la capa fully connected usando la inicialización Xavier uniforme\n",
    "        y los sesgos a cero.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, h_0=None, c_0=None):\n",
    "        \"\"\"\n",
    "        Realiza un pase hacia adelante del modelo LSTM.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Entrada, forma (batch_size, seq_length, input_size).\n",
    "            h_0 (Tensor, opcional): Estado oculto inicial, forma (batch_size, hidden_size).\n",
    "            c_0 (Tensor, opcional): Estado de la celda inicial, forma (batch_size, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            logits (Tensor): Puntuaciones para cada clase, forma (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        if h_0 is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        else:\n",
    "            h_t = h_0\n",
    "        if c_0 is None:\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        else:\n",
    "            c_t = c_0\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]  # (batch_size, input_size)\n",
    "            h_t, c_t = self.lstm_cell(x_t, h_t, c_t)\n",
    "\n",
    "        logits = self.fc(h_t)  # (batch_size, output_size)\n",
    "        return logits\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, epoch, total_epochs, clip=5.0):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento para una época.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de entrenamiento.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "        epoch (int): Número de la época actual.\n",
    "        total_epochs (int): Número total de épocas.\n",
    "        clip (float, opcional): Valor máximo para el recorte de gradientes. Por defecto es 5.0.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "\n",
    "    for batch_idx, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device).unsqueeze(1)  # targets: [batch_size, 1]\n",
    "\n",
    "        # Inicializar los estados ocultos y de la celda\n",
    "        h_t = torch.zeros(inputs.size(0), model.hidden_size, device=device)\n",
    "        c_t = torch.zeros(inputs.size(0), model.hidden_size, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Procesar cada paso temporal\n",
    "        for t in range(inputs.size(1)):\n",
    "            x_t = inputs[:, t, :]  # (batch_size, input_size)\n",
    "            h_t, c_t = model.lstm_cell(x_t, h_t, c_t)\n",
    "\n",
    "        # Calcular la salida\n",
    "        logits = model.fc(h_t)  # (batch_size, output_size)\n",
    "\n",
    "        # Calcular la pérdida\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Recorte de gradientes para evitar explosiones\n",
    "        torch.nn.utils.clip_grad_norm_(model.lstm_cell.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Cálculo de exactitud\n",
    "        predicted = torch.round(torch.sigmoid(logits))  # Para BCEWithLogitsLoss\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        # Actualizar la barra de progreso\n",
    "        average_loss = epoch_loss / (batch_idx + 1)\n",
    "        accuracy = 100.0 * correct / total\n",
    "        progress_bar.set_postfix(loss=average_loss, accuracy=f\"{accuracy:.2f}%\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs} - Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Función de evaluación para calcular la pérdida en el conjunto de validación.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a evaluar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de validación.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "\n",
    "    Returns:\n",
    "        float: Pérdida promedio en el conjunto de validación.\n",
    "        float: Exactitud promedio en el conjunto de validación.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluando\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).unsqueeze(1)\n",
    "\n",
    "            # Inicializar los estados ocultos y de la celda\n",
    "            h_t = torch.zeros(inputs.size(0), model.hidden_size, device=device)\n",
    "            c_t = torch.zeros(inputs.size(0), model.hidden_size, device=device)\n",
    "\n",
    "            # Procesar cada paso temporal\n",
    "            for t in range(inputs.size(1)):\n",
    "                x_t = inputs[:, t, :]  # (batch_size, input_size)\n",
    "                h_t, c_t = model.lstm_cell(x_t, h_t, c_t)\n",
    "\n",
    "            # Calcular la salida\n",
    "            logits = model.fc(h_t)  # (batch_size, output_size)\n",
    "\n",
    "            # Calcular la pérdida\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Cálculo de exactitud\n",
    "            predicted = torch.round(torch.sigmoid(logits))  # Para BCEWithLogitsLoss\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "            # Actualizar la barra de progreso\n",
    "            average_loss = epoch_loss / len(dataloader)\n",
    "            accuracy = 100.0 * correct / total\n",
    "            progress_bar.set_postfix(loss=average_loss, accuracy=f\"{accuracy:.2f}%\")\n",
    "\n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    average_accuracy = 100.0 * correct / total\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "def save_model(model, optimizer, epoch, loss, path):\n",
    "    \"\"\"\n",
    "    Guarda el estado del modelo y del optimizador.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a guardar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a guardar.\n",
    "        epoch (int): Número de la época actual.\n",
    "        loss (float): Pérdida actual.\n",
    "        path (str): Ruta donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "def load_model(model, optimizer, path, device):\n",
    "    \"\"\"\n",
    "    Carga el estado del modelo y del optimizador desde un archivo.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a cargar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a cargar.\n",
    "        path (str): Ruta desde donde se cargará el modelo.\n",
    "        device (torch.device): Dispositivo (CPU o GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (época desde la cual se reanudó el entrenamiento, pérdida en la última época registrada)\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Modelo cargado desde {path} (Época {epoch}, Pérdida {loss})\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo {path}. Se iniciará el entrenamiento desde cero.\")\n",
    "        return 0, None\n",
    "\n",
    "def main():\n",
    "    # Parámetros\n",
    "    input_size = 10        # Tamaño del vector de entrada\n",
    "    hidden_size = 20       # Tamaño del estado oculto\n",
    "    sequence_length = 5    # Longitud de la secuencia\n",
    "    batch_size = 32        # Tamaño del lote\n",
    "    num_epochs = 5         # Número de épocas\n",
    "    learning_rate = 0.001  # Tasa de aprendizaje\n",
    "    save_path = 'lstm_cell.pth'  # Ruta para guardar el modelo\n",
    "\n",
    "    # Crear datos de ejemplo (para demostración; usa datos reales en práctica)\n",
    "    num_samples = 1000\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for _ in range(num_samples):\n",
    "        seq = torch.randn(sequence_length, input_size).tolist()  # Secuencia aleatoria\n",
    "        label = random.randint(0, 1)  # Etiqueta aleatoria (por ejemplo, clasificación binaria)\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Crear el dataset y el dataloader\n",
    "    dataset = SequenceDataset(sequences, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inicializar el modelo, función de pérdida y optimizador\n",
    "    model = LSTMModel(input_size, hidden_size, output_size=1).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Función de pérdida para clasificación binaria\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Opcional: Cargar un modelo previamente guardado\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(save_path):\n",
    "        start_epoch, _ = load_model(model, optimizer, save_path, device)\n",
    "\n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"\\nEpoca {epoch+1}/{num_epochs}\")\n",
    "        train(model, dataloader, criterion, optimizer, device, epoch, num_epochs)\n",
    "        val_loss, val_accuracy = evaluate(model, dataloader, criterion, device)\n",
    "        print(f\"Validación - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Guardar el modelo después de cada época\n",
    "        save_model(model, optimizer, epoch + 1, val_loss, save_path)\n",
    "\n",
    "    # Ejemplo de uso: realizar una predicción\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generar una secuencia de entrada aleatoria\n",
    "        test_seq = torch.randn(1, sequence_length, input_size).to(device)\n",
    "        logits = model(test_seq)  # (1, 1)\n",
    "        # Aplicar una función de activación (sigmoide) para obtener la probabilidad\n",
    "        prob = torch.sigmoid(logits)\n",
    "        prediction = torch.round(prob)\n",
    "        print(f\"\\nPredicción de la secuencia de prueba: {prediction.item()}, Probabilidad: {prob.item():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa620cc",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "* En el ejemplo proporcionado, se generan datos aleatorios para la demostración. Para una aplicación real, deberías reemplazar esta parte con datos auténticos que representen la tarea de clasificación que deseas realizar.\n",
    "* Considera normalizar tus datos de entrada para mejorar el rendimiento del modelo.\n",
    "* Además del recorte de gradientes y el dropout ya implementado, podrías explorar otras técnicas de regularización como la normalización de pesos o el uso de capas adicionales para mejorar la capacidad de generalización del modelo.\n",
    "* Experimenta con diferentes tasas de aprendizaje, tamaños de lote, dimensiones de embeddings y tamaños ocultos para encontrar la mejor configuración para tu tarea específica. Herramientas como Optuna o Ray Tune pueden ayudarte a automatizar la búsqueda de hiperparámetros.\n",
    "* Si trabajas con secuencias de longitud variable, considera implementar técnicas de padding y usar pack_padded_sequence y pad_packed_sequence de PyTorch para manejar eficientemente las secuencias de diferentes longitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0febe",
   "metadata": {},
   "source": [
    "### NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d28d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # Barra de progreso\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Establecer semillas para reproducibilidad\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Verificar dispositivo (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Crear un vocabulario de juguete para Inglés y Español\n",
    "source_vocab = {'<pad>':0, 'the':1, 'green':2, 'witch':3, 'arrived':4}\n",
    "target_vocab = {'<pad>':0, '<s>':1, 'llegó':2, 'la':3, 'bruja':4, 'verde':5, '</s>':6}\n",
    "source_vocab_size = len(source_vocab)  # 5\n",
    "target_vocab_size = len(target_vocab)  # 7\n",
    "\n",
    "# Diccionarios inversos para decodificación\n",
    "source_vocab_inv = {v: k for k, v in source_vocab.items()}\n",
    "target_vocab_inv = {v: k for k, v in target_vocab.items()}\n",
    "\n",
    "# Convertir oraciones a secuencias de IDs de tokens\n",
    "src_sentence = [source_vocab[word] for word in [\"the\", \"green\", \"witch\", \"arrived\"]]\n",
    "trg_sentence = [target_vocab['<s>']] + [target_vocab[word] for word in [\"llegó\", \"la\", \"bruja\", \"verde\", \"</s>\"]]\n",
    "\n",
    "# Definir parámetros del modelo\n",
    "embed_size = 32\n",
    "hidden_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Modelo de Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embed_size]\n",
    "        outputs, hidden = self.rnn(embedded)  # outputs: [batch_size, seq_length, hidden_size], hidden: [1, batch_size, hidden_size]\n",
    "        return outputs, hidden\n",
    "\n",
    "# Modelo de Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = x.unsqueeze(1)  # [batch_size] -> [batch_size, 1]\n",
    "        embedded = self.embedding(x)  # [batch_size, 1, embed_size]\n",
    "        output, hidden = self.rnn(embedded, hidden)  # output: [batch_size, 1, hidden_size]\n",
    "        prediction = self.fc(output.squeeze(1))  # [batch_size, output_size]\n",
    "        return prediction, hidden\n",
    "\n",
    "# Modelo NMT con encoder y decoder\n",
    "class NMTModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(NMTModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [batch_size, src_len]\n",
    "            trg: [batch_size, trg_len]\n",
    "            teacher_forcing_ratio: probabilidad de usar el valor real como siguiente input en lugar de la predicción\n",
    "        Returns:\n",
    "            outputs: [batch_size, trg_len, output_dim]\n",
    "        \"\"\"\n",
    "        batch_size = trg.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        # Inicializar el tensor de outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n",
    "        \n",
    "        # Obtener las salidas del encoder\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        # El primer input al decoder es el token <s>\n",
    "        input = trg[:,0]  # [batch_size]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # Pasar el input y el estado oculto actual al decoder\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            # Guardar la predicción\n",
    "            outputs[:, t] = output\n",
    "            # Decidir si usar teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # Obtener la siguiente entrada\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Dataset personalizado para pares de oración\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, trg_sentences):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_sentences: list of list of int\n",
    "            trg_sentences: list of list of int\n",
    "        \"\"\"\n",
    "        assert len(src_sentences) == len(trg_sentences), \"Las oraciones fuente y destino deben tener la misma longitud.\"\n",
    "        self.src_sentences = src_sentences\n",
    "        self.trg_sentences = trg_sentences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src_sentences[idx], dtype=torch.long), torch.tensor(self.trg_sentences[idx], dtype=torch.long)\n",
    "\n",
    "# Crear múltiples copias del mismo par de oraciones para simular datos\n",
    "num_samples = 1000\n",
    "src_sentences = [src_sentence for _ in range(num_samples)]\n",
    "trg_sentences = [trg_sentence for _ in range(num_samples)]\n",
    "\n",
    "# Crear el dataset y el dataloader\n",
    "dataset = TranslationDataset(src_sentences, trg_sentences)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Instanciar modelos, optimizador y función de pérdida\n",
    "encoder = Encoder(source_vocab_size, embed_size, hidden_size)\n",
    "decoder = Decoder(target_vocab_size, embed_size, hidden_size)\n",
    "model = NMTModel(encoder, decoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs, clip=1.0):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    \n",
    "    for batch_idx, (src, trg) in progress_bar:\n",
    "        src = src.to(device)  # [batch_size, src_len]\n",
    "        trg = trg.to(device)  # [batch_size, trg_len]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Obtener las predicciones del modelo\n",
    "        output = model(src, trg)  # [batch_size, trg_len, trg_vocab_size]\n",
    "        \n",
    "        # Reorganizar las salidas para calcular la pérdida\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:,1:].reshape(-1, output_dim)  # [batch_size * (trg_len -1), trg_vocab_size]\n",
    "        trg = trg[:,1:].reshape(-1)  # [batch_size * (trg_len -1)]\n",
    "        \n",
    "        # Calcular la pérdida\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Recorte de gradientes para evitar explosiones\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Cálculo de exactitud\n",
    "        predicted = output.argmax(1)  # [batch_size * (trg_len -1)]\n",
    "        correct += (predicted == trg).sum().item()\n",
    "        total += trg.size(0)\n",
    "        \n",
    "        # Actualizar la barra de progreso\n",
    "        average_loss = epoch_loss / (batch_idx +1)\n",
    "        accuracy = 100.0 * correct / total\n",
    "        progress_bar.set_postfix(loss=average_loss, accuracy=f\"{accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{total_epochs} - Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Función de evaluación\n",
    "def evaluate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluando\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for src, trg in progress_bar:\n",
    "            src = src.to(device)  # [batch_size, src_len]\n",
    "            trg = trg.to(device)  # [batch_size, trg_len]\n",
    "            \n",
    "            # Obtener las predicciones del modelo\n",
    "            output = model(src, trg, teacher_forcing_ratio=0.0)  # [batch_size, trg_len, trg_vocab_size]\n",
    "            \n",
    "            # Reorganizar las salidas para calcular la pérdida\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:,1:].reshape(-1, output_dim)  # [batch_size * (trg_len -1), trg_vocab_size]\n",
    "            trg = trg[:,1:].reshape(-1)  # [batch_size * (trg_len -1)]\n",
    "            \n",
    "            # Calcular la pérdida\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Cálculo de exactitud\n",
    "            predicted = output.argmax(1)  # [batch_size * (trg_len -1)]\n",
    "            correct += (predicted == trg).sum().item()\n",
    "            total += trg.size(0)\n",
    "            \n",
    "            # Actualizar la barra de progreso\n",
    "            average_loss = epoch_loss / len(dataloader)\n",
    "            accuracy = 100.0 * correct / total\n",
    "            progress_bar.set_postfix(loss=average_loss, accuracy=f\"{accuracy:.2f}%\")\n",
    "    \n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    average_accuracy = 100.0 * correct / total\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "# Función para guardar el modelo\n",
    "def save_model(model, optimizer, epoch, loss, path):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "# Función para cargar el modelo\n",
    "def load_model(model, optimizer, path, device):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Modelo cargado desde {path} (Época {epoch}, Pérdida {loss})\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo {path}. Se iniciará el entrenamiento desde cero.\")\n",
    "        return 0, None\n",
    "\n",
    "# Función de traducción (inferencia)\n",
    "def translate(model, src_sentence, target_vocab_inv, device, max_len=10):\n",
    "    model.eval()\n",
    "    src_tensor = torch.LongTensor([src_sentence]).to(device)  # [1, src_len]\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        input = torch.LongTensor([target_vocab['<s>']]).to(device)  # [1]\n",
    "        translated_sentence = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            output, hidden = model.decoder(input, hidden)  # output: [1, target_vocab_size]\n",
    "            top1 = output.argmax(1).item()\n",
    "            if top1 == target_vocab['</s>']:\n",
    "                break\n",
    "            translated_sentence.append(target_vocab_inv[top1])\n",
    "            input = torch.LongTensor([top1]).to(device)\n",
    "    \n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "def main():\n",
    "    # Parámetros\n",
    "    embed_size = 32\n",
    "    hidden_size = 64\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 1000\n",
    "    save_path = 'nmt_model.pth'\n",
    "    \n",
    "    # Crear datos de ejemplo (usando múltiples copias de la misma oración para simular datos)\n",
    "    num_samples = 1000\n",
    "    src_sentences = [src_sentence for _ in range(num_samples)]\n",
    "    trg_sentences = [trg_sentence for _ in range(num_samples)]\n",
    "    \n",
    "    # Crear el dataset y el dataloader\n",
    "    dataset = TranslationDataset(src_sentences, trg_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Instanciar modelos, optimizador y función de pérdida\n",
    "    encoder = Encoder(source_vocab_size, embed_size, hidden_size)\n",
    "    decoder = Decoder(target_vocab_size, embed_size, hidden_size)\n",
    "    model = NMTModel(encoder, decoder).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignora el padding\n",
    "    \n",
    "    # Opcional: Cargar un modelo previamente guardado\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(save_path):\n",
    "        start_epoch, _ = load_model(model, optimizer, save_path, device)\n",
    "    \n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"\\nEpoca {epoch+1}/{num_epochs}\")\n",
    "        train_epoch(model, dataloader, criterion, optimizer, device, epoch, num_epochs)\n",
    "        val_loss, val_accuracy = evaluate_epoch(model, dataloader, criterion, device)\n",
    "        print(f\"Validación - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "        # Guardar el modelo después de cada época\n",
    "        save_model(model, optimizer, epoch + 1, val_loss, save_path)\n",
    "    \n",
    "    # Ejemplo de uso: realizar una predicción\n",
    "    translated_sentence = translate(model, src_sentence, target_vocab_inv, device)\n",
    "    print(\"Oracion traducida:\", translated_sentence)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b48d73",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "- Dado que  se está utilizando datos sintéticos (múltiples copias de un solo par de oraciones), el modelo puede aprender rápidamente a mapear correctamente las entradas a las salidas. Sin embargo, en aplicaciones reales, necesitarías un conjunto de datos mucho más grande y variado para que el modelo generalice bien.\n",
    "- Para aplicaciones prácticas, considera utilizar conjuntos de datos reales como IWSLT o WMT para entrenar tu modelo de traducción.\n",
    "- Si trabajas con secuencias de diferentes longitudes, implementa técnicas de padding y utiliza pack_padded_sequence y pad_packed_sequence de PyTorch para optimizar el procesamiento.\n",
    "- Experimenta con diferentes tasas de aprendizaje, tamaños de embedding, tamaños ocultos, y proporciones de teacher forcing para mejorar el rendimiento del modelo.\n",
    "- Además de la pérdida y la exactitud, considera implementar métricas como BLEU para evaluar la calidad de las traducciones\n",
    "- Explora arquitecturas más avanzadas como Transformer, que han demostrado un rendimiento superior en tareas de traducción automática.\n",
    "- Implementa técnicas de regularización como dropout para evitar el sobreajuste, especialmente cuando trabajas con conjuntos de datos más grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb2a06",
   "metadata": {},
   "source": [
    "### Mecanismo de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # Barra de progreso\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Establecer semillas para reproducibilidad\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Verificar dispositivo (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Crear un vocabulario de juguete para Inglés y Español\n",
    "source_vocab = {'<pad>':0, '<s>':1, '</s>':2, 'the':3, 'green':4, 'witch':5, 'arrived':6}\n",
    "target_vocab = {'<pad>':0, '<s>':1, '</s>':2, 'llegó':3, 'la':4, 'bruja':5, 'verde':6}\n",
    "source_vocab_size = len(source_vocab)  # 7\n",
    "target_vocab_size = len(target_vocab)  # 7\n",
    "\n",
    "# Diccionarios inversos para decodificación\n",
    "source_vocab_inv = {v: k for k, v in source_vocab.items()}\n",
    "target_vocab_inv = {v: k for k, v in target_vocab.items()}\n",
    "\n",
    "# Convertir oraciones a secuencias de IDs de tokens\n",
    "def tokenize(sentence, vocab):\n",
    "    return [vocab['<s>']] + [vocab[word] for word in sentence.split()] + [vocab['</s>']]\n",
    "\n",
    "src_sentence = \"the green witch arrived\"\n",
    "trg_sentence = \"llegó la bruja verde\"\n",
    "\n",
    "src_ids = tokenize(src_sentence, source_vocab)  # [1, 3, 4, 5, 6, 2]\n",
    "trg_ids = tokenize(trg_sentence, target_vocab)  # [1, 3, 4, 5, 6, 2]\n",
    "\n",
    "print(f\"Source IDs: {src_ids}\")\n",
    "print(f\"Target IDs: {trg_ids}\")\n",
    "\n",
    "# Modelo de Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Encoder con GRU.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Tamaño del vocabulario de entrada.\n",
    "            embed_size (int): Tamaño de los embeddings.\n",
    "            hidden_size (int): Tamaño del estado oculto de la GRU.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pase hacia adelante del encoder.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Secuencia de entrada, forma (batch_size, seq_length).\n",
    "\n",
    "        Returns:\n",
    "            outputs (Tensor): Salidas de todos los pasos temporales, forma (batch_size, seq_length, hidden_size).\n",
    "            hidden (Tensor): Último estado oculto, forma (1, batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embed_size]\n",
    "        outputs, hidden = self.rnn(embedded)  # outputs: [batch_size, seq_length, hidden_size], hidden: [1, batch_size, hidden_size]\n",
    "        return outputs, hidden\n",
    "\n",
    "# Mecanismo de Atención\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        \"\"\"\n",
    "        Mecanismo de atención.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): Tamaño del estado oculto.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Cálculo de pesos de atención.\n",
    "\n",
    "        Args:\n",
    "            hidden (Tensor): Estado oculto actual del decoder, forma (batch_size, hidden_size).\n",
    "            encoder_outputs (Tensor): Salidas del encoder, forma (batch_size, seq_len, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            attention (Tensor): Pesos de atención normalizados, forma (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # [batch_size, seq_len, hidden_size]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, seq_len, hidden_size]\n",
    "        attention = self.v(energy).squeeze(2)  # [batch_size, seq_len]\n",
    "        return torch.softmax(attention, dim=1)  # [batch_size, seq_len]\n",
    "\n",
    "# Modelo de Decoder con Atención\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Decoder con atención y GRU.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): Tamaño del vocabulario de salida.\n",
    "            embed_size (int): Tamaño de los embeddings.\n",
    "            hidden_size (int): Tamaño del estado oculto de la GRU.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size + embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Pase hacia adelante del decoder.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Token actual, forma (batch_size).\n",
    "            hidden (Tensor): Estado oculto anterior, forma (batch_size, hidden_size).\n",
    "            encoder_outputs (Tensor): Salidas del encoder, forma (batch_size, seq_len, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): Logits para el siguiente token, forma (batch_size, output_size).\n",
    "            hidden (Tensor): Nuevo estado oculto, forma (batch_size, hidden_size).\n",
    "            attn_weights (Tensor): Pesos de atención, forma (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # [batch_size, embed_size]\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)  # [batch_size, seq_len]\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, hidden_size]\n",
    "        attn_applied = attn_applied.squeeze(1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, attn_applied), dim=1).unsqueeze(1)  # [batch_size, 1, embed_size + hidden_size]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))  # output: [batch_size, 1, hidden_size]\n",
    "        output = output.squeeze(1)  # [batch_size, hidden_size]\n",
    "        output = self.fc(torch.cat((output, attn_applied), dim=1))  # [batch_size, output_size]\n",
    "        \n",
    "        return output, hidden.squeeze(0), attn_weights\n",
    "\n",
    "# Modelo Seq2Seq con Encoder y Decoder\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \"\"\"\n",
    "        Modelo Seq2Seq que integra el encoder y el decoder.\n",
    "\n",
    "        Args:\n",
    "            encoder (Encoder): Modelo de encoder.\n",
    "            decoder (Decoder): Modelo de decoder.\n",
    "            device (torch.device): Dispositivo (CPU/GPU).\n",
    "        \"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Pase hacia adelante del modelo Seq2Seq.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Secuencia de entrada, forma (batch_size, src_len).\n",
    "            trg (Tensor): Secuencia de salida, forma (batch_size, trg_len).\n",
    "            teacher_forcing_ratio (float): Proporción de uso de teacher forcing.\n",
    "\n",
    "        Returns:\n",
    "            outputs (Tensor): Logits para cada token de la secuencia de salida, forma (batch_size, trg_len, output_size).\n",
    "        \"\"\"\n",
    "        batch_size = trg.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        # Inicializar el tensor de salidas\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Obtener las salidas del encoder\n",
    "        encoder_outputs, hidden = self.encoder(src)  # hidden: [1, batch_size, hidden_size]\n",
    "        hidden = hidden.squeeze(0)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # El primer input al decoder es el token <s>\n",
    "        input = trg[:,0]  # [batch_size]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, attn_weights = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Dataset personalizado para pares de oración\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, trg_sentences):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_sentences (list of list of int): Lista de secuencias de entrada.\n",
    "            trg_sentences (list of list of int): Lista de secuencias de salida.\n",
    "        \"\"\"\n",
    "        assert len(src_sentences) == len(trg_sentences), \"Las oraciones fuente y destino deben tener la misma longitud.\"\n",
    "        self.src_sentences = src_sentences\n",
    "        self.trg_sentences = trg_sentences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src_sentences[idx], dtype=torch.long), torch.tensor(self.trg_sentences[idx], dtype=torch.long)\n",
    "\n",
    "# Función para visualizar los pesos de atención\n",
    "def visualize_attention(attn_weights, src_sentence, trg_sentence, target_vocab_inv):\n",
    "    \"\"\"\n",
    "    Visualiza los pesos de atención.\n",
    "\n",
    "    Args:\n",
    "        attn_weights (Tensor): Pesos de atención, forma (trg_len, src_len).\n",
    "        src_sentence (list of str): Oración de entrada.\n",
    "        trg_sentence (list of str): Oración de salida.\n",
    "        target_vocab_inv (dict): Diccionario inverso del vocabulario de destino.\n",
    "    \"\"\"\n",
    "    attn_weights = attn_weights.cpu().detach().numpy()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attn_weights, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticklabels([''] + src_sentence, rotation=90)\n",
    "    ax.set_yticklabels([''] + trg_sentence)\n",
    "\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Función de entrenamiento por época\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs, clip=1.0):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento para una época.\n",
    "\n",
    "    Args:\n",
    "        model (Seq2Seq): Modelo a entrenar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de entrenamiento.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador.\n",
    "        device (torch.device): Dispositivo (CPU/GPU).\n",
    "        epoch (int): Número de la época actual.\n",
    "        total_epochs (int): Número total de épocas.\n",
    "        clip (float, opcional): Valor máximo para el recorte de gradientes. Por defecto es 1.0.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    \n",
    "    for batch_idx, (src, trg) in progress_bar:\n",
    "        src = src.to(device)  # [batch_size, src_len]\n",
    "        trg = trg.to(device)  # [batch_size, trg_len]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Obtener las predicciones del modelo\n",
    "        output = model(src, trg)  # [batch_size, trg_len, output_size]\n",
    "        \n",
    "        # Reorganizar las salidas para calcular la pérdida\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:,1:].reshape(-1, output_dim)  # [batch_size * (trg_len -1), output_size]\n",
    "        trg = trg[:,1:].reshape(-1)  # [batch_size * (trg_len -1)]\n",
    "        \n",
    "        # Calcular la pérdida\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Recorte de gradientes para evitar explosiones\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Cálculo de exactitud\n",
    "        predicted = output.argmax(1)  # [batch_size * (trg_len -1)]\n",
    "        correct += (predicted == trg).sum().item()\n",
    "        total += trg.size(0)\n",
    "        \n",
    "        # Actualizar la barra de progreso\n",
    "        average_loss = epoch_loss / (batch_idx +1)\n",
    "        accuracy = 100.0 * correct / total\n",
    "        progress_bar.set_postfix(loss=average_loss, accuracy=f\"{accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{total_epochs} - Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Función de evaluación por época\n",
    "def evaluate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Función de evaluación para calcular la pérdida y exactitud en el conjunto de validación.\n",
    "\n",
    "    Args:\n",
    "        model (Seq2Seq): Modelo a evaluar.\n",
    "        dataloader (DataLoader): DataLoader que proporciona los datos de validación.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        device (torch.device): Dispositivo (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        float: Pérdida promedio en el conjunto de validación.\n",
    "        float: Exactitud promedio en el conjunto de validación.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluando\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for src, trg in progress_bar:\n",
    "            src = src.to(device)  # [batch_size, src_len]\n",
    "            trg = trg.to(device)  # [batch_size, trg_len]\n",
    "            \n",
    "            # Obtener las predicciones del modelo\n",
    "            output = model(src, trg, teacher_forcing_ratio=0.0)  # [batch_size, trg_len, output_size]\n",
    "            \n",
    "            # Reorganizar las salidas para calcular la pérdida\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:,1:].reshape(-1, output_dim)  # [batch_size * (trg_len -1), output_size]\n",
    "            trg = trg[:,1:].reshape(-1)  # [batch_size * (trg_len -1)]\n",
    "            \n",
    "            # Calcular la pérdida\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Cálculo de exactitud\n",
    "            predicted = output.argmax(1)  # [batch_size * (trg_len -1)]\n",
    "            correct += (predicted == trg).sum().item()\n",
    "            total += trg.size(0)\n",
    "            \n",
    "            # Actualizar la barra de progreso\n",
    "            average_loss = epoch_loss / len(dataloader)\n",
    "            accuracy = 100.0 * correct / total\n",
    "            progress_bar.set_postfix(loss=average_loss, accuracy=f\"{accuracy:.2f}%\")\n",
    "    \n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    average_accuracy = 100.0 * correct / total\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "# Función para guardar el modelo\n",
    "def save_model(model, optimizer, epoch, loss, path):\n",
    "    \"\"\"\n",
    "    Guarda el estado del modelo y del optimizador.\n",
    "\n",
    "    Args:\n",
    "        model (Seq2Seq): Modelo a guardar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a guardar.\n",
    "        epoch (int): Número de la época actual.\n",
    "        loss (float): Pérdida actual.\n",
    "        path (str): Ruta donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "# Función para cargar el modelo\n",
    "def load_model(model, optimizer, path, device):\n",
    "    \"\"\"\n",
    "    Carga el estado del modelo y del optimizador desde un archivo.\n",
    "\n",
    "    Args:\n",
    "        model (Seq2Seq): Modelo a cargar.\n",
    "        optimizer (torch.optim.Optimizer): Optimizador a cargar.\n",
    "        path (str): Ruta desde donde se cargará el modelo.\n",
    "        device (torch.device): Dispositivo (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (época desde la cual se reanudó el entrenamiento, pérdida en la última época registrada)\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Modelo cargado desde {path} (Época {epoch}, Pérdida {loss})\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo {path}. Se iniciará el entrenamiento desde cero.\")\n",
    "        return 0, None\n",
    "\n",
    "# Función de traducción (inferencia)\n",
    "def translate_sentence(model, src_sentence, source_vocab, target_vocab_inv, device, max_len=10):\n",
    "    \"\"\"\n",
    "    Traduce una oración fuente utilizando el modelo entrenado.\n",
    "\n",
    "    Args:\n",
    "        model (Seq2Seq): Modelo entrenado.\n",
    "        src_sentence (str): Oración de entrada en inglés.\n",
    "        source_vocab (dict): Vocabulario de fuente (inglés).\n",
    "        target_vocab_inv (dict): Vocabulario inverso de destino (español).\n",
    "        device (torch.device): Dispositivo (CPU/GPU).\n",
    "        max_len (int, opcional): Longitud máxima de la traducción. Por defecto es 10.\n",
    "\n",
    "    Returns:\n",
    "        str: Oración traducida en español.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = src_sentence.split()\n",
    "    src_ids = [source_vocab['<s>']] + [source_vocab.get(word, source_vocab['<pad>']) for word in tokens] + [source_vocab['</s>']]\n",
    "    src_tensor = torch.LongTensor(src_ids).unsqueeze(0).to(device)  # [1, src_len]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        hidden = hidden.squeeze(0)  # [1, hidden_size]\n",
    "        input = torch.LongTensor([target_vocab['<s>']]).to(device)  # [1]\n",
    "        translated_sentence = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            output, hidden, attn_weights = model.decoder(input, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1).item()\n",
    "            if top1 == target_vocab['</s>']:\n",
    "                break\n",
    "            translated_sentence.append(target_vocab_inv[top1])\n",
    "            input = torch.LongTensor([top1]).to(device)\n",
    "    \n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "# Visualizar pesos de atención para una traducción específica\n",
    "def visualize_attention_weights(model, src_sentence, trg_sentence, source_vocab_inv, target_vocab_inv, device):\n",
    "    \"\"\"\n",
    "    Visualiza los pesos de atención para una oración específica.\n",
    "\n",
    "    Args:\n",
    "        model (Seq2Seq): Modelo entrenado.\n",
    "        src_sentence (str): Oración de entrada en inglés.\n",
    "        trg_sentence (str): Oración de salida en español.\n",
    "        source_vocab_inv (dict): Vocabulario inverso de fuente.\n",
    "        target_vocab_inv (dict): Vocabulario inverso de destino.\n",
    "        device (torch.device): Dispositivo (CPU/GPU).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = src_sentence.split()\n",
    "    src_ids = [source_vocab['<s>']] + [source_vocab.get(word, source_vocab['<pad>']) for word in tokens] + [source_vocab['</s>']]\n",
    "    src_tensor = torch.LongTensor(src_ids).unsqueeze(0).to(device)  # [1, src_len]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        hidden = hidden.squeeze(0)  # [1, hidden_size]\n",
    "        input = torch.LongTensor([target_vocab['<s>']]).to(device)  # [1]\n",
    "        translated_sentence = []\n",
    "        attn_weights_all = []\n",
    "        \n",
    "        for _ in range(len(trg_sentence.split()) + 2):  # +2 for <s> and </s>\n",
    "            output, hidden, attn_weights = model.decoder(input, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1).item()\n",
    "            if top1 == target_vocab['</s>']:\n",
    "                break\n",
    "            translated_sentence.append(target_vocab_inv[top1])\n",
    "            attn_weights_all.append(attn_weights.cpu().numpy())\n",
    "            input = torch.LongTensor([top1]).to(device)\n",
    "    \n",
    "    # Convertir a matriz de atención\n",
    "    attn_matrix = np.stack(attn_weights_all, axis=0)  # [trg_len, src_len]\n",
    "    \n",
    "    # Obtener las palabras\n",
    "    src_words = [source_vocab_inv[id] for id in src_ids]\n",
    "    trg_words = [target_vocab_inv[id] for id in [target_vocab['<s>']] + [target_vocab.get(word, target_vocab['<pad>']) for word in translated_sentence] + [target_vocab['</s>']]]\n",
    "    \n",
    "    # Visualizar\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    cax = ax.matshow(attn_matrix, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticklabels([''] + src_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + translated_sentence)\n",
    "\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Función principal\n",
    "def main():\n",
    "    # Parámetros\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 1000\n",
    "    save_path = 'seq2seq_attention.pth'\n",
    "    \n",
    "    # Crear datos de ejemplo (usando múltiples copias del par de oraciones para simular datos)\n",
    "    num_samples = 1000\n",
    "    src_sentences = [src_ids for _ in range(num_samples)]\n",
    "    trg_sentences = [trg_ids for _ in range(num_samples)]\n",
    "    \n",
    "    # Crear el dataset y el dataloader\n",
    "    dataset = TranslationDataset(src_sentences, trg_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Instanciar modelos, optimizador y función de pérdida\n",
    "    encoder = Encoder(source_vocab_size, embed_size, hidden_size)\n",
    "    decoder = Decoder(target_vocab_size, embed_size, hidden_size)\n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=source_vocab['<pad>'])\n",
    "    \n",
    "    # Opcional: Cargar un modelo previamente guardado\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(save_path):\n",
    "        start_epoch, _ = load_model(model, optimizer, save_path, device)\n",
    "    \n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"\\nEpoca {epoch+1}/{num_epochs}\")\n",
    "        train_epoch(model, dataloader, criterion, optimizer, device, epoch, num_epochs)\n",
    "        val_loss, val_accuracy = evaluate_epoch(model, dataloader, criterion, device)\n",
    "        print(f\"Validación - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Guardar el modelo después de cada época\n",
    "        save_model(model, optimizer, epoch + 1, val_loss, save_path)\n",
    "        \n",
    "        # Opcional: Visualizar atención cada 100 épocas\n",
    "        if (epoch +1) % 100 == 0:\n",
    "            visualize_attention_weights(model, src_sentence, trg_sentence, source_vocab_inv, target_vocab_inv, device)\n",
    "    \n",
    "    # Realizar una traducción de ejemplo\n",
    "    translated = translate_sentence(model, src_sentence, source_vocab, target_vocab_inv, device)\n",
    "    print(\"\\nTraducción de ejemplo:\")\n",
    "    print(f\"Fuente: {src_sentence}\")\n",
    "    print(f\"Traducción: {translated}\")\n",
    "    \n",
    "    # Visualizar los pesos de atención para la traducción de ejemplo\n",
    "    visualize_attention_weights(model, src_sentence, translated, source_vocab_inv, target_vocab_inv, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7150f057",
   "metadata": {},
   "source": [
    "Al ejecutar el código mejorado, el modelo se entrenará para traducir la oración \"the green witch arrived\" al español \"llegó la bruja verde\". Además, cada 100 épocas, se visualizarán los pesos de atención para entender cómo el modelo está prestando atención a las palabras de entrada durante la traducción.\n",
    "\n",
    "\n",
    "Durante el entrenamiento, deberías ver una salida similar a la siguiente cada 10 épocas:\n",
    "\n",
    "```\n",
    "Usando dispositivo: cpu\n",
    "\n",
    "Epoca 1/1000\n",
    "Epoch 1/1000: 100%|█████████████████████████████████████| 31/31 [00:00<00:00,  9.95it/s]\n",
    "Epoch 1/1000 - Loss: 1.9452, Accuracy: 32.00%\n",
    "Validación - Loss: 1.9345, Accuracy: 33.20%\n",
    "\n",
    "Modelo guardado en seq2seq_attention.pth\n",
    "```\n",
    "\n",
    "Al final del entrenamiento, se realizará una traducción de ejemplo y se visualizarán los pesos de atención:\n",
    "\n",
    "```\n",
    "Traducción de ejemplo:\n",
    "Fuente: the green witch arrived\n",
    "Traducción: llegó la bruja verde\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**Observaciones**\n",
    "\n",
    "- Actualmente, el conjunto de datos está compuesto por múltiples copias del mismo par de oraciones. Para mejorar la capacidad de generalización del modelo, es recomendable utilizar un conjunto de datos más amplio y variado.\n",
    "- Si trabajas con secuencias de diferentes longitudes, considera implementar técnicas de padding y utilizar `pack_padded_sequence` y `pad_packed_sequence` de PyTorch para optimizar el procesamiento.\n",
    "- Experimenta con diferentes tamaños de embedding, tamaños ocultos, tasas de aprendizaje y proporciones de teacher forcing para encontrar la mejor configuración para tu tarea específica.\n",
    "- Además de la pérdida y la precisión, considera implementar métricas como BLEU para evaluar la calidad de las traducciones de manera más exhaustiva.\n",
    "- Para evitar el sobreajuste y reducir el tiempo de entrenamiento, puedes implementar una técnica de early stopping que detenga el entrenamiento cuando la pérdida de validación deje de mejorar.\n",
    "- Explora arquitecturas más avanzadas como los Transformers, que han demostrado un rendimiento superior en tareas de traducción automática.\n",
    "- Implementa técnicas de regularización como Dropout para mejorar la capacidad de generalización del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f874cc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Ejercicios adicionales\n",
    "\n",
    "#### 1. **Ampliar el vocabulario y el conjunto de datos**\n",
    "\n",
    "**Objetivo**: Incrementar la capacidad del modelo para manejar una variedad mayor de palabras y frases.\n",
    "\n",
    "**Tareas**:\n",
    "- **Agregar más palabras**: Amplía los vocabularios `source_vocab` y `target_vocab` con más palabras relevantes.\n",
    "- **Crear más pares de oraciones**: Genera múltiples pares de oraciones en inglés y español que contengan las nuevas palabras añadidas.\n",
    "- **Actualizar el dataset**: Modifica la clase `TranslationDataset` para manejar el nuevo conjunto de datos.\n",
    "\n",
    "**Beneficios**: Mejora la capacidad del modelo para generalizar y manejar traducciones más complejas.\n",
    "\n",
    "#### 2. **Implementar un encoder bidireccional**\n",
    "\n",
    "**Objetivo**: Mejorar la capacidad del encoder para capturar información contextual de ambas direcciones de la secuencia de entrada.\n",
    "\n",
    "**Tareas**:\n",
    "- **Modificar el encoder**: Cambia el GRU del encoder para que sea bidireccional añadiendo el parámetro `bidirectional=True`.\n",
    "- **Ajustar el decoder**: Actualiza el decoder para manejar el estado oculto bidireccional concatenado o sumado.\n",
    "- **Actualizar la función de atención**: Asegúrate de que la atención maneje las salidas bidireccionales correctamente.\n",
    "\n",
    "**Beneficios**: Captura una comprensión más rica de la secuencia de entrada, lo que puede mejorar la precisión de la traducción.\n",
    "\n",
    "#### 3. **Manejo de secuencias de longitud variable con padding y máscaras**\n",
    "\n",
    "**Objetivo**: Permitir que el modelo maneje secuencias de diferentes longitudes de manera eficiente.\n",
    "\n",
    "**Tareas**:\n",
    "- **Implementar padding**: Añade padding a las secuencias más cortas para igualar la longitud máxima en el batch.\n",
    "- **Crear máscaras de padding**: Genera máscaras que indiquen qué posiciones son padding para evitar que el modelo preste atención a ellas.\n",
    "- **Actualizar el modelo**: Modifica el encoder y el mecanismo de atención para utilizar las máscaras y ignorar el padding.\n",
    "\n",
    "**Beneficios**: Permite trabajar con conjuntos de datos reales donde las secuencias de entrada y salida varían en longitud.\n",
    "\n",
    "#### 4. **Implementar el beam search para inferencia**\n",
    "\n",
    "**Objetivo**: Mejorar la calidad de las traducciones generadas durante la inferencia al considerar múltiples posibles secuencias de salida.\n",
    "\n",
    "**Tareas**:\n",
    "- **Crear la función de beam search**: Implementa una función que explore múltiples secuencias de salida simultáneamente.\n",
    "- **Integrar con el modelo**: Reemplaza la traducción greedy con la búsqueda en haz en la función de traducción.\n",
    "- **Ajustar parámetros**: Experimenta con diferentes tamaños de haz (beam sizes) para encontrar el equilibrio entre calidad y eficiencia.\n",
    "\n",
    "**Beneficios**: Genera traducciones más precisas y coherentes al considerar múltiples opciones durante la generación.\n",
    "\n",
    "#### 5. **Visualizar los pesos de atención para múltiples oraciones**\n",
    "\n",
    "**Objetivo**: Comprender cómo el modelo presta atención a diferentes partes de la oración de entrada durante la traducción.\n",
    "\n",
    "**Tareas**:\n",
    "- **Modificar la función de visualización**: Permite la visualización de pesos de atención para múltiples pares de oraciones.\n",
    "- **Crear múltiples casos de prueba**: Traduce diferentes oraciones y visualiza los pesos de atención correspondientes.\n",
    "- **Interpretar los resultados**: Analiza cómo varían los pesos de atención según las oraciones de entrada.\n",
    "\n",
    "**Beneficios**: Facilita la interpretación del comportamiento del modelo y ayuda a identificar posibles mejoras.\n",
    "\n",
    "#### 6. **Agregar dropout para regularización**\n",
    "\n",
    "**Objetivo**: Prevenir el sobreajuste del modelo y mejorar su capacidad de generalización.\n",
    "\n",
    "**Tareas**:\n",
    "- **Incorporar dropout en el encoder y decoder**: Añade capas de Dropout después de las capas de embedding y antes de las capas de GRU.\n",
    "- **Experimentar con diferentes tasas de dropout**: Prueba diferentes valores de dropout para encontrar el óptimo.\n",
    "- **Evaluar el impacto**: Observa cómo afecta el dropout a la pérdida de entrenamiento y a la precisión.\n",
    "\n",
    "**Beneficios**: Mejora la robustez del modelo y evita que memorice el conjunto de entrenamiento.\n",
    "\n",
    "#### 7. **Implementar early stopping**\n",
    "\n",
    "**Objetivo**: Evitar el sobreentrenamiento deteniendo el entrenamiento cuando la pérdida de validación deja de mejorar.\n",
    "\n",
    "**Tareas**:\n",
    "- **Monitorear la pérdida de validación**: Durante el entrenamiento, observa la pérdida en el conjunto de validación.\n",
    "- **Definir un criterio de detención**: Establece un número de épocas sin mejora antes de detener el entrenamiento.\n",
    "- **Implementar la lógica de early stopping**: Modifica el ciclo de entrenamiento para incluir la lógica de detención.\n",
    "\n",
    "**Beneficios**: Reduce el tiempo de entrenamiento y previene el sobreajuste.\n",
    "\n",
    "#### 8. **Evaluar el modelo con la métrica BLEU**\n",
    "\n",
    "**Objetivo**: Medir la calidad de las traducciones generadas de manera más exhaustiva.\n",
    "\n",
    "**Tareas**:\n",
    "- **Implementar el cálculo de BLEU**: Utiliza bibliotecas como `nltk` para calcular el puntaje BLEU de las traducciones.\n",
    "- **Integrar con el ciclo de evaluación**: Añade el cálculo de BLEU durante las fases de evaluación.\n",
    "- **Interpretar los resultados**: Analiza cómo varía el puntaje BLEU con diferentes configuraciones del modelo.\n",
    "\n",
    "**Beneficios**: Proporciona una medida estándar de la calidad de las traducciones, más allá de la pérdida y la precisión.\n",
    "\n",
    "#### 9. **Experimentar con diferentes mecanismos de atención**\n",
    "\n",
    "**Objetivo**: Comprender cómo diferentes mecanismos de atención afectan el rendimiento del modelo.\n",
    "\n",
    "**Tareas**:\n",
    "- **Implementar atención dot-product**: Crea una variante del mecanismo de atención que utiliza productos punto.\n",
    "- **Comparar con atención aditiva**: Compara el rendimiento entre atención aditiva (tu implementación actual) y dot-product.\n",
    "- **Evaluar el impacto**: Observa cómo cambian la pérdida y la precisión con diferentes mecanismos de atención.\n",
    "\n",
    "**Beneficios**: Permite identificar qué tipo de atención es más adecuada para tu tarea específica.\n",
    "\n",
    "#### 10. **Guardar y cargar el modelo para inferencia posterior**\n",
    "\n",
    "**Objetivo**: Permitir el uso del modelo entrenado en futuras sesiones sin necesidad de reentrenarlo.\n",
    "\n",
    "**Tareas**:\n",
    "- **Implementar funciones de guardado y carga**: Asegúrate de que el modelo y el optimizador se puedan guardar y cargar correctamente.\n",
    "- **Realizar la inferencia con el modelo guardado**: Guarda el modelo después del entrenamiento y cárgalo para traducir nuevas oraciones.\n",
    "- **Verificar la consistencia**: Asegúrate de que el modelo cargado produce las mismas traducciones que antes de guardarlo.\n",
    "\n",
    "**Beneficios**: Facilita la reutilización del modelo y la realización de inferencias en diferentes contextos.\n",
    "\n",
    "#### 11. **Optimización de hiperparámetros**\n",
    "\n",
    "**Objetivo**: Mejorar el rendimiento del modelo ajustando los hiperparámetros clave.\n",
    "\n",
    "**Tareas**:\n",
    "- **Definir hiperparámetros a ajustar**: Por ejemplo, tamaño de embedding, tamaño oculto, tasa de aprendizaje, proporción de teacher forcing.\n",
    "- **Realizar pruebas sistemáticas**: Prueba diferentes combinaciones de hiperparámetros y registra los resultados.\n",
    "- **Utilizar herramientas de búsqueda**: Considera usar bibliotecas como `Optuna` o `Ray Tune` para automatizar la búsqueda de hiperparámetros.\n",
    "- **Identificar la mejor configuración**: Selecciona la combinación de hiperparámetros que proporciona el mejor rendimiento.\n",
    "\n",
    "**Beneficios**: Optimiza el modelo para obtener mejores resultados en la tarea de traducción.\n",
    "\n",
    "#### 12. **Implementar un decoder bidireccional**\n",
    "\n",
    "**Objetivo**: Explorar cómo un decoder bidireccional podría influir en la generación de secuencias.\n",
    "\n",
    "**Tareas**:\n",
    "- **Modificar el decoder**: Cambia el GRU del decoder para que sea bidireccional.\n",
    "- **Ajustar el mecanismo de atención**: Asegúrate de que la atención maneje las salidas bidireccionales correctamente.\n",
    "- **Evaluar el impacto**: Observa cómo afecta esto a la calidad de las traducciones.\n",
    "\n",
    "**Nota**: Aunque los decoders bidireccionales no son comunes en modelos Seq2Seq tradicionales, este ejercicio es útil para comprender las implicaciones de las arquitecturas bidireccionales en ambos componentes.\n",
    "\n",
    "**Beneficios**: Amplía tu comprensión de las arquitecturas bidireccionales y su impacto en las tareas de generación de secuencias.\n",
    "\n",
    "#### 13. **Implementar el mecanismo de atención de Bahdanau y Luong**\n",
    "\n",
    "**Objetivo**: Profundizar en diferentes tipos de mecanismos de atención y comparar sus efectos.\n",
    "\n",
    "**Tareas**:\n",
    "- **Atención de Bahdanau**: Implementa la atención aditiva propuesta por Bahdanau et al.\n",
    "- **Atención de Luong**: Implementa la atención multiplicativa propuesta por Luong et al.\n",
    "- **Comparar desempeños**: Entrena modelos con cada tipo de atención y compara sus métricas de rendimiento.\n",
    "\n",
    "**Beneficios**: Comprende las diferencias entre los mecanismos de atención y cómo afectan al modelo.\n",
    "\n",
    "#### 14. **Implementar un modelo transformer para traducción**\n",
    "\n",
    "**Objetivo**: Explorar una arquitectura más avanzada que ha demostrado un rendimiento superior en tareas de traducción automática.\n",
    "\n",
    "**Tareas**:\n",
    "- **Estudiar la arquitectura transformer**: Revisa los conceptos clave como auto-atención, capas de encoder y decoder, y mecanismos de posición.\n",
    "- **Implementar el transformer**: Usa la clase `nn.Transformer` de PyTorch o implementa tu propia versión.\n",
    "- **Comparar con Seq2Seq**: Entrena y evalúa ambos modelos en el mismo conjunto de datos y compara sus desempeños.\n",
    "\n",
    "**Beneficios**: Familiarízate con las arquitecturas de vanguardia en procesamiento de lenguaje natural.\n",
    "\n",
    "#### 15. **Agregar un mecanismo de copia para manejar palabras raras o desconocidas**\n",
    "\n",
    "**Objetivo**: Mejorar la capacidad del modelo para traducir palabras raras o fuera del vocabulario mediante un mecanismo de copia.\n",
    "\n",
    "**Tareas**:\n",
    "- **Estudiar el mecanismo de copia**: Comprende cómo permite al modelo copiar directamente palabras de la entrada a la salida.\n",
    "- **Implementar el mecanismo**: Modifica el decoder para que pueda decidir entre generar una palabra del vocabulario o copiar una palabra de la entrada.\n",
    "- **Evaluar el impacto**: Observa cómo afecta esto a la precisión en palabras raras o desconocidas.\n",
    "\n",
    "**Beneficios**: Mejora la capacidad del modelo para manejar vocabularios limitados y traducciones precisas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93928ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
