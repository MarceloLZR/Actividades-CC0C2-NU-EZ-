{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7827fcaf",
   "metadata": {},
   "source": [
    "### Atención global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definir el módulo de atención\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        hidden: [batch_size, hidden_size]\n",
    "        encoder_outputs: [src_len, batch_size, hidden_size]\n",
    "        mask: [batch_size, src_len] (opcional)\n",
    "        \"\"\"\n",
    "        src_len = encoder_outputs.size(0)\n",
    "        batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Expand hidden to [batch_size, src_len, hidden_size]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hidden_size]\n",
    "\n",
    "        # Concatenar y aplicar capa lineal y tanh\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hidden_size]\n",
    "\n",
    "        # Calcular puntuaciones de atención\n",
    "        energy = energy.permute(0, 2, 1)  # [batch_size, hidden_size, src_len]\n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)  # [batch_size, 1, hidden_size]\n",
    "        attention = torch.bmm(v, energy).squeeze(1)  # [batch_size, src_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = self.softmax(attention)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        return attention  # [batch_size, src_len]\n",
    "\n",
    "# Definir el Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        \"\"\"\n",
    "        src: [src_len, batch_size]\n",
    "        src_lengths: [batch_size]\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(src))  # [src_len, batch_size, embed_dim]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths.cpu(), enforce_sorted=False)\n",
    "        outputs, hidden = self.gru(packed)  # outputs: [src_len, batch, hidden_dim]\n",
    "        src_len = src.size(0)  # Obtener el src_len original\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, total_length=src_len)\n",
    "        return outputs, hidden  # outputs: [src_len, batch_size, hidden_dim]\n",
    "\n",
    "# Definir el Decoder con Atención\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, attention, num_layers=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.attention = attention\n",
    "        self.gru = nn.GRU(embed_dim + hidden_dim, hidden_dim, num_layers=num_layers, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        input: [batch_size]\n",
    "        hidden: [num_layers, batch_size, hidden_dim]\n",
    "        encoder_outputs: [src_len, batch_size, hidden_dim]\n",
    "        mask: [batch_size, src_len] (opcional)\n",
    "        \"\"\"\n",
    "        input = input.unsqueeze(0)  # [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input))  # [1, batch_size, embed_dim]\n",
    "\n",
    "        # Atención\n",
    "        hidden_last = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        attn_weights = self.attention(hidden_last, encoder_outputs, mask)  # [batch_size, src_len]\n",
    "\n",
    "        # Contexto\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hidden_dim]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, hidden_dim]\n",
    "        context = context.permute(1, 0, 2)  # [1, batch_size, hidden_dim]\n",
    "\n",
    "        # Concatenar embedding y contexto\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)  # [1, batch_size, embed_dim + hidden_dim]\n",
    "\n",
    "        # Decodificar\n",
    "        output, hidden = self.gru(rnn_input, hidden)  # output: [1, batch_size, hidden_dim]\n",
    "\n",
    "        # Concatenar output y contexto para predecir\n",
    "        output = output.squeeze(0)  # [batch_size, hidden_dim]\n",
    "        context = context.squeeze(0)  # [batch_size, hidden_dim]\n",
    "        output = self.fc_out(torch.cat((output, context), dim=1))  # [batch_size, output_dim]\n",
    "\n",
    "        return output, hidden, attn_weights  # output: [batch_size, output_dim]\n",
    "\n",
    "# Definir el modelo Encoder-Decoder con Atención\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def create_mask(self, src, src_lengths):\n",
    "        \"\"\"\n",
    "        Crear máscara para ignorar los padding en la atención\n",
    "        src: [src_len, batch_size]\n",
    "        src_lengths: [batch_size]\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[1]\n",
    "        src_len = src.shape[0]\n",
    "        # Crear una matriz [batch_size, src_len] donde cada posición es True si está dentro de la longitud real\n",
    "        mask = torch.arange(src_len).unsqueeze(0).to(self.device) < src_lengths.unsqueeze(1)\n",
    "        return mask  # [batch_size, src_len]\n",
    "\n",
    "    def forward(self, src, src_lengths, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        src: [src_len, batch_size]\n",
    "        src_lengths: [batch_size]\n",
    "        trg: [trg_len, batch_size]\n",
    "        teacher_forcing_ratio: probabilidad de usar el token real como siguiente input\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        output_dim = self.decoder.output_dim\n",
    "\n",
    "        # Tensor para guardar las predicciones\n",
    "        outputs = torch.zeros(trg_len, batch_size, output_dim).to(self.device)\n",
    "\n",
    "        # Codificar\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
    "\n",
    "        # Inicializar entrada del decoder (generalmente <sos>)\n",
    "        input = trg[0, :]  # [batch_size]\n",
    "\n",
    "        # Crear máscara\n",
    "        mask = self.create_mask(src, src_lengths)  # [batch_size, src_len]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, attn_weights = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Decidir si usar teacher forcing\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)  # [batch_size]\n",
    "\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Configuración de dimensiones y ejemplo de uso\n",
    "INPUT_DIM = 1000   # Tamaño del vocabulario de entrada\n",
    "OUTPUT_DIM = 1000  # Tamaño del vocabulario de salida\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "# Seleccionar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instanciar componentes\n",
    "attn = Attention(HIDDEN_DIM, dropout=DEC_DROPOUT)\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, num_layers=N_LAYERS, dropout=ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, attn, num_layers=N_LAYERS, dropout=DEC_DROPOUT)\n",
    "\n",
    "# Instanciar el modelo Seq2Seq\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Inicializar pesos\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Definir el optimizador y la función de pérdida\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Asumiendo que 0 es el padding\n",
    "\n",
    "# Ejemplo de datos (utilizar índices de vocabulario reales en aplicaciones reales)\n",
    "src = torch.randint(1, INPUT_DIM, (10, 32)).to(device)  # src_len=10, batch_size=32\n",
    "trg = torch.randint(1, OUTPUT_DIM, (20, 32)).to(device)  # trg_len=20, batch_size=32\n",
    "src_lengths = torch.randint(5, 10, (32,)).to(device)  # Longitudes variables entre 5 y 9\n",
    "\n",
    "# Hacer una pasada hacia adelante\n",
    "output = model(src, src_lengths, trg, teacher_forcing_ratio=0.75)\n",
    "print(\"Forma de la salida:\", output.shape)  # Esperado: (trg_len, batch_size, output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8251b22",
   "metadata": {},
   "source": [
    "### Redes de memoria extremo a extremo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MemoryNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hops=3, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Modelo de Red de Memoria con múltiples hops.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Tamaño del vocabulario.\n",
    "            embedding_dim (int): Dimensionalidad de los embeddings.\n",
    "            hops (int, opcional): Número de hops de atención. Por defecto es 3.\n",
    "            dropout (float, opcional): Tasa de dropout. Por defecto es 0.2.\n",
    "        \"\"\"\n",
    "        super(MemoryNetwork, self).__init__()\n",
    "        self.hops = hops\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Embeddings para las sentencias y las preguntas\n",
    "        self.embeddings_A = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings_C = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(hops)])\n",
    "\n",
    "        # Red para combinar los embeddings de la pregunta\n",
    "        self.question_combine = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # Dropout para regularización\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Inicialización de pesos\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas de embedding y lineales.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.embeddings_A.weight)\n",
    "        for embed in self.embeddings_C:\n",
    "            nn.init.xavier_uniform_(embed.weight)\n",
    "        nn.init.xavier_uniform_(self.question_combine.weight)\n",
    "        nn.init.zeros_(self.question_combine.bias)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, sentences, question, mask=None):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del modelo de Red de Memoria.\n",
    "\n",
    "        Args:\n",
    "            sentences (Tensor): Tensores de sentencias de forma (batch_size, num_sentences, sentence_length).\n",
    "            question (Tensor): Tensores de preguntas de forma (batch_size, question_length).\n",
    "            mask (Tensor, opcional): Máscara para las sentencias de forma (batch_size, num_sentences).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log probabilidades de las respuestas de forma (batch_size, vocab_size).\n",
    "        \"\"\"\n",
    "        batch_size, num_sentences, sentence_length = sentences.size()\n",
    "        _, question_length = question.size()\n",
    "\n",
    "        # Paso 1: Obtener embeddings de las preguntas\n",
    "        # Embeddings_A se utiliza para las preguntas\n",
    "        q_embeddings = self.embeddings_A(question)  # (batch_size, question_length, embedding_dim)\n",
    "        q_embeddings = q_embeddings.mean(dim=1)  # (batch_size, embedding_dim)\n",
    "        q_embeddings = self.dropout(q_embeddings)\n",
    "        q = self.relu(self.question_combine(q_embeddings))  # (batch_size, embedding_dim)\n",
    "\n",
    "        for hop in range(self.hops):\n",
    "            # Paso 2: Obtener embeddings de las sentencias\n",
    "            m = self.embeddings_A(sentences.view(-1, sentence_length))  # (batch_size * num_sentences, sentence_length, embedding_dim)\n",
    "            m = m.mean(dim=1)  # (batch_size * num_sentences, embedding_dim)\n",
    "            m = m.view(batch_size, num_sentences, self.embedding_dim)  # (batch_size, num_sentences, embedding_dim)\n",
    "\n",
    "            # Embeddings_C específicos para cada hop\n",
    "            c = self.embeddings_C[hop](sentences.view(-1, sentence_length))  # (batch_size * num_sentences, sentence_length, embedding_dim)\n",
    "            c = c.mean(dim=1)  # (batch_size * num_sentences, embedding_dim)\n",
    "            c = c.view(batch_size, num_sentences, self.embedding_dim)  # (batch_size, num_sentences, embedding_dim)\n",
    "\n",
    "            # Paso 3: Cálculo de pesos de atención\n",
    "            # Similaridad entre la pregunta y cada sentencia\n",
    "            attn_scores = torch.bmm(m, q.unsqueeze(2)).squeeze(2)  # (batch_size, num_sentences)\n",
    "\n",
    "            if mask is not None:\n",
    "                attn_scores = attn_scores.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "            attn_weights = F.softmax(attn_scores, dim=1)  # (batch_size, num_sentences)\n",
    "            attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "            # Paso 4: Suma ponderada de las sentencias\n",
    "            o = torch.bmm(attn_weights.unsqueeze(1), c).squeeze(1)  # (batch_size, embedding_dim)\n",
    "\n",
    "            # Integración de la memoria con el estado de la pregunta\n",
    "            q = q + o  # (batch_size, embedding_dim)\n",
    "            q = self.dropout(q)\n",
    "\n",
    "        # Paso 5: Predicción final\n",
    "        output = self.fc(q)  # (batch_size, vocab_size)\n",
    "        return F.log_softmax(output, dim=1)\n",
    "\n",
    "# Configuración de dimensiones y ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros del modelo\n",
    "    vocab_size = 1000      # Tamaño del vocabulario\n",
    "    embedding_dim = 128    # Dimensionalidad de los embeddings\n",
    "    hops = 3               # Número de hops de atención\n",
    "    dropout = 0.3          # Tasa de dropout\n",
    "\n",
    "    # Crear instancia del modelo\n",
    "    model = MemoryNetwork(vocab_size, embedding_dim, hops, dropout)\n",
    "\n",
    "    # Seleccionar dispositivo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Ejemplo de datos\n",
    "    batch_size = 32\n",
    "    num_sentences = 10\n",
    "    sentence_length = 15\n",
    "    question_length = 5\n",
    "\n",
    "    # Sentencias: (batch_size, num_sentences, sentence_length)\n",
    "    sentences = torch.randint(0, vocab_size, (batch_size, num_sentences, sentence_length)).to(device)\n",
    "\n",
    "    # Preguntas: (batch_size, question_length)\n",
    "    question = torch.randint(0, vocab_size, (batch_size, question_length)).to(device)\n",
    "\n",
    "    # Máscara opcional (por ejemplo, para padding)\n",
    "    mask = torch.ones(batch_size, num_sentences).to(device)  # Aquí no hay padding\n",
    "\n",
    "    # Predicción\n",
    "    output = model(sentences, question, mask)\n",
    "    print(\"Predicción de respuesta:\", output.argmax(dim=1))  # (batch_size)\n",
    "    print(\"Forma de la salida:\", output.shape)  # (batch_size, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae22dcdb",
   "metadata": {},
   "source": [
    "Las **memory networks** son una clase de modelos de aprendizaje profundo diseñados para tareas de procesamiento de lenguaje natural que requieren razonamiento a múltiples pasos o \"hops\" sobre una memoria externa. Estas memorias están compuestas por sentencias o fragmentos de texto que el modelo puede consultar iterativamente para extraer información relevante.\n",
    "\n",
    "**Características clave:**\n",
    "- **Memoria externa:** Almacena información que el modelo puede consultar.\n",
    "- **Múltiples hops de atención:** Permite al modelo realizar múltiples iteraciones de atención para refinar su comprensión y respuesta.\n",
    "- **Actualización del estado:** Después de cada hop, el estado interno del modelo se actualiza incorporando la información extraída de la memoria.\n",
    "\n",
    "#### **Mecanismo de atención de Bahdanau**\n",
    "El mecanismo de **atención de Bahdanau**, también conocido como **atención aditiva**, es una técnica diseñada para mejorar los modelos de secuencia a secuencia (Seq2Seq) al permitir que el decodificador se enfoque en diferentes partes de la secuencia de entrada en cada paso de generación.\n",
    "\n",
    "**Características clave:**\n",
    "- **Atención puntual:** En cada paso de generación, el decodificador calcula pesos de atención sobre todas las posiciones de la secuencia de entrada.\n",
    "- **Contexto dinámico:** Genera un vector de contexto dinámico que es una combinación ponderada de las representaciones de la entrada, guiado por los pesos de atención.\n",
    "- **Un solo paso de atención:** Generalmente, realiza un solo cálculo de atención por paso de generación.\n",
    "\n",
    "\n",
    "A continuación, comparamos ambos enfoques en términos de sus mecanismos y cómo se reflejan en el código de `MemoryNetwork`.\n",
    "\n",
    "#### **a. Estructura de la memoria**\n",
    "\n",
    "- **Memory networks:**\n",
    "  - **Memoria estructurada:** Las memorias están organizadas en sentencias o hechos individuales.\n",
    "  - **Múltiples hops:** Permiten múltiples iteraciones para refinar la atención sobre la memoria.\n",
    "  \n",
    "- **Bahdanau attention:**\n",
    "  - **Memoria implícita:** La memoria está implícita en las salidas del codificador.\n",
    "  - **Atención singular:** Realiza una única atención por paso de decodificación.\n",
    "\n",
    "#### **b. Mecanismo de atención**\n",
    "\n",
    "- **Memory networks:**\n",
    "  - **Atención iterativa:** En cada hop, el modelo recalcula los pesos de atención y actualiza su estado interno.\n",
    "  - **Integración de la memoria:** El estado interno se actualiza agregando la información extraída de la memoria en cada hop.\n",
    "  \n",
    "- **Bahdanau attention:**\n",
    "  - **Atención adicional por paso:** En cada paso de decodificación, se calcula una nueva distribución de atención sobre las entradas.\n",
    "  - **Contexto basado en estado actual:** El vector de contexto se basa únicamente en el estado actual del decodificador.\n",
    "\n",
    "#### **c. Actualización del estado interno**\n",
    "\n",
    "- **Memory networks:**\n",
    "  - **Estado refinado:** El estado interno (`u` en el código) se actualiza iterativamente con la información extraída de la memoria.\n",
    "  \n",
    "- **Bahdanau attention:**\n",
    "  - **Estado independiente por paso:** Cada paso de atención depende del estado actual del decodificador, pero no se mantiene un estado acumulativo a través de múltiples atenciones.\n",
    "\n",
    "#### **d. Aplicación en el código proporcionado**\n",
    "\n",
    "\n",
    "\n",
    "#### **MemoryNetwork**\n",
    "\n",
    "```python\n",
    "class MemoryNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hops=3):\n",
    "        super(MemoryNetwork, self).__init__()\n",
    "        self.hops = hops\n",
    "        self.embeddings_A = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings_C = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(hops)])\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, sentences, question):\n",
    "        # Paso 1: Obtener embedding de la pregunta\n",
    "        u = self.embeddings_A(question).mean(dim=1)  # (batch, embedding_dim)\n",
    "        \n",
    "        for hop in range(self.hops):\n",
    "            # Paso 2: Embedding de las sentencias (memoria)\n",
    "            m = self.embeddings_A(sentences)  # (batch, num_sentences, embedding_dim)\n",
    "            c = self.embeddings_C[hop](sentences)  # (batch, num_sentences, embedding_dim)\n",
    "\n",
    "            # Paso 3: Cálculo de pesos de atención (similaridad entre pregunta y sentencias)\n",
    "            p = F.softmax(torch.bmm(m, u.unsqueeze(2)).squeeze(2), dim=1)  # (batch, num_sentences)\n",
    "            \n",
    "            # Paso 4: Suma ponderada\n",
    "            o = torch.bmm(p.unsqueeze(1), c).squeeze(1)  # (batch, embedding_dim)\n",
    "            \n",
    "            # Actualizar u para el siguiente salto\n",
    "            u = u + o  # Integración de la memoria de salida con el estado de la pregunta\n",
    "\n",
    "        # Paso 5: Predicción final\n",
    "        output = self.fc(u)  # (batch, vocab_size)\n",
    "        return F.log_softmax(output, dim=1)\n",
    "```\n",
    "\n",
    "**Análisis en el contexto de memory networks:**\n",
    "\n",
    "1. **Memoria y hops:**\n",
    "   - **`self.hops`:** Define el número de hops de atención, permitiendo múltiples iteraciones para refinar la atención sobre las sentencias.\n",
    "   - **Embeddings múltiples (`embeddings_A` y `embeddings_C`):** Diferentes embeddings para cada hop permiten al modelo capturar diferentes aspectos de las sentencias en cada iteración.\n",
    "\n",
    "2. **Iteraciones de atención:**\n",
    "   - En cada hop, el modelo calcula una distribución de atención (`p`) sobre las sentencias basándose en la similitud entre la pregunta (`u`) y las sentencias (`m`).\n",
    "   - El vector de estado interno (`u`) se actualiza agregando la información extraída (`o`), lo que permite al modelo acumular información relevante a lo largo de los hops.\n",
    "\n",
    "3. **Predicción final:**\n",
    "   - Después de los hops de atención, el estado interno refinado (`u`) se pasa a una capa lineal para generar la predicción final.\n",
    "\n",
    "**Comparación con Bahdanau:**\n",
    "- **Múltiples iteraciones vs. atención singular:** A diferencia del mecanismo de Bahdanau, que realiza una sola atención por paso de decodificación, `MemoryNetwork` permite múltiples iteraciones de atención, refinando continuamente el estado interno.\n",
    "- **Actualización del estado:** `MemoryNetwork` mantiene y actualiza un estado interno acumulativo (`u`), mientras que Bahdanau no mantiene un estado acumulativo a través de las atenciones.\n",
    "\n",
    "#### **Bahdanau Attention (conceptual)**\n",
    "\n",
    "Para comparar, consideremos una implementación conceptual de Bahdanau Attention en un modelo Seq2Seq:\n",
    "\n",
    "```python\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch_size, hidden_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, hidden_dim]\n",
    "        \n",
    "        src_len = encoder_outputs.size(0)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hidden_dim]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hidden_dim]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hidden_dim]\n",
    "        energy = energy.permute(0, 2, 1)  # [batch_size, hidden_dim, src_len]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "        attention = torch.bmm(v, energy).squeeze(1)  # [batch_size, src_len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)  # [batch_size, src_len]\n",
    "```\n",
    "\n",
    "**Características en comparación:**\n",
    "\n",
    "1. **Atención singular:**\n",
    "   - En cada paso de decodificación, calcula una única distribución de atención sobre las salidas del codificador.\n",
    "\n",
    "2. **No hay actualización acumulativa:**\n",
    "   - No mantiene un estado interno que se actualiza iterativamente a través de múltiples atenciones.\n",
    "\n",
    "#### **3. Implicaciones**\n",
    "\n",
    "#### **a. Número de hops**\n",
    "\n",
    "En el código de `MemoryNetwork`, el parámetro `hops` permite al modelo realizar múltiples iteraciones de atención sobre la memoria (sentencias). Esto es fundamental en las Memory Networks de Sukhbaatar, ya que cada hop puede enfocarse en diferentes aspectos o fragmentos de la memoria para refinar la respuesta.\n",
    "\n",
    "En contraste, el mecanismo de Bahdanau generalmente realiza una única atención por paso de decodificación, enfocándose en diferentes partes de la entrada en cada paso de generación, pero sin iteraciones internas adicionales por paso.\n",
    "\n",
    "#### **b. Actualización del estado interno**\n",
    "\n",
    "En `MemoryNetwork`, el estado interno `u` se actualiza agregando el vector `o` obtenido de la atención en cada hop:\n",
    "\n",
    "```python\n",
    "u = u + o\n",
    "```\n",
    "\n",
    "Esto permite que el estado interno acumule información relevante de múltiples hops, mejorando la capacidad del modelo para razonar sobre la información almacenada en la memoria.\n",
    "\n",
    "En Bahdanau Attention, el estado del decodificador (`hidden`) se utiliza directamente para calcular la atención en cada paso, pero no se actualiza de manera acumulativa a través de múltiples iteraciones de atención internas.\n",
    "\n",
    "#### **c. Mecanismo de atención iterativo vs. singular**\n",
    "\n",
    "El código de `MemoryNetwork` implementa un ciclo `for` sobre los hops:\n",
    "\n",
    "```python\n",
    "for hop in range(self.hops):\n",
    "    # Cálculo de atención y actualización de u\n",
    "```\n",
    "\n",
    "Esto refleja el proceso iterativo de las Memory Networks, donde cada hop puede potencialmente extraer información adicional de la memoria, refinando el entendimiento de la pregunta.\n",
    "\n",
    "Por otro lado, en Bahdanau Attention, no existe tal ciclo interno; la atención se calcula una vez por paso de decodificación.\n",
    "\n",
    "#### **d. Integración de la memoria y la pregunta**\n",
    "\n",
    "En `MemoryNetwork`, la pregunta (`u`) y las sentencias se combinan en cada hop para calcular los pesos de atención:\n",
    "\n",
    "```python\n",
    "p = F.softmax(torch.bmm(m, u.unsqueeze(2)).squeeze(2), dim=1)\n",
    "```\n",
    "\n",
    "Esto representa una interacción directa y repetitiva entre la pregunta y la memoria en múltiples hops.\n",
    "\n",
    "En Bahdanau Attention, la interacción entre el estado del decodificador y las salidas del codificador se realiza una sola vez por paso de decodificación.\n",
    "\n",
    "#### **e. Capas de embedding diferentes por hop**\n",
    "\n",
    "El uso de `embeddings_C` como una lista de capas de embedding específicas para cada hop permite al modelo capturar diferentes representaciones de las sentencias en cada iteración. Esto añade flexibilidad y capacidad de razonamiento más profundo.\n",
    "\n",
    "Bahdanau Attention no tiene una correspondencia directa para esto, ya que utiliza una única representación de las entradas en cada paso de atención.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491463eb",
   "metadata": {},
   "source": [
    "### Sum Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Mecanismo de atención para calcular la relevancia entre dos representaciones.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "        nn.init.xavier_uniform_(self.attn.weight)\n",
    "        nn.init.xavier_uniform_(self.v.unsqueeze(0))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden: [batch_size, hidden_dim] - Estado oculto de la pregunta.\n",
    "            encoder_outputs: [batch_size, seq_len, hidden_dim] - Salidas del documento.\n",
    "            mask: [batch_size, seq_len] - Máscara para ignorar padding.\n",
    "        \n",
    "        Returns:\n",
    "            attention_weights: [batch_size, seq_len] - Pesos de atención.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_dim = encoder_outputs.size()\n",
    "        \n",
    "        # Expandir el estado oculto de la pregunta para concatenarlo con las salidas del documento\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Concatenar y aplicar capa lineal y tanh\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Calcular puntuaciones de atención\n",
    "        energy = energy.view(-1, hidden_dim)  # [batch_size * seq_len, hidden_dim]\n",
    "        v = self.v.repeat(batch_size, 1)  # [batch_size, hidden_dim]\n",
    "        attention = torch.bmm(energy.view(batch_size, seq_len, hidden_dim), v.unsqueeze(2)).squeeze(2)  # [batch_size, seq_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention_weights = F.softmax(attention, dim=1)  # [batch_size, seq_len]\n",
    "        return attention_weights\n",
    "\n",
    "class SumReader(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo SumReader mejorado para procesar documentos y preguntas,\n",
    "    utilizando RNNs bidireccionales y un mecanismo de atención.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Tamaño del vocabulario.\n",
    "            embedding_dim (int): Dimensionalidad de los embeddings.\n",
    "            hidden_dim (int): Dimensionalidad de las capas ocultas de las RNNs.\n",
    "            dropout (float, opcional): Tasa de dropout. Por defecto es 0.3.\n",
    "        \"\"\"\n",
    "        super(SumReader, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # RNNs bidireccionales para el documento y la pregunta\n",
    "        self.document_rnn = nn.GRU(embedding_dim, hidden_dim, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.question_rnn = nn.GRU(embedding_dim, hidden_dim, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Capa de atención\n",
    "        self.attention = Attention(hidden_dim * 2)\n",
    "        \n",
    "        # Capa lineal para la predicción final\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        \n",
    "        # Inicialización de pesos\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas de embedding y lineales.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, document, question, doc_lengths, ques_lengths):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del modelo SumReader.\n",
    "        \n",
    "        Args:\n",
    "            document (Tensor): [batch_size, doc_len] - Índices de palabras del documento.\n",
    "            question (Tensor): [batch_size, ques_len] - Índices de palabras de la pregunta.\n",
    "            doc_lengths (Tensor): [batch_size] - Longitudes de cada documento.\n",
    "            ques_lengths (Tensor): [batch_size] - Longitudes de cada pregunta.\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, vocab_size] - Log probabilidades de las respuestas.\n",
    "        \"\"\"\n",
    "        # Paso 1: Embedding del documento y de la pregunta\n",
    "        doc_embed = self.dropout(self.embedding(document))  # [batch_size, doc_len, embedding_dim]\n",
    "        ques_embed = self.dropout(self.embedding(question)) # [batch_size, ques_len, embedding_dim]\n",
    "        \n",
    "        # Paso 2: Pasar el documento y la pregunta por las RNNs con packing\n",
    "        # Document\n",
    "        packed_doc = nn.utils.rnn.pack_padded_sequence(doc_embed, doc_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        doc_output, _ = self.document_rnn(packed_doc)   # [batch_size, doc_len, hidden_dim * 2]\n",
    "        doc_output, _ = nn.utils.rnn.pad_packed_sequence(doc_output, batch_first=True)\n",
    "        \n",
    "        # Pregunta\n",
    "        packed_ques = nn.utils.rnn.pack_padded_sequence(ques_embed, ques_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        ques_output, _ = self.question_rnn(packed_ques) # [batch_size, ques_len, hidden_dim * 2]\n",
    "        ques_output, _ = nn.utils.rnn.pad_packed_sequence(ques_output, batch_first=True)\n",
    "        \n",
    "        # Representación de la pregunta como el último estado oculto de la RNN\n",
    "        # Alternativamente, se puede usar una atención sobre la pregunta\n",
    "        question_representation = torch.cat((ques_output[range(len(ques_output)), ques_lengths - 1, :hidden_dim],\n",
    "                                             ques_output[range(len(ques_output)), ques_lengths - 1, hidden_dim:]), dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Paso 3: Calcular pesos de atención entre la pregunta y cada palabra del documento\n",
    "        attention_weights = self.attention(question_representation, doc_output)  # [batch_size, doc_len]\n",
    "        \n",
    "        # Paso 4: Calcular el vector de contexto como suma ponderada de las representaciones del documento\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), doc_output).squeeze(1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Paso 5: Pasar el vector de contexto por una capa lineal para predecir la respuesta\n",
    "        output = self.fc(context)  # [batch_size, vocab_size]\n",
    "        \n",
    "        return F.log_softmax(output, dim=1)  # [batch_size, vocab_size]\n",
    "\n",
    "# Configuración del modelo\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros del modelo\n",
    "    vocab_size = 10000     # Tamaño del vocabulario\n",
    "    embedding_dim = 300    # Dimensionalidad de los embeddings\n",
    "    hidden_dim = 128       # Dimensionalidad oculta de las RNNs\n",
    "    dropout = 0.3          # Tasa de dropout\n",
    "\n",
    "    # Crear instancia del modelo\n",
    "    model = SumReader(vocab_size, embedding_dim, hidden_dim, dropout)\n",
    "\n",
    "    # Seleccionar dispositivo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Ejemplo de datos\n",
    "    batch_size = 2\n",
    "    doc_len = 10\n",
    "    ques_len = 5\n",
    "\n",
    "    # Sentencias: [batch_size, doc_len]\n",
    "    document = torch.randint(1, vocab_size, (batch_size, doc_len)).to(device)  # Asumiendo que 0 es <pad>\n",
    "    # Preguntas: [batch_size, ques_len]\n",
    "    question = torch.randint(1, vocab_size, (batch_size, ques_len)).to(device)   # Asumiendo que 0 es <pad>\n",
    "\n",
    "    # Longitudes de las secuencias (sin padding)\n",
    "    doc_lengths = torch.tensor([10, 8]).to(device)   # Ejemplo de longitudes\n",
    "    ques_lengths = torch.tensor([5, 4]).to(device)  # Ejemplo de longitudes\n",
    "\n",
    "    # Predicción de probabilidad de respuesta\n",
    "    output = model(document, question, doc_lengths, ques_lengths)\n",
    "    print(\"Probabilidad de respuesta:\", output)\n",
    "    print(\"Forma de la salida:\", output.shape)  # [batch_size, vocab_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd2cfc",
   "metadata": {},
   "source": [
    "### Two-Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoWayAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Mecanismo de atención bidireccional entre secuencias fuente y destino.\n",
    "\n",
    "    Este módulo calcula atención desde la fuente hacia el destino y desde el destino hacia la fuente,\n",
    "    combina las representaciones contextuales y produce logits para clasificación.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_classes=3, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim (int): Dimensionalidad de los estados ocultos.\n",
    "            num_classes (int, opcional): Número de clases de salida. Por defecto es 3.\n",
    "            dropout (float, opcional): Tasa de dropout. Por defecto es 0.3.\n",
    "        \"\"\"\n",
    "        super(TwoWayAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Capas lineales para proyectar las representaciones de atención\n",
    "        self.attention_source = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attention_target = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Función de activación\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Capas completamente conectadas para la clasificación\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # Inicialización de pesos\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales utilizando inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.attention_source.weight)\n",
    "        nn.init.constant_(self.attention_source.bias, 0)\n",
    "        nn.init.xavier_uniform_(self.attention_target.weight)\n",
    "        nn.init.constant_(self.attention_target.bias, 0)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "        nn.init.constant_(self.output_layer.bias, 0)\n",
    "    \n",
    "    def forward(self, source_states, target_states, source_mask=None, target_mask=None):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del mecanismo de atención bidireccional.\n",
    "\n",
    "        Args:\n",
    "            source_states (Tensor): Estados ocultos de la secuencia fuente [batch, src_len, hidden_dim]\n",
    "            target_states (Tensor): Estados ocultos de la secuencia destino [batch, tgt_len, hidden_dim]\n",
    "            source_mask (Tensor, opcional): Máscara para la secuencia fuente [batch, src_len]\n",
    "            target_mask (Tensor, opcional): Máscara para la secuencia destino [batch, tgt_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log probabilidades para cada clase [batch, num_classes]\n",
    "        \"\"\"\n",
    "        # Paso 1: Obtener representaciones globales de la fuente y el destino\n",
    "        # Usando mean pooling con máscara si está disponible\n",
    "        if source_mask is not None:\n",
    "            # Evitar división por cero\n",
    "            src_lengths = source_mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "            source_rep = (source_states * source_mask.unsqueeze(2)).sum(dim=1) / src_lengths\n",
    "        else:\n",
    "            source_rep = source_states.mean(dim=1)  # [batch, hidden_dim]\n",
    "        \n",
    "        if target_mask is not None:\n",
    "            tgt_lengths = target_mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "            target_rep = (target_states * target_mask.unsqueeze(2)).sum(dim=1) / tgt_lengths\n",
    "        else:\n",
    "            target_rep = target_states.mean(dim=1)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Aplicar dropout a las representaciones\n",
    "        source_rep = self.dropout(source_rep)\n",
    "        target_rep = self.dropout(target_rep)\n",
    "        \n",
    "        # Paso 2: Proyectar las representaciones\n",
    "        source_proj = self.attention_source(source_rep)  # [batch, hidden_dim]\n",
    "        target_proj = self.attention_target(target_rep)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Paso 3: Atención de fuente hacia destino\n",
    "        # Calcular pesos de atención de destino basados en la fuente\n",
    "        source_proj_expanded = source_proj.unsqueeze(2)  # [batch, hidden_dim, 1]\n",
    "        attention_weights_target = torch.bmm(target_states, source_proj_expanded).squeeze(2)  # [batch, tgt_len]\n",
    "        \n",
    "        if target_mask is not None:\n",
    "            attention_weights_target = attention_weights_target.masked_fill(target_mask == 0, -1e10)\n",
    "        \n",
    "        attention_weights_target = F.softmax(attention_weights_target, dim=1)  # [batch, tgt_len]\n",
    "        attention_weights_target = self.dropout(attention_weights_target)\n",
    "        \n",
    "        # Vector de contexto para el destino\n",
    "        target_context = torch.bmm(attention_weights_target.unsqueeze(1), target_states).squeeze(1)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Paso 4: Atención de destino hacia fuente\n",
    "        # Calcular pesos de atención de fuente basados en el destino\n",
    "        target_proj_expanded = target_proj.unsqueeze(2)  # [batch, hidden_dim, 1]\n",
    "        attention_weights_source = torch.bmm(source_states, target_proj_expanded).squeeze(2)  # [batch, src_len]\n",
    "        \n",
    "        if source_mask is not None:\n",
    "            attention_weights_source = attention_weights_source.masked_fill(source_mask == 0, -1e10)\n",
    "        \n",
    "        attention_weights_source = F.softmax(attention_weights_source, dim=1)  # [batch, src_len]\n",
    "        attention_weights_source = self.dropout(attention_weights_source)\n",
    "        \n",
    "        # Vector de contexto para la fuente\n",
    "        source_context = torch.bmm(attention_weights_source.unsqueeze(1), source_states).squeeze(1)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Paso 5: Concatenar los vectores de contexto\n",
    "        combined_context = torch.cat([target_context, source_context], dim=1)  # [batch, hidden_dim * 2]\n",
    "        combined_context = self.dropout(combined_context)\n",
    "        \n",
    "        # Paso 6: Pasar por una capa completamente conectada y aplicar activación\n",
    "        combined_representation = self.activation(self.fc(combined_context))  # [batch, hidden_dim]\n",
    "        combined_representation = self.dropout(combined_representation)\n",
    "        \n",
    "        # Paso 7: Capa de salida para clasificación\n",
    "        output = self.output_layer(combined_representation)  # [batch, num_classes]\n",
    "        \n",
    "        # Retornar log probabilidades\n",
    "        return F.log_softmax(output, dim=1)  # [batch, num_classes]\n",
    "\n",
    "# Ejemplo de configuración del modelo\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros del modelo\n",
    "    hidden_dim = 128      # Dimensión oculta de los estados\n",
    "    num_classes = 3       # Número de clases para clasificación\n",
    "    dropout = 0.3         # Tasa de dropout\n",
    "    \n",
    "    # Crear instancia del modelo\n",
    "    model = TwoWayAttention(hidden_dim, num_classes=num_classes, dropout=dropout)\n",
    "    \n",
    "    # Seleccionar dispositivo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Ejemplo de datos de entrada\n",
    "    batch_size = 4\n",
    "    seq_len_source = 5\n",
    "    seq_len_target = 7\n",
    "    \n",
    "    # Simulación de estados ocultos de source y target\n",
    "    source_states = torch.randn(batch_size, seq_len_source, hidden_dim).to(device)\n",
    "    target_states = torch.randn(batch_size, seq_len_target, hidden_dim).to(device)\n",
    "    \n",
    "    # Opcional: máscaras para source y target (1 indica válido, 0 indica padding)\n",
    "    source_mask = torch.ones(batch_size, seq_len_source).to(device)  # Aquí no hay padding\n",
    "    target_mask = torch.ones(batch_size, seq_len_target).to(device)  # Aquí no hay padding\n",
    "    \n",
    "    # Obtener salida del modelo\n",
    "    output = model(source_states, target_states, source_mask=source_mask, target_mask=target_mask)\n",
    "    print(\"Probabilidades de clasificación:\", output)\n",
    "    print(\"Forma de la salida:\", output.shape)  # [batch, num_classes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac06794",
   "metadata": {},
   "source": [
    "### Redes dinámicas de coatención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Coattention(nn.Module):\n",
    "    \"\"\"\n",
    "    Mecanismo de Co-Atención bidireccional entre secuencias fuente y destino.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim (int): Dimensionalidad de los estados ocultos.\n",
    "            dropout (float, opcional): Tasa de dropout. Por defecto es 0.3.\n",
    "        \"\"\"\n",
    "        super(Coattention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Capas lineales para proyectar las representaciones de atención\n",
    "        self.attention_source = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attention_target = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Función de activación\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Capa lineal para combinar las representaciones contextuales\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "    def forward(self, source_states, target_states, source_mask=None, target_mask=None):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del mecanismo de co-atención.\n",
    "    \n",
    "        Args:\n",
    "            source_states (Tensor): Estados ocultos de la secuencia fuente [batch, src_len, hidden_dim]\n",
    "            target_states (Tensor): Estados ocultos de la secuencia destino [batch, tgt_len, hidden_dim]\n",
    "            source_mask (Tensor, opcional): Máscara para la secuencia fuente [batch, src_len]\n",
    "            target_mask (Tensor, opcional): Máscara para la secuencia destino [batch, tgt_len]\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: Representación combinada [batch, hidden_dim]\n",
    "        \"\"\"\n",
    "        # Paso 1: Obtener representaciones globales de la fuente y el destino usando mean pooling con máscara\n",
    "        if source_mask is not None:\n",
    "            src_lengths = source_mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "            source_rep = (source_states * source_mask.unsqueeze(2)).sum(dim=1) / src_lengths\n",
    "        else:\n",
    "            source_rep = source_states.mean(dim=1)  # [batch, hidden_dim]\n",
    "        \n",
    "        if target_mask is not None:\n",
    "            tgt_lengths = target_mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "            target_rep = (target_states * target_mask.unsqueeze(2)).sum(dim=1) / tgt_lengths\n",
    "        else:\n",
    "            target_rep = target_states.mean(dim=1)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Aplicar dropout a las representaciones\n",
    "        source_rep = self.dropout(source_rep)\n",
    "        target_rep = self.dropout(target_rep)\n",
    "        \n",
    "        # Paso 2: Proyectar las representaciones\n",
    "        source_proj = self.activation(self.attention_source(source_rep))  # [batch, hidden_dim]\n",
    "        target_proj = self.activation(self.attention_target(target_rep))  # [batch, hidden_dim]\n",
    "        \n",
    "        # Paso 3: Atención de fuente hacia destino\n",
    "        source_proj_expanded = source_proj.unsqueeze(2)  # [batch, hidden_dim, 1]\n",
    "        attention_weights_target = torch.bmm(target_states, source_proj_expanded).squeeze(2)  # [batch, tgt_len]\n",
    "        \n",
    "        if target_mask is not None:\n",
    "            attention_weights_target = attention_weights_target.masked_fill(target_mask == 0, -1e10)\n",
    "        \n",
    "        attention_weights_target = F.softmax(attention_weights_target, dim=1)  # [batch, tgt_len]\n",
    "        attention_weights_target = self.dropout(attention_weights_target)\n",
    "        \n",
    "        # Vector de contexto para el destino\n",
    "        target_context = torch.bmm(attention_weights_target.unsqueeze(1), target_states).squeeze(1)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Paso 4: Atención de destino hacia fuente\n",
    "        target_proj_expanded = target_proj.unsqueeze(2)  # [batch, hidden_dim, 1]\n",
    "        attention_weights_source = torch.bmm(source_states, target_proj_expanded).squeeze(2)  # [batch, src_len]\n",
    "        \n",
    "        if source_mask is not None:\n",
    "            attention_weights_source = attention_weights_source.masked_fill(source_mask == 0, -1e10)\n",
    "        \n",
    "        attention_weights_source = F.softmax(attention_weights_source, dim=1)  # [batch, src_len]\n",
    "        attention_weights_source = self.dropout(attention_weights_source)\n",
    "        \n",
    "        # Vector de contexto para la fuente\n",
    "        source_context = torch.bmm(attention_weights_source.unsqueeze(1), source_states).squeeze(1)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Paso 5: Concatenar los vectores de contexto\n",
    "        combined_context = torch.cat([target_context, source_context], dim=1)  # [batch, hidden_dim * 2]\n",
    "        combined_context = self.dropout(combined_context)\n",
    "        \n",
    "        # Paso 6: Pasar por una capa completamente conectada y aplicar activación\n",
    "        combined_representation = self.activation(self.fc(combined_context))  # [batch, hidden_dim]\n",
    "        combined_representation = self.dropout(combined_representation)\n",
    "        \n",
    "        return combined_representation  # [batch, hidden_dim]\n",
    "\n",
    "class DynamicCoattentionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Red de Co-Atención Dinámica para tareas de procesamiento de lenguaje natural.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes=3, num_layers=1, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Tamaño del vocabulario.\n",
    "            embedding_dim (int): Dimensionalidad de los embeddings.\n",
    "            hidden_dim (int): Dimensionalidad de los estados ocultos de los LSTMs.\n",
    "            num_classes (int, opcional): Número de clases para clasificación. Por defecto es 3.\n",
    "            num_layers (int, opcional): Número de capas en los LSTMs. Por defecto es 1.\n",
    "            dropout (float, opcional): Tasa de dropout. Por defecto es 0.3.\n",
    "        \"\"\"\n",
    "        super(DynamicCoattentionNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Embedding layer con manejo de padding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # LSTMs bidireccionales para documento y pregunta\n",
    "        self.document_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                                     bidirectional=True, batch_first=True, dropout=dropout if num_layers >1 else 0)\n",
    "        self.question_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                                     bidirectional=True, batch_first=True, dropout=dropout if num_layers >1 else 0)\n",
    "        \n",
    "        # Mecanismo de Co-Atención\n",
    "        self.coattention = Coattention(hidden_dim * 2, dropout=dropout)\n",
    "        \n",
    "        # LSTM para la salida final (opcional, según la tarea)\n",
    "        self.coattention_lstm = nn.LSTM(hidden_dim * 2, hidden_dim, \n",
    "                                        bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Capa completamente conectada para clasificación\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        \n",
    "        # Inicialización de pesos\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas de embedding y lineales utilizando inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.coattention.fc.weight)\n",
    "        nn.init.constant_(self.coattention.fc.bias, 0)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "    \n",
    "    def forward(self, document, question, doc_lengths, ques_lengths):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del modelo Dynamic Co-Attention Network.\n",
    "    \n",
    "        Args:\n",
    "            document (Tensor): Índices de palabras del documento [batch, doc_len]\n",
    "            question (Tensor): Índices de palabras de la pregunta [batch, ques_len]\n",
    "            doc_lengths (Tensor): Longitudes de cada documento [batch]\n",
    "            ques_lengths (Tensor): Longitudes de cada pregunta [batch]\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: Log probabilidades para cada clase [batch, num_classes]\n",
    "        \"\"\"\n",
    "        # Paso 1: Embedding y aplicación de dropout\n",
    "        doc_embed = self.dropout(self.embedding(document))    # [batch, doc_len, embedding_dim]\n",
    "        ques_embed = self.dropout(self.embedding(question))   # [batch, ques_len, embedding_dim]\n",
    "        \n",
    "        # Paso 2: Pasar por LSTMs con manejo de secuencias\n",
    "        # Document\n",
    "        packed_doc = nn.utils.rnn.pack_padded_sequence(doc_embed, doc_lengths.cpu(), \n",
    "                                                      batch_first=True, enforce_sorted=False)\n",
    "        doc_output, _ = self.document_lstm(packed_doc)        # [batch, doc_len, hidden_dim*2]\n",
    "        doc_output, _ = nn.utils.rnn.pad_packed_sequence(doc_output, batch_first=True)\n",
    "        \n",
    "        # Pregunta\n",
    "        packed_ques = nn.utils.rnn.pack_padded_sequence(ques_embed, ques_lengths.cpu(), \n",
    "                                                       batch_first=True, enforce_sorted=False)\n",
    "        ques_output, _ = self.question_lstm(packed_ques)      # [batch, ques_len, hidden_dim*2]\n",
    "        ques_output, _ = nn.utils.rnn.pad_packed_sequence(ques_output, batch_first=True)\n",
    "        \n",
    "        # Paso 3: Mecanismo de Co-Atención para obtener representación combinada\n",
    "        # Crear máscaras si las secuencias tienen padding\n",
    "        source_mask = (document != 0).float()                  # [batch, doc_len]\n",
    "        target_mask = (question != 0).float()                  # [batch, ques_len]\n",
    "        \n",
    "        combined_representation = self.coattention(doc_output, ques_output, \n",
    "                                                   source_mask=source_mask, target_mask=target_mask)  # [batch, hidden_dim*2]\n",
    "        \n",
    "        # Paso 4: Pasar por otro LSTM para capturar relaciones dinámicas\n",
    "        # Expandir las dimensiones para LSTM: [batch, 1, hidden_dim*2]\n",
    "        combined_representation = combined_representation.unsqueeze(1)\n",
    "        coattention_output, _ = self.coattention_lstm(combined_representation)  # [batch, 1, hidden_dim*2]\n",
    "        coattention_output = coattention_output.squeeze(1)  # [batch, hidden_dim*2]\n",
    "        \n",
    "        # Paso 5: Capa de clasificación\n",
    "        logits = self.fc(coattention_output)  # [batch, num_classes]\n",
    "        \n",
    "        # Retornar log probabilidades\n",
    "        return F.log_softmax(logits, dim=1)  # [batch, num_classes]\n",
    "\n",
    "# Configuración del modelo y ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros del modelo\n",
    "    vocab_size = 10000    # Tamaño del vocabulario\n",
    "    embedding_dim = 300   # Dimensionalidad de los embeddings\n",
    "    hidden_dim = 128      # Dimensionalidad oculta de los LSTMs\n",
    "    num_classes = 3       # Número de clases para clasificación\n",
    "    num_layers = 1        # Número de capas en los LSTMs\n",
    "    dropout = 0.3         # Tasa de dropout\n",
    "    \n",
    "    # Crear instancia del modelo\n",
    "    model = DynamicCoattentionNetwork(vocab_size, embedding_dim, hidden_dim, \n",
    "                                      num_classes=num_classes, num_layers=num_layers, dropout=dropout)\n",
    "    \n",
    "    # Seleccionar dispositivo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Ejemplo de datos de entrada\n",
    "    batch_size = 4\n",
    "    doc_len = 50\n",
    "    ques_len = 10\n",
    "    \n",
    "    # Simulación de datos de entrada (documento y pregunta)\n",
    "    # Asumiendo que 0 es el índice de padding\n",
    "    document = torch.randint(1, vocab_size, (batch_size, doc_len)).to(device)  # [batch, doc_len]\n",
    "    question = torch.randint(1, vocab_size, (batch_size, ques_len)).to(device)   # [batch, ques_len]\n",
    "    \n",
    "    # Longitudes de las secuencias (sin padding)\n",
    "    doc_lengths = torch.tensor([50, 45, 30, 20]).to(device)   # [batch]\n",
    "    ques_lengths = torch.tensor([10, 8, 5, 7]).to(device)    # [batch]\n",
    "    \n",
    "    # Obtener salida del modelo\n",
    "    output = model(document, question, doc_lengths, ques_lengths)\n",
    "    print(\"Probabilidades de clasificación:\", output)\n",
    "    print(\"Forma de la salida:\", output.shape)  # [batch, num_classes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010deee",
   "metadata": {},
   "source": [
    "### Autoatención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Módulo de Self-Attention mejorado con soporte para máscaras, dropout,\n",
    "    layer normalization y conexiones residuales.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_heads=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim (int): Dimensionalidad de los embeddings de entrada.\n",
    "            hidden_dim (int): Dimensionalidad de las proyecciones de Query, Key y Value.\n",
    "            num_heads (int, opcional): Número de cabezas de atención. Por defecto es 1.\n",
    "            dropout (float, opcional): Tasa de dropout. Por defecto es 0.1.\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim debe ser divisible por num_heads\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        # Proyecciones para Query, Key y Value\n",
    "        self.query = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.key = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.value = nn.Linear(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Dropout para los pesos de atención\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Proyección final\n",
    "        self.fc_out = nn.Linear(hidden_dim, embedding_dim)\n",
    "\n",
    "        # Inicialización de pesos\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales utilizando inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.query.weight)\n",
    "        nn.init.xavier_uniform_(self.key.weight)\n",
    "        nn.init.xavier_uniform_(self.value.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "        nn.init.zeros_(self.query.bias)\n",
    "        nn.init.zeros_(self.key.bias)\n",
    "        nn.init.zeros_(self.value.bias)\n",
    "        nn.init.zeros_(self.fc_out.bias)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del módulo de Self-Attention.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Entrada de embeddings [batch_size, seq_len, embedding_dim].\n",
    "            mask (Tensor, opcional): Máscara de atención [batch_size, 1, 1, seq_len] o [batch_size, 1, seq_len].\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: \n",
    "                - Output de atención contextualizada [batch_size, seq_len, embedding_dim].\n",
    "                - Pesos de atención [batch_size, num_heads, seq_len, seq_len].\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Aplicar Layer Normalization y Residual Connection\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # Proyección y reshape para Multi-Head Attention\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)    # [batch, num_heads, seq_len, head_dim]\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # Calcular scores de atención\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [batch, num_heads, seq_len, seq_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            # Asegurarse de que la máscara tiene el mismo número de dimensiones que scores\n",
    "            # mask debe ser de forma [batch_size, 1, 1, seq_len] o [batch_size, 1, seq_len, seq_len]\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Aplicar softmax para obtener los pesos de atención\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # [batch, num_heads, seq_len, seq_len]\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Multiplicar los pesos de atención con los valores\n",
    "        attention_output = torch.matmul(attention_weights, V)  # [batch, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # Concatenar las cabezas de atención\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)  # [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Proyección final\n",
    "        attention_output = self.fc_out(attention_output)  # [batch, seq_len, embedding_dim]\n",
    "        attention_output = self.dropout(attention_output)\n",
    "\n",
    "        # Residual Connection\n",
    "        output = attention_output + residual  # [batch, seq_len, embedding_dim]\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Ejemplo de uso del módulo SelfAttention mejorado\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros de ejemplo\n",
    "    embedding_dim = 64\n",
    "    hidden_dim = 64  # Debe ser divisible por num_heads\n",
    "    num_heads = 8\n",
    "    dropout = 0.1\n",
    "    seq_len = 10\n",
    "    batch_size = 2\n",
    "\n",
    "    # Instanciar el módulo de Self-Attention\n",
    "    self_attention = SelfAttention(embedding_dim, hidden_dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "    # Datos de entrada simulados (batch de secuencias de palabras)\n",
    "    x = torch.randn(batch_size, seq_len, embedding_dim)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "    # Ejemplo de máscara (opcional)\n",
    "    # Supongamos que la primera secuencia tiene padding en las últimas 2 posiciones\n",
    "    mask = torch.tensor([\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    ])  # [batch_size, seq_len]\n",
    "    mask = mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "\n",
    "    # Obtener salida de atención y matriz de pesos\n",
    "    output, attention_weights = self_attention(x, mask=mask)\n",
    "\n",
    "    print(\"Output de atención:\", output.shape)  # [batch_size, seq_len, embedding_dim]\n",
    "    print(\"Pesos de atención:\", attention_weights.shape)  # [batch_size, num_heads, seq_len, seq_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9fb43",
   "metadata": {},
   "source": [
    "### Key-Value (Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183eaf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class KeyValuePredictAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Módulo de Atención Key-Value-Predict mejorado con soporte para máscaras, dropout,\n",
    "    layer normalization y conexiones residuales.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_heads=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim (int): Dimensionalidad de los embeddings de entrada.\n",
    "            hidden_dim (int): Dimensionalidad de las proyecciones de Key, Value y Predict.\n",
    "            num_heads (int, opcional): Número de cabezas de atención. Por defecto es 1.\n",
    "            dropout (float, opcional): Tasa de dropout. Por defecto es 0.1.\n",
    "        \"\"\"\n",
    "        super(KeyValuePredictAttention, self).__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim debe ser divisible por num_heads\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        # Proyecciones para Key, Value y Predict\n",
    "        self.key_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.predict_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Dropout para los pesos de atención\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Proyección final\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, embedding_dim)\n",
    "\n",
    "        # Inicialización de pesos\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales utilizando inicialización Xavier.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.key_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.value_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.predict_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "        nn.init.zeros_(self.key_proj.bias)\n",
    "        nn.init.zeros_(self.value_proj.bias)\n",
    "        nn.init.zeros_(self.predict_proj.bias)\n",
    "        nn.init.zeros_(self.fc_out.bias)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del módulo de Atención Key-Value-Predict.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Entrada de embeddings [batch_size, seq_len, embedding_dim].\n",
    "            mask (Tensor, opcional): Máscara de atención [batch_size, 1, 1, seq_len] o [batch_size, 1, seq_len].\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: \n",
    "                - Output de atención contextualizada [batch_size, seq_len, embedding_dim].\n",
    "                - Pesos de atención [batch_size, num_heads, seq_len, seq_len].\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Aplicar Layer Normalization y Residual Connection\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # Proyección y reshape para Multi-Head Attention\n",
    "        K = self.key_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)    # [batch, num_heads, seq_len, head_dim]\n",
    "        V = self.value_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
    "        P = self.predict_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # Calcular scores de atención usando Key\n",
    "        scores = torch.matmul(K, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [batch, num_heads, seq_len, seq_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            # Asegurarse de que la máscara tiene el mismo número de dimensiones que scores\n",
    "            # mask debe ser de forma [batch_size, 1, 1, seq_len] o [batch_size, 1, seq_len, seq_len]\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Aplicar softmax para obtener los pesos de atención\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # [batch, num_heads, seq_len, seq_len]\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Multiplicar los pesos de atención con Value\n",
    "        context = torch.matmul(attention_weights, V)  # [batch, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # Multiplicar los pesos de atención con Predict\n",
    "        predict = torch.matmul(attention_weights, P)  # [batch, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # Concatenar Contexto y Predict\n",
    "        combined = torch.cat([context, predict], dim=-1)  # [batch, num_heads, seq_len, head_dim * 2]\n",
    "\n",
    "        # Proyección final\n",
    "        combined = combined.view(batch_size, self.num_heads, seq_len, self.head_dim * 2)\n",
    "        combined = combined.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim * 2)  # [batch, seq_len, hidden_dim * 2]\n",
    "        output = self.fc_out(combined)  # [batch, seq_len, embedding_dim]\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # Residual Connection\n",
    "        output = output + residual  # [batch, seq_len, embedding_dim]\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Ejemplo de uso del módulo KeyValuePredictAttention mejorado\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros de ejemplo\n",
    "    embedding_dim = 64\n",
    "    hidden_dim = 64  # Debe ser divisible por num_heads\n",
    "    num_heads = 8\n",
    "    dropout = 0.1\n",
    "    seq_len = 10\n",
    "    batch_size = 2\n",
    "\n",
    "    # Instanciar el módulo de Key-Value-Predict Attention\n",
    "    kvp_attention = KeyValuePredictAttention(embedding_dim, hidden_dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "    # Mover el modelo al dispositivo adecuado\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kvp_attention.to(device)\n",
    "\n",
    "    # Datos de entrada simulados (batch de secuencias de palabras)\n",
    "    x = torch.randn(batch_size, seq_len, embedding_dim).to(device)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "    # Ejemplo de máscara (opcional)\n",
    "    # Supongamos que la primera secuencia tiene padding en las últimas 2 posiciones\n",
    "    mask = torch.tensor([\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    ]).to(device)  # [batch_size, seq_len]\n",
    "    mask = mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "\n",
    "    # Obtener salida de atención y matriz de pesos\n",
    "    output, attention_weights = kvp_attention(x, mask=mask)\n",
    "\n",
    "    print(\"Output de atención:\", output.shape)  # [batch_size, seq_len, embedding_dim]\n",
    "    print(\"Pesos de atención:\", attention_weights.shape)  # [batch_size, num_heads, seq_len, seq_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcde262",
   "metadata": {},
   "source": [
    "### Atención sobre atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionOverAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Módulo de Atención sobre Atención (Attention over Attention) mejorado.\n",
    "    \n",
    "    Este módulo realiza una atención bidireccional entre un documento y una pregunta,\n",
    "    combinando las representaciones contextuales para producir un vector de contexto final.\n",
    "    \n",
    "    Mejoras:\n",
    "        - Soporte para máscaras de padding.\n",
    "        - Incorporación de Dropout para regularización.\n",
    "        - Layer Normalization para estabilidad del entrenamiento.\n",
    "        - Conexiones residuales para facilitar el flujo de gradientes.\n",
    "        - Inicialización adecuada de pesos.\n",
    "        - Configurabilidad de parámetros como número de capas GRU y tasa de dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inicializa el módulo AttentionOverAttention.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim (int): Dimensionalidad de los embeddings de entrada.\n",
    "            hidden_dim (int): Dimensionalidad de los estados ocultos de las Bi-GRU.\n",
    "            num_layers (int, opcional): Número de capas en las Bi-GRU. Por defecto es 1.\n",
    "            dropout (float, opcional): Tasa de dropout. Por defecto es 0.1.\n",
    "        \"\"\"\n",
    "        super(AttentionOverAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        # Capas Bi-GRU para documento y pregunta\n",
    "        self.document_gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.question_gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "        # Proyección final\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        \n",
    "        # Inicialización de pesos\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales y GRUs utilizando inicialización Xavier.\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and isinstance(param, nn.Linear):\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name and isinstance(param, nn.Linear):\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight_ih' in name and isinstance(param, nn.GRU):\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name and isinstance(param, nn.GRU):\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'bias_ih' in name and isinstance(param, nn.GRU):\n",
    "                nn.init.zeros_(param.data)\n",
    "            elif 'bias_hh' in name and isinstance(param, nn.GRU):\n",
    "                nn.init.zeros_(param.data)\n",
    "    \n",
    "    def forward(self, document, question, doc_lengths=None, ques_lengths=None):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del módulo AttentionOverAttention.\n",
    "        \n",
    "        Args:\n",
    "            document (Tensor): Embeddings del documento [batch_size, doc_len, embedding_dim].\n",
    "            question (Tensor): Embeddings de la pregunta [batch_size, ques_len, embedding_dim].\n",
    "            doc_lengths (Tensor, opcional): Longitudes reales de cada documento [batch_size].\n",
    "            ques_lengths (Tensor, opcional): Longitudes reales de cada pregunta [batch_size].\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]:\n",
    "                - Context Vector: Representación combinada [batch_size, hidden_dim*2].\n",
    "                - Attention-over-Attention: Matriz de atención [batch_size, doc_len].\n",
    "        \"\"\"\n",
    "        # Paso 1: Pasar el documento y la pregunta por las Bi-GRUs\n",
    "        doc_output, _ = self._run_gru(self.document_gru, document, doc_lengths)  # [batch, doc_len, hidden_dim*2]\n",
    "        ques_output, _ = self._run_gru(self.question_gru, question, ques_lengths)  # [batch, ques_len, hidden_dim*2]\n",
    "        \n",
    "        # Paso 2: Calcular la matriz de atención mediante el producto punto\n",
    "        attention_matrix = torch.bmm(doc_output, ques_output.transpose(1, 2))  # [batch, doc_len, ques_len]\n",
    "        \n",
    "        # Paso 3: Aplicar Softmax en ambas direcciones\n",
    "        # Softmax por columnas (atención del documento hacia la pregunta)\n",
    "        column_softmax = F.softmax(attention_matrix, dim=2)  # [batch, doc_len, ques_len]\n",
    "        \n",
    "        # Softmax por filas (atención de la pregunta hacia el documento)\n",
    "        row_softmax = F.softmax(attention_matrix.transpose(1, 2), dim=2)  # [batch, ques_len, doc_len]\n",
    "        \n",
    "        # Paso 4: Calcular Atención sobre Atención (AoA)\n",
    "        # Promediar la atención en las columnas para obtener un vector de atención final\n",
    "        attention_over_attention = torch.mean(column_softmax * row_softmax.transpose(1, 2), dim=2)  # [batch, doc_len]\n",
    "        \n",
    "        # Paso 5: Ponderar el documento con AoA\n",
    "        # Expandir la dimensión para multiplicar y obtener una representación final\n",
    "        context_vector = torch.bmm(attention_over_attention.unsqueeze(1), doc_output).squeeze(1)  # [batch, hidden_dim*2]\n",
    "        \n",
    "        # Paso 6: Aplicar Layer Normalization y Dropout\n",
    "        context_vector = self.layer_norm(context_vector)\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        # Paso 7: Pasar por una capa completamente conectada\n",
    "        context_vector = F.relu(self.fc(context_vector))  # [batch, hidden_dim*2]\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        \n",
    "        return context_vector, attention_over_attention\n",
    "    \n",
    "    def _run_gru(self, gru, x, lengths):\n",
    "        \"\"\"\n",
    "        Ejecuta una GRU bidireccional con manejo de secuencias empaquetadas.\n",
    "        \n",
    "        Args:\n",
    "            gru (nn.GRU): La capa GRU a utilizar.\n",
    "            x (Tensor): Entrada a la GRU [batch_size, seq_len, embedding_dim].\n",
    "            lengths (Tensor, opcional): Longitudes reales de las secuencias [batch_size].\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]:\n",
    "                - Output de la GRU [batch_size, seq_len, hidden_dim*2].\n",
    "                - Estado oculto final.\n",
    "        \"\"\"\n",
    "        if lengths is not None:\n",
    "            # Ordenar las secuencias por longitud descendente\n",
    "            lengths_sorted, sorted_idx = lengths.sort(descending=True)\n",
    "            x_sorted = x[sorted_idx]\n",
    "            \n",
    "            # Empaquetar las secuencias\n",
    "            packed_input = nn.utils.rnn.pack_padded_sequence(x_sorted, lengths_sorted.cpu(), batch_first=True)\n",
    "            \n",
    "            # Pasar por la GRU\n",
    "            packed_output, hidden = gru(packed_input)\n",
    "            \n",
    "            # Desempaquetar las secuencias\n",
    "            output_sorted, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "            \n",
    "            # Restaurar el orden original\n",
    "            _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "            output = output_sorted[original_idx]\n",
    "            hidden = hidden[:, original_idx]\n",
    "        else:\n",
    "            output, hidden = gru(x)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "# Ejemplo de uso del módulo AttentionOverAttention mejorado\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros de ejemplo\n",
    "    embedding_dim = 128\n",
    "    hidden_dim = 64\n",
    "    num_layers = 2\n",
    "    dropout = 0.3\n",
    "    batch_size = 2\n",
    "    doc_len = 10\n",
    "    ques_len = 5\n",
    "\n",
    "    # Crear instancia del modelo Attention-over-Attention\n",
    "    aoa_model = AttentionOverAttention(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "    # Mover el modelo al dispositivo adecuado\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    aoa_model.to(device)\n",
    "\n",
    "    # Simulación de datos de entrada (documento y pregunta)\n",
    "    document = torch.randn(batch_size, doc_len, embedding_dim).to(device)   # [batch, doc_len, embedding_dim]\n",
    "    question = torch.randn(batch_size, ques_len, embedding_dim).to(device)  # [batch, ques_len, embedding_dim]\n",
    "\n",
    "    # Simulación de longitudes reales (sin padding)\n",
    "    doc_lengths = torch.tensor([10, 7]).to(device)  # [batch]\n",
    "    ques_lengths = torch.tensor([5, 3]).to(device)  # [batch]\n",
    "\n",
    "    # Obtener salida del modelo\n",
    "    context_vector, attention_over_attention = aoa_model(document, question, doc_lengths, ques_lengths)\n",
    "\n",
    "    print(\"Vector de contexto:\", context_vector.shape)  # [batch, hidden_dim*2]\n",
    "    print(\"Attention-over-Attention:\", attention_over_attention.shape)  # [batch, doc_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c0bd2",
   "metadata": {},
   "source": [
    "### Modelo de flujo de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469fd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    Módulo de Atención Flow para el modelo BiDAF.\n",
    "\n",
    "    Este módulo implementa el mecanismo de atención bidireccional entre el contexto y la pregunta,\n",
    "    calculando las atenciones Query2Context y Context2Query.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        \"\"\"\n",
    "        Inicializa el módulo AttentionFlow.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Dimensionalidad de las representaciones ocultas.\n",
    "        \"\"\"\n",
    "        super(AttentionFlow, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Parámetros para calcular la similitud\n",
    "        self.W_c = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "        self.W_q = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "        self.W_cq = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, context, question, context_mask, question_mask):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del módulo AttentionFlow.\n",
    "\n",
    "        Args:\n",
    "            context (Tensor): Representaciones del contexto [batch_size, context_len, hidden_dim * 2].\n",
    "            question (Tensor): Representaciones de la pregunta [batch_size, question_len, hidden_dim * 2].\n",
    "            context_mask (Tensor): Máscara del contexto [batch_size, context_len].\n",
    "            question_mask (Tensor): Máscara de la pregunta [batch_size, question_len].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Representaciones combinadas [batch_size, context_len, hidden_dim * 8].\n",
    "        \"\"\"\n",
    "        batch_size, context_len, _ = context.size()\n",
    "        question_len = question.size(1)\n",
    "\n",
    "        # Expandir dimensiones para calcular la similitud\n",
    "        context_exp = context.unsqueeze(2).repeat(1, 1, question_len, 1)   # [batch, context_len, question_len, hidden_dim * 2]\n",
    "        question_exp = question.unsqueeze(1).repeat(1, context_len, 1, 1)  # [batch, context_len, question_len, hidden_dim * 2]\n",
    "\n",
    "        # Calcular la matriz de similitud\n",
    "        S = self._similarity_matrix(context_exp, question_exp)  # [batch, context_len, question_len]\n",
    "\n",
    "        # Aplicar máscaras para ignorar posiciones de padding\n",
    "        question_mask = question_mask.unsqueeze(1).expand(-1, context_len, -1)  # [batch, context_len, question_len]\n",
    "        S = S.masked_fill(~question_mask, float('-inf'))\n",
    "\n",
    "        # Atención Contexto a Pregunta (C2Q)\n",
    "        a = F.softmax(S, dim=-1)  # [batch, context_len, question_len]\n",
    "        c2q = torch.bmm(a, question)  # [batch, context_len, hidden_dim * 2]\n",
    "\n",
    "        # Atención Pregunta a Contexto (Q2C)\n",
    "        b = F.softmax(S.max(dim=2)[0], dim=-1).unsqueeze(1)  # [batch, 1, context_len]\n",
    "        q2c = torch.bmm(b, context).repeat(1, context_len, 1)  # [batch, context_len, hidden_dim * 2]\n",
    "\n",
    "        # Concatenar representaciones\n",
    "        G = torch.cat([context, c2q, context * c2q, context * q2c], dim=-1)  # [batch, context_len, hidden_dim * 8]\n",
    "\n",
    "        return G\n",
    "\n",
    "    def _similarity_matrix(self, context_exp, question_exp):\n",
    "        \"\"\"\n",
    "        Calcula la matriz de similitud entre el contexto y la pregunta.\n",
    "\n",
    "        Args:\n",
    "            context_exp (Tensor): Contexto expandido [batch, context_len, question_len, hidden_dim * 2].\n",
    "            question_exp (Tensor): Pregunta expandida [batch, context_len, question_len, hidden_dim * 2].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Matriz de similitud [batch, context_len, question_len].\n",
    "        \"\"\"\n",
    "        S = self.W_c(context_exp).squeeze(-1) + self.W_q(question_exp).squeeze(-1) + \\\n",
    "            self.W_cq(context_exp * question_exp).squeeze(-1)\n",
    "        return S\n",
    "\n",
    "class ModelingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Módulo de Modeling Layer para el modelo BiDAF.\n",
    "\n",
    "    Este módulo utiliza una Bi-GRU para modelar las relaciones dinámicas entre las palabras en el contexto.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Inicializa el módulo ModelingLayer.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Dimensionalidad de las representaciones ocultas.\n",
    "            num_layers (int, opcional): Número de capas en la Bi-GRU. Por defecto es 2.\n",
    "            dropout (float, opcional): Tasa de dropout. Por defecto es 0.2.\n",
    "        \"\"\"\n",
    "        super(ModelingLayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_dim * 8,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "    def forward(self, G):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del módulo ModelingLayer.\n",
    "\n",
    "        Args:\n",
    "            G (Tensor): Representaciones combinadas de la capa de atención [batch_size, context_len, hidden_dim * 8].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Representaciones modeladas [batch_size, context_len, hidden_dim * 2].\n",
    "        \"\"\"\n",
    "        M, _ = self.gru(G)  # [batch_size, context_len, hidden_dim * 2]\n",
    "        return M\n",
    "\n",
    "class OutputLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Módulo de Output Layer para el modelo BiDAF.\n",
    "\n",
    "    Este módulo predice las posiciones de inicio y fin de la respuesta en el contexto.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Inicializa el módulo OutputLayer.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Dimensionalidad de las representaciones ocultas.\n",
    "            dropout (float, opcional): Tasa de dropout. Por defecto es 0.2.\n",
    "        \"\"\"\n",
    "        super(OutputLayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.p1_weight_g = nn.Linear(hidden_dim * 8, 1)\n",
    "        self.p1_weight_m = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.p2_weight_g = nn.Linear(hidden_dim * 8, 1)\n",
    "        self.p2_weight_m = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=hidden_dim * 2,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, G, M, mask):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del módulo OutputLayer.\n",
    "\n",
    "        Args:\n",
    "            G (Tensor): Representaciones combinadas de la capa de atención [batch_size, context_len, hidden_dim * 8].\n",
    "            M (Tensor): Representaciones modeladas [batch_size, context_len, hidden_dim * 2].\n",
    "            mask (Tensor): Máscara del contexto [batch_size, context_len].\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: Logits de inicio y fin [batch_size, context_len].\n",
    "        \"\"\"\n",
    "        # Predicción de inicio\n",
    "        logits1 = (self.p1_weight_g(G) + self.p1_weight_m(M)).squeeze(-1)  # [batch_size, context_len]\n",
    "        logits1 = logits1.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "        # Pasar M por otra capa GRU\n",
    "        M2, _ = self.rnn(M)  # [batch_size, context_len, hidden_dim * 2]\n",
    "\n",
    "        # Predicción de fin\n",
    "        logits2 = (self.p2_weight_g(G) + self.p2_weight_m(M2)).squeeze(-1)  # [batch_size, context_len]\n",
    "        logits2 = logits2.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "        return logits1, logits2\n",
    "\n",
    "class BiDAF(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo BiDAF (Bidirectional Attention Flow) para tareas de preguntas y respuestas.\n",
    "\n",
    "    Este modelo sigue la arquitectura presentada en el artículo original de BiDAF,\n",
    "    implementando capas de embedding contextual, atención bidireccional, modelado y capa de salida.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_dim, char_dim, hidden_dim, vocab_size, char_vocab_size, embedding_matrix=None):\n",
    "        \"\"\"\n",
    "        Inicializa el modelo BiDAF.\n",
    "\n",
    "        Args:\n",
    "            word_dim (int): Dimensionalidad de los embeddings de palabra.\n",
    "            char_dim (int): Dimensionalidad de los embeddings de carácter.\n",
    "            hidden_dim (int): Dimensionalidad de las representaciones ocultas en las GRUs.\n",
    "            vocab_size (int): Tamaño del vocabulario de palabras.\n",
    "            char_vocab_size (int): Tamaño del vocabulario de caracteres.\n",
    "            embedding_matrix (Tensor, opcional): Matriz de embeddings preentrenados. Por defecto es None.\n",
    "        \"\"\"\n",
    "        super(BiDAF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding de palabras\n",
    "        self.word_embedding = nn.Embedding(vocab_size, word_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            self.word_embedding.weight.data.copy_(embedding_matrix)\n",
    "\n",
    "        # Embedding de caracteres\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_dim, padding_idx=0)\n",
    "        self.char_conv = nn.Conv1d(in_channels=char_dim, out_channels=char_dim, kernel_size=5, padding=2)\n",
    "\n",
    "        # Contextual Embedding Layer (Bi-GRU)\n",
    "        self.contextual_gru = nn.GRU(\n",
    "            input_size=word_dim + char_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Attention Flow Layer\n",
    "        self.attention_flow = AttentionFlow(hidden_dim)\n",
    "\n",
    "        # Modeling Layer\n",
    "        self.modeling_layer = ModelingLayer(hidden_dim)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = OutputLayer(hidden_dim)\n",
    "\n",
    "        # Inicialización de pesos\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de las capas lineales y convolucionales.\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, context, query, context_char=None, query_char=None):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante del modelo BiDAF.\n",
    "\n",
    "        Args:\n",
    "            context (Tensor): Índices de palabras del contexto [batch_size, context_len].\n",
    "            query (Tensor): Índices de palabras de la pregunta [batch_size, query_len].\n",
    "            context_char (Tensor, opcional): Índices de caracteres del contexto [batch_size, context_len, char_len]. Por defecto es None.\n",
    "            query_char (Tensor, opcional): Índices de caracteres de la pregunta [batch_size, query_len, char_len]. Por defecto es None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]:\n",
    "                - Logits de inicio [batch_size, context_len].\n",
    "                - Logits de fin [batch_size, context_len].\n",
    "        \"\"\"\n",
    "        batch_size, context_len = context.size()\n",
    "        query_len = query.size(1)\n",
    "\n",
    "        # Embedding de palabras\n",
    "        word_emb_context = self.word_embedding(context)  # [batch_size, context_len, word_dim]\n",
    "        word_emb_query = self.word_embedding(query)      # [batch_size, query_len, word_dim]\n",
    "\n",
    "        if context_char is not None and query_char is not None:\n",
    "            # Embedding de caracteres para el contexto\n",
    "            char_emb_context = self.char_embedding(context_char)  # [batch_size, context_len, char_len, char_dim]\n",
    "            batch_size, context_len, char_len, char_dim = char_emb_context.size()\n",
    "            char_emb_context = char_emb_context.view(-1, char_len, char_dim)  # [batch_size * context_len, char_len, char_dim]\n",
    "            char_emb_context = char_emb_context.transpose(1, 2)               # [batch_size * context_len, char_dim, char_len]\n",
    "            char_features_context = F.relu(self.char_conv(char_emb_context))  # [batch_size * context_len, char_dim, char_len]\n",
    "            char_features_context, _ = torch.max(char_features_context, dim=2)  # [batch_size * context_len, char_dim]\n",
    "            char_features_context = char_features_context.view(batch_size, context_len, -1)  # [batch_size, context_len, char_dim]\n",
    "\n",
    "            # Embedding de caracteres para la pregunta\n",
    "            char_emb_query = self.char_embedding(query_char)  # [batch_size, query_len, char_len, char_dim]\n",
    "            batch_size, query_len, char_len, char_dim = char_emb_query.size()\n",
    "            char_emb_query = char_emb_query.view(-1, char_len, char_dim)  # [batch_size * query_len, char_len, char_dim]\n",
    "            char_emb_query = char_emb_query.transpose(1, 2)               # [batch_size * query_len, char_dim, char_len]\n",
    "            char_features_query = F.relu(self.char_conv(char_emb_query))  # [batch_size * query_len, char_dim, char_len]\n",
    "            char_features_query, _ = torch.max(char_features_query, dim=2)  # [batch_size * query_len, char_dim]\n",
    "            char_features_query = char_features_query.view(batch_size, query_len, -1)  # [batch_size, query_len, char_dim]\n",
    "\n",
    "            # Concatenar embeddings de palabras y caracteres\n",
    "            word_emb_context = torch.cat([word_emb_context, char_features_context], dim=-1)  # [batch_size, context_len, word_dim + char_dim]\n",
    "            word_emb_query = torch.cat([word_emb_query, char_features_query], dim=-1)        # [batch_size, query_len, word_dim + char_dim]\n",
    "\n",
    "        # Contextual Embedding Layer\n",
    "        context_output, _ = self.contextual_gru(word_emb_context)  # [batch_size, context_len, hidden_dim * 2]\n",
    "        query_output, _ = self.contextual_gru(word_emb_query)      # [batch_size, query_len, hidden_dim * 2]\n",
    "\n",
    "        # Máscaras\n",
    "        context_mask = (context != 0)  # [batch_size, context_len]\n",
    "        question_mask = (query != 0)   # [batch_size, query_len]\n",
    "\n",
    "        # Attention Flow Layer\n",
    "        G = self.attention_flow(context_output, query_output, context_mask, question_mask)  # [batch_size, context_len, hidden_dim * 8]\n",
    "\n",
    "        # Modeling Layer\n",
    "        M = self.modeling_layer(G)  # [batch_size, context_len, hidden_dim * 2]\n",
    "\n",
    "        # Output Layer\n",
    "        start_logits, end_logits = self.output_layer(G, M, context_mask)  # [batch_size, context_len], [batch_size, context_len]\n",
    "\n",
    "        return start_logits, end_logits\n",
    "\n",
    "# Configuración de ejemplo\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros del modelo\n",
    "    word_dim = 100       # Dimensionalidad del embedding de palabra\n",
    "    char_dim = 50        # Dimensionalidad del embedding de carácter\n",
    "    hidden_dim = 64      # Dimensionalidad oculta de las GRUs\n",
    "    vocab_size = 10000   # Tamaño del vocabulario de palabras\n",
    "    char_vocab_size = 100  # Tamaño del vocabulario de caracteres\n",
    "    batch_size = 2\n",
    "    context_len = 20\n",
    "    query_len = 10\n",
    "    char_len = 10        # Longitud de secuencia de caracteres\n",
    "\n",
    "    # Simulación de matriz de embeddings preentrenados (opcional)\n",
    "    embedding_matrix = torch.randn(vocab_size, word_dim)\n",
    "\n",
    "    # Crear instancia del modelo BiDAF\n",
    "    bidaf_model = BiDAF(word_dim, char_dim, hidden_dim, vocab_size, char_vocab_size, embedding_matrix=embedding_matrix)\n",
    "\n",
    "    # Seleccionar dispositivo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    bidaf_model.to(device)\n",
    "\n",
    "    # Simulación de datos de entrada (contexto y pregunta)\n",
    "    context = torch.randint(1, vocab_size, (batch_size, context_len)).to(device)      # [batch_size, context_len]\n",
    "    query = torch.randint(1, vocab_size, (batch_size, query_len)).to(device)          # [batch_size, query_len]\n",
    "\n",
    "    # Simulación de datos de caracteres\n",
    "    context_char = torch.randint(1, char_vocab_size, (batch_size, context_len, char_len)).to(device)  # [batch_size, context_len, char_len]\n",
    "    query_char = torch.randint(1, char_vocab_size, (batch_size, query_len, char_len)).to(device)      # [batch_size, query_len, char_len]\n",
    "\n",
    "    # Obtener salida del modelo\n",
    "    start_logits, end_logits = bidaf_model(context, query, context_char, query_char)\n",
    "\n",
    "    print(\"Logits de inicio:\", start_logits.shape)  # [batch_size, context_len]\n",
    "    print(\"Logits de fin:\", end_logits.shape)       # [batch_size, context_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ce0d4",
   "metadata": {},
   "source": [
    "### **Ejercicios sobre mecanismos de atención**\n",
    "\n",
    "#### **1. Self-Attention**\n",
    "\n",
    "**a. Implementación de Self-Attention Multi-cabecera**\n",
    "\n",
    "*Objetivo:* Modificar la implementación de `SelfAttention` para soportar atención multi-cabecera de manera más eficiente, siguiendo la arquitectura utilizada en Transformers.\n",
    "\n",
    "*Instrucciones:*\n",
    "- Crea una clase `MultiHeadSelfAttention` que extienda `nn.Module`.\n",
    "- Asegúrate de que las proyecciones de Query, Key y Value se realicen de manera eficiente para múltiples cabezas.\n",
    "- Implementa el mecanismo de concatenación y proyección final.\n",
    "- Añade una capa de *dropout* y *layer normalization* después de la atención.\n",
    "- Compara tu implementación con la clase `SelfAttention` mejorada proporcionada anteriormente.\n",
    "\n",
    "*Recursos:*\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [PyTorch MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)\n",
    "\n",
    "*Desafío Adicional:*\n",
    "- Añade la posibilidad de enmascarar ciertas posiciones en la atención (por ejemplo, para atención causal en modelos de generación).\n",
    "\n",
    "\n",
    "**b. Visualización de los pesos de atención**\n",
    "\n",
    "*Objetivo:* Visualizar los pesos de atención aprendidos por el módulo `SelfAttention` para comprender cómo el modelo asigna importancia a diferentes posiciones en la secuencia.\n",
    "\n",
    "*Instrucciones:*\n",
    "- Utiliza una secuencia de entrada sencilla (por ejemplo, una oración corta) y pásala a través del módulo `SelfAttention`.\n",
    "- Extrae los pesos de atención y utiliza una biblioteca de visualización (como `matplotlib`) para representar gráficamente la matriz de atención.\n",
    "- Analiza cómo los diferentes tokens se relacionan entre sí según los pesos de atención.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **2. KeyValuePredictAttention**\n",
    "\n",
    "**a. Extensión para soportar máscaras dinámicas**\n",
    "\n",
    "*Objetivo:* Mejorar la clase `KeyValuePredictAttention` para manejar máscaras dinámicas que permitan trabajar con secuencias de diferentes longitudes en un solo batch.\n",
    "\n",
    "*Instrucciones:*\n",
    "- Modifica el método `forward` para aceptar una máscara opcional que indique las posiciones válidas en la secuencia.\n",
    "- Asegúrate de que la máscara se aplique correctamente al calcular los scores de atención.\n",
    "- Prueba tu implementación con secuencias de diferentes longitudes y verifica que las posiciones de padding no influyan en la atención.\n",
    "\n",
    "*Pista:*\n",
    "- Puedes utilizar `torch.masked_fill` para aplicar la máscara a los scores de atención antes de la softmax.\n",
    "\n",
    "\n",
    "**b. Integración con una capa de salida personalizada**\n",
    "\n",
    "*Objetivo:* Añadir una capa de salida personalizada que permita transformar el output de la atención contextualizada para una tarea específica, como clasificación o regresión.\n",
    "\n",
    "*Instrucciones:*\n",
    "- Después de obtener el `output` de atención contextualizada, pásalo por una o más capas lineales con activaciones no lineales.\n",
    "- Implementa una función de pérdida adecuada según la tarea (por ejemplo, `nn.CrossEntropyLoss` para clasificación).\n",
    "- Entrena el módulo en un conjunto de datos simulado para verificar que la arquitectura es funcional.\n",
    "\n",
    "*Desafío adicional:*\n",
    "- Añade una capa de normalización adicional (como `BatchNorm1d`) antes de la capa de salida para mejorar la estabilidad del entrenamiento.\n",
    "\n",
    "\n",
    "#### **3. AttentionOverAttention**\n",
    "\n",
    "**a. Implementación de atención bidireccional completa**\n",
    "\n",
    "*Objetivo:* Completar la implementación de `AttentionOverAttention` para que capture completamente las interacciones bidireccionales entre el contexto y la pregunta.\n",
    "\n",
    "*Instrucciones:*\n",
    "- Asegúrate de que tanto la atención de `Query2Context` como `Context2Query` están correctamente implementadas y se combinan adecuadamente.\n",
    "- Modifica el mecanismo de `AttentionOverAttention` para incluir multiplicaciones element-wise adicionales o interacciones que puedan enriquecer la representación final.\n",
    "- Evalúa la efectividad de las representaciones resultantes en una tarea de preguntas y respuestas simple.\n",
    "\n",
    "---\n",
    "\n",
    "**b. Incorporación de positional encoding**\n",
    "\n",
    "*Objetivo:* Añadir codificaciones posicionales a las entradas de `AttentionOverAttention` para proporcionar información sobre la posición de cada token en la secuencia.\n",
    "\n",
    "*Instrucciones:*\n",
    "- Implementa una clase `PositionalEncoding` que añada codificaciones posicionales sinusoidales a los embeddings.\n",
    "- Integra esta clase en el flujo de `BiDAF` antes de pasar los embeddings a las GRUs.\n",
    "- Verifica que las codificaciones posicionales mejoran la capacidad del modelo para capturar relaciones de orden en la secuencia.\n",
    "\n",
    "*Recursos:*\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "\n",
    "\n",
    "#### **4. BiDAF**\n",
    "\n",
    "**a. Mejorar la integración de embeddings de caracteres**\n",
    "\n",
    "*Objetivo:* Refinar la forma en que los embeddings de caracteres se integran con los embeddings de palabras para mejorar la representación final.\n",
    "\n",
    "*Instrucciones:*\n",
    "- En lugar de utilizar una simple convolución y pooling sobre los caracteres, implementa un `Bi-GRU` o una red convolucional más profunda para procesar los embeddings de caracteres.\n",
    "- Concatenar las representaciones de caracteres procesadas con los embeddings de palabras antes de pasarlos a las GRUs contextuales.\n",
    "- Evalúa el impacto de esta mejora en la calidad de las representaciones aprendidas y en el rendimiento del modelo.\n",
    "\n",
    "*Recursos:*\n",
    "- [Bidirectional Attention Flow for Machine Comprehension](https://arxiv.org/abs/1611.01603)\n",
    "\n",
    "\n",
    "**b. Implementación de conexiones residuales en BiDAF**\n",
    "\n",
    "*Objetivo:* Añadir conexiones residuales en las diferentes capas del modelo BiDAF para facilitar el flujo de gradientes y mejorar la capacidad de representación.\n",
    "\n",
    "*Instrucciones:*\n",
    "- Añade conexiones residuales después de la capa de `AttentionFlow` y después de la `ModelingLayer`.\n",
    "- Asegúrate de que las dimensiones coincidan al sumar las salidas y las entradas de las capas.\n",
    "- Verifica que la adición de conexiones residuales no introduzca errores dimensionales y que el entrenamiento sea más estable.\n",
    "\n",
    "*Pista:*\n",
    "- Puedes utilizar `nn.LayerNorm` y `nn.Dropout` antes de las conexiones residuales para mejorar la estabilidad.\n",
    "\n",
    "\n",
    "**c. Evaluación del modelo BiDAF en un conjunto de datos simulado**\n",
    "\n",
    "*Objetivo:* Crear un conjunto de datos simulado y entrenar el modelo BiDAF mejorado para una tarea de preguntas y respuestas, evaluando su capacidad para predecir correctamente las posiciones de inicio y fin.\n",
    "\n",
    "*Instrucciones:*\n",
    "- Genera datos simulados donde el contexto contiene una respuesta clara a una pregunta dada.\n",
    "- Define las posiciones de inicio y fin en el contexto que corresponden a la respuesta.\n",
    "- Entrena el modelo BiDAF en estos datos y observa si aprende a predecir correctamente las posiciones de inicio y fin.\n",
    "- Ajusta hiperparámetros como la tasa de aprendizaje, el número de épocas y la tasa de dropout para optimizar el rendimiento.\n",
    "\n",
    "*Desafío adicional:*\n",
    "- Implementa métricas de evaluación como Exact Match (EM) y F1 para medir la precisión de las predicciones del modelo.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b43f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
