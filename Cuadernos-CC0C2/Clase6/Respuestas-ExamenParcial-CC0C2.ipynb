{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35a97d0",
   "metadata": {},
   "source": [
    "## Respuestas del examen parcial CC0C2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55574d-3519-42cb-bd20-357ab7b6abcd",
   "metadata": {},
   "source": [
    "### Problema 1\n",
    "\n",
    "El subsampling en el contexto de los modelos de Word2Vec es una técnica utilizada para reducir el número de veces que se entrenan palabras muy frecuentes. Se basa en la idea de que las palabras extremadamente comunes (como preposiciones y conjunciones) proporcionan menos información de contexto valiosa en comparación con las palabras menos frecuentes. En la práctica, cada palabra en el conjunto de entrenamiento tiene una probabilidad calculada de ser \"saltada\" durante el entrenamiento, dependiendo de su frecuencia. Esto ayuda a acelerar el entrenamiento y a mejorar la calidad de las representaciones de palabras menos frecuentes, que podrían verse oscurecidas por palabras de alta frecuencia.\n",
    "\n",
    "El negative sampling es una técnica de optimización para reducir la complejidad computacional de actualizar los pesos en la red neuronal en modelos como Word2Vec. En lugar de actualizar los pesos de todas las palabras del vocabulario para cada ejemplo de entrenamiento (lo cual es muy costoso computacionalmente), el negative sampling actualiza solo un pequeño número de \"palabras negativas\" (ejemplos negativos seleccionados aleatoriamente) junto con la palabra objetivo (ejemplo positivo). Esto no solo acelera significativamente el entrenamiento sino que también mejora la calidad de las representaciones vectoriales al enfocarse en distinguir la palabra objetivo de un pequeño subconjunto de palabras negativas.\n",
    "\n",
    "La correlación de Spearman es una medida estadística que evalúa la fuerza y la dirección de la asociación entre dos variables clasificadas. A diferencia de la correlación de Pearson, que requiere que las variables sean de escala intervalo o de razón y aproximadamente normales, la correlación de Spearman no hace suposiciones sobre la distribución de los datos y se basa en rangos. Es especialmente útil en el contexto de Word2Vec cuando se evalúa cómo las similitudes coseno calculadas entre vectores de palabras se comparan con juicios humanos de similitud (usualmente dados en estudios donde las personas califican qué tan similares son las palabras). Al correlacionar estos dos conjuntos de rankings (el calculado y el humano), se puede obtener una medida de cuán bien el modelo captura relaciones semánticas que coinciden con las percepciones humanas.\n",
    "\n",
    "#### Ejercicios:\n",
    "\n",
    "1. Implementa los modelos CBOW y Skip-gram en Python sin utilizar bibliotecas de alto nivel como Gensim (2 puntos).\n",
    "    - Escribe el código para inicializar los pesos de la red, realizar el entrenamiento mediante descenso de gradiente y calcular la función de pérdida.\n",
    "    - Añade mecanismos de subsampling y negative sampling para mejorar la eficiencia del entrenamiento. \n",
    "2. Analiza cómo diferentes hiperparámetros afectan la calidad de los embeddings vectoriales (2 puntos).\n",
    "    - Entrena modelos Word2Vec con diferentes tamaños de ventana, dimensiones de vector y tasas de aprendizaje. Utiliza un conjunto de datos estándar como el corpus de texto de Wikipedia.\n",
    "    - Evalúa los modelos usando tareas de analogía de palabras y calcula la correlación de Spearman entre las similitudes humanas y las  calculadas por el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "def build_vocab(words):\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    word_to_id = {word: vocab[word] for word in words}\n",
    "    id_to_word = {id: word for word, id in word_to_id.items()}\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "# Generar contextos y palabras objetivo para CBOW\n",
    "def generate_cbow_context(words, window_size):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
    "        target = words[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "    return contexts, targets\n",
    "\n",
    "# Generar contextos y palabras objetivo para Skip-gram\n",
    "def generate_skipgram_context(words, window_size):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        target = words[i]\n",
    "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
    "        for ctx in context:\n",
    "            contexts.append(ctx)\n",
    "            targets.append(target)\n",
    "    return contexts, targets\n",
    "\n",
    "# Inicializar parámetros del modelo\n",
    "def initialize_params(vocab_size, embedding_dim):\n",
    "    W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "    W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "    return W1, W2\n",
    "\n",
    "# Funciones de activación y derivadas\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def one_hot_encoding(index, vocab_size):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[index] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Entrenamiento del modelo CBOW\n",
    "def train_cbow(contexts, targets, word_to_id, id_to_word, vocab_size, embedding_dim, learning_rate, epochs):\n",
    "    W1, W2 = initialize_params(vocab_size, embedding_dim)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for context, target in zip(contexts, targets):\n",
    "            context_indices = [word_to_id[word] for word in context]\n",
    "            target_index = word_to_id[target]\n",
    "            \n",
    "            h = np.mean(W1[context_indices], axis=0)\n",
    "            u = np.dot(W2.T, h)\n",
    "            y_pred = softmax(u)\n",
    "            \n",
    "            e = y_pred - one_hot_encoding(target_index, vocab_size)\n",
    "            dW2 = np.outer(h, e)\n",
    "            \n",
    "            # Actualizar W1 para cada palabra en el contexto\n",
    "            for idx in context_indices:\n",
    "                W1[idx] -= learning_rate * np.dot(W2, e)\n",
    "            \n",
    "            W2 -= learning_rate * dW2\n",
    "            \n",
    "            loss += -np.log(y_pred[target_index])\n",
    "        \n",
    "        print(f'Epoch {epoch}, Loss: {loss}')\n",
    "    return W1, W2\n",
    "\n",
    "# Entrenamiento del modelo Skip-gram\n",
    "def train_skipgram(contexts, targets, word_to_id, id_to_word, vocab_size, embedding_dim, learning_rate, epochs):\n",
    "    W1, W2 = initialize_params(vocab_size, embedding_dim)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for context, target in zip(contexts, targets):\n",
    "            context_index = word_to_id[context]\n",
    "            target_index = word_to_id[target]\n",
    "            \n",
    "            h = W1[context_index]\n",
    "            u = np.dot(W2.T, h)\n",
    "            y_pred = softmax(u)\n",
    "            \n",
    "            e = y_pred - one_hot_encoding(target_index, vocab_size)\n",
    "            dW2 = np.outer(h, e)\n",
    "            dW1 = np.outer(e, W2[:, context_index])\n",
    "            \n",
    "            W1[context_index] -= learning_rate * dW1.sum(axis=0)\n",
    "            W2 -= learning_rate * dW2\n",
    "            \n",
    "            loss += -np.log(y_pred[target_index])\n",
    "        \n",
    "        print(f'Epoca {epoch}, Perdida: {loss}')\n",
    "    return W1, W2\n",
    "\n",
    "# Uso del modelo\n",
    "text = \"We are learning Natural Language Processing and it is very exciting\"\n",
    "words = preprocess(text)\n",
    "word_to_id, id_to_word = build_vocab(words)\n",
    "\n",
    "# CBOW\n",
    "contexts, targets = generate_cbow_context(words, window_size=2)\n",
    "W1_cbow, W2_cbow = train_cbow(contexts, targets, word_to_id, id_to_word, vocab_size=len(word_to_id), embedding_dim=10, learning_rate=0.01, epochs=100)\n",
    "\n",
    "# Skip-gram\n",
    "contexts, targets = generate_skipgram_context(words, window_size=2)\n",
    "W1_skipgram, W2_skipgram = train_skipgram(contexts, targets, word_to_id, id_to_word, vocab_size=len(word_to_id), embedding_dim=10, learning_rate=0.01, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661af653",
   "metadata": {},
   "source": [
    "Para mejorar la eficiencia del entrenamiento, podemos añadir dos técnicas clave:\n",
    "\n",
    "- Subsampling: Reducir la frecuencia de las palabras muy comunes para evitar que dominen el entrenamiento.\n",
    "- Negative Sampling: En lugar de actualizar todos los pesos para cada palabra en el vocabulario, solo actualizamos algunos pesos seleccionados aleatoriamente como \"negativos\".\n",
    "\n",
    "Primero, agregamos la función de subsampling para reducir la frecuencia de palabras muy comunes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e10d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "def build_vocab(words):\n",
    "    word_freq = Counter(words)\n",
    "    vocab = {word: i for i, word in enumerate(word_freq)}\n",
    "    word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "    id_to_word = {i: word for word, i in word_to_id.items()}\n",
    "    return word_to_id, id_to_word, word_freq\n",
    "\n",
    "# Subsampling de palabras frecuentes\n",
    "def subsample(words, word_freq, threshold=1e-5):\n",
    "    total_count = sum(word_freq.values())\n",
    "    prob_drop = {word: 1 - np.sqrt(threshold / (freq / total_count)) for word, freq in word_freq.items()}\n",
    "    subsampled_words = [word for word in words if random.random() > prob_drop[word]]\n",
    "    return subsampled_words\n",
    "\n",
    "# Generar contextos y palabras objetivo para CBOW\n",
    "def generate_cbow_context(words, window_size):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
    "        target = words[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "    return contexts, targets\n",
    "\n",
    "# Generar contextos y palabras objetivo para Skip-gram\n",
    "def generate_skipgram_context(words, window_size):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        target = words[i]\n",
    "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
    "        for ctx in context:\n",
    "            contexts.append(ctx)\n",
    "            targets.append(target)\n",
    "    return contexts, targets\n",
    "\n",
    "# Inicializar parámetros del modelo\n",
    "def initialize_params(vocab_size, embedding_dim):\n",
    "    W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "    W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "    return W1, W2\n",
    "\n",
    "# Funciones de activación y derivadas\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def one_hot_encoding(index, vocab_size):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[index] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Muestras negativas\n",
    "def get_negative_samples(target, vocab_size, num_neg_samples):\n",
    "    neg_samples = []\n",
    "    while len(neg_samples) < num_neg_samples:\n",
    "        neg = random.randint(0, vocab_size - 1)\n",
    "        if neg != target:\n",
    "            neg_samples.append(neg)\n",
    "    return neg_samples\n",
    "\n",
    "# Entrenamiento del modelo CBOW con Negative Sampling\n",
    "def train_cbow(contexts, targets, word_to_id, id_to_word, vocab_size, embedding_dim, learning_rate, epochs, num_neg_samples):\n",
    "    W1, W2 = initialize_params(vocab_size, embedding_dim)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for context, target in zip(contexts, targets):\n",
    "            context_indices = [word_to_id[word] for word in context]\n",
    "            target_index = word_to_id[target]\n",
    "            \n",
    "            h = np.mean(W1[context_indices], axis=0)\n",
    "            u = np.dot(W2.T, h)\n",
    "            y_pred = softmax(u)\n",
    "            \n",
    "            e = y_pred - one_hot_encoding(target_index, vocab_size)\n",
    "            dW2 = np.outer(h, e)\n",
    "            \n",
    "            # Actualizar W1 para cada palabra en el contexto\n",
    "            for idx in context_indices:\n",
    "                W1[idx] -= learning_rate * np.dot(W2, e)\n",
    "            \n",
    "            W2 -= learning_rate * dW2\n",
    "            \n",
    "            # Negative sampling\n",
    "            neg_samples = get_negative_samples(target_index, vocab_size, num_neg_samples)\n",
    "            for neg in neg_samples:\n",
    "                W2[:, neg] -= learning_rate * np.dot(h.reshape(-1, 1), np.outer(np.zeros_like(e), neg))\n",
    "            \n",
    "            loss += -np.log(y_pred[target_index])\n",
    "        \n",
    "        print(f'Epoch {epoch}, Loss: {loss}')\n",
    "    return W1, W2\n",
    "\n",
    "# Entrenamiento del modelo Skip-gram con Negative Sampling\n",
    "def train_skipgram(contexts, targets, word_to_id, id_to_word, vocab_size, embedding_dim, learning_rate, epochs, num_neg_samples):\n",
    "    W1, W2 = initialize_params(vocab_size, embedding_dim)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for context, target in zip(contexts, targets):\n",
    "            context_index = word_to_id[context]\n",
    "            target_index = word_to_id[target]\n",
    "            \n",
    "            h = W1[context_index]\n",
    "            u = np.dot(W2.T, h)\n",
    "            y_pred = softmax(u)\n",
    "            \n",
    "            e = y_pred - one_hot_encoding(target_index, vocab_size)\n",
    "            dW2 = np.outer(h, e)\n",
    "            dW1 = np.outer(e, W2[:, context_index])\n",
    "            \n",
    "            W1[context_index] -= learning_rate * dW1.sum(axis=0)\n",
    "            W2 -= learning_rate * dW2\n",
    "            \n",
    "            # Negative sampling\n",
    "            neg_samples = get_negative_samples(target_index, vocab_size, num_neg_samples)\n",
    "            for neg in neg_samples:\n",
    "                W2[:, neg] -= learning_rate * np.dot(h.reshape(-1, 1), np.outer(np.zeros_like(e), neg))\n",
    "            \n",
    "            loss += -np.log(y_pred[target_index])\n",
    "        \n",
    "        print(f'Epoca{epoch}, Perdida: {loss}')\n",
    "    return W1, W2\n",
    "\n",
    "# Uso del modelo\n",
    "text = \"We are learning Natural Language Processing and it is very exciting\"\n",
    "words = preprocess(text)\n",
    "word_to_id, id_to_word, word_freq = build_vocab(words)\n",
    "subsampled_words = subsample(words, word_freq)\n",
    "\n",
    "# CBOW\n",
    "contexts, targets = generate_cbow_context(subsampled_words, window_size=2)\n",
    "W1_cbow, W2_cbow = train_cbow(contexts, targets, word_to_id, id_to_word, vocab_size=len(word_to_id), embedding_dim=10, learning_rate=0.01, epochs=100, num_neg_samples=5)\n",
    "\n",
    "# Skip-gram\n",
    "contexts, targets = generate_skipgram_context(subsampled_words, window_size=2)\n",
    "W1_skipgram, W2_skipgram = train_skipgram(contexts, targets, word_to_id, id_to_word, vocab_size=len(word_to_id), embedding_dim=10, learning_rate=0.01, epochs=100, num_neg_samples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e0461",
   "metadata": {},
   "source": [
    "La función subsample reduce la frecuencia de las palabras muy comunes utilizando una probabilidad de eliminación basada en su frecuencia.\n",
    "\n",
    "La función get_negative_samples genera muestras negativas aleatorias que no sean la palabra objetivo.\n",
    "Durante el entrenamiento, se actualizan los pesos W2 utilizando las muestras negativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b81d8e-c946-40df-a842-581816d28be9",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "\n",
    "La factorización de matrices GloVe y PPMI son dos métodos utilizados en el procesamiento del lenguaje natural (NLP) para capturar relaciones semánticas entre palabras a partir de grandes corpus de texto. Ambos métodos se utilizan para generar representaciones vectoriales de palabras, lo que permite que las relaciones semánticas y sintácticas entre palabras se reflejen en el espacio vectorial. \n",
    "\n",
    "1 . GloVe (Global Vectors for Word Representation)\n",
    "GloVe es un modelo de aprendizaje no supervisado para obtener representaciones vectoriales de palabras. Fue desarrollado por investigadores de Stanford y combina elementos de dos enfoques principales en NLP: factorización de matrices y modelos basados en ventana de contexto (como word2vec). La idea principal detrás de GloVe es que las co-ocurrencias de palabras en un corpus pueden proporcionar información semántica valiosa.\n",
    "\n",
    "El modelo GloVe construye una matriz de co-ocurrencia global que tabula cuántas veces cada palabra aparece en el contexto de otras palabras dentro de un corpus. Luego, esta matriz se factoriza para reducir su dimensión, resultando en vectores de palabras más densos. El objetivo de la factorización es mantener la estructura semántica donde la distancia entre dos vectores de palabras refleje la similitud semántica entre las palabras correspondientes.\n",
    "\n",
    "2 . PPMI (Positive Pointwise Mutual Information)\n",
    "La PPMI es una técnica que se usa para calcular la asociación entre palabras basada en cuán frecuentemente aparecen juntas en comparación con cuán frecuentemente aparecen por separado. El \"Pointwise Mutual Information\" (PMI) de dos palabras mide la probabilidad de co-ocurrencia de las palabras en relación con las probabilidades de que cada palabra ocurra por sí sola. Sin embargo, PMI puede tener valores negativos, lo que puede ser problemático en algunos escenarios de modelado.\n",
    "\n",
    "Para solucionarlo, se utiliza PPMI, donde todos los valores negativos de PMI se reemplazan por cero, enfocándose solo en las asociaciones positivas. En NLP, la PPMI a menudo se usa como una técnica de pre-procesamiento para construir matrices de características que luego pueden ser factorizadas (similar a SVD en GloVe) para obtener representaciones vectoriales de palabras.\n",
    "\n",
    "Para implementar PPMI, primero construiremos una matriz de co-ocurrencia y luego convertiremos sus valores a PPMI. Usaremos numpy para las operaciones matemáticas y collections para construir la matriz de co-ocurrencia.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0261ec-a5a5-478a-acf4-d70a216e6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import product\n",
    "\n",
    "# Función para construir la matriz de co-ocurrencia\n",
    "def co_occurrence_matrix(corpus, window_size=2):\n",
    "    vocab = set(corpus)\n",
    "    vocab = {word: i for i, word in enumerate(vocab)}\n",
    "    co_occurrences = defaultdict(Counter)\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        token = corpus[i]\n",
    "        left = max(0, i-window_size)\n",
    "        right = min(len(corpus), i+window_size+1)\n",
    "\n",
    "        for j in range(left, right):\n",
    "            if i != j:\n",
    "                co_occurrences[token][corpus[j]] += 1\n",
    "\n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for token1, neighbors in co_occurrences.items():\n",
    "        for token2, count in neighbors.items():\n",
    "            matrix[vocab[token1], vocab[token2]] = count\n",
    "\n",
    "    return matrix, vocab\n",
    "\n",
    "# Función para calcular PPMI\n",
    "def ppmi_matrix(co_matrix, eps=1e-8):\n",
    "    total_sum = np.sum(co_matrix)\n",
    "    row_sums = np.sum(co_matrix, axis=1)\n",
    "    col_sums = np.sum(co_matrix, axis=0)\n",
    "\n",
    "    ppmi = np.maximum(\n",
    "        np.log((co_matrix * total_sum) / (row_sums[:, None] * col_sums[None, :] + eps)),\n",
    "        0\n",
    "    )\n",
    "    return ppmi\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = \"the quick brown fox jumps over the lazy dog\".split()\n",
    "co_matrix, vocab = co_occurrence_matrix(corpus, window_size=2)\n",
    "ppmi = ppmi_matrix(co_matrix)\n",
    "\n",
    "print(ppmi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a29399-8799-4b5b-b251-175f1a5d5749",
   "metadata": {},
   "source": [
    "Implementar GloVe desde cero es más complejo debido a la optimización necesaria para ajustar los vectores de palabras. Sin embargo, puedes usar la biblioteca gensim, que tiene una implementación eficiente de GloVe. Utiliza el código realizado en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e09653-32b6-4a06-b7fc-c48e011c5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Crear modelo Word2Vec con los mismos parámetros que GloVe\n",
    "modelo = Word2Vec(sentences=[corpus], vector_size=100, window=5, min_count=1, sg=0, workers=4, epochs=10)\n",
    "\n",
    "# Guardar y cargar el modelo (simulando una carga de GloVe)\n",
    "modelo.wv.save_word2vec_format('model.bin')\n",
    "glove_model = KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
    "\n",
    "# Usar el modelo\n",
    "#print(glove_model['fox'])  # Muestra el vector para la palabra \"fox\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3b58c-fb61-4251-97df-74050d05c742",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "1. Modifica el tamaño de la ventana de contexto en la función co_occurrence_matrix para diferentes valores (por ejemplo, 1, 3, y 5) y observa cómo cambia la matriz PPMI resultante. Analiza cómo el tamaño de la ventana afecta las relaciones semánticas capturadas (1 punto).\n",
    "2. Implementa una función que identifique y muestre las palabras con mayor asociación (mayores valores PPMI) para una palabra dada. Utiliza esta función para explorar las relaciones semánticas de varias palabras clave en un corpus más grande (1 punto).\n",
    "3. Usa la biblioteca gensim para entrenar un modelo GloVe con un corpus más grande (por ejemplo, un conjunto de datos de reseñas de productos o artículos de noticias). Ajusta diferentes hiperparámetros como el tamaño del vector, el tamaño de la ventana, y el número de iteraciones. Evalúa los vectores de palabras resultantes en tareas de analogía y similaridad (1 punto).\n",
    "4. Realiza una comparación cualitativa y cuantitativa de las representaciones de palabras obtenidas a través de PPMI y GloVe. Considera aspectos como la capacidad de capturar sinónimos, antónimos y relaciones semánticas complejas. Discute en qué casos un método podría ser preferido sobre el otro (1 punto)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8e976",
   "metadata": {},
   "source": [
    "**Ventana de contexto 1:**\n",
    "\n",
    "Una ventana de tamaño 1 considera solo las palabras adyacentes para la co-ocurrencia. Esto puede capturar relaciones sintácticas muy locales pero puede no ser suficiente para capturar relaciones semánticas más amplias.\n",
    "\n",
    "**Ventana de contexto 3:**\n",
    "\n",
    "Una ventana de tamaño 3 considera un contexto más amplio, incluyendo dos palabras a cada lado de la palabra objetivo. Esto puede capturar relaciones semánticas más significativas y proporcionar un balance entre las relaciones sintácticas y semánticas.\n",
    "\n",
    "**Ventana de contexto 5:**\n",
    "\n",
    "Una ventana de tamaño 5 considera un contexto aún más amplio, incluyendo cuatro palabras a cada lado. Esto puede capturar relaciones semánticas más globales, pero también puede introducir más ruido, ya que las palabras más alejadas pueden no estar tan fuertemente relacionadas.\n",
    "\n",
    "- Tamaño de la ventana pequeña (1): Captura relaciones muy locales, proporcionando información detallada sobre la proximidad inmediata de las palabras. Puede ser útil para tareas donde las relaciones sintácticas inmediatas son críticas.\n",
    "- Tamaño de la ventana mediana (3): Captura un balance entre relaciones locales y más amplias, siendo útil para muchas tareas NLP estándar.\n",
    "- Tamaño de la ventana grande (5): Captura relaciones semánticas más amplias, pero puede introducir ruido. Es útil para tareas que requieren un entendimiento más global del contexto, pero puede no ser tan efectivo para relaciones muy locales.\n",
    "\n",
    "El tamaño de la ventana de contexto es un hiperparámetro importante que puede afectar significativamente el rendimiento de las representaciones vectoriales en diferentes tareas de NLP. Es recomendable experimentar con diferentes tamaños de ventana y evaluar su impacto en el rendimiento del modelo en la tarea específica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c652e1",
   "metadata": {},
   "source": [
    "Vamos a modificar el código para mostrar las matrices PPMI resultantes para diferentes tamaños de ventana (1, 3 y 5) y analizarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Función para construir la matriz de co-ocurrencia\n",
    "def co_occurrence_matrix(corpus, window_size=2):\n",
    "    vocab = set(corpus)\n",
    "    vocab = {word: i for i, word in enumerate(vocab)}\n",
    "    co_occurrences = defaultdict(Counter)\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        token = corpus[i]\n",
    "        left = max(0, i - window_size)\n",
    "        right = min(len(corpus), i + window_size + 1)\n",
    "\n",
    "        for j in range(left, right):\n",
    "            if i != j:\n",
    "                co_occurrences[token][corpus[j]] += 1\n",
    "\n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for token1, neighbors in co_occurrences.items():\n",
    "        for token2, count in neighbors.items():\n",
    "            matrix[vocab[token1], vocab[token2]] = count\n",
    "\n",
    "    return matrix, vocab\n",
    "\n",
    "# Función para calcular PPMI\n",
    "def ppmi_matrix(co_matrix, eps=1e-8):\n",
    "    total_sum = np.sum(co_matrix)\n",
    "    row_sums = np.sum(co_matrix, axis=1)\n",
    "    col_sums = np.sum(co_matrix, axis=0)\n",
    "\n",
    "    ppmi = np.maximum(\n",
    "        np.log((co_matrix * total_sum) / (row_sums[:, None] * col_sums[None, :] + eps)),\n",
    "        0\n",
    "    )\n",
    "    return ppmi\n",
    "\n",
    "# Función para imprimir matrices PPMI para diferentes tamaños de ventana\n",
    "def example_with_different_window_sizes(corpus, window_sizes):\n",
    "    for window_size in window_sizes:\n",
    "        co_matrix, vocab = co_occurrence_matrix(corpus, window_size=window_size)\n",
    "        ppmi = ppmi_matrix(co_matrix)\n",
    "        print(f\"Matriz PPMI  con tamaño de ventana {window_size}:\")\n",
    "        print(ppmi, \"\\n\")\n",
    "\n",
    "corpus = \"the quick brown fox jumps over the lazy dog\".split()\n",
    "window_sizes = [1, 3, 5]\n",
    "example_with_different_window_sizes(corpus, window_sizes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcefd28a",
   "metadata": {},
   "source": [
    "Ventana de contexto 1: La matriz PPMI muestra valores significativos solo para palabras adyacentes directas. Esto captura relaciones locales pero puede perder información semántica más global.\n",
    "\n",
    "Ventana de contexto 3: La matriz PPMI incluye valores para palabras dentro de un rango de dos palabras a cada lado. Esto proporciona un balance entre capturar relaciones locales y semánticas más amplias.\n",
    "\n",
    "Ventana de contexto 5: La matriz PPMI considera un contexto más amplio, incluyendo palabras hasta cuatro posiciones de distancia. Esto puede capturar relaciones semánticas más amplias pero también puede diluir las relaciones locales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74766a",
   "metadata": {},
   "source": [
    "Para implementar una función que identifique y muestre las palabras con mayor asociación (mayores valores PPMI) para una palabra dada, podemos seguir los siguientes pasos:\n",
    "\n",
    "- Construir la matriz de co-ocurrencia y la matriz PPMI para el corpus.\n",
    "- Implementar una función que encuentre y muestre las palabras con mayor asociación para una palabra dada.\n",
    "- Probar la función con un corpus más grande.\n",
    "\n",
    "Vamos a empezar por definir nuestro código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67464c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Función para construir la matriz de co-ocurrencia\n",
    "def co_occurrence_matrix(corpus, window_size=2):\n",
    "    vocab = set(corpus)\n",
    "    vocab = {word: i for i, word in enumerate(vocab)}\n",
    "    co_occurrences = defaultdict(Counter)\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        token = corpus[i]\n",
    "        left = max(0, i - window_size)\n",
    "        right = min(len(corpus), i + window_size + 1)\n",
    "\n",
    "        for j in range(left, right):\n",
    "            if i != j:\n",
    "                co_occurrences[token][corpus[j]] += 1\n",
    "\n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for token1, neighbors in co_occurrences.items():\n",
    "        for token2, count in neighbors.items():\n",
    "            matrix[vocab[token1], vocab[token2]] = count\n",
    "\n",
    "    return matrix, vocab\n",
    "\n",
    "# Función para calcular PPMI\n",
    "def ppmi_matrix(co_matrix, eps=1e-8):\n",
    "    total_sum = np.sum(co_matrix)\n",
    "    row_sums = np.sum(co_matrix, axis=1)\n",
    "    col_sums = np.sum(co_matrix, axis=0)\n",
    "\n",
    "    ppmi = np.zeros_like(co_matrix)\n",
    "\n",
    "    for i in range(co_matrix.shape[0]):\n",
    "        for j in range(co_matrix.shape[1]):\n",
    "            if co_matrix[i, j] > 0:\n",
    "                pmi = np.log((co_matrix[i, j] * total_sum) / (row_sums[i] * col_sums[j] + eps))\n",
    "                ppmi[i, j] = max(pmi, 0)\n",
    "    \n",
    "    return ppmi\n",
    "\n",
    "# Función para mostrar las palabras con mayor asociación para una palabra dada\n",
    "def top_associations(word, ppmi_matrix, vocab, top_n=5):\n",
    "    if word not in vocab:\n",
    "        print(f\"The word '{word}' is not in the vocabulary.\")\n",
    "        return\n",
    "    \n",
    "    word_index = vocab[word]\n",
    "    word_ppmi = ppmi_matrix[word_index]\n",
    "    \n",
    "    top_indices = word_ppmi.argsort()[::-1][:top_n+1]  # +1 because the word itself will be included\n",
    "    top_words = [(list(vocab.keys())[list(vocab.values()).index(i)], word_ppmi[i]) for i in top_indices if i != word_index]\n",
    "    \n",
    "    print(f\"Primeras asociaciones para '{word}':\")\n",
    "    for w, score in top_words:\n",
    "        print(f\"  {w}: {score:.4f}\")\n",
    "\n",
    "# Ejemplo de uso con un corpus más grande\n",
    "corpus = (\"the quick brown fox jumps over the lazy dog the quick brown fox jumps \"\n",
    "          \"over the lazy dog the quick brown fox jumps over the lazy dog\").split()\n",
    "window_size = 3\n",
    "\n",
    "co_matrix, vocab = co_occurrence_matrix(corpus, window_size=window_size)\n",
    "ppmi = ppmi_matrix(co_matrix)\n",
    "\n",
    "# Probar la función con varias palabras clave\n",
    "top_associations('quick', ppmi, vocab)\n",
    "top_associations('fox', ppmi, vocab)\n",
    "top_associations('dog', ppmi, vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea32260",
   "metadata": {},
   "source": [
    "Para entrenar un modelo GloVe utilizando la biblioteca Gensim en un corpus más grande y ajustar diferentes hiperparámetros, como el tamaño del vector, el tamaño de la ventana y el número de iteraciones, puedes seguir estos pasos:\n",
    "\n",
    "- Instalar la biblioteca Gensim si aún no está instalada.\n",
    "- Preprocesar y tokenizar el corpus de texto.\n",
    "- Entrenar el modelo GloVe con los hiperparámetros deseados.\n",
    "- Evaluar los vectores de palabras resultantes en tareas de analogía y similaridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d001d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "# Paso 1: Preparar el corpus\n",
    "# Supongamos que tienes un archivo de texto grande llamado \"corpus.txt\"\n",
    "\n",
    "# Paso 2: Entrenar el modelo GloVe\n",
    "def train_glove_model(corpus_file, vector_size=100, window=5, epochs=5):\n",
    "    # Convertir el archivo GloVe a formato Word2Vec\n",
    "    tmp_file = get_tmpfile(\"temp_word2vec.txt\")\n",
    "    glove2word2vec(corpus_file, tmp_file)\n",
    "    \n",
    "    # Entrenar el modelo GloVe\n",
    "    model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Entrenar el modelo GloVe con tus datos\n",
    "corpus_file = \"path/to/corpus.txt\"  # Reemplaza con la ubicación de tu archivo de corpus\n",
    "glove_model = train_glove_model(corpus_file, vector_size=100, window=5, epochs=5)\n",
    "\n",
    "# Paso 3: Evaluación de vectores de palabras\n",
    "# Ejemplo de tareas de analogía\n",
    "analogies = glove_model.wv.evaluate_word_analogies(datapath('questions-words.txt'))\n",
    "print(\"Exactitud de analogia:\", analogies[0])\n",
    "\n",
    "# Ejemplo de tareas de similitud\n",
    "similarity = glove_model.wv.evaluate_word_pairs(datapath('similarity.txt'))\n",
    "print(\"Puntuacion de similaridad:\", similarity[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbcee9",
   "metadata": {},
   "source": [
    "Para realizar una comparación cualitativa y cuantitativa de las representaciones de palabras obtenidas a través de PPMI y GloVe, seguiremos estos pasos:\n",
    "\n",
    "- Usaremos un corpus grande, como reseñas de productos o artículos de noticias.\n",
    "- Entrenamiento de Modelos:\n",
    "    * PPMI: Construiremos una matriz PPMI.\n",
    "    * GloVe: Entrenaremos un modelo GloVe utilizando la biblioteca Gensim.\n",
    "- Evaluaremos los modelos en tareas de analogía y similitud de palabras.\n",
    "- Compararemos ejemplos específicos para analizar la capacidad de los modelos para capturar sinónimos, antónimos y relaciones semánticas complejas.\n",
    "\n",
    "**Paso 1: Preparación del corpus**\n",
    "\n",
    "Usaremos el corpus de Reuters proporcionado por NLTK y tokenizaremos las oraciones correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Preparar el corpus\n",
    "corpus = reuters.sents()\n",
    "\n",
    "# Función para construir la matriz de co-ocurrencia\n",
    "def co_occurrence_matrix(corpus, window_size=2):\n",
    "    vocab = set([word for sentence in corpus for word in sentence])\n",
    "    vocab = {word: i for i, word in enumerate(vocab)}\n",
    "    co_occurrences = defaultdict(Counter)\n",
    "\n",
    "    for sentence in corpus:\n",
    "        for i in range(len(sentence)):\n",
    "            token = sentence[i]\n",
    "            left = max(0, i - window_size)\n",
    "            right = min(len(sentence), i + window_size + 1)\n",
    "\n",
    "            for j in range(left, right):\n",
    "                if i != j:\n",
    "                    co_occurrences[token][sentence[j]] += 1\n",
    "\n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for token1, neighbors in co_occurrences.items():\n",
    "        for token2, count in neighbors.items():\n",
    "            matrix[vocab[token1], vocab[token2]] = count\n",
    "\n",
    "    return matrix, vocab\n",
    "\n",
    "# Función para calcular PPMI\n",
    "def ppmi_matrix(co_matrix, eps=1e-8):\n",
    "    total_sum = np.sum(co_matrix)\n",
    "    row_sums = np.sum(co_matrix, axis=1)\n",
    "    col_sums = np.sum(co_matrix, axis=0)\n",
    "\n",
    "    ppmi = np.zeros_like(co_matrix)\n",
    "\n",
    "    for i in range(co_matrix.shape[0]):\n",
    "        for j in range(co_matrix.shape[1]):\n",
    "            if co_matrix[i, j] > 0:\n",
    "                pmi = np.log((co_matrix[i, j] * total_sum) / (row_sums[i] * col_sums[j] + eps))\n",
    "                ppmi[i, j] = max(pmi, 0)\n",
    "    \n",
    "    return ppmi\n",
    "\n",
    "# Construir la matriz PPMI\n",
    "window_size = 3\n",
    "co_matrix, vocab = co_occurrence_matrix(corpus, window_size=window_size)\n",
    "ppmi = ppmi_matrix(co_matrix)\n",
    "\n",
    "# Preparar el corpus para Gensim\n",
    "def prepare_corpus(corpus):\n",
    "    return [[word for word in sentence] for sentence in corpus]\n",
    "\n",
    "corpus_gensim = prepare_corpus(corpus)\n",
    "\n",
    "# Entrenar el modelo GloVe\n",
    "def train_glove_model(corpus, vector_size=100, window=5, epochs=5):\n",
    "    model = Word2Vec(corpus, vector_size=vector_size, window=window, sg=0, epochs=epochs)\n",
    "    return model.wv\n",
    "\n",
    "vector_size = 100\n",
    "window = 5\n",
    "epochs = 5\n",
    "glove_model = train_glove_model(corpus_gensim, vector_size=vector_size, window=window, epochs=epochs)\n",
    "\n",
    "# Evaluar similaridades PPMI\n",
    "def get_word_vector(word, vocab, ppmi):\n",
    "    if word in vocab:\n",
    "        return ppmi[vocab[word]]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "def evaluate_similarity(word_pairs, vocab, ppmi):\n",
    "    similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        vec1 = get_word_vector(word1, vocab, ppmi)\n",
    "        vec2 = get_word_vector(word2, vocab, ppmi)\n",
    "        if vec1 is not None and vec2 is not None:\n",
    "            sim = cosine_similarity(vec1, vec2)\n",
    "            similarities.append((word1, word2, sim))\n",
    "    return similarities\n",
    "\n",
    "word_pairs = [('king', 'queen'), ('man', 'woman'), ('paris', 'france'), ('car', 'vehicle')]\n",
    "ppmi_similarities = evaluate_similarity(word_pairs, vocab, ppmi)\n",
    "print(\"Similaridades PPMI :\", ppmi_similarities)\n",
    "\n",
    "# Evaluar GloVe en tareas de similaridad\n",
    "def get_top_similar_words(word, model, top_n=5):\n",
    "    try:\n",
    "        similar_words = model.most_similar(word, topn=top_n)\n",
    "        return similar_words\n",
    "    except KeyError:\n",
    "        return []\n",
    "\n",
    "print(\"Palabras principales similares GloVe:\")\n",
    "for word in words_to_evaluate:\n",
    "    similar_words = get_top_similar_words(word, glove_model)\n",
    "    print(f\"{word}: {similar_words}\")\n",
    "\n",
    "# Evaluar GloVe en tareas de analogía\n",
    "analogies = glove_model.evaluate_word_analogies(datapath('questions-words.txt'))\n",
    "print(\"Exactitud de analogia GloVe :\", analogies[0])\n",
    "\n",
    "# Evaluar GloVe en tareas de similitud\n",
    "similarity = glove_model.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "print(\"GloVe Similarity score:\", similarity[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9120c",
   "metadata": {},
   "source": [
    "**Observación (demora):** \n",
    "\n",
    "PPMI:\n",
    "\n",
    "- Es mejor capturando asociaciones fuertes en contextos pequeños.\n",
    "- Tiene limitaciones en capturar relaciones semánticas más complejas debido a su enfoque basado en frecuencias.\n",
    "\n",
    "GloVe:\n",
    "\n",
    "- Captura mejor relaciones semánticas complejas y analogías gracias a su entrenamiento en grandes corpus y su enfoque de aprendizaje profundo.\n",
    "- Produce vectores de palabras más robustos y útiles en tareas NLP avanzadas.\n",
    "\n",
    "Este análisis muestra que, aunque PPMI puede ser útil para asociaciones locales y frecuencias, GloVe ofrece una ventaja significativa en la captura de relaciones semánticas complejas y es generalmente más útil en aplicaciones NLP avanzadas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dbda1f-635d-4517-9739-c3f448c99b21",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "\n",
    "El desarrollo de modelos de redes neuronales recurrentes (RNNs) ha sido fundamental en el avance del procesamiento de secuencias de tiempo y lenguaje natural. Estos modelos son especialmente útiles en tareas como el reconocimiento de voz, la traducción automática y la generación de texto. Sin embargo, las RNNs básicas enfrentan desafíos significativos, como la desaparición y la explosión del gradiente, que obstaculizan su capacidad para aprender dependencias a largo plazo en los datos. Las unidades de memoria de largo y corto plazo (LSTM) y las unidades recurrentes con compuertas (GRU) se desarrollaron como soluciones a estos problemas, mejorando la capacidad de las redes para aprender de datos secuenciales a largo plazo.\n",
    "\n",
    "Una RNN básica procesa información secuencial mediante la actualización de su estado oculto con cada nuevo elemento de la secuencia. La naturaleza recurrente de estas redes les permite mantener una forma de 'memoria' sobre los elementos anteriores de la secuencia, utilizando la siguiente fórmula básica para actualizar el estado oculto en cada paso de tiempo $t$:\n",
    "\n",
    "$$\n",
    "h_t = \\sigma(W_{ih} x_t + W_{hh} h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "Donde $x_t$ es la entrada en el tiempo $t$, $h_t$ es el estado oculto en el tiempo $t$, $W_{ih}$ y $W_{hh}$ son los pesos de entrada y recurrentes, respectivamente, $b_h$ es el término de sesgo, y $\\sigma$ es una función de activación no lineal como tanh o ReLU.\n",
    "\n",
    "\n",
    "El entrenamiento de RNNs implica ajustar estos pesos mediante retropropagación a través del tiempo, lo que puede llevar a dos problemas principales:\n",
    "\n",
    "1. **Desaparición del gradiente:** Si los gradientes de los pesos son muy pequeños, disminuyen exponencialmente a medida que se propagan hacia atrás a través de cada paso de tiempo. Esto hace que sea difícil para la RNN aprender dependencias a largo plazo, ya que los gradientes se vuelven insignificantes para ajustar los pesos efectivamente en pasos de tiempo anteriores.\n",
    "\n",
    "2. **Explosión del gradiente:** En contraste, si los gradientes son demasiado grandes, pueden crecer exponencialmente durante la retropropagación, lo que lleva a actualizaciones de peso grandes e inestables, y por ende, a un modelo que diverge y no aprende de manera efectiva.\n",
    "\n",
    "#### Unidad de memoria de largo y corto plazo (LSTM)\n",
    "\n",
    "Para abordar estos problemas, se introdujeron las LSTMs, que incorporan un diseño más complejo que permite controlar el flujo de información. Las LSTMs utilizan varias \"puertas\" para regular tanto el almacenamiento como la eliminación de información en el estado de la celda:\n",
    "\n",
    "- **Puerta de olvido $(f_t)$** decide qué parte de la información anterior se mantiene:\n",
    "  $$\n",
    "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "  $$\n",
    "\n",
    "- **Puerta de entrada ($i_t$) y candidato de celda ($\\tilde{c}_t$)** deciden qué nueva información se añade al estado de la celda:\n",
    "\n",
    "  $$\n",
    "  i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "  $$\n",
    "  $$\n",
    "  \\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)\n",
    "  $$\n",
    "\n",
    "- **Actualización del estado de la celda ($c_t$)** combina la información antigua y nueva:\n",
    "  $$\n",
    "  c_t = f_t \\ast c_{t-1} + i_t \\ast \\tilde{c}_t\n",
    "  $$\n",
    "\n",
    "- **Puerta de salida ($o_t$)** y el estado oculto resultante ($h_t$) que determina qué parte del estado de la celda afectará la salida:\n",
    "  $$\n",
    "  o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "  $$\n",
    "  $$\n",
    "  h_t = o_t \\ast \\tanh(c_t)\n",
    "  $$\n",
    "\n",
    "#### Unidad recurrente compuerta (GRU)\n",
    "\n",
    "Las GRUs simplifican la arquitectura de las LSTMs combinando las puertas de entrada y olvido en una sola puerta de actualización y omitiendo el uso de un estado de celda separado:\n",
    "\n",
    "- **Puerta de actualización ($z_t$)** decide cuánto del estado anterior se debe mantener:\n",
    "  $$\n",
    "  z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "  $$\n",
    "\n",
    "- **Puerta de reinicio ($r_t$)** decide cuánto del pasado se debe olvidar antes de calcular el nuevo candidato de estado:\n",
    "  $$\n",
    "  r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "  $$\n",
    "\n",
    "- **Candidato de estado oculto ($\\tilde{h}_t$)** y la actualización del estado oculto:\n",
    "  $$\n",
    "  \\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\ast h_{t-1}, x_t] + b_h)\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  h_t = (1 - z_t) \\ast h_{t-1} + z_t \\ast \\tilde{h}_t\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead46557-86c0-4b63-a6cd-78fafa796d30",
   "metadata": {},
   "source": [
    "#### Ejercicios\n",
    "\n",
    "1. ¿Qué papel juegan los reguladores como dropout o L2 regularization específicamente en el contexto de RNNs y LSTM para evitar el sobreajuste en tareas de modelado de lenguaje? (1 punto)\n",
    "2. Considerando la complejidad computacional de BPTT, ¿cuáles son las limitaciones prácticas cuando se usa con RNNs en secuencias muy largas? ¿Cómo podrías mitigar estos problemas en un entorno de producción? (1 punto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f40f0",
   "metadata": {},
   "source": [
    "Los reguladores como el **dropout** y la **regularización L2** juegan un papel crucial en el contexto de RNNs y LSTM para evitar el sobreajuste, especialmente en tareas de modelado de lenguaje. A continuación, se describen cómo funcionan estos reguladores y cómo contribuyen a mejorar el rendimiento y la generalización de los modelos.\n",
    "\n",
    "#### Dropout en RNNs y LSTM\n",
    "\n",
    "**Dropout** es una técnica de regularización que introduce aleatoriedad en el proceso de entrenamiento, apagando de manera aleatoria un conjunto de neuronas en cada paso de actualización. Esto obliga a la red a aprender representaciones redundantes y robustas, ya que no puede depender de ninguna neurona en particular para una característica específica.\n",
    "\n",
    "En el contexto de RNNs y LSTM, **dropout** se aplica típicamente de dos maneras:\n",
    "\n",
    "1. **Dropout en las conexiones no recurrentes**: Esta es la forma más común de aplicar dropout en RNNs. Se aplica dropout en las conexiones de entrada y salida de cada capa recurrente, pero no en las conexiones recurrentes internas. Esto ayuda a prevenir el sobreajuste al asegurarse de que las neuronas no se vuelvan excesivamente dependientes de entradas específicas.\n",
    "  \n",
    "   $$\n",
    "   y_{t} = \\text{Dropout}(W_{ih} x_t) + W_{hh} h_{t-1} + b_h\n",
    "   $$\n",
    "\n",
    "2. **Variational Dropout**: En LSTM y otras arquitecturas de RNN más complejas, se puede usar una versión modificada de dropout llamada \"variational dropout\". Aquí, un mismo patrón de apagado se aplica a lo largo de toda la secuencia temporal, en lugar de recalcularse en cada paso de tiempo. Esto mantiene la coherencia en el aprendizaje a través de la secuencia temporal.\n",
    "\n",
    "#### Regularización L2 en RNNs y LSTM\n",
    "\n",
    "La **regularización L2** agrega un término de penalización a la función de pérdida del modelo, basado en la suma de los cuadrados de todos los pesos en la red. Esto desalienta que los pesos se vuelvan demasiado grandes, promoviendo soluciones más simples y evitando el sobreajuste.\n",
    "\n",
    "La regularización L2 se define como:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{total} = \\mathcal{L}_{original} + \\lambda \\sum_{i} w_i^2\n",
    "$$\n",
    "\n",
    "Donde $\\mathcal{L}_{original}$ es la pérdida original (por ejemplo, error cuadrático medio o entropía cruzada), $w_i$ son los pesos del modelo, y $\\lambda$ es el hiperparámetro de regularización que controla la importancia del término de penalización.\n",
    "\n",
    "#### Efectos y beneficios\n",
    "\n",
    "- **Dropout**: \n",
    "  - Reduce el riesgo de sobreajuste al forzar la red a ser robusta frente a la pérdida de información de neuronas específicas.\n",
    "  - Mejora la generalización del modelo al hacer que la red no dependa excesivamente de ningún conjunto específico de neuronas.\n",
    "  - Es particularmente útil en modelos grandes y complejos donde el riesgo de sobreajuste es alto.\n",
    "\n",
    "- **Regularización L2**:\n",
    "  - Impide que los pesos del modelo crezcan demasiado, lo que puede llevar a modelos que se ajusten excesivamente a los datos de entrenamiento.\n",
    "  - Ayuda a mantener los pesos del modelo en un rango más controlado, lo que puede conducir a un aprendizaje más estable y eficiente.\n",
    "  - Es útil para controlar la complejidad del modelo y mejorar su capacidad de generalización.\n",
    "\n",
    "#### Aplicación en Tareas de Modelado de Lenguaje\n",
    "\n",
    "En tareas de modelado de lenguaje, donde los datos son secuenciales y las dependencias a largo plazo son cruciales, el sobreajuste es un problema común. Los reguladores como dropout y L2 son esenciales para:\n",
    "\n",
    "- **Modelado de lenguaje**: Al entrenar modelos para predecir la siguiente palabra en una secuencia, el dropout puede ayudar a evitar que el modelo se ajuste demasiado a patrones específicos de las secuencias de entrenamiento.\n",
    "- **Traducción automática**: Las RNNs y LSTMs con dropout y L2 regularization pueden generalizar mejor a frases y contextos nuevos no vistos durante el entrenamiento.\n",
    "- **Reconocimiento de voz**: El uso de regularización puede mejorar la robustez del modelo frente a diferentes variaciones en las entradas de voz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5517bcd3",
   "metadata": {},
   "source": [
    "La retropropagación a través del tiempo (Backpropagation Through Time, BPTT) es una extensión del algoritmo de retropropagación utilizado para entrenar redes neuronales recurrentes (RNNs). Sin embargo, su aplicación en secuencias muy largas presenta varias limitaciones prácticas debido a su complejidad computacional.\n",
    "\n",
    "Limitaciones de BPTT en secuencias muy largas\n",
    "\n",
    "\n",
    "- Explosión del Gradiente: En secuencias largas, los gradientes pueden crecer exponencialmente durante la retropropagación, llevando a actualizaciones de peso grandes e inestables que hacen que el modelo diverja.\n",
    "- Desaparición del Gradiente: Los gradientes pueden disminuir exponencialmente, haciendo que las actualizaciones de peso sean insignificantes. Esto dificulta el aprendizaje de dependencias a largo plazo.\n",
    "- La retropropagación a través de largas secuencias requiere almacenar todos los estados ocultos y las intermedias en memoria, lo que puede llevar a un uso intensivo de memoria y recursos computacionales.\n",
    "- El tiempo de entrenamiento aumenta significativamente con la longitud de la secuencia, lo que puede hacer que el proceso de entrenamiento sea prohibitivo para secuencias muy largas.\n",
    "- Los cálculos con números muy grandes o muy pequeños pueden provocar inestabilidades numéricas, afectando la precisión y la estabilidad del entrenamiento.\n",
    "\n",
    "Estrategias para mitigar problemas en un entorno de producción\n",
    "\n",
    "- En lugar de propagar los gradientes a través de toda la secuencia, se puede truncar BPTT y limitar la propagación a una ventana de longitud fija.\n",
    "- Esto reduce el uso de memoria y el costo computacional, aunque puede comprometer la capacidad de la red para aprender dependencias a largo plazo.  \n",
    "     Ejemplo: Si se tiene una secuencia de longitud T, se podría truncar BPTT a una ventana de longitud  k, dondees significativamente menor que \n",
    "- Las unidades de memoria de largo y corto plazo (LSTM) y las unidades recurrentes con compuertas (GRU) están diseñadas para manejar dependencias a largo plazo de manera más efectiva que las RNNs simples. Estas arquitecturas incluyen mecanismos internos que ayudan a mitigar los problemas de desaparición y explosión del gradiente.\n",
    "- Implementar técnicas de regularización como dropout y regularización L2 para evitar el sobreajuste y mejorar la generalización del modelo.\n",
    "- Regularización de gradiente (Gradient Clipping): Restringir los gradientes a un rango específico para evitar la explosión del gradiente.\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "Batch processing:\n",
    "\n",
    "- Dividir las secuencias largas en mini-batches para paralelizar el entrenamiento y reducir la carga de memoria.\n",
    "- Utilizar técnicas como bucketizing para agrupar secuencias de longitud similar y procesarlas en lotes, mejorando la eficiencia computacional.\n",
    "\n",
    "Optimización del hardware:\n",
    "\n",
    "- Utilizar hardware especializado como GPUs y TPUs, que están optimizados para operaciones de matriz y pueden manejar mejor los requisitos computacionales de BPTT.\n",
    "- Implementar operaciones eficientes de memoria y cómputo, aprovechando bibliotecas como CUDA para mejorar el rendimiento.\n",
    "\n",
    "Modelos alternativos:\n",
    "\n",
    "- Considerar el uso de arquitecturas más avanzadas como Transformers, que pueden manejar dependencias a largo plazo más eficientemente mediante mecanismos de atención en lugar de recurrencia.\n",
    "- Los Transformers, como el modelo BERT o GPT, pueden procesar secuencias enteras en paralelo y son menos susceptibles a problemas de gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0b968-53e3-4fd5-bff8-0618b4d8ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parte 3\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_to_hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.tanh(self.input_to_hidden(combined))\n",
    "        return hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Gates\n",
    "        self.input_to_inputgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_forgetgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_outputgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_cellgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "\n",
    "        # Calculate gates\n",
    "        input_gate = torch.sigmoid(self.input_to_inputgate(combined))\n",
    "        forget_gate = torch.sigmoid(self.input_to_forgetgate(combined))\n",
    "        output_gate = torch.sigmoid(self.input_to_outputgate(combined))\n",
    "        cell_gate = torch.tanh(self.input_to_cellgate(combined))\n",
    "\n",
    "        # Update cell state\n",
    "        cell = forget_gate * cell + input_gate * cell_gate\n",
    "        hidden = output_gate * torch.tanh(cell)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "    def init_hidden_and_cell(self):\n",
    "        return torch.zeros(1, self.hidden_size), torch.zeros(1, self.hidden_size)\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_to_updategate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_resetgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_newgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "\n",
    "        # Calculate gates\n",
    "        update_gate = torch.sigmoid(self.input_to_updategate(combined))\n",
    "        reset_gate = torch.sigmoid(self.input_to_resetgate(combined))\n",
    "        new_hidden = torch.tanh(self.input_to_newgate(torch.cat((input, reset_gate * hidden), 1)))\n",
    "\n",
    "        # Update hidden state\n",
    "        hidden = update_gate * hidden + (1 - update_gate) * new_hidden\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a9703-79cb-4435-b9bc-239985f6c3f2",
   "metadata": {},
   "source": [
    "Extiende la implementación de LSTM para incluir embeddings de palabras y una capa de clasificación, y entrenar el modelo en una tarea de predicción de la siguiente palabra en secuencias de texto (3 puntos).\n",
    "\n",
    "- Agrega una capa de embedding al modelo LSTM para procesar entradas de texto.\n",
    "- Incluye una capa de salida que mapee el estado oculto a las predicciones de palabras.\n",
    "- Implementa una función de pérdida adecuada para la clasificación de palabras.\n",
    "- Preprocesa  un corpus de texto grande (utiliza los datos dados en clase por ejemplo) para convertir texto a índices utilizando un vocabulario predefinido.\n",
    "- Genera datos de entrenamiento como pares de secuencias de entrada y palabras objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da3c614-442c-44d5-92bf-4b61280d9c0e",
   "metadata": {},
   "source": [
    "Realiza un análisis de sensibilidad de los hiperparámetros en modelos LSTM y GRU para entender su impacto en la capacidad de aprendizaje de dependencias a largo plazo en textos (3 puntos)\n",
    "\n",
    "- Selecciona un corpus de texto y prepara datos para el entrenamiento de modelos de lenguaje basados en LSTM y GRU.\n",
    "- Experimenta con diferentes valores para los hiperparámetros como el tamaño de las puertas, la tasa de aprendizaje, el tamaño del estado oculto y la longitud de BPTT.\n",
    "- Utiliza técnicas como validación cruzada para evaluar el impacto de estos cambios en la precisión del modelo y en su capacidad para generar texto coherente.\n",
    "- Analiza cómo la modificación de los parámetros de las puertas y la longitud de BPTT afecta la estabilidad del entrenamiento y la convergencia del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a3c0f",
   "metadata": {},
   "source": [
    "Para extender la implementación de LSTM y entrenar el modelo en una tarea de predicción de la siguiente palabra en secuencias de texto, necesitamos agregar las siguientes funcionalidades:\n",
    "\n",
    "- Una capa de embeddings para procesar las entradas de texto.\n",
    "- Una capa de salida que mapee el estado oculto a las predicciones de palabras.\n",
    "- Una función de pérdida adecuada para la clasificación de palabras.\n",
    "- Preprocesar un corpus de texto grande para convertir texto a índices utilizando un vocabulario predefinido.\n",
    "- Generar datos de entrenamiento como pares de secuencias de entrada y palabras objetivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c60ecc1",
   "metadata": {},
   "source": [
    "Para extender la implementación de la clase LSTM e incluir funcionalidades adicionales que permitan entrenar el modelo para la tarea de predicción de la siguiente palabra en secuencias de texto usaremos los pasos anteriores. El objetivo es adaptar el modelo para que pueda manejar texto a nivel de palabra, aprendiendo a predecir la siguiente palabra en una secuencia dada. \n",
    "\n",
    "1. Agrega una capa de embedding a la clase LSTM que convierta los índices de palabras en vectores densos. Esto permite que el modelo maneje eficazmente el espacio de características de las palabras.\n",
    "\n",
    "2. Implementa una capa lineal que tome el estado oculto de la LSTM y lo mapee a las predicciones de palabras, es decir, un vector cuyo tamaño sea igual al tamaño del vocabulario.\n",
    "\n",
    "3. Usara la entropía cruzada, que es común para las tareas de clasificación de múltiples clases en modelos de lenguaje.\n",
    "\n",
    "4. Desarrollara funciones para convertir un corpus de texto grande en secuencias de índices de palabras utilizando un vocabulario predefinido.\n",
    "\n",
    "5. Crea pares de secuencias de entrada y palabras objetivo para el entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f574dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class LSTMWithEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(LSTMWithEmbedding, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        logits = self.fc(output[:, -1, :])  # Take the last time step's output\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size),\n",
    "                torch.zeros(1, batch_size, self.hidden_size))\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, vocab, sequence_length=30):\n",
    "        self.vocab = vocab\n",
    "        self.data = self.process_text(text, sequence_length)\n",
    "\n",
    "    def process_text(self, text, sequence_length):\n",
    "        tokens = text.split()\n",
    "        indices = [self.vocab[word] for word in tokens if word in self.vocab]\n",
    "        data = [(indices[i:i+sequence_length], indices[i+sequence_length])\n",
    "                for i in range(len(indices) - sequence_length)]\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def train_model(model, data_loader, epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in data_loader:\n",
    "            batch_size = x.size(0)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(x, hidden)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoca {epoch+1}, Perdida: {total_loss / len(data_loader)}')\n",
    "\n",
    "# Ejemplo\n",
    "vocab = {'<UNK>': 0, 'hello': 1, 'world': 2}  \n",
    "text = \"hello world hello world hello\"\n",
    "sequence_length = 2\n",
    "\n",
    "dataset = TextDataset(text, vocab, sequence_length)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 50\n",
    "hidden_size = 100\n",
    "model = LSTMWithEmbedding(vocab_size, embedding_dim, hidden_size)\n",
    "\n",
    "train_model(model, data_loader, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e63894",
   "metadata": {},
   "source": [
    "Realizar un análisis de sensibilidad de los hiperparámetros en modelos LSTM y GRU puede proporcionar información valiosa sobre cómo cada configuración afecta la capacidad del modelo para aprender y predecir secuencias, especialmente en tareas de procesamiento de lenguaje natural como la generación de texto. \n",
    "\n",
    "Paso 1: selección y preparación del Corpus de Texto\n",
    "- Selecciona un corpus de texto adecuado. Por ejemplo, podrías usar un conjunto de datos clásico como el texto de \"Alice in Wonderland\" o artículos de Wikipedia.\n",
    "- Tokenización y Vocabulario: Convierte el texto en tokens y construye un vocabulario. Esto incluye convertir cada palabra en un índice numérico para un procesamiento eficiente.\n",
    "- Crea secuencias de entrada y etiquetas objetivo. Por ejemplo, si usas una longitud de secuencia de 10 palabras, cada secuencia de entrada de 10 palabras debería tener una palabra objetivo que sea la siguiente palabra en el texto.\n",
    "\n",
    "Implementa dos modelos básicos utilizando LSTM y GRU:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d916121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, model_type, vocab_size, embedding_dim, hidden_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if model_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        elif model_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(x[:, -1, :])  \n",
    "        return x, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da8894",
   "metadata": {},
   "source": [
    "Paso 3: Experimentación con hiperparámetros\n",
    "\n",
    "- Varía el tamaño del estado oculto (por ejemplo, 128, 256, 512).\n",
    "- Experimenta con diferentes tasas de aprendizaje (por ejemplo, 0.01, 0.001, 0.0001).\n",
    "- Modifica la longitud de BPTT para ver cómo afecta la capacidad del modelo para aprender dependencias a largo plazo.\n",
    "\n",
    "Paso 4: Validación cruzada y evaluación\n",
    "\n",
    "- Divide el conjunto de datos en entrenamiento, validación y pruebas.\n",
    "-  Utiliza técnicas de validación cruzada para evaluar la generalización del modelo en diferentes subconjuntos del dato.\n",
    "- Evalúa la precisión de los modelos en la tarea de predicción de palabras y observa la coherencia del texto generado para evaluar la calidad del aprendizaje.\n",
    "\n",
    "Paso 5: Análisis de resultados\n",
    "- Observa cómo la variación en la longitud de BPTT y los tamaños de las puertas afecta la estabilidad durante el entrenamiento.\n",
    "- Analiza cómo cada configuración de hiperparámetros impacta en la capacidad del modelo para capturar y aprender dependencias a largo plazo en el texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81365ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type, vocab_size, embedding_dim, hidden_size):\n",
    "    model = RNNModel(model_type, vocab_size, embedding_dim, hidden_size)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0ce29",
   "metadata": {},
   "source": [
    "Luego, escribiremos un bucle para entrenar modelos con diferentes configuraciones de hiperparámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [128, 256, 512]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "bptt_lengths = [10, 20, 30]  # Asumiendo que modificamos esto en el diseño de nuestro DataLoader\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for bptt_length in bptt_lengths:\n",
    "            model = create_model('LSTM', vocab_size, 100, hidden_size)\n",
    "            print(f\"Entrenamiento con la capa oculta={hidden_size}, lr={lr}, bptt_length={bptt_length}\")\n",
    "            train(model, data_loader, 10, lr)  # Asegúrate de que data_loader use bptt_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12894117",
   "metadata": {},
   "source": [
    "Para realizar una validación cruzada, necesitaríamos implementar un esquema de k-fold o utilizar una división de datos de entrenamiento y validación. Vamos a suponer que se usa una división simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suponiendo que `dataset` es una instancia de TextDataset\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "def validate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            output, _ = model(x, None)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Ejemplo de validación después del entrenamiento\n",
    "validation_loss = validate(model, val_loader)\n",
    "print(f'Perdida de la validacion: {validation_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed1383",
   "metadata": {},
   "source": [
    "Analiza cómo los cambios en los hiperparámetros afectan la estabilidad y el rendimiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for bptt_length in bptt_lengths:\n",
    "            model = create_model('LSTM', vocab_size, 100, hidden_size)\n",
    "            train(model, train_loader, 10, lr)\n",
    "            val_loss = validate(model, val_loader)\n",
    "            results.append((hidden_size, lr, bptt_length, val_loss))\n",
    "            print(f\"Resultado: Tam oculto={hidden_size}, LR={lr}, BPTT={bptt_length}, Val Loss={val_loss}\")\n",
    "\n",
    "# Analizar los resultados para ver qué configuración da el mejor rendimiento\n",
    "best_config = min(results, key=lambda x: x[3])\n",
    "print(f\"Mejor configuracion: Hidden Size={best_config[0]}, LR={best_config[1]}, BPTT={best_config[2]}, Loss={best_config[3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf1093-7292-469a-b041-3c7120f4ee45",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "El script proporcionado es un ejemplo completo de cómo implementar un modelo de red neuronal recurrente (RNN) utilizando PyTorch para generar texto de manera automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cc564-c049-4af8-8153-26e0f5816d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias para trabajar con tensores y redes neuronales.\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# Datos de entrada: una lista de frases.\n",
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# Creación de un conjunto de caracteres únicos presentes en las frases.\n",
    "chars = set(''.join(text))\n",
    "# Creación de un diccionario que mapea cada caracter a un índice único.\n",
    "int2char = dict(enumerate(chars))\n",
    "# Creación de un diccionario inverso que mapea cada índice a su caracter correspondiente.\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "# Determinación de la longitud máxima de las frases para normalizar la longitud de todas.\n",
    "maxlen = len(max(text, key=len))\n",
    "print(\"La longitud mayor tiene {} caracteres\".format(maxlen))\n",
    "\n",
    "# Añadir espacios a las frases más cortas para igualar la longitud máxima.\n",
    "for i in range(len(text)):\n",
    "  while len(text[i])<maxlen:\n",
    "    text[i] += ' '\n",
    "\n",
    "# Inicialización de listas para secuencias de entrada y objetivo.\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "# Creación de secuencias de entrada y objetivo.\n",
    "for i in range(len(text)):\n",
    "    input_seq.append(text[i][:-1])\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Secuencia entrada: {}\\nSecuencia objetivo: {}\".format(input_seq[i], target_seq[i]))\n",
    "\n",
    "# Conversión de caracteres a índices para procesamiento numérico.\n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    "\n",
    "# Definición de tamaños para la codificación one-hot.\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "# Función para codificar las secuencias en formato one-hot.\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "          features[i, u, sequence[i][u]] = 1\n",
    "    return features\n",
    "\n",
    "# Aplicación de la codificación one-hot a las secuencias de entrada.\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "print(\"Forma de entrada: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
    "\n",
    "# Conversión de las secuencias de entrada a tensores de PyTorch.\n",
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)\n",
    "\n",
    "# Chequeo de disponibilidad de GPU y selección del dispositivo (GPU o CPU).\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU es disponible\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU no disponible, CPU es usada\")\n",
    "\n",
    "# Definición de la clase del modelo RNN.\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Capa RNN que toma entradas y retorna la salida y un estado oculto.\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Capa lineal que procesa la salida del RNN.\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Inicialización del estado oculto a cero.\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden\n",
    "\n",
    "# Instancia del modelo con parámetros específicos.\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "model.to(device)\n",
    "\n",
    "# Definición de hiperparámetros para el entrenamiento.\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Configuración de la función de pérdida y el optimizador.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Bucle de entrenamiento del modelo.\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  optimizer.zero_grad()\n",
    "  input_seq = input_seq.to(device)\n",
    "  output, hidden = model(input_seq)\n",
    "  loss = criterion(output, target_seq.view(-1).long())\n",
    "  loss.backward() # Realización de backpropagation y cálculo de gradientes.\n",
    "  optimizer.step() # Actualización de los pesos del modelo.\n",
    "  \n",
    "  if epoch%10 == 0:\n",
    "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "    print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "\n",
    "# Funciones para predicción y generación de texto basadas en el modelo entrenado.\n",
    "def predict(model, character):\n",
    "  character = np.array([[char2int[c] for c in character]])\n",
    "  character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "  character = torch.from_numpy(character)\n",
    "  character.to(device)\n",
    "    \n",
    "  out, hidden = model(character)\n",
    "\n",
    "  prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "  char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "  return int2char[char_ind], hidden\n",
    "\n",
    "def sample(model, out_len, start='hey'):\n",
    "  model.eval() \n",
    "  start = start.lower()\n",
    "  chars = [ch for ch in start]\n",
    "  size = out_len - len(chars)\n",
    "  for ii in range(size):\n",
    "    char, h = predict(model, chars)\n",
    "    chars.append(char)\n",
    "\n",
    "  return ''.join(chars)\n",
    "\n",
    "# Ejemplo de uso de la función de generación de texto.\n",
    "sample(model, 15, 'good')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017567c-c7c0-4d27-81f1-1abeee08518f",
   "metadata": {},
   "source": [
    "#### Ejercicios: \n",
    "\n",
    "1. Modifica el modelo existente para que funcione como un autoencoder. Esto implica que el modelo debe aprender a codificar una secuencia de entrada en un vector de características (estado oculto) y luego decodificar ese vector de vuelta a la secuencia original (1.5 puntos).\n",
    "     - Implementa las capas de codificación y decodificación dentro del mismo modelo.\n",
    "     - Experimenta  con diferentes estructuras como LSTM para mejorar la retención de información.\n",
    "     - Mide la calidad de la reconstrucción del texto y la eficiencia de compresión.\n",
    "\n",
    "2. Utiliza el modelo RNN actual y modifícalo para introducir secuencias más largas. Monitoriza los gradientes durante el entrenamiento para detectar signos de desaparición o explosión. (1.5 puntos)\n",
    "    - Implementa el  clipping de gradiente para prevenir la explosión del gradiente.\n",
    "    - Reemplaza la RNN por LSTM para abordar la desaparición del gradiente.\n",
    "    - Utiliza técnicas de visualización para observar la magnitud de los gradientes a lo largo de varias épocas.\n",
    "3. Implementa el dropout en las capas recurrentes y comparar los resultados. (1 punto)\n",
    "\n",
    "    - Ajusta el parámetro de weight decay en el optimizador y observar el efecto sobre el overfitting.\n",
    "    - Aplica early stopping basado en la validación del loss para detener el entrenamiento antes de que el modelo comience a sobreajustarse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3374e",
   "metadata": {},
   "source": [
    "Para modificar el modelo RNN existente y convertirlo en un autoencoder que aprenda a codificar una secuencia de entrada en un vector de características (estado oculto) y luego decodificar ese vector de vuelta a la secuencia original, se necesitará realizar varias modificaciones estructurales y funcionales en el modelo. \n",
    "\n",
    "1. Definición del modelo Autoencoder RNN\n",
    "\n",
    "La estructura básica del autoencoder RNN incluirá capas de codificación y decodificación. Usaremos LSTM para la capa de codificación para capturar dependencias a largo plazo, y otra red LSTM o RNN para la decodificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5508b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, n_layers):\n",
    "        super(RNN_Autoencoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Codificador\n",
    "        self.encoder = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "        # Decodificador\n",
    "        self.decoder = nn.LSTM(hidden_dim, input_size, n_layers, batch_first=True)  # Podría ser también un RNN simple o GRU\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Codificación\n",
    "        _, (hidden, _) = self.encoder(x)\n",
    "\n",
    "        # Decodificación\n",
    "        # Repetir el vector de características a través de la secuencia para decodificar\n",
    "        repeat_hidden = hidden.repeat(1, x.size(1), 1)\n",
    "        output, _ = self.decoder(repeat_hidden)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ef7cf",
   "metadata": {},
   "source": [
    "2 . Instanciación del modelo y configuración de entrenamiento\n",
    "\n",
    "Configura el autoencoder y define los hiperparámetros de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "input_size = dict_size\n",
    "hidden_dim = 12\n",
    "n_layers = 1\n",
    "\n",
    "# Instancia del modelo\n",
    "autoencoder = RNN_Autoencoder(input_size, hidden_dim, n_layers)\n",
    "autoencoder.to(device)\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.01)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    output = autoencoder(input_seq)\n",
    "    loss = criterion(output, input_seq)  # Ahora se compara la salida con la entrada\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoca: {epoch}/{n_epochs} Perdida: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba2acb",
   "metadata": {},
   "source": [
    "3 . Prueba y evaluación\n",
    "\n",
    "Evaluaremos el modelo autoencoder en términos de calidad de la reconstrucción y la eficiencia de la compresión del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e19532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de evaluación\n",
    "def evaluate_autoencoder(model, input_seq):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_seq)\n",
    "    return output\n",
    "\n",
    "# Convertir el tensor de salida en texto para verificar la reconstrucción\n",
    "output_seq = evaluate_autoencoder(autoencoder, input_seq)\n",
    "reconstructed_text = torch.argmax(output_seq, dim=2)\n",
    "reconstructed_text = ''.join([int2char[int(idx)] for idx in reconstructed_text[0]])\n",
    "\n",
    "print(f'Texto reconstruido: {reconstructed_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035628c",
   "metadata": {},
   "source": [
    "Modificar el modelo RNN para manejar secuencias más largas y monitorizar los gradientes durante el entrenamiento es una excelente forma de mejorar la robustez del modelo y prevenir problemas comunes como la desaparición y explosión de gradientes. Implementamos estos cambios utilizando PyTorch, introduciendo clipping de gradiente, sustituyendo RNN por LSTM, y utilizando técnicas de visualización para los gradientes.\n",
    "\n",
    "1 . Reemplazar RNN por LSTM\n",
    "\n",
    "Primero, reemplacemos la capa RNN por una LSTM para mejorar la capacidad del modelo para aprender dependencias a largo plazo y abordar la desaparición del gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])  # Tomamos la última salida temporal para la predicción\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Inicializar el estado oculto y el estado de la celda a cero\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
    "                  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb05f3",
   "metadata": {},
   "source": [
    "2 . Implementación del clipping de gradiente\n",
    "\n",
    "Añade clipping de gradiente en el bucle de entrenamiento para prevenir la explosión del gradiente. Esto limita la norma del gradiente a un valor máximo deseado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c85b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de hiperparámetros para el entrenamiento.\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "clip=5  # Valor máximo para la norma de los gradientes\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_seq)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward()\n",
    "    \n",
    "    # Aplicar clipping de gradiente\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoca: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Perdida: {:.4f}\".format(loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d621302",
   "metadata": {},
   "source": [
    "3 . Técnicas de Visualización para los Gradientes\n",
    "\n",
    "Usamos matplotlib para visualizar la magnitud de los gradientes puede ayudarte a entender cómo evolucionan durante el entrenamiento:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396476aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Guardar magnitudes de gradiente para visualización\n",
    "grad_magnitudes = []\n",
    "\n",
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    grad_magnitudes.append(ave_grads)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_seq)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward()\n",
    "    plot_grad_flow(model.named_parameters())\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(np.array(grad_magnitudes).T, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.yticks(range(len(layers)), layers)\n",
    "plt.title(\"Gradiente Magnitud a lo largo de Épocas\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Capas\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703054ca",
   "metadata": {},
   "source": [
    "Incorporar técnicas como dropout en las capas recurrentes, ajustar el parámetro de weight decay en el optimizador y aplicar early stopping son métodos efectivos para mejorar la generalización del modelo y prevenir el sobreajuste. A continuación, te muestro cómo implementar estos métodos en el contexto de un modelo LSTM en PyTorch.\n",
    "\n",
    "1 . Implementación del Dropout en las capas recurrentes\n",
    "\n",
    "Modificar el modelo para incluir dropout en las capas LSTM. Esto ayuda a prevenir el sobreajuste al introducir regularización en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2375cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout_prob):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # LSTM con dropout\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Inicializar el estado oculto y el estado de la celda a cero\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
    "                  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
    "        return hidden\n",
    "\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=2, dropout_prob=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9219b2b",
   "metadata": {},
   "source": [
    "2 . Ajuste del parámetro de weight decay en el optimizador\n",
    "\n",
    "Configurar el optimizador para usar weight decay, lo cual añade una penalización L2 a los pesos durante el entrenamiento, ayudando a reducir el sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)  # Ajuste de weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fd489",
   "metadata": {},
   "source": [
    "3 . Aplicación de early stopping\n",
    "\n",
    "Implementar early stopping para finalizar el entrenamiento si el loss de validación no mejora después de un número determinado de épocas. Esto evita que el modelo sobreajuste al conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9adb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'Contador EarlyStopping : {self.counter} de {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "# Ejemplo de uso de EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_seq)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_output = model(val_seq)  # Suponiendo que val_seq es la secuencia de validación\n",
    "    val_loss = criterion(val_output, val_target_seq.view(-1).long())\n",
    "\n",
    "    print(f'Epoca {epoch}, Perdida de entrenamiento: {loss.item()}, Perdida de validacion: {val_loss.item()}')\n",
    "\n",
    "    early_stopping(val_loss.item(), model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e77ccb",
   "metadata": {},
   "source": [
    "Estos métodos proporcionan maneras robustas de mejorar la generalización del modelo y prevenir el sobreajuste. Implementando dropout en las capas LSTM, ajustando weight decay en el optimizador y utilizando early stopping basado en el desempeño en el conjunto de validación, podemos obtener un modelo más robusto y efectivo para tareas de aprendizaje secuencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de04032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
