{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9374fcc-cd34-43d1-aa06-166d26046234",
   "metadata": {},
   "source": [
    "## Aprendizaje por refuerzo desde retroalimentación humana (RLHF)\n",
    "\n",
    "- Imitación aprendizaje (IL): Algoritmo de aprendizaje por refuerzo donde el agente aprende a imitar el comportamiento de un experto.\n",
    "- Aprendizaje inverso de reforzamiento (IRL): Algoritmo donde se infiere la función de recompensa que un experto está optimizando a partir de sus demostraciones.\n",
    "- Aprendizaje por preferencias: Un enfoque donde se usan comparaciones de pares de trayectorias en lugar de recompensas numéricas para guiar el aprendizaje del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c78f70c-1121-40f4-aa80-f29c07f74199",
   "metadata": {},
   "source": [
    "### El aprendizaje por imitación (IL) \n",
    "\n",
    "Es un campo del aprendizaje automático en el cual un agente aprende a realizar tareas replicando el comportamiento de un experto. Este enfoque es especialmente útil en escenarios donde es difícil definir una función de recompensa precisa o cuando la exploración directa en el entorno es costosa o peligrosa.\n",
    "\n",
    "**1 . Behavioral Cloning (BC)**\n",
    "Behavioral Cloning (BC) es una técnica básica de aprendizaje por imitación que utiliza aprendizaje supervisado para entrenar al agente a partir de demostraciones de expertos.\n",
    "\n",
    "**Proceso de BC:**\n",
    "\n",
    "- Recopilación de datos: Se recopilan datos de demostraciones de un experto, que consisten en pares de estados y acciones (s, a).\n",
    "- Entrenamiento del modelo: Se entrena un modelo de política (por ejemplo, una red neuronal) utilizando los pares de estados y acciones. El objetivo es minimizar la diferencia entre las acciones predichas por el modelo y las acciones del experto en los estados correspondientes.\n",
    "- Ejecución de la política: El modelo entrenado se utiliza para predecir acciones en nuevos estados durante la ejecución.\n",
    "\n",
    "**Ventajas y desventajas**:\n",
    "\n",
    "* Ventajas: Simple de implementar y puede funcionar bien si las demostraciones del experto cubren adecuadamente el espacio de estados.\n",
    "* Desventajas: Puede sufrir de errores de compounding, donde pequeños errores en la predicción de acciones pueden acumularse y llevar a un desempeño pobre. BC también depende en gran medida de la calidad y cobertura de las demostraciones del experto.\n",
    "\n",
    "**2 . DAgger (Dataset Aggregation)**\n",
    "\n",
    "DAgger (Dataset Aggregation) es una técnica que mejora BC al abordar el problema de los errores de compounding. DAgger es un enfoque iterativo que combina el aprendizaje supervisado con la retroalimentación en tiempo real del experto.\n",
    "\n",
    "**Proceso de DAgger:**\n",
    "\n",
    "- Recopilación de datos inicial: Se recopila un conjunto inicial de demostraciones del experto.\n",
    "- Entrenamiento inicial: Se entrena una política inicial usando BC con los datos iniciales.\n",
    "- Iteraciones de DAgger:\n",
    "    * Ejecución de la política actual: La política entrenada se ejecuta en el entorno, generando nuevas trayectorias de estados.\n",
    "    * Corrección del experto: El experto revisa las acciones tomadas por la política en estos nuevos estados y proporciona las acciones correctas.\n",
    "    * Agregación de datos: Los nuevos pares de estados y acciones proporcionados por el experto se agregan al conjunto de datos de entrenamiento.\n",
    "    * Reentrenamiento: La política se reentrena con el conjunto de datos ampliado.\n",
    " \n",
    "**3 . Generative adversarial imitation learning (GAIL):**\n",
    "\n",
    "GAIL combina aprendizaje por imitación con técnicas de aprendizaje generativo adversarial. En GAIL, un generador (la política del agente) y un discriminador (que distingue entre trayectorias generadas por el agente y trayectorias del experto) se entrenan conjuntamente. El objetivo del generador es producir trayectorias que sean indistinguibles de las trayectorias del experto, según el discriminador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05dd7d-8dc2-4690-8eba-760d32abebce",
   "metadata": {},
   "source": [
    "### Ejemplos\n",
    "\n",
    "Aquí hay un ejemplo simple de cómo implementar Behavioral Cloning en PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2353e-cb3c-4d64-9ab8-2c87b3f0eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Definir la red neuronal\n",
    "class BCModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(BCModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Datos de ejemplo (estados y acciones)\n",
    "states = torch.rand((1000, 4))  # 1000 estados con 4 características cada uno\n",
    "actions = torch.rand((1000, 2))  # 1000 acciones con 2 dimensiones cada una\n",
    "\n",
    "# Crear el dataset y dataloader\n",
    "dataset = TensorDataset(states, actions)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Inicializar el modelo, la pérdida y el optimizador\n",
    "model = BCModel(input_dim=4, output_dim=2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamiento\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_states, batch_actions in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_states)\n",
    "        loss = criterion(outputs, batch_actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoca [{epoch+1}/{num_epochs}], Perdida: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Entrenamiento completado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a840e7d-32db-4ed8-91d0-130decf8c1c0",
   "metadata": {},
   "source": [
    "Este código entrena un modelo de clonación de comportamiento utilizando un conjunto de datos de estados y acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84866d7-8d69-41d8-8da2-e4752f676a43",
   "metadata": {},
   "source": [
    "Aquí hay un ejemplo de cómo implementar DAgger en PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220b652-25a6-4de7-a537-3ea9ad4b1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Definir la red neuronal\n",
    "class DAggerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DAggerModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Función simulada para obtener la acción del experto\n",
    "def expert_policy(state):\n",
    "    return state.sum(dim=-1, keepdim=True) * 0.5  # Acción simulada\n",
    "\n",
    "# Datos iniciales de ejemplo\n",
    "states = torch.rand((1000, 4))\n",
    "actions = expert_policy(states)\n",
    "\n",
    "# Crear el dataset inicial y dataloader\n",
    "dataset = TensorDataset(states, actions)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Inicializar el modelo, la pérdida y el optimizador\n",
    "model = DAggerModel(input_dim=4, output_dim=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamiento inicial con BC\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_states, batch_actions in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_states)\n",
    "        loss = criterion(outputs, batch_actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoca [{epoch+1}/{num_epochs}], Perida: {loss.item():.4f}\")\n",
    "\n",
    "# Iteraciones de DAgger\n",
    "num_iterations = 10\n",
    "for iteration in range(num_iterations):\n",
    "    # Ejecutar la política actual y recopilar nuevos datos\n",
    "    new_states = torch.rand((500, 4))  # Nuevos estados alcanzados por la política\n",
    "    new_actions = expert_policy(new_states)  # Acciones del experto para los nuevos estados\n",
    "\n",
    "    # Agregar los nuevos datos al dataset existente\n",
    "    states = torch.cat([states, new_states], dim=0)\n",
    "    actions = torch.cat([actions, new_actions], dim=0)\n",
    "\n",
    "    # Crear un nuevo dataloader con el dataset ampliado\n",
    "    dataset = TensorDataset(states, actions)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Reentrenar el modelo\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_states, batch_actions in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_states)\n",
    "            loss = criterion(outputs, batch_actions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Iteracion [{iteration+1}/{num_iterations}], Epoca [{epoch+1}/{num_epochs}], Perdida: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Entrenamiento completado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646227bf-130f-41cb-ae0e-6fb54b4fb235",
   "metadata": {},
   "source": [
    "Este código muestra un ejemplo simplificado de DAgger. En cada iteración, el agente ejecuta su política actual para recolectar nuevos estados y obtiene las acciones correctas del experto para estos estados. Luego, reentrena la política con el conjunto de datos ampliado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae371a0-201f-49c2-badc-201a546b2db1",
   "metadata": {},
   "source": [
    "A continuación, se presenta una implementación básica de GAIL en PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe8643-ff2e-4ff3-b721-e358d424f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Definir la red neuronal para la política (generador)\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        action_probs = torch.softmax(self.fc3(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "# Definir la red neuronal para el discriminador\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        action_onehot = torch.zeros(action.size(0), action_dim)\n",
    "        action_onehot.scatter_(1, action.long(), 1)  # Convertir las acciones a one-hot encoding\n",
    "        x = torch.cat([state, action_onehot], dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        prob = torch.sigmoid(self.fc3(x))\n",
    "        return prob\n",
    "\n",
    "# Generar una trayectoria utilizando la política actual\n",
    "def generate_trajectory(policy_net, env, max_steps=200):\n",
    "    state = env.reset()\n",
    "    trajectory = []\n",
    "    for _ in range(max_steps):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action_probs = policy_net(state_tensor)\n",
    "        action = np.random.choice(len(action_probs[0]), p=action_probs.detach().numpy()[0])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        trajectory.append((state, action))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return trajectory\n",
    "\n",
    "# Generar trayectorias de un experto\n",
    "def generate_expert_trajectories(env, expert_policy, num_trajectories=10, max_steps=200):\n",
    "    expert_trajectories = []\n",
    "    for _ in range(num_trajectories):\n",
    "        trajectory = []\n",
    "        state = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            action = expert_policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        expert_trajectories.append(trajectory)\n",
    "    return expert_trajectories\n",
    "\n",
    "# Entrenar el discriminador\n",
    "def train_discriminator(discriminator, expert_trajectories, generated_trajectories, optimizer, num_epochs=10):\n",
    "    criterion = nn.BCELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for expert_traj, gen_traj in zip(expert_trajectories, generated_trajectories):\n",
    "            expert_states, expert_actions = zip(*expert_traj)\n",
    "            gen_states, gen_actions = zip(*gen_traj)\n",
    "            \n",
    "            expert_states = torch.tensor(np.array(expert_states), dtype=torch.float32)\n",
    "            expert_actions = torch.tensor(np.array(expert_actions), dtype=torch.float32).unsqueeze(-1)\n",
    "            gen_states = torch.tensor(np.array(gen_states), dtype=torch.float32)\n",
    "            gen_actions = torch.tensor(np.array(gen_actions), dtype=torch.float32).unsqueeze(-1)\n",
    "            \n",
    "            expert_labels = torch.ones(len(expert_states), 1)\n",
    "            gen_labels = torch.zeros(len(gen_states), 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            expert_preds = discriminator(expert_states, expert_actions)\n",
    "            gen_preds = discriminator(gen_states, gen_actions)\n",
    "            \n",
    "            loss = criterion(expert_preds, expert_labels) + criterion(gen_preds, gen_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoca {epoch+1}, Perdida: {total_loss / len(expert_trajectories)}')\n",
    "\n",
    "# Entrenar la política\n",
    "def train_policy(policy_net, discriminator, optimizer, env, num_epochs=10, max_steps=200):\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        trajectory = generate_trajectory(policy_net, env, max_steps)\n",
    "        states, actions = zip(*trajectory)\n",
    "        \n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.float32).unsqueeze(-1)\n",
    "        \n",
    "        rewards = -torch.log(discriminator(states, actions))\n",
    "        loss = -rewards.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoca {epoch+1}, Politica perdida: {loss.item()}')\n",
    "\n",
    "# Inicialización del entorno y modelos\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "discriminator = Discriminator(state_dim, action_dim)\n",
    "\n",
    "optimizer_policy = optim.Adam(policy_net.parameters(), lr=3e-4)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=3e-4)\n",
    "\n",
    "# Generar trayectorias de expertos (esto debería ser reemplazado con una política de experto real)\n",
    "expert_policy = lambda state: env.action_space.sample()  # Ejemplo simple\n",
    "expert_trajectories = generate_expert_trajectories(env, expert_policy, num_trajectories=10)\n",
    "\n",
    "for iteration in range(10):\n",
    "    generated_trajectories = [generate_trajectory(policy_net, env) for _ in range(10)]\n",
    "    train_discriminator(discriminator, expert_trajectories, generated_trajectories, optimizer_discriminator)\n",
    "    train_policy(policy_net, discriminator, optimizer_policy, env)\n",
    "\n",
    "print(\"Entrenamiento completado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d3a09-71cd-4bd3-9f9a-0279e7f99542",
   "metadata": {},
   "source": [
    "Esta implementación básica de GAIL en PyTorch proporciona una base para experimentar con la combinación de aprendizaje por imitación y aprendizaje generativo adversarial. Se pueden mejorar diversos aspectos del código, como la arquitectura de las redes, los hiperparámetros y las técnicas de optimización, para ajustarse mejor a aplicaciones específicas y mejorar el rendimiento del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da624ed9-158c-4741-82cc-9a10e22446f3",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 . Implementa y entrena un modelo de Behavioral Cloning en el entorno LunarLander-v2.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Utiliza el entorno LunarLander-v2 de OpenAI Gym.\n",
    "- Recopila un conjunto de datos de demostraciones de un experto.\n",
    "- Entrena una política utilizando Behavioral Cloning.\n",
    "- Evalúa la política clonada en el entorno y compara su rendimiento con el experto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc5ed2-7706-468b-a8b8-651e1b36fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519da21-a95a-4fa4-a1ac-42e2b2174bc9",
   "metadata": {},
   "source": [
    "2 . Implementa y entrena un modelo usando DAgger en el entorno MountainCarContinuous-v0.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Utiliza el entorno MountainCarContinuous-v0 de OpenAI Gym.\n",
    "- Recopila un conjunto inicial de datos de demostraciones de un experto.\n",
    "- Implementa el algoritmo DAgger para mejorar la política iterativamente.\n",
    "- Evalúa la política final en el entorno y compara su rendimiento con el experto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9f3df-65e1-4e35-a670-4204fc3c6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2607cc3-c4b8-4222-9ccd-eebba7cffa5c",
   "metadata": {},
   "source": [
    "3 . Implementa y entrena un modelo de GAIL en el entorno BipedalWalker-v3.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Utiliza el entorno BipedalWalker-v3 de OpenAI Gym.\n",
    "- Recopila un conjunto de datos de demostraciones de un experto.\n",
    "- Implementa el algoritmo GAIL para entrenar tanto el generador como el discriminador.\n",
    "- Evalúa la política final en el entorno y compara su rendimiento con el experto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f27d0b2-279b-4337-875a-63cfa6f70b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8625fa-7945-45c3-966b-f7ba57277a14",
   "metadata": {},
   "source": [
    "### Aprendiza inverso de reforzamiento (IRL)\n",
    "\n",
    "El aprendizaje por refuerzo inverso (Inverse Reinforcement Learning, IRL) es un campo de la inteligencia artificial donde el objetivo es inferir la función de recompensa que un agente está tratando de maximizar a partir de su comportamiento observado. A diferencia del aprendizaje por refuerzo tradicional, donde la función de recompensa está dada y el objetivo es encontrar la política óptima, en IRL se observa la política del experto y se trata de descubrir la función de recompensa que podría haber llevado a esa política."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878769f4-91f2-4027-b9f7-55d20794485c",
   "metadata": {},
   "source": [
    "#### Máxima entropía inverse reinforcement learning (MaxEnt IRL)\n",
    "\n",
    "MaxEnt IRL es un algoritmo que busca inferir una función de recompensa a partir de demostraciones de expertos, utilizando el principio de máxima entropía para manejar la incertidumbre de manera robusta. Este enfoque garantiza que entre todas las distribuciones posibles de trayectorias que coinciden con las demostraciones, se selecciona aquella que tiene la máxima entropía, evitando suposiciones innecesarias y promoviendo la diversidad.\n",
    "\n",
    "**Proceso de MaxEnt IRL**\n",
    "\n",
    "- Recopilación de datos: Obtener demostraciones de expertos que consisten en trayectorias de estados y acciones.\n",
    "- Modelado de trayectorias: Representar las trayectorias como secuencias de estados y acciones.\n",
    "- Cálculo de la entropía: Utilizar el principio de máxima entropía para modelar la probabilidad de las trayectorias.\n",
    "- Optimización: Ajustar la función de recompensa para maximizar la probabilidad de las trayectorias observadas.\n",
    "\n",
    "A continuación, se presenta un ejemplo simplificado de cómo implementar MaxEnt IRL en PyTorch. Este ejemplo asume un entorno simple donde se conocen las transiciones de estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f747af-a543-487f-8cb3-f37d0845594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Definir la red neuronal para la función de recompensa\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        reward = self.fc2(x)\n",
    "        return reward\n",
    "\n",
    "# Función para calcular la probabilidad de trayectorias usando máxima entropía\n",
    "def compute_trajectory_probabilities(trajectories, reward_model, gamma=0.99):\n",
    "    probabilities = []\n",
    "    for trajectory in trajectories:\n",
    "        states = torch.tensor(trajectory, dtype=torch.float32, requires_grad=True)\n",
    "        rewards = reward_model(states).squeeze()\n",
    "        discounted_rewards = (gamma ** torch.arange(len(rewards))).to(rewards) * rewards\n",
    "        trajectory_prob = torch.exp(discounted_rewards.sum())\n",
    "        probabilities.append(trajectory_prob)\n",
    "    probabilities = torch.stack(probabilities)\n",
    "    return probabilities / probabilities.sum()\n",
    "\n",
    "# Función para entrenar el modelo de recompensa\n",
    "def train_maxent_irl(trajectories, state_dim, lr=0.01, num_epochs=100):\n",
    "    reward_model = RewardModel(state_dim)\n",
    "    optimizer = optim.Adam(reward_model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calcular las probabilidades de las trayectorias\n",
    "        trajectory_probs = compute_trajectory_probabilities(trajectories, reward_model)\n",
    "        \n",
    "        # Calcular la función de pérdida (negativa de la suma de log-probabilidades)\n",
    "        loss = -torch.log(trajectory_probs).mean()\n",
    "        \n",
    "        # Optimizar el modelo de recompensa\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoca {epoch}, Perdida: {loss.item()}')\n",
    "\n",
    "    return reward_model\n",
    "\n",
    "# Datos de ejemplo: trayectorias observadas del experto (cada trayectoria es una secuencia de estados)\n",
    "trajectories = [\n",
    "    [[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]],\n",
    "    [[0.1, 0.0], [0.2, 0.1], [0.3, 0.2]],\n",
    "    # Más trayectorias del experto\n",
    "]\n",
    "\n",
    "# Entrenar el modelo de recompensa usando MaxEnt IRL\n",
    "state_dim = 2  # Dimensionalidad del estado\n",
    "reward_model = train_maxent_irl(trajectories, state_dim)\n",
    "\n",
    "# Evaluar el modelo de recompensa en un nuevo estado\n",
    "new_state = torch.tensor([[0.1, 0.1]], dtype=torch.float32)\n",
    "predicted_reward = reward_model(new_state)\n",
    "print(f'Recompensa predicha para el estado {new_state.numpy()}: {predicted_reward.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6dc590-c65c-44e7-9c15-5babb1611841",
   "metadata": {},
   "source": [
    "### Apprenticeship Learning via Inverse Reinforcement Learning\n",
    "\n",
    "El concepto de \"Apprenticeship Learning via Inverse Reinforcement Learning\" implica aprender una política similar a la de un experto mediante el aprendizaje de la función de recompensa subyacente que el experto está optimizando. Aquí te presento una implementación básica utilizando PyTorch.\n",
    "\n",
    "La idea principal es iterar entre dos fases:\n",
    "\n",
    "* Actualizar la política: Usar una técnica de RL estándar para optimizar la política dada una función de recompensa.\n",
    "* Actualizar la función de recompensa: Usar las trayectorias generadas por la política actual para ajustar la función de recompensa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d80b1-8a13-482c-a69d-88581230a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Definir la red neuronal para la función de recompensa\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        reward = self.fc2(x)\n",
    "        return reward\n",
    "\n",
    "# Definir la política para espacios de acción discretos\n",
    "class DiscretePolicyModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DiscretePolicyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "def select_action(policy, state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action_probs = policy(state_tensor)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action\n",
    "\n",
    "# Función para calcular el retorno acumulado de una trayectoria\n",
    "def compute_return(trajectory, reward_model, gamma=0.99):\n",
    "    states = torch.tensor(np.array(trajectory), dtype=torch.float32, requires_grad=True)\n",
    "    rewards = reward_model(states).squeeze()\n",
    "    discounted_rewards = (gamma ** torch.arange(len(rewards))).to(rewards) * rewards\n",
    "    return discounted_rewards.sum()\n",
    "\n",
    "# Función para recolectar una trayectoria usando la política actual\n",
    "def collect_trajectory(env, policy):\n",
    "    state = env.reset()\n",
    "    trajectory = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(policy, state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        trajectory.append(state)\n",
    "        state = next_state\n",
    "    return trajectory\n",
    "\n",
    "# Función para entrenar la política usando la recompensa aprendida\n",
    "def train_policy(env, reward_model, policy, optimizer, num_episodes=100, gamma=0.99):\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = select_action(policy, state)\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            trajectory.append(state)\n",
    "            state = next_state\n",
    "        G = compute_return(trajectory, reward_model, gamma)\n",
    "        optimizer.zero_grad()\n",
    "        G.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Función principal de entrenamiento de IRL\n",
    "def train_apprenticeship_irl(env_name, expert_trajectories, lr=0.01, num_epochs=100, gamma=0.99):\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    reward_model = RewardModel(state_dim)\n",
    "    policy_model = DiscretePolicyModel(state_dim, action_dim)\n",
    "    reward_optimizer = optim.Adam(reward_model.parameters(), lr=lr)\n",
    "    policy_optimizer = optim.Adam(policy_model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Fase 1: Optimizar la política con la recompensa actual\n",
    "        train_policy(env, reward_model, policy_model, policy_optimizer)\n",
    "        \n",
    "        # Fase 2: Optimizar el modelo de recompensa con las trayectorias del experto y las generadas\n",
    "        reward_optimizer.zero_grad()\n",
    "        expert_returns = torch.stack([compute_return(traj, reward_model, gamma) for traj in expert_trajectories])\n",
    "        policy_trajectories = [collect_trajectory(env, policy_model) for _ in range(len(expert_trajectories))]\n",
    "        policy_returns = torch.stack([compute_return(traj, reward_model, gamma) for traj in policy_trajectories])\n",
    "        \n",
    "        loss = -expert_returns.mean() + policy_returns.mean()\n",
    "        loss.backward()\n",
    "        reward_optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoca {epoch}, Perdida: {loss.item()}')\n",
    "\n",
    "    return policy_model, reward_model\n",
    "\n",
    "# Ejemplo de uso con el entorno \"CartPole-v1\"\n",
    "expert_trajectories = [\n",
    "    [[0.0, 0.0, 0.1, 0.1], [0.1, 0.1, 0.2, 0.2], [0.2, 0.2, 0.3, 0.3]],\n",
    "    [[0.0, 0.1, 0.1, 0.2], [0.1, 0.2, 0.2, 0.3], [0.2, 0.3, 0.3, 0.4]],\n",
    "    # Más trayectorias del experto\n",
    "]\n",
    "\n",
    "policy_model, reward_model = train_apprenticeship_irl(\"CartPole-v1\", expert_trajectories)\n",
    "\n",
    "# Evaluar la política entrenada en un nuevo estado\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action = select_action(policy_model, state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f'Recompensa total obtenida por la política aprendida: {total_reward}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b4783-f19f-49e5-812d-7d7acb49db62",
   "metadata": {},
   "source": [
    "### Deep Inverse Reinforcement Learning (Deep IRL)\n",
    "\n",
    "Deep IRL extiende los métodos tradicionales de IRL utilizando redes neuronales profundas para modelar la función de recompensa. Este enfoque permite manejar problemas más complejos con estados de alta dimensionalidad.\n",
    "\n",
    "* Definir una red neural para la función de recompensa.\n",
    "* Optimizar la función de recompensa usando trayectorias observadas.\n",
    "* Entrenar una política usando RL con la función de recompensa aprendida.\n",
    "\n",
    "Revisar: [Maximum Entropy Deep Inverse Reinforcement Learning](https://arxiv.org/abs/1507.04888)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c46db-d813-431e-bc79-97f209dcb86b",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 . Implementa y entrena un modelo usando el algoritmo de Máxima Entropía IRL en el entorno LunarLander-v2.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "* Utiliza el entorno LunarLander-v2 de OpenAI Gym.\n",
    "* Recopila un conjunto de datos de demostraciones de un experto.\n",
    "* Implementa el algoritmo de Máxima Entropía IRL para aprender la función de recompensa.\n",
    "* Entrena una política utilizando la función de recompensa aprendida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8886d23-aae4-4f7c-b7d6-7e5fe286896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd08c10-6860-4e18-89d8-64163926231d",
   "metadata": {},
   "source": [
    "2 .Implementa y entrena un modelo usando el algoritmo de Máxima Entropía IRL en el entorno MountainCarContinuous-v0.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "* Utiliza el entorno MountainCarContinuous-v0 de OpenAI Gym.\n",
    "* Recopila un conjunto de datos de demostraciones de un experto.\n",
    "* Implementa el algoritmo de Máxima Entropía IRL para aprender la función de recompensa.\n",
    "* Entrena una política utilizando la función de recompensa aprendida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151ef13-1a84-4c3c-954f-3d37d93a32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdfd555-01f4-4c45-8582-4373ed20d546",
   "metadata": {},
   "source": [
    "### Aprendizaje por preferencia\n",
    "\n",
    "El aprendizaje por preferencias es una subárea del aprendizaje automático que se centra en aprender a partir de preferencias en lugar de ejemplos etiquetados de manera explícita. En lugar de aprender a partir de pares de datos de entrada y salida, el modelo aprende a partir de comparaciones o rankings de diferentes opciones. Este enfoque es especialmente útil en situaciones donde las etiquetas exactas son difíciles de obtener, pero es fácil obtener preferencias relativas.\n",
    "\n",
    "Por ejemplo, en lugar de saber que la opción A tiene una recompensa de 10 y la opción B tiene una recompensa de 5, el modelo sólo sabe que A es preferida sobre B. Este tipo de aprendizaje es común en aplicaciones como sistemas de recomendación, donde las preferencias de los usuarios son más fáciles de obtener que las evaluaciones cuantitativas exactas.\n",
    "\n",
    "En el contexto del aprendizaje por refuerzo, el aprendizaje por refuerzo basado en preferencias (PBRL) se centra en aprender políticas óptimas a partir de preferencias en lugar de recompensas numéricas explícitas. Esto es útil en situaciones donde es difícil definir una función de recompensa exacta, pero se pueden obtener comparaciones de resultados o trayectorias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a3c71-13b6-4cdb-abef-3ed9576b51b5",
   "metadata": {},
   "source": [
    "Aquí hay un ejemplo simple de cómo se podría implementar un algoritmo de aprendizaje por preferencias en PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528db5d8-6d49-4f72-a04c-a4e4c5449d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir el modelo de preferencia\n",
    "class PreferenceModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PreferenceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        preference = self.fc2(x)\n",
    "        return preference\n",
    "\n",
    "# Generar datos de preferencia simulados\n",
    "def generate_preference_data(num_samples, input_dim):\n",
    "    data = torch.randn(num_samples, input_dim)\n",
    "    preferences = torch.randint(0, 2, (num_samples,))  # 0 o 1 para indicar preferencia\n",
    "    return data, preferences\n",
    "\n",
    "# Función para entrenar el modelo de preferencia\n",
    "def train_preference_model(data, preferences, input_dim, lr=0.01, num_epochs=100):\n",
    "    model = PreferenceModel(input_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data).squeeze()\n",
    "        loss = loss_fn(outputs, preferences.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoca {epoch}, Perdida: {loss.item()}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Ejemplo de uso\n",
    "input_dim = 4\n",
    "data, preferences = generate_preference_data(100, input_dim)\n",
    "trained_model = train_preference_model(data, preferences, input_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ff564-7943-483c-b383-a1b30001f8bd",
   "metadata": {},
   "source": [
    "#### El modelo Bradley-Terry \n",
    "\n",
    "Es un modelo probabilístico utilizado para predecir el resultado de comparaciones binarias entre pares de opciones. Es ampliamente utilizado en la teoría de la decisión y en el aprendizaje por preferencias para modelar preferencias en varios contextos, como competiciones deportivas, sistemas de recomendación y encuestas de opinión.\n",
    "\n",
    "En el modelo Bradley-Terry, se asume que cada opción $i$ tiene una habilidad o \"score\" $\\theta_i$. La probabilidad de que la opción $i$ sea preferida sobre la opción $j$ se modela como:\n",
    "\n",
    "$$P(i \\succ j) = \\frac{\\theta_i}{\\theta_i + \\theta_j}$$\n",
    "\n",
    "donde $i \\succ j$ indica que la opción $i$ es preferida sobre la opción $j$.\n",
    "\n",
    "**Estimación de parámetros**\n",
    "\n",
    "Para estimar los parámetros $\\theta_i$ de cada opción $i$, se puede utilizar el método de máxima verosimilitud. La función de verosimilitud para el conjunto de comparaciones observadas se maximiza con respecto a los parámetros $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013788f5-8930-40e8-aff0-610b45fcf025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir el modelo Bradley-Terry\n",
    "class BradleyTerryModel(nn.Module):\n",
    "    def __init__(self, num_items):\n",
    "        super(BradleyTerryModel, self).__init__()\n",
    "        self.scores = nn.Parameter(torch.randn(num_items))\n",
    "    \n",
    "    def forward(self, i, j):\n",
    "        score_i = self.scores[i]\n",
    "        score_j = self.scores[j]\n",
    "        prob = torch.sigmoid(score_i - score_j)\n",
    "        return prob\n",
    "\n",
    "# Generar datos de comparación simulados\n",
    "def generate_comparison_data(num_pairs, num_items):\n",
    "    comparisons = torch.randint(0, num_items, (num_pairs, 2))\n",
    "    preferences = torch.randint(0, 2, (num_pairs,))\n",
    "    return comparisons, preferences\n",
    "\n",
    "# Función para entrenar el modelo Bradley-Terry\n",
    "def train_bradley_terry_model(comparisons, preferences, num_items, lr=0.01, num_epochs=100):\n",
    "    model = BradleyTerryModel(num_items)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        i = comparisons[:, 0]\n",
    "        j = comparisons[:, 1]\n",
    "        outputs = model(i, j)\n",
    "        loss = loss_fn(outputs, preferences.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoca {epoch}, Perdida: {loss.item()}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Ejemplo de uso\n",
    "num_items = 10\n",
    "comparisons, preferences = generate_comparison_data(100, num_items)\n",
    "trained_model = train_bradley_terry_model(comparisons, preferences, num_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5133388-e8e0-4cea-80bb-ecb3e6f45264",
   "metadata": {},
   "source": [
    "El PBRL es una extensión del aprendizaje por refuerzo (RL) que utiliza preferencias en lugar de recompensas numéricas explícitas para guiar el aprendizaje del agente. En PBRL, el agente recibe retroalimentación en forma de comparaciones entre diferentes trayectorias o acciones, indicando cuál es preferida.\n",
    "\n",
    "**Conceptos clave en PBRL**\n",
    "\n",
    "* Preferencias en lugar de recompensas: En lugar de recibir recompensas numéricas después de cada acción, el agente recibe comparaciones de trayectorias o acciones.\n",
    "* Función de recompensa implícita: El agente aprende una función de recompensa implícita que satisface las preferencias observadas.\n",
    "* Actualización de la política: La política del agente se actualiza para maximizar la probabilidad de generar trayectorias que sean consistentes con las preferencias observadas.\n",
    "  \n",
    "#### Algoritmos de PBRL\n",
    "\n",
    "**Preferencia por trayectorias (trajectory preference learning):**\n",
    "\n",
    "En este enfoque, el agente aprende a partir de comparaciones entre trayectorias completas. Las trayectorias son secuencias de estados y acciones, y el agente recibe retroalimentación sobre qué trayectorias son preferidas.\n",
    "\n",
    "- Ejemplo de algoritmo: Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL), donde se aprende una función de recompensa que maximiza la entropía de la política condicionada a las preferencias observadas.\n",
    "\n",
    "**Preferencia por parejas de estados (state pair preference learning):**\n",
    "\n",
    "Aquí, el agente aprende a partir de comparaciones entre pares de estados o pares de estado-acción. La retroalimentación indica qué par es preferido.\n",
    "\n",
    "- Ejemplo de algoritmo: Preference-Based Policy Iteration (PBPI), donde se actualiza la política basada en las preferencias entre pares de estados.\n",
    "\n",
    "**Preferencia por acciones (action preference learning):**\n",
    "\n",
    "En este enfoque, el agente aprende preferencias entre diferentes acciones en un estado dado. La retroalimentación se proporciona sobre cuál acción es preferida en un estado específico.\n",
    "\n",
    "- Ejemplo de algoritmo: Bayesian Preference Learning, donde se usa inferencia bayesiana para aprender las preferencias y actualizar la política en consecuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec4b81-045f-44c6-86a2-e7819cdd3df3",
   "metadata": {},
   "source": [
    "A continuación, se presenta un ejemplo de cómo implementar un algoritmo de PBRL usando el modelo Bradley-Terry para aprender una función de recompensa implícita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19402ae-ee3b-4063-91a8-5293cea82bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Definir el modelo de preferencia Bradley-Terry\n",
    "class BradleyTerryModel(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(BradleyTerryModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        score = self.fc2(x)\n",
    "        return score\n",
    "\n",
    "# Función para calcular la probabilidad de preferencia\n",
    "def preference_probability(model, state_i, state_j):\n",
    "    score_i = model(state_i)\n",
    "    score_j = model(state_j)\n",
    "    prob = torch.sigmoid(score_i - score_j)\n",
    "    return prob\n",
    "\n",
    "# Función para entrenar el modelo de preferencia\n",
    "def train_preference_model(model, optimizer, states, preferences, num_epochs=100):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        state_i, state_j = states\n",
    "        outputs = preference_probability(model, state_i, state_j).squeeze()\n",
    "        loss = loss_fn(outputs, preferences.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoca {epoch}, Perdida: {loss.item()}')\n",
    "\n",
    "# Generar datos de comparación simulados\n",
    "def generate_comparison_data(num_pairs, state_dim):\n",
    "    state_i = torch.randn(num_pairs, state_dim)\n",
    "    state_j = torch.randn(num_pairs, state_dim)\n",
    "    preferences = torch.randint(0, 2, (num_pairs,))\n",
    "    return (state_i, state_j), preferences\n",
    "\n",
    "# Entrenar una política usando la recompensa implícita aprendida\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        action_probs = torch.softmax(self.fc3(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "def select_action(policy, state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action_probs = policy(state_tensor)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action\n",
    "\n",
    "def collect_trajectory(env, policy, max_steps=200):\n",
    "    state = env.reset()\n",
    "    trajectory = []\n",
    "    for _ in range(max_steps):\n",
    "        action = select_action(policy, state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        trajectory.append(state)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return trajectory\n",
    "\n",
    "def compute_reward(trajectory, model):\n",
    "    rewards = [model(torch.tensor(state, dtype=torch.float32, requires_grad=True).unsqueeze(0)) for state in trajectory]\n",
    "    rewards_tensor = torch.cat(rewards)\n",
    "    return rewards_tensor.sum()\n",
    "\n",
    "def train_policy(env, model, policy, optimizer, num_episodes=100):\n",
    "    for episode in range(num_episodes):\n",
    "        trajectory = collect_trajectory(env, policy)\n",
    "        reward = compute_reward(trajectory, model)\n",
    "        optimizer.zero_grad()\n",
    "        loss = -reward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if episode % 10 == 0:\n",
    "            print(f'Episodio {episode}, Politica perdida: {loss.item()}')\n",
    "\n",
    "# Ejemplo de uso\n",
    "state_dim = 4\n",
    "action_dim = 2\n",
    "\n",
    "# Generar datos de preferencia\n",
    "states, preferences = generate_comparison_data(100, state_dim)\n",
    "\n",
    "# Entrenar el modelo de preferencia\n",
    "preference_model = BradleyTerryModel(state_dim)\n",
    "optimizer = optim.Adam(preference_model.parameters(), lr=0.01)\n",
    "train_preference_model(preference_model, optimizer, states, preferences)\n",
    "\n",
    "# Entrenar la política usando la recompensa implícita aprendida\n",
    "env = gym.make('CartPole-v1')\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "optimizer_policy = optim.Adam(policy.parameters(), lr=0.01)\n",
    "train_policy(env, preference_model, policy, optimizer_policy)\n",
    "\n",
    "# Evaluar la política entrenada\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(policy, state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # env.render()  # Comentado para evitar el uso de pyglet si no está instalado\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561207c-e7f2-450c-ab25-86458de74197",
   "metadata": {},
   "source": [
    "Hay varios algoritmos más de PBRL que utilizan técnicas como el modelo de Bradley-Terry para manejar preferencias en lugar de recompensas numéricas tradicionales. \n",
    "\n",
    "**Learning from ranked trajectories (RaPP)**\n",
    "\n",
    "El algoritmo RaPP se basa en la idea de aprender una política óptima a partir de trayectorias que están ordenadas según las preferencias. En este enfoque, el agente aprende una función de valor que respeta el orden de las trayectorias proporcionadas.\n",
    "\n",
    "Proceso de RaPP\n",
    "\n",
    "- Recopilación de datos: Obtener conjuntos de trayectorias ordenadas por preferencias.\n",
    "- Modelado de la función de valor: Utilizar un modelo para estimar los valores de las trayectorias.\n",
    "- Optimización: Ajustar la política para maximizar la preferencia de las trayectorias ordenadas.\n",
    "\n",
    "**Coactive learning**\n",
    "\n",
    "Coactive learning es un enfoque donde un agente interactúa con un experto y recibe feedback en forma de preferencias sobre acciones. Este feedback se utiliza para ajustar la política del agente.\n",
    "\n",
    "Proceso de coactive learning\n",
    "\n",
    "- Interacción: El agente toma una acción y recibe feedback en forma de preferencia.\n",
    "- Actualización: La política se actualiza para preferir las acciones que el experto prefiere.\n",
    "- Repetición: Este proceso se repite para refinar la política del agente.\n",
    "\n",
    "**Active preference-based learning (APBL)**\n",
    "\n",
    "APBL es un enfoque activo donde el agente selecciona pares de trayectorias para comparación con el fin de aprender de manera más eficiente. Este enfoque utiliza técnicas de selección activa para mejorar el aprendizaje.\n",
    "\n",
    "Proceso de APBL\n",
    "\n",
    "- Selección activa: El agente selecciona pares de trayectorias que son más informativas.\n",
    "- Feedback de preferencias: El agente recibe feedback en forma de preferencias.\n",
    "- Actualización de la política: La política se ajusta para reflejar el feedback recibido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33474ba7-a4a0-419c-a837-9f08427c8189",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 . Implementa un algoritmo de iteración de políticas basado en preferencias (PBPI) en un entorno de OpenAI Gym.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Utiliza el entorno CartPole-v1.\n",
    "- Implementa un modelo de preferencias para aprender la función de recompensa.\n",
    "- Implementa la iteración de políticas basada en preferencias para mejorar la política del agente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916134da-b44c-4c16-b1a9-7733d8da9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615a080-8b77-4844-8eb0-525f770fd22c",
   "metadata": {},
   "source": [
    "2 .  Implementa un algoritmo de aprendizaje por preferencias Bayesiano en un entorno de OpenAI Gym.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Utiliza el entorno MountainCar-v0.\n",
    "- Implementa un modelo Bayesiano para aprender la función de recompensa a partir de preferencias.\n",
    "- Utiliza inferencia Bayesiana para actualizar la política del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fcb70-5912-4a7f-97e3-e43899549de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86e4c1-4401-4307-bc70-9c735900ca61",
   "metadata": {},
   "source": [
    "3 . Implementa un algoritmo de aprendizaje a partir de trayectorias ordenadas en un entorno de OpenAI Gym.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Utiliza el entorno LunarLander-v2.\n",
    "- Recopila un conjunto de trayectorias ordenadas por un experto.\n",
    "- Implementa un modelo para aprender la función de recompensa a partir de estas trayectorias ordenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa33d0-4d5d-45c1-a885-e7e24eeb77e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004fd38-3475-4cc1-ab0b-7e86b9f88045",
   "metadata": {},
   "source": [
    "4 . Implementar un algoritmo de coactive learning en un entorno de OpenAI Gym.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Utiliza el entorno Acrobot-v1.\n",
    "- Implementa un modelo para aprender a partir de interacciones iterativas con el experto.\n",
    "- Ajusta la política del agente en función de las sugerencias del experto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a0a3f-ab2d-4c35-9d82-39e286327854",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959b728-bf74-4495-9350-ea550018dc11",
   "metadata": {},
   "source": [
    "5 . Implementa un algoritmo de aprendizaje activo basado en preferencias en un entorno de OpenAI Gym.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Utiliza el entorno Pendulum-v0.\n",
    "- Implementa un modelo para aprender la función de recompensa a partir de consultas activas de preferencias.\n",
    "- Mejora la política del agente utilizando estas preferencias activas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf31e7d-8ba1-4b91-8c20-03bdbbd5c2a9",
   "metadata": {},
   "source": [
    "Referencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214aca0-77b5-4f13-b10e-acf341080900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class PreferenceModel(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(PreferenceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        score = self.fc2(x)\n",
    "        return score\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        action_probs = torch.softmax(self.fc3(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "def select_action(policy, state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action_probs = policy(state_tensor)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action\n",
    "\n",
    "def collect_trajectory(env, policy, max_steps=200):\n",
    "    state = env.reset()\n",
    "    trajectory = []\n",
    "    for _ in range(max_steps):\n",
    "        action = select_action(policy, state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        trajectory.append((state, action, reward))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return trajectory\n",
    "\n",
    "def train_preference_model(model, optimizer, trajectories, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = 0\n",
    "        for traj in trajectories:\n",
    "            reward = compute_reward(traj, model)\n",
    "            loss = -reward\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoca {epoch}, Perdida: {total_loss / len(trajectories)}')\n",
    "\n",
    "def train_policy(env, model, policy, optimizer, num_episodes=100):\n",
    "    for episode in range(num_episodes):\n",
    "        trajectory = collect_trajectory(env, policy)\n",
    "        reward = compute_reward(trajectory, model)\n",
    "        optimizer.zero_grad()\n",
    "        loss = -reward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if episode % 10 == 0:\n",
    "            print(f'Episodio {episode}, Politica perdida: {loss.item()}')\n",
    "\n",
    "# Puedes utilizar esta estructura base para implementar los algoritmos específicos mencionados en los ejercicios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5267b35-bacb-4ac2-9fca-d5264b1a5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
