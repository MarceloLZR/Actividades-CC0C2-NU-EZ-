{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1700247a",
   "metadata": {},
   "source": [
    "## Fundamentos del aprendizaje por refuerzo\n",
    "\n",
    "El aprendizaje por refuerzo (RL, por sus siglas en inglés) es un paradigma del aprendizaje automático donde un agente aprende a tomar decisiones mediante la interacción con su entorno, recibiendo recompensas o penalizaciones según sus acciones. A diferencia del aprendizaje supervisado, donde el aprendizaje se basa en un conjunto de datos etiquetados, RL se basa en la retroalimentación recibida a partir de la experiencia directa. \n",
    "\n",
    "#### Elementos básicos del aprendizaje por refuerzo\n",
    "\n",
    "En RL, los principales elementos son el agente, el entorno, los estados, las acciones y las recompensas. Estos elementos se relacionan de la siguiente manera:\n",
    "\n",
    "1. **Agente**: Es el tomador de decisiones que interactúa con el entorno. El agente recibe información del entorno, toma decisiones (acciones) y aprende de las consecuencias de esas decisiones.\n",
    "\n",
    "2. **Entorno**: Es el mundo con el que interactúa el agente. El entorno responde a las acciones del agente y proporciona nuevas observaciones y recompensas.\n",
    "\n",
    "3. **Estados (s)**: Representan la situación actual del entorno. Un estado contiene toda la información necesaria para describir la situación en un momento dado.\n",
    "\n",
    "4. **Acciones (a)**: Son las decisiones o movimientos que el agente puede realizar en un estado dado. El conjunto de todas las posibles acciones se denota como A.\n",
    "\n",
    "5. **Recompensas (r)**: Son señales de retroalimentación que indican el valor de una acción en un estado específico. El objetivo del agente es maximizar la recompensa total a lo largo del tiempo.\n",
    "\n",
    "La interacción entre estos elementos puede describirse como una serie de pasos, donde el agente percibe un estado del entorno, selecciona una acción, recibe una recompensa y transita a un nuevo estado.\n",
    "\n",
    "#### Procesos de decisión de Markov (MDPs)\n",
    "\n",
    "Los procesos de decisión de Markov (MDPs) son una herramienta matemática utilizada para modelar problemas de RL. Un MDP se define por los siguientes componentes:\n",
    "\n",
    "1. **Conjunto de estados (S)**: Todos los posibles estados en los que el agente puede encontrarse.\n",
    "2. **Conjunto de acciones (A)**: Todas las posibles acciones que el agente puede tomar.\n",
    "3. **Función de transición (P)**: Describe la probabilidad de transición de un estado a otro, dado una acción. $P(s'|s,a)$ representa la probabilidad de moverse al estado $s'$ desde el estado $s$ tomando la acción $a$.\n",
    "4. **Función de recompensa (R)**: Define la recompensa esperada al realizar una acción en un estado particular. $R(s,a)$ es la recompensa inmediata recibida al realizar la acción $a$ en el estado $s$.\n",
    "5. **Factor de descuento (γ)**: Es un valor entre 0 y 1 que determina la importancia de las recompensas futuras. Un factor de descuento cercano a 0 hace que el agente se enfoque en recompensas inmediatas, mientras que un valor cercano a 1 da más peso a las recompensas futuras.\n",
    "\n",
    "Los MDPs permiten formalizar el problema de RL y proporcionar una base para diseñar y analizar algoritmos.\n",
    "\n",
    "#### Políticas y funciones de valor\n",
    "\n",
    "En RL, una política ($\\pi$) es una estrategia que el agente sigue para decidir qué acciones tomar en cada estado. Una política puede ser determinista ($\\pi(s) = a$) o estocástica ($\\pi(a|s)$), donde la acción es elegida con una cierta probabilidad.\n",
    "\n",
    "Las funciones de valor son herramientas clave para evaluar la calidad de los estados y las acciones bajo una política determinada. Hay dos tipos principales de funciones de valor:\n",
    "\n",
    "1. **Función de valor del estado (V)**: $V^\\pi(s)$ es el valor esperado de las recompensas futuras comenzando desde el estado $s$ y siguiendo la política $\\pi$. Se define como:\n",
    "   $$\n",
    "   V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_t = s \\right]\n",
    "   $$\n",
    "\n",
    "2. **Función de valor de la acción (Q)**: $Q^\\pi(s,a)$ es el valor esperado de las recompensas futuras al tomar la acción $a$ en el estado $s$ y luego seguir la política $\\pi$. Se define como:\n",
    "   $$\n",
    "   Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_t = s, a_t = a \\right]\n",
    "   $$\n",
    "\n",
    "El objetivo del agente es encontrar la política óptima ($\\pi^*$) que maximice estas funciones de valor.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d10036",
   "metadata": {},
   "source": [
    "#### Métodos basados en el valor\n",
    "\n",
    "Los métodos basados en el valor se centran en aprender una función de valor que estima la calidad de los estados y acciones. Los dos algoritmos más representativos en esta categoría son Q-Learning y SARSA.\n",
    "\n",
    "**Q-Learning**:\n",
    "Q-Learning es un algoritmo off-policy que busca aprender la función de valor de acción $Q(s, a)$. La actualización de Q-Learning se basa en la ecuación de Bellman:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "Aquí, $s$ es el estado actual, $a$ es la acción tomada, $r$ es la recompensa recibida, $s'$ es el nuevo estado, $\\alpha$ es la tasa de aprendizaje, y $\\gamma$ es el factor de descuento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58faf9b",
   "metadata": {},
   "source": [
    "A continuación, presentamos un ejemplo de código en PyTorch para implementar Q-Learning, uno de los algoritmos de RL más conocidos, que aprende la función de valor de acción (Q)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bf4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Definir la red neuronal para aproximar la función Q\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Función para elegir una acción basada en epsilon-greedy\n",
    "def select_action(state, epsilon, q_network):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(action_dim)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            q_values = q_network(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "# Parámetros del entorno y de aprendizaje\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "episodes = 500\n",
    "\n",
    "q_network = QNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Entrenamiento del agente\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(state, epsilon, q_network)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Actualización de la red Q\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action_tensor = torch.tensor([action], dtype=torch.int64)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "        done_tensor = torch.tensor([done], dtype=torch.float32)\n",
    "        \n",
    "        q_values = q_network(state_tensor)\n",
    "        next_q_values = q_network(next_state_tensor)\n",
    "        \n",
    "        q_value = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = torch.max(next_q_values, 1)[0]\n",
    "        expected_q_value = reward_tensor + gamma * next_q_value * (1 - done_tensor)\n",
    "        \n",
    "        loss = criterion(q_value, expected_q_value.detach())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    print(f'Episodio {episode + 1}, Recompensa total: {total_reward}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09d66d",
   "metadata": {},
   "source": [
    "**SARSA**:\n",
    "    \n",
    "SARSA (State-Action-Reward-State-Action) es un algoritmo on-policy que actualiza la función de valor de acción utilizando la acción actual y la siguiente acción seleccionada por la política. La actualización de SARSA se basa en la ecuación:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma Q(s', a') - Q(s, a)]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e520365",
   "metadata": {},
   "source": [
    "#### Métodos basados en la política\n",
    "\n",
    "Los métodos basados en la política se enfocan en aprender directamente una política que mapea estados a acciones, sin necesidad de una función de valor intermedia. Los dos algoritmos más comunes en esta categoría son REINFORCE y Actor-Critic.\n",
    "\n",
    "**REINFORCE**:\n",
    "REINFORCE es un algoritmo de gradiente de política que ajusta los parámetros de la política para maximizar la recompensa esperada. La actualización de la política se basa en la ecuación:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a|s) G_t$$\n",
    "\n",
    "Donde $G_t$ es la recompensa acumulada desde el tiempo $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd105bf2",
   "metadata": {},
   "source": [
    "Presentamos una implementación de REINFORCE elemental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.softmax(self.fc3(x), dim=-1)\n",
    "\n",
    "def select_action(state, policy_network):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action_probs = policy_network(state)\n",
    "    action = torch.multinomial(action_probs, 1).item()\n",
    "    return action, action_probs[0, action]\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "episodes = 500\n",
    "\n",
    "policy_network = PolicyNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=lr)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, log_prob = select_action(state, policy_network)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    discounted_rewards = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        discounted_rewards.insert(0, R)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    \n",
    "    policy_loss = 0\n",
    "    for log_prob, reward in zip(log_probs, discounted_rewards):\n",
    "        policy_loss += -log_prob * reward\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Episodio {episode + 1}, Recompensa total: {total_reward}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f819b60",
   "metadata": {},
   "source": [
    "**Actor-Critic**:\n",
    "El método Actor-Critic combina una red de política (actor) y una red de valor (critic). El actor actualiza la política basándose en la ventaja estimada por el crítico. La actualización de la política sigue:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a|s) A(s, a)$$\n",
    "\n",
    "Y la actualización de la función de valor sigue:\n",
    "\n",
    "$$\\phi \\leftarrow \\phi + \\beta \\nabla_\\phi (r + \\gamma V_\\phi(s') - V_\\phi(s))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119edd3",
   "metadata": {},
   "source": [
    "#### Métodos híbridos\n",
    "\n",
    "Los métodos híbridos combinan enfoques basados en el valor y en la política para aprovechar las fortalezas de ambos. Entre los algoritmos híbridos más avanzados se encuentran DDPG, PPO y A3C.\n",
    "\n",
    "**DDPG (Deep Deterministic Policy Gradient)**:\n",
    "DDPG es un algoritmo off-policy que combina DQN y el actor-critic. Utiliza una red de actor para seleccionar acciones y una red crítico para evaluar la calidad de esas acciones. También emplea una red de destino para estabilizar el entrenamiento.\n",
    "\n",
    "**PPO (Proximal Policy Optimization)**:\n",
    "PPO es un algoritmo de política basada en la proximidad que busca mejorar la estabilidad del entrenamiento limitando el tamaño del paso de actualización. La función objetivo de PPO es:\n",
    "\n",
    "$$ L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} A_t, \\text{clip}\\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_t \\right) \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8be27",
   "metadata": {},
   "source": [
    "Realizamos una implementación de POO elemental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.actor = nn.Linear(128, action_dim)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        policy_dist = torch.softmax(self.actor(x), dim=-1)\n",
    "        value = self.critic(x)\n",
    "        return policy_dist, value\n",
    "\n",
    "def select_action(state, model):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    policy_dist, _ = model(state)\n",
    "    action = torch.multinomial(policy_dist, 1).item()\n",
    "    return action, torch.log(policy_dist[0, action])\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "episodes = 500\n",
    "\n",
    "model = ActorCritic(state_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    masks = []\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, log_prob = select_action(state, model)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        total_reward += reward\n",
    "        \n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        _, value = model(state_tensor)\n",
    "        values.append(value)\n",
    "        \n",
    "        masks.append(1 - done)\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    log_probs = torch.stack(log_probs)\n",
    "    values = torch.stack(values).squeeze(1)\n",
    "    masks = torch.tensor(masks, dtype=torch.float32)\n",
    "    \n",
    "    advantages = returns - values\n",
    "    critic_loss = advantages.pow(2).mean()\n",
    "    \n",
    "    ratio = torch.exp(log_probs - log_probs.detach())\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon) * advantages\n",
    "    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    loss = actor_loss + critic_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Episodio {episode + 1}, Recompensa total: {total_reward}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad29401",
   "metadata": {},
   "source": [
    "**A3C (Asynchronous Advantage Actor-Critic)**:\n",
    "\n",
    "A3C es un algoritmo de aprendizaje asincrónico donde múltiples agentes independientes aprenden simultáneamente y actualizan una política global. Utiliza la misma estructura que el actor-critic, pero distribuye el entrenamiento en múltiples instancias del entorno.\n",
    "\n",
    "En resumen, los métodos de aprendizaje por refuerzo abarcan una variedad de enfoques, cada uno con sus propias fortalezas y desafíos. Desde los métodos basados en el valor como Q-Learning y SARSA, hasta los métodos basados en la política como REINFORCE y Actor-Critic, y finalmente los métodos híbridos como DDPG, PPO y A3C, todos juegan un papel crucial en el desarrollo de agentes inteligentes capaces de tomar decisiones en entornos complejos y dinámicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f36640",
   "metadata": {},
   "source": [
    "### Métodos de Monte Carlo: simulación y estimación de valores*\n",
    "\n",
    "Los métodos de Monte Carlo son una clase de algoritmos computacionales que dependen de muestreos aleatorios repetidos para obtener resultados numéricos. Se utilizan principalmente en optimización, integración numérica y generación de muestras de una distribución de probabilidad. En el contexto del aprendizaje por refuerzo, los métodos de Monte Carlo se usan para estimar el valor esperado de una política dada, promediando las recompensas observadas a lo largo de múltiples episodios.\n",
    "\n",
    "**Simulación y estimación de valores con Monte Carlo**\n",
    "\n",
    "En el aprendizaje por refuerzo, la simulación mediante métodos de Monte Carlo implica ejecutar varios episodios de interacción entre el agente y el entorno para recopilar datos sobre las recompensas obtenidas. A partir de estas simulaciones, se pueden calcular las estimaciones de valor. Estas estimaciones se basan en la premisa de que, a largo plazo, las recompensas acumuladas reflejan el valor esperado de un estado o una acción.\n",
    "\n",
    "Para estimar los valores de una política, se sigue un procedimiento que incluye:\n",
    "\n",
    "1. **Generación de episodios:** Se generan múltiples episodios siguiendo la política actual, donde un episodio es una secuencia de estados, acciones y recompensas que termina en un estado terminal.\n",
    "2. **Cálculo de retornos:** Para cada estado visitado en un episodio, se calcula el retorno, que es la suma de recompensas futuras descontadas.\n",
    "3. **Promedio de retornos:** Se promedian los retornos de todos los episodios en los que se visitó un estado específico para obtener una estimación de su valor.\n",
    "\n",
    "Una característica fundamental de los métodos de Monte Carlo es que requieren episodios completos, lo que significa que sólo se actualizan los valores al final de un episodio. Esto puede ser una limitación en entornos donde los episodios son largos o no terminan.\n",
    "\n",
    "**Métodos de diferencias temporales (TD): TD(λ) y n-step TD**\n",
    "\n",
    "Los métodos de diferencias temporales combinan las ventajas del aprendizaje Monte Carlo y el aprendizaje dinámico, actualizando las estimaciones de valores en función de las diferencias temporales, es decir, la diferencia entre las estimaciones de valor consecutivas.\n",
    "\n",
    "**TD(λ)**\n",
    "\n",
    "TD(λ) es una técnica que unifica los métodos TD y Monte Carlo mediante el uso de trazas de elegibilidad, que son variables que asignan crédito a los estados y acciones visitados recientemente. λ es un parámetro que controla la ponderación de las actualizaciones.\n",
    "\n",
    "1. **Trazas de elegibilidad:** Se utilizan para dar crédito a los estados visitados recientemente. A medida que el agente se mueve a través del entorno, las trazas de elegibilidad se actualizan, decayendo con cada paso.\n",
    "2. **Actualización de valores:** Las actualizaciones de valores se realizan no solo en función del estado actual y el siguiente estado, sino también en función de los estados anteriores, ponderados por sus trazas de elegibilidad.\n",
    "\n",
    "La fórmula general de actualización para TD(λ) es:\n",
    "\n",
    "$$V(s) \\leftarrow V(s) + \\alpha \\delta_t e_t(s)$$\n",
    "donde:\n",
    "- $\\alpha$ es la tasa de aprendizaje,\n",
    "- $\\delta_t$ es el error de TD,\n",
    "- $e_t(s)$ es la traza de elegibilidad.\n",
    "\n",
    "**n-step TD**\n",
    "\n",
    "El método n-step TD extiende la idea básica de TD al utilizar recompensas de los siguientes n pasos en lugar de solo la recompensa inmediata y el valor del siguiente estado. Este método actualiza los valores basándose en n pasos futuros de interacción con el entorno.\n",
    "\n",
    "1. **Recompensas acumuladas:** Para cada estado, se acumulan las recompensas de los próximos n pasos.\n",
    "2. **Actualización de valores:** Los valores se actualizan utilizando esta suma de recompensas, proporcionando un equilibrio entre la actualización a corto plazo (TD(0)) y la actualización a largo plazo (Monte Carlo).\n",
    "\n",
    "**Exploración vs. explotación: estrategias epsilon-greedy, Upper Confidence Bound (UCB)**\n",
    "\n",
    "En el aprendizaje por refuerzo, un desafío clave es balancear la exploración de nuevas acciones con la explotación de las acciones conocidas que proporcionan las mayores recompensas. Este dilema se conoce como el trade-off exploración-explotación.\n",
    "\n",
    "**Estrategia epsilon-greedy**\n",
    "\n",
    "La estrategia epsilon-greedy es una de las técnicas más simples y efectivas para gestionar este trade-off. En esta estrategia:\n",
    "\n",
    "1. **Exploración:** Con una probabilidad \\(\\epsilon\\), el agente selecciona una acción al azar, lo que permite explorar nuevas acciones.\n",
    "2. **Explotación:** Con una probabilidad \\(1 - \\epsilon\\), el agente selecciona la acción con el mayor valor esperado (explota el conocimiento actual).\n",
    "\n",
    "El parámetro \\(\\epsilon\\) se puede ajustar durante el entrenamiento, a menudo comenzando con un valor alto para fomentar la exploración y disminuyéndolo gradualmente para favorecer la explotación a medida que el agente aprende más sobre el entorno.\n",
    "\n",
    "**Upper Confidence Bound (UCB)**\n",
    "\n",
    "El método Upper Confidence Bound es otra estrategia para balancear exploración y explotación, utilizando una forma más teórica y matemática basada en la teoría de la toma de decisiones en condiciones de incertidumbre.\n",
    "\n",
    "1. **Valor de confianza:** UCB asigna a cada acción un valor de confianza que aumenta con la incertidumbre de la estimación del valor de esa acción.\n",
    "2. **Selección de acciones:** El agente selecciona la acción con el valor de confianza más alto, lo que favorece las acciones con altos valores esperados y aquellas que han sido menos exploradas.\n",
    "\n",
    "La fórmula general para el valor de confianza en UCB es:\n",
    "$$Q(a) + c \\sqrt{\\frac{\\ln(t)}{N(a)}}$$\n",
    "donde:\n",
    "- $Q(a)$ es el valor estimado de la acción $a$,\n",
    "- $c$ es un parámetro que controla el grado de exploración,\n",
    "- $t$ es el número total de selecciones de acciones,\n",
    "- $N(a)$ es el número de veces que la acción $a$ ha sido seleccionada.\n",
    "\n",
    "UCB proporciona un marco matemáticamente riguroso para el trade-off exploración-explotación, asegurando que cada acción sea seleccionada un número suficiente de veces para obtener estimaciones precisas de su valor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b48a17",
   "metadata": {},
   "source": [
    "Presentamos implementaciones de las estrategia Upper Confidence Bound (UCB) en PyTorch para el problema del bandido multi-brazo. La estrategia UCB es útil en situaciones donde se quiere balancear la exploración y la explotación sin necesidad de un parámetro de exploración explícito como epsilon.\n",
    "\n",
    "**Implementación de UCB1**\n",
    "\n",
    "El algoritmo UCB1 selecciona la acción que maximiza el valor esperado más un término de exploración que disminuye a medida que se incrementa el número de veces que se ha seleccionado esa acción.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def select_action(state, q_network, counts, total_counts, epsilon=1e-5):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_values = q_network(state).numpy().flatten()\n",
    "    \n",
    "    adjusted_counts = np.maximum(counts, epsilon)\n",
    "    adjusted_total_counts = max(total_counts, epsilon)\n",
    "    ucb_values = q_values + np.sqrt((2 * np.log(adjusted_total_counts)) / adjusted_counts)\n",
    "    action = np.argmax(ucb_values)\n",
    "    \n",
    "    return action\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "episodes = 500\n",
    "\n",
    "q_network = QNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "counts = np.zeros(action_dim)\n",
    "total_counts = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(state, q_network, counts, total_counts)\n",
    "        counts[action] += 1\n",
    "        total_counts += 1\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        next_q_values = q_network(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0))\n",
    "        max_next_q_value = next_q_values.max().item()\n",
    "        target = reward + (gamma * max_next_q_value * (1 - done))\n",
    "        \n",
    "        q_values = q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "        target_f = q_values.clone().detach()\n",
    "        target_f[0][action] = target\n",
    "\n",
    "        loss = criterion(q_values, target_f)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f'Episodio {episode + 1}, Recompensa total: {total_reward}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ec7ac",
   "metadata": {},
   "source": [
    "UCB-V es una variante del algoritmo UCB que considera la varianza en la estimación de las recompensas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def select_action(state, q_network, counts, total_counts, q_variances, epsilon=1e-5):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_values = q_network(state).numpy().flatten()\n",
    "    \n",
    "    adjusted_counts = counts + epsilon\n",
    "    ucb_values = q_values + np.sqrt((2 * np.log(total_counts + epsilon) * q_variances) / adjusted_counts)\n",
    "    action = np.argmax(ucb_values)\n",
    "    \n",
    "    return action\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "episodes = 500\n",
    "\n",
    "q_network = QNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "counts = np.zeros(action_dim)\n",
    "q_variances = np.ones(action_dim)  # Initial variances can be set to 1\n",
    "total_counts = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    while not done:\n",
    "        action = select_action(state, q_network, counts, total_counts, q_variances)\n",
    "        counts[action] += 1\n",
    "        total_counts += 1\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        next_q_values = q_network(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0))\n",
    "        max_next_q_value = next_q_values.max().item()\n",
    "        target = reward + (gamma * max_next_q_value * (1 - done))\n",
    "        \n",
    "        q_values = q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "        target_f = q_values.clone().detach()\n",
    "        target_f[0][action] = target\n",
    "\n",
    "        loss = criterion(q_values, target_f)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Update Q variances after each episode\n",
    "    episode_mean_reward = np.mean(episode_rewards)\n",
    "    for action in range(action_dim):\n",
    "        action_rewards = [r for i, r in enumerate(episode_rewards) if i % action_dim == action]\n",
    "        if len(action_rewards) > 1:\n",
    "            q_variances[action] = np.var(action_rewards)\n",
    "    \n",
    "    print(f'Episodio {episode + 1}, Recompensa total: {total_reward}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c766cb",
   "metadata": {},
   "source": [
    "### Deep Q-Networks (DQN)\n",
    "\n",
    "Deep Q-Networks (DQN) representan una evolución significativa en el campo del Aprendizaje por Refuerzo (RL), combinando técnicas tradicionales de RL con redes neuronales profundas. Este enfoque fue popularizado por la investigación de DeepMind, donde se demostró que una red neuronal podía aprender a jugar videojuegos de Atari a nivel humano.\n",
    "\n",
    "En los métodos tradicionales de RL, la función Q se representa mediante tablas (tabular methods) que mapean cada par estado-acción a un valor Q. Sin embargo, este enfoque no es escalable a entornos con grandes espacios de estados y acciones. Aquí es donde entran las redes neuronales: en lugar de almacenar valores Q explícitamente, una red neuronal se entrena para aproximar la función Q.\n",
    "\n",
    "La arquitectura de un DQN es bastante simple: se utiliza una red neuronal con varias capas ocultas que toma el estado del entorno como entrada y produce un valor Q para cada posible acción. La actualización de los pesos de la red neuronal se realiza mediante un proceso de backpropagation, donde el objetivo es minimizar el error entre el valor Q estimado y el valor Q objetivo, el cual se define mediante la ecuación de Bellman.\n",
    "\n",
    "Veamos una simple implementación en Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e776797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "episodes = 500\n",
    "epsilon = 0.1\n",
    "\n",
    "dqn = DQN(state_dim, action_dim)\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            q_values = dqn(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        target = reward + gamma * torch.max(dqn(next_state_tensor)).item() * (1 - done)\n",
    "        \n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = dqn(state_tensor)\n",
    "        target_f = q_values.clone().detach()\n",
    "        target_f[0][action] = target\n",
    "\n",
    "        loss = criterion(q_values, target_f)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f'Episodio {episode + 1}, Recompensa total: {total_reward}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb5c39",
   "metadata": {},
   "source": [
    "### Redes Actor-Critic\n",
    "El enfoque Actor-Critic combina las ventajas de los métodos basados en políticas y los basados en valor. En lugar de tener una única red que aprenda la política o el valor Q, se utilizan dos redes: una para la política (actor) y otra para el valor (critic).\n",
    "\n",
    "La red Actor se encarga de seleccionar acciones según una política aprendida, mientras que la red Critic evalúa estas acciones proporcionando una estimación del valor Q. \n",
    "\n",
    "El Actor mejora su política utilizando las críticas del Critic, haciendo que este enfoque sea más estable y eficiente en comparación con los métodos puramente basados en políticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "from torch.utils.data import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.actor = nn.Linear(128, action_dim)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        action_probs = torch.softmax(self.actor(x), dim=-1)\n",
    "        state_values = self.critic(x)\n",
    "        return action_probs, state_values\n",
    "\n",
    "def compute_gae(rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "lr = 0.0003\n",
    "gamma = 0.99\n",
    "tau = 0.95\n",
    "episodes = 500\n",
    "clip_param = 0.2\n",
    "ppo_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "model = ActorCriticNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    states = []\n",
    "    actions = []\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        dist, value = model(state_tensor)\n",
    "        action = torch.multinomial(dist, 1).item()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        log_prob = torch.log(dist.squeeze(0)[action])\n",
    "        entropy += -torch.sum(dist * torch.log(dist), dim=-1).mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        masks.append(1 - done)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "\n",
    "    _, next_value = model(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0))\n",
    "    values.append(next_value)\n",
    "    returns = compute_gae(rewards, masks, values, gamma, tau)\n",
    "\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    returns = torch.stack(returns).detach()\n",
    "    values = torch.stack(values)[:-1].detach()\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64)\n",
    "\n",
    "    advantages = returns - values\n",
    "\n",
    "    for _ in range(ppo_epochs):\n",
    "        sampler = BatchSampler(SubsetRandomSampler(range(len(rewards))), batch_size, False)\n",
    "        for indices in sampler:\n",
    "            sampled_states = states[indices]\n",
    "            sampled_actions = actions[indices]\n",
    "            sampled_returns = returns[indices]\n",
    "            sampled_advantages = advantages[indices]\n",
    "            sampled_log_probs = log_probs[indices]\n",
    "\n",
    "            dist, value = model(sampled_states)\n",
    "            new_log_probs = torch.log(dist.gather(1, sampled_actions.unsqueeze(-1)).squeeze(-1))\n",
    "            ratio = (new_log_probs - sampled_log_probs).exp()\n",
    "            surr1 = ratio * sampled_advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * sampled_advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (sampled_returns - value).pow(2).mean()\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Retain graph on the last PPO epoch\n",
    "            retain_graph = _ < ppo_epochs - 1\n",
    "            loss.backward(retain_graph=retain_graph)\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f'Episodio {episode + 1}, Recompensa total: {total_reward}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fd17c",
   "metadata": {},
   "source": [
    "### Algoritmos avanzados\n",
    "\n",
    "**Proximal Policy Optimization (PPO)**\n",
    "\n",
    "PPO es uno de los algoritmos más robustos y populares en el aprendizaje por refuerzo. La idea principal detrás de PPO es limitar el cambio en la política en cada actualización para mantener la estabilidad y eficiencia del entrenamiento. PPO emplea una técnica llamada \"clipping\" para evitar cambios demasiado grandes en la política."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299beda7",
   "metadata": {},
   "source": [
    "#### Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "A3C es otro algoritmo avanzado que utiliza múltiples agentes (hilos) que interactúan con el entorno de forma paralela. Cada agente tiene su propia copia de la red neuronal y actualiza los parámetros de la red global de forma asíncrona. Este enfoque mejora la eficiencia del entrenamiento y la estabilidad del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac7d94",
   "metadata": {},
   "source": [
    "Estos enfoques avanzados han demostrado ser altamente efectivos para resolver problemas complejos en el campo del aprendizaje por refuerzo, proporcionando un marco robusto y eficiente para la toma de decisiones autónoma en una variedad de entornos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622e9ee9",
   "metadata": {},
   "source": [
    "### Integración de aprendizaje por refuerzo y transformers\n",
    "La integración de los Transformers, un tipo de modelo de aprendizaje profundo que ha revolucionado el procesamiento del lenguaje natural (NLP), con el aprendizaje por refuerzo (RL) ofrece nuevas oportunidades para mejorar el rendimiento y la eficiencia de los agentes inteligentes. Los Transformers, originalmente diseñados para tareas de NLP, han demostrado ser extremadamente poderosos en capturar dependencias a largo plazo y modelar secuencias complejas. Estos atributos los hacen adecuados para diversas aplicaciones de RL, donde la secuenciación y el modelado de decisiones a largo plazo son cruciales.\n",
    "\n",
    "**Transformers para RL**\n",
    "\n",
    "El uso de transformers en RL se puede dividir en dos áreas principales: modelar políticas y modelar funciones de valor. En ambas áreas, los Transformers pueden aprovechar su capacidad para manejar secuencias y capturar relaciones a largo plazo para mejorar el rendimiento de los agentes de RL.\n",
    "\n",
    "\n",
    "En el contexto del RL, una política define el comportamiento de un agente, mapeando estados del entorno a acciones. Tradicionalmente, las políticas se han modelado usando redes neuronales recurrentes (RNNs) o redes neuronales convolucionales (CNNs). Sin embargo, los Transformers pueden ofrecer ventajas significativas debido a su capacidad para procesar secuencias completas y capturar dependencias complejas entre las observaciones del entorno.\n",
    "\n",
    "Un transformer puede ser entrenado para modelar una política de RL mediante el aprendizaje supervisado, donde se le proporciona una secuencia de estados y acciones óptimas. El Transformer aprende a predecir la siguiente acción dada la secuencia actual de estados. Una vez entrenado, el Transformer puede ser usado para inferir la acción óptima en cada paso del tiempo durante la interacción del agente con el entorno.\n",
    "\n",
    "**Uso de transformers para modelar funciones de valor**\n",
    "\n",
    "Las funciones de valor en RL evalúan la calidad de los estados o las acciones, proporcionando una estimación de las recompensas futuras esperadas. Los Transformers pueden ser utilizados para aproximar estas funciones de valor mediante la integración de la información de secuencias completas de estados y recompensas.\n",
    "\n",
    "La arquitectura de un transformer, con su mecanismo de atención, puede capturar las relaciones entre estados a lo largo de una secuencia, permitiendo una estimación más precisa de las recompensas futuras. Esto es particularmente útil en entornos donde las decisiones a largo plazo tienen un impacto significativo en las recompensas.\n",
    "\n",
    "Para ilustrar el uso de transformers en RL, consideremos una implementación simplificada donde un transformer se utiliza para modelar una política en el entorno de CartPole de OpenAI Gym.\n",
    "\n",
    "Primero, definimos el transformer que será utilizado para modelar la política. El transformer recibe una secuencia de estados como entrada y produce una distribución de probabilidad sobre las acciones posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc270796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class TransformerPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, d_model=128, nhead=8, num_layers=2):\n",
    "        super(TransformerPolicy, self).__init__()\n",
    "        self.embedding = nn.Linear(state_dim, d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(d_model, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # Permute to [sequence, batch, features]\n",
    "        transformer_output = self.transformer(x, x)\n",
    "        output = self.fc(transformer_output)\n",
    "        return torch.softmax(output, dim=-1)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "policy_net = TransformerPolicy(state_dim, action_dim)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for episode in range(500):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    states = []\n",
    "    actions = []\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        states.append(state_tensor)\n",
    "        state_sequence = torch.cat(states).unsqueeze(1)  # Add batch dimension\n",
    "        action_probs = policy_net(state_sequence)\n",
    "        action = torch.multinomial(action_probs[-1, -1], 1).item()\n",
    "        actions.append(torch.tensor([action], dtype=torch.int64))\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            state_sequence = torch.cat(states).unsqueeze(1)  # Add batch dimension\n",
    "            action_sequence = torch.cat(actions).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = policy_net(state_sequence)\n",
    "            outputs = outputs.view(-1, action_dim)  # Flatten the outputs for CrossEntropyLoss\n",
    "            loss = criterion(outputs, action_sequence.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    print(f'Episodio {episode + 1}, Recompensa total: {total_reward}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73530b6",
   "metadata": {},
   "source": [
    "Para modelar funciones de valor con transformers, el proceso es similar. Sin embargo, en lugar de predecir acciones, el transformer predice el valor esperado de los estados o las acciones. \n",
    "\n",
    "A continuación se muestra un ejemplo de cómo un transformer puede ser utilizado para aproximar la función de valor en el entorno de CartPole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74390e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class TransformerValue(nn.Module):\n",
    "    def __init__(self, state_dim, d_model=128, nhead=8, num_layers=2):\n",
    "        super(TransformerValue, self).__init__()\n",
    "        self.embedding = nn.Linear(state_dim, d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # Permute to [sequence, batch, features]\n",
    "        transformer_output = self.transformer(x, x)\n",
    "        output = self.fc(transformer_output).squeeze(1)\n",
    "        return output\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "value_net = TransformerValue(state_dim)\n",
    "optimizer = optim.Adam(value_net.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for episode in range(500):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    states = []\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        states.append(state_tensor)\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            state_sequence = torch.cat(states).unsqueeze(1)  # Add batch dimension\n",
    "            state_sequence = state_sequence.permute(1, 0, 2)  # [sequence, batch, features]\n",
    "            returns = [sum(rewards[i:]) for i in range(len(rewards))]\n",
    "            returns = torch.tensor(returns, dtype=torch.float32).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            values = value_net(state_sequence)\n",
    "            loss = criterion(values, returns)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    print(f'Episodio {episode + 1}, Recompensa total: {total_reward}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e85a7c",
   "metadata": {},
   "source": [
    "La integración de transformers, como BERT, GPT-3, GPT-4, y GPT-5, con el aprendizaje por refuerzo (RL) representa un avance significativo en la creación de agentes inteligentes más eficientes y versátiles. Estos modelos, que han demostrado un rendimiento superior en tareas de procesamiento del lenguaje natural (NLP), también tienen el potencial de mejorar los sistemas de RL al aprovechar sus capacidades para modelar secuencias y capturar dependencias a largo plazo.\n",
    "\n",
    "\n",
    "Los transformers se pueden utilizar para modelar políticas en RL, aprovechando su capacidad para procesar secuencias completas de estados y acciones. En lugar de depender únicamente de las observaciones actuales, un Transformer puede tener en cuenta una historia más extensa de estados para decidir la mejor acción. Esto es particularmente útil en entornos donde la información relevante se distribuye a lo largo del tiempo.\n",
    "\n",
    "Por ejemplo, en un entorno de juego, un transformer puede utilizar la historia completa de movimientos para decidir la mejor acción actual. La arquitectura del Transformer permite manejar esta secuencia de forma eficiente, capturando relaciones complejas y dependencias a largo plazo.\n",
    "\n",
    "\n",
    "En general el uso de transformers en RL ofrece varias ventajas potenciales:\n",
    "\n",
    "* Captura de dependencias a largo plazo: Los Transformers son capaces de capturar dependencias a largo plazo en las secuencias de estados y acciones, lo cual es crucial en tareas de RL donde las decisiones a largo plazo impactan significativamente las recompensas.\n",
    "\n",
    "* Escalabilidad: Los Transformers pueden manejar grandes secuencias de datos y múltiples tipos de entradas, lo que los hace adecuados para entornos complejos y de alta dimensionalidad.\n",
    "\n",
    "* Flexibilidad: Los Transformers pueden ser fácilmente adaptados para modelar políticas, funciones de valor y otros componentes críticos en los algoritmos de RL, proporcionando un marco unificado y flexible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40887548",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 . Implementa un entorno simple, como el problema del camino más corto en un laberinto.\n",
    "\n",
    "- Define los elementos básicos: agente, entorno, estados, acciones y recompensas.\n",
    "- Describe cómo el agente interactúa con el entorno y cómo se calculan las recompensas.\n",
    "\n",
    "2 . Procesos de decisión de Markov (MDPs)\n",
    "\n",
    "- Modela un MDP para un problema de control de inventario.\n",
    "- Define los estados, acciones, probabilidades de transición y recompensas.\n",
    "- Escribe una política simple ($\\pi$) que el agente puede seguir.\n",
    "\n",
    "3 . Implementación de Q-Learning\n",
    "\n",
    "- Implementa el algoritmo Q-Learning para resolver el problema del laberinto del Ejercicio 1.\n",
    "- Ajusta los parámetros de aprendizaje y observa cómo cambian las políticas óptimas.\n",
    "\n",
    "4 .Algoritmo REINFORCE\n",
    "\n",
    "- Implementa el algoritmo REINFORCE en un entorno de bandido multi-brazo.\n",
    "- Analiza cómo varía el rendimiento del agente con diferentes tasas de aprendizaje y estrategias de exploración.\n",
    "\n",
    "5 . Actor-Critic\n",
    "\n",
    "- Implementa un algoritmo Actor-Critic para un entorno de control continuo, como el problema del equilibrio de un péndulo.\n",
    "- Compara el rendimiento con otros métodos basados en políticas y basados en valores.\n",
    "\n",
    "6 . Monte Carlo y diferencias temporales (TD)\n",
    "\n",
    "- Implementa un método de Monte Carlo para estimar los valores de estado en un problema de caminata aleatoria.\n",
    "- Implementa el método TD(λ) y compara los resultados con los obtenidos por Monte Carlo.\n",
    "\n",
    "7 . Exploración vs. Explotación\n",
    "\n",
    "- Implementa y compara las estrategias epsilon-greedy y Upper Confidence Bound (UCB) en un problema de bandido multi-brazo.\n",
    "- Analiza cómo cada estrategia afecta la exploración y explotación.\n",
    "\n",
    "8. Deep Q-Networks (DQN)\n",
    "\n",
    "- Implementa un DQN para resolver el problema del Cart-Pole.\n",
    "- Analiza cómo el uso de redes neuronales mejora la aproximación de la función Q comparado con métodos tabulares.\n",
    "\n",
    "9 . Redes Actor-Critic\n",
    "\n",
    "- Implementa una red Actor-Critic usando una arquitectura de red neuronal profunda.\n",
    "- Aplica este modelo a un entorno más complejo, como el entorno de Ataris en OpenAI Gym.\n",
    "\n",
    "10 . PPO y A3C\n",
    "\n",
    "- Implementa los algoritmos PPO y A3C para un entorno de control continuo.\n",
    "- Compara el rendimiento y la estabilidad de ambos algoritmos en el entorno seleccionado.\n",
    "\n",
    "11 .Transformers para RL\n",
    "\n",
    "- Investiga y resume cómo los Transformers se pueden utilizar para modelar políticas y funciones de valor en entornos de RL.\n",
    "- Implementa una política basada en Transformers para un problema de generación de lenguaje natural (NLG), como la creación de diálogos en un chatbot.\n",
    "12. Transformers en modelos de grandes lenguajes (LLMs)\n",
    "\n",
    "- Utiliza un modelo de Transformer preentrenado (como GPT-3) y adapta su arquitectura para un problema de RL, como la generación de respuestas en un diálogo interactivo.\n",
    "- Experimenta con diferentes enfoques de fine-tuning para mejorar la coherencia y relevancia de las respuestas generadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
