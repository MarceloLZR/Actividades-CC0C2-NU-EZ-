{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b346f12e",
   "metadata": {},
   "source": [
    "## Repaso\n",
    "\n",
    "**Redes Generativas Adversarias**\n",
    "\n",
    "A continuación se presenta el resumen del artículo original sobre las Generative Adversarial Networks. Al leer este resumen, notarás muchos términos y conceptos con los que quizás no estés familiarizado. \n",
    "\n",
    "Fuente: https://arxiv.org/abs/1406.2661\n",
    "\n",
    "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n",
    "\n",
    "Proponemos un nuevo marco para estimar modelos generativos a través de un proceso adversarial, en el cual entrenamos simultáneamente dos modelos: un modelo generativo G que captura la distribución de los datos, y un modelo discriminativo D que estima la probabilidad de que una muestra provenga de los datos de entrenamiento en lugar de G. El procedimiento de entrenamiento para G es maximizar la probabilidad de que D cometa un error. Este marco corresponde a un juego minimax de dos jugadores. En el espacio de funciones arbitrarias G y D, existe una solución única, con G recuperando la distribución de los datos de entrenamiento y D igual a 1/2 en todas partes. En el caso donde G y D están definidos por perceptrones multicapa, todo el sistema puede ser entrenado con retropropagación. No se necesita ninguna cadena de Markov ni redes de inferencia aproximada desenvueltas durante el entrenamiento o la generación de muestras. Los experimentos demuestran el potencial del marco a través de una evaluación cualitativa y cuantitativa de las muestras generadas.\n",
    "\n",
    "**Attention Is All You Need**\n",
    "\n",
    "\n",
    "Fuente: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "\n",
    "Los modelos de transducción de secuencias dominantes se basan en redes neuronales recurrentes o convolucionales complejas en una configuración de codificador-decodificador. Los modelos de mejor rendimiento también conectan el codificador y el decodificador a través de un mecanismo de atención. Proponemos una nueva arquitectura de red simple, el Transformer, basada únicamente en mecanismos de atención, eliminando por completo la recurrencia y las convoluciones. Experimentos en dos tareas de traducción automática muestran que estos modelos son superiores en calidad, al tiempo que son más paralelizables y requieren significativamente menos tiempo de entrenamiento. Nuestro modelo logra 28.4 BLEU en la tarea de traducción de inglés a alemán del WMT 2014, mejorando sobre los mejores resultados existentes, incluidas las combinaciones, en más de 2 BLEU. En la tarea de traducción de inglés a francés del WMT 2014, nuestro modelo establece una nueva puntuación BLEU de estado del arte para un solo modelo de 41.8 después de entrenar durante 3.5 días en ocho GPUs, una fracción pequeña de los costos de entrenamiento de los mejores modelos de la literatura. Demostramos que el Transformer se generaliza bien a otras tareas aplicándolo con éxito al análisis sintáctico del inglés tanto con datos de entrenamiento grandes como limitados.\n",
    "\n",
    "**GPT-4 Technical Report (Abstract)**\n",
    "\n",
    "\n",
    "Fuente: https://arxiv.org/abs/2303.08774\n",
    "\n",
    "Informamos sobre el desarrollo de GPT-4, un modelo multimodal a gran escala que puede aceptar entradas de texto e imagen y producir salidas de texto. Aunque es menos capaz que los humanos en muchos escenarios del mundo real, GPT-4 exhibe un rendimiento a nivel humano en varios puntos de referencia profesionales y académicos, incluyendo aprobar un examen simulado de abogacía con una puntuación en el 10% superior de los examinados. GPT-4 es un modelo basado en Transformer preentrenado para predecir el siguiente token en un documento. El proceso de alineación postentrenamiento resulta en un mejor rendimiento en medidas de factualidad y adherencia al comportamiento deseado. Un componente central de este proyecto fue desarrollar infraestructura y métodos de optimización que se comporten de manera predecible en una amplia gama de escalas. Esto nos permitió predecir con precisión algunos aspectos del rendimiento de GPT-4 basándonos en modelos entrenados con no más de 1/1,000 del cómputo de GPT-4.\n",
    "\n",
    "**Training Language Models to Follow Instructions with Human Feedback**\n",
    "\n",
    "Fuente: https://arxiv.org/abs/2203.02155\n",
    "\n",
    "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe\n",
    "\n",
    "Hacer que los modelos de lenguaje sean más grandes no los hace inherentemente mejores para seguir la intención del usuario. Por ejemplo, los modelos de lenguaje grandes pueden generar salidas que son falsas, tóxicas o simplemente no útiles para el usuario. En otras palabras, estos modelos no están alineados con sus usuarios. En este artículo, mostramos una vía para alinear los modelos de lenguaje con la intención del usuario en una amplia gama de tareas afinándolos con retroalimentación humana. Comenzando con un conjunto de indicaciones escritas por etiquetadores e indicaciones enviadas a través de la API de OpenAI, recopilamos un conjunto de datos de demostraciones de etiquetadores del comportamiento deseado del modelo, que usamos para afinar GPT-3 usando aprendizaje supervisado. Luego recopilamos un conjunto de datos de clasificaciones de salidas del modelo, que usamos para afinar aún más este modelo supervisado utilizando aprendizaje por refuerzo con retroalimentación humana. Llamamos a los modelos resultantes InstructGPT. En evaluaciones humanas de nuestra distribución de indicaciones, las salidas del modelo InstructGPT de 1.3B parámetros son preferidas a las salidas del GPT-3 de 175B parámetros, a pesar de tener 100 veces menos parámetros. Además, los modelos InstructGPT muestran mejoras en veracidad y reducciones en la generación de salidas tóxicas, mientras tienen regresiones mínimas de rendimiento en conjuntos de datos públicos de NLP. Aunque InstructGPT aún comete errores simples, nuestros resultados muestran que afinar con retroalimentación humana es una dirección prometedora para alinear los modelos de lenguaje con la intención humana.\n",
    "\n",
    "**Denoising Diffusion Probabilistic Models**\n",
    "\n",
    "\n",
    "Fuente: https://arxiv.org/abs/2006.11239\n",
    "\n",
    "Jonathan Ho, Ajay Jain, Pieter Abbeel\n",
    "\n",
    "Presentamos resultados de síntesis de imágenes de alta calidad utilizando modelos probabilísticos de difusión, una clase de modelos de variables latentes inspirados en consideraciones de la termodinámica fuera del equilibrio. Nuestros mejores resultados se obtienen entrenando con una cota variacional ponderada diseñada según una nueva conexión entre modelos probabilísticos de difusión y coincidencia de puntuación de desenfoque con dinámica de Langevin, y nuestros modelos admiten naturalmente un esquema de descompresión con pérdida progresiva que puede interpretarse como una generalización de la decodificación autoregresiva. En el conjunto de datos incondicional CIFAR10, obtenemos una puntuación de Inception de 9.46 y una puntuación FID de 3.17, ambas de vanguardia. En LSUN de 256x256, obtenemos una calidad de muestra similar a ProgressiveGAN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad26d1d",
   "metadata": {},
   "source": [
    "### Hugging Face orientado a LLM\n",
    "\n",
    "1 . Instalación y configuración\n",
    "\n",
    "Para comenzar a trabajar con la biblioteca Hugging Face Transformers, primero debes instalarla junto con sus dependencias. Utiliza pip para instalarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c16a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch optuna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348aa07",
   "metadata": {},
   "source": [
    "2 . Carga y uso de modelos preentrenados\n",
    "\n",
    "Hugging Face ofrece una amplia variedad de modelos preentrenados que puedes cargar y usar fácilmente. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c380c5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Cargar el tokenizador y el modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Entrada de texto\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generar texto con el pad_token_id y attention_mask\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50, \n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ddcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Cargar el modelo y el tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenizar una oración\n",
    "inputs = tokenizer(\"Este es un ejemplo.\", return_tensors=\"pt\")\n",
    "\n",
    "# Realizar la predicción\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6afa66",
   "metadata": {},
   "source": [
    "3 . Fine-Tuning de modelos\n",
    "\n",
    "Fine-tuning es el proceso de ajustar un modelo preentrenado en un conjunto de datos específico para mejorar su desempeño en una tarea concreta. A continuación se muestra un ejemplo básico de cómo realizar fine-tuning en un conjunto de datos personalizado.\n",
    "\n",
    "**Prepara el conjunto de datos**\n",
    "\n",
    "Para este ejemplo, usaremos un conjunto de datos de Hugging Face Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0bd388",
   "metadata": {},
   "source": [
    "**Tokenización del conjunto de datos**\n",
    "Es necesario tokenizar los datos para que el modelo los entienda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3475f6c9",
   "metadata": {},
   "source": [
    "**Dividir el conjunto de datos**\n",
    "\n",
    "Dividimos el conjunto de datos en conjuntos de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59856b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en entrenamiento y evaluación\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(10000))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de614f42",
   "metadata": {},
   "source": [
    "**Preparar el modelo para Fine-Tuning**\n",
    "\n",
    "Cargamos el modelo preentrenado y lo preparamos para la tarea de clasificación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6134cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\"  # Usar la implementación de AdamW de PyTorch\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950bc776",
   "metadata": {},
   "source": [
    "**Configurar el entrenamiento**\n",
    "\n",
    "Configuramos los parámetros de entrenamiento y usamos Trainer para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d8445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el entrenador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5b4c2",
   "metadata": {},
   "source": [
    "4 . Evaluación del modelo\n",
    "\n",
    "Después del entrenamiento, evaluamos el modelo para ver cómo se desempeña en el conjunto de datos de prueba.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a052bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa78670",
   "metadata": {},
   "source": [
    "5 . Uso del modelo Fine-Tuned\n",
    "\n",
    "Finalmente, podemos usar el modelo entrenado para hacer predicciones en nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar el modelo fine-tuned para realizar predicciones\n",
    "inputs = tokenizer(\"Este es un ejemplo.\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f876cc",
   "metadata": {},
   "source": [
    "6 . Guardado y carga del modelo\n",
    "\n",
    "Es importante guardar el modelo entrenado para su uso futuro. Aquí te mostramos cómo guardar y cargar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb044a",
   "metadata": {},
   "source": [
    "**Guardar el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo y el tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d8685",
   "metadata": {},
   "source": [
    "**Cargar el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a76e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo y el tokenizer guardados\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c1b07",
   "metadata": {},
   "source": [
    "7 . Optimización y mejoras\n",
    "\n",
    "**Ajuste de hiperparámetros**\n",
    "\n",
    "El ajuste de hiperparámetros puede mejorar significativamente el rendimiento del modelo. Esto implica experimentar con diferentes valores de hiperparámetros como la tasa de aprendizaje, el tamaño del batch, etc.\n",
    "\n",
    "**Uso de Optuna para optimización de hiperparámetros**\n",
    "\n",
    "[Optuna](https://optuna.org/) es una biblioteca para la optimización automática de hiperparámetros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Definir los hiperparámetros que se desean ajustar\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    num_train_epochs = trial.suggest_int('num_train_epochs', 1, 5)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    return eval_results['eval_loss']\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa9f53e",
   "metadata": {},
   "source": [
    "8 . Implementación del modelo en producción\n",
    "\n",
    "**Usando Hugging Face Inference API**\n",
    "\n",
    "Puedes usar la API de Hugging Face para implementar modelos en producción de manera sencilla.\n",
    "\n",
    "**Despliegue en Amazon SageMaker**\n",
    "\n",
    "Hugging Face también ofrece integración con Amazon SageMaker para despliegue escalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46323152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Definir el modelo Hugging Face\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=\"s3://path/to/your/model.tar.gz\",   # path to your trained model\n",
    "   role=role,                                     # IAM role with SageMaker permissions\n",
    "   transformers_version=\"4.6\",\n",
    "   pytorch_version=\"1.7\",\n",
    "   py_version=\"py36\",\n",
    ")\n",
    "\n",
    "# Despliegue del modelo\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47273e",
   "metadata": {},
   "source": [
    "9 . Trabajando con modelos de traducción\n",
    "\n",
    "Hugging Face también ofrece modelos para traducción automática. \n",
    "\n",
    "A continuación se muestra cómo usar un modelo de traducción preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be565d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Cargar el modelo y el tokenizer para traducción\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Realizar traducción\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea915a",
   "metadata": {},
   "source": [
    "10 . Uso de modelos de resumen de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe308ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Cargar el modelo y el tokenizer para resumen\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Realizar resumen\n",
    "text = \"Tu texto largo aquí.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38918724",
   "metadata": {},
   "source": [
    "11 . Ajuste de parámetros de generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar parámetros de generación como num_beams, length_penalty, etc.\n",
    "generated_ids = model.generate(inputs[\"input_ids\"], num_beams=5, length_penalty=1.5, max_length=150, min_length=50)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672aa636",
   "metadata": {},
   "source": [
    "12 . Evaluación automática de modelos\n",
    "\n",
    "Para evaluar automáticamente los modelos, puedes utilizar métricas como BLEU, ROUGE, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"rouge\")\n",
    "predictions = [\"Tu texto generado\"]\n",
    "references = [\"Tu texto de referencia\"]\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5ec64",
   "metadata": {},
   "source": [
    "13 . Integración con aplicaciones Web\n",
    "\n",
    "Puedes integrar los modelos con aplicaciones web utilizando frameworks como Flask o FastAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    inputs = tokenizer(data['text'], return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return jsonify({'generated_text': generated_text})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6c053",
   "metadata": {},
   "source": [
    "14 . Entrenamiento distribuido y acelerado\n",
    "\n",
    "Para el entrenamiento distribuido y acelerado, puedes utilizar el soporte de Hugging Face para aceleradores de hardware como GPUs y TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    # Añadir soporte para múltiples GPUs\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    fp16=True,  # Usar precisión de 16 bits\n",
    "    deepspeed=\"path/to/deepspeed_config.json\"  # Configuración de DeepSpeed para entrenamiento distribuido\n",
    ")\n",
    "\n",
    "# Configurar y entrenar el modelo de manera distribuida\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d1f90",
   "metadata": {},
   "source": [
    "### Ejemplos\n",
    "\n",
    "1 . Fine-Tuning y evaluación de un Modelo BERT para Clasificación de Texto\n",
    "\n",
    "Este ejemplo abarca la carga de un conjunto de datos, el preprocesamiento, el fine-tuning de un modelo BERT para clasificación de texto, y la evaluación del modelo utilizando métricas avanzadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ea111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Cargar el conjunto de datos y el tokenizador\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Preprocesar el conjunto de datos\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000))  # Usamos un subconjunto para el ejemplo\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Cargar el modelo\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Definir una función de evaluación\n",
    "def compute_metrics(p):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Crear el objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n",
    "\n",
    "# Guardar el modelo y el tokenizador\n",
    "model.save_pretrained(\"./fine_tuned_bert\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_bert\")\n",
    "\n",
    "# Ejemplo de uso del modelo fine-tuned\n",
    "def classify_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    return \"positive\" if prediction == 1 else \"negative\"\n",
    "\n",
    "sample_text = \"This movie was absolutely fantastic!\"\n",
    "print(f\"Sample text classification: {classify_text(sample_text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f341de",
   "metadata": {},
   "source": [
    "2 . Implementación de un modelo de traducción con evaluación BLEU\n",
    "\n",
    "Este ejemplo muestra cómo cargar un modelo de traducción, realizar traducción automática y evaluar el rendimiento del modelo utilizando la métrica BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-de'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Cargar el conjunto de datos WMT14\n",
    "dataset = load_dataset('wmt14', 'de-en', split='test[:1%]')  # Usamos una muestra pequeña para el ejemplo\n",
    "\n",
    "# Tokenizar el texto de origen\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['en'], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['de'], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\n",
    "train_dataset = tokenized_datasets.shuffle(seed=42).select(range(500))  # Usamos un subconjunto para el ejemplo\n",
    "\n",
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Definir una función de evaluación BLEU\n",
    "def compute_metrics(p):\n",
    "    metric = load_metric(\"sacrebleu\")\n",
    "    predictions = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crear el objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,  # Para este ejemplo, usamos el mismo conjunto para evaluación\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo\n",
    "results = trainer.evaluate()\n",
    "print(\"BLEU evaluation results:\", results)\n",
    "\n",
    "# Guardar el modelo y el tokenizador\n",
    "model.save_pretrained(\"./fine_tuned_translation_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_translation_model\")\n",
    "\n",
    "# Ejemplo de traducción\n",
    "def translate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    translated_tokens = model.generate(**inputs)\n",
    "    translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    return translation\n",
    "\n",
    "sample_text = \"This is a test sentence for translation.\"\n",
    "print(f\"Sample translation: {translate_text(sample_text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f6e60",
   "metadata": {},
   "source": [
    "3 . Entrenamiento y evaluación de un modelo de resumen de texto con métricas ROUGE\n",
    "\n",
    "Este ejemplo muestra cómo entrenar un modelo de resumen de texto y evaluar su rendimiento utilizando métricas ROUGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Cargar el conjunto de datos CNN/DailyMail\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train[:1%]')  # Usamos una muestra pequeña para el ejemplo\n",
    "\n",
    "# Preprocesar el conjunto de datos\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['article'], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['highlights'], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"highlights\"])\n",
    "train_dataset = tokenized_datasets.shuffle(seed=42).select(range(500))  # Usamos un subconjunto para el ejemplo\n",
    "\n",
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Definir una función de evaluación ROUGE\n",
    "def compute_metrics(p):\n",
    "    metric = load_metric(\"rouge\")\n",
    "    predictions = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crear el objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,  # Para este ejemplo, usamos el mismo conjunto para evaluación\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo\n",
    "results = trainer.evaluate()\n",
    "print(\"ROUGE evaluation results:\", results)\n",
    "\n",
    "# Guardar el modelo y el tokenizador\n",
    "model.save_pretrained(\"./fine_tuned_summarization_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_summarization_model\")\n",
    "\n",
    "# Ejemplo de resumen\n",
    "def summarize_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=50, min_length=25, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "sample_text = \"Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.\"\n",
    "print(f\"Sample summary: {summarize_text(sample_text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4aed44",
   "metadata": {},
   "source": [
    "4 . Implementación de un pipeline de clasificación de texto con FastAPI\n",
    "\n",
    "Este ejemplo muestra cómo entrenar un modelo de clasificación de texto y luego desplegarlo en un servicio web utilizando FastAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# Crear la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Definir la entrada del modelo\n",
    "class TextItem(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# Cargar el modelo y el tokenizador entrenados\n",
    "model_path = \"./fine_tuned_bert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Crear una función de predicción\n",
    "def predict(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    return \"positive\" if prediction == 1 else \"negative\"\n",
    "\n",
    "# Crear un endpoint de predicción\n",
    "@app.post(\"/predict/\")\n",
    "def classify_text(item: TextItem):\n",
    "    try:\n",
    "        prediction = predict(item.text)\n",
    "        return {\"text\": item.text, \"classification\": prediction}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Para correr el servidor, usa:\n",
    "# uvicorn script_name:app --reload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8457e",
   "metadata": {},
   "source": [
    "5 . Traducción automática con evaluación de BLEU y despliegue en Flask\n",
    "\n",
    "Este ejemplo muestra cómo entrenar un modelo de traducción automática, evaluar su rendimiento utilizando la métrica BLEU y desplegarlo en un servicio web utilizando Flask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6175e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-de'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Cargar el conjunto de datos WMT14\n",
    "dataset = load_dataset('wmt14', 'de-en', split='test[:1%]')\n",
    "\n",
    "# Preprocesar el conjunto de datos\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['en'], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['de'], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\n",
    "train_dataset = tokenized_datasets.shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Definir una función de evaluación BLEU\n",
    "def compute_metrics(p):\n",
    "    metric = load_metric(\"sacrebleu\")\n",
    "    predictions = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crear el objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo\n",
    "results = trainer.evaluate()\n",
    "print(\"BLEU evaluation results:\", results)\n",
    "\n",
    "# Guardar el modelo y el tokenizador\n",
    "model.save_pretrained(\"./fine_tuned_translation_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_translation_model\")\n",
    "\n",
    "# Crear la aplicación Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Definir la entrada del modelo\n",
    "class TranslationItem(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# Crear una función de traducción\n",
    "def translate(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    translated_tokens = model.generate(**inputs)\n",
    "    translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    return translation\n",
    "\n",
    "# Crear un endpoint de traducción\n",
    "@app.route(\"/translate\", methods=[\"POST\"])\n",
    "def translate_text():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        translation = translate(data['text'])\n",
    "        return jsonify({\"text\": data['text'], \"translation\": translation})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# Para correr el servidor, usa:\n",
    "# flask run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeec7d4",
   "metadata": {},
   "source": [
    "6 . Resumen de texto con evaluación ROUGE y despliegue en FastAPI\n",
    "\n",
    "Este ejemplo muestra cómo entrenar un modelo de resumen de texto, evaluar su rendimiento utilizando métricas ROUGE y desplegarlo en un servicio web utilizando FastAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Cargar el conjunto de datos CNN/DailyMail\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train[:1%]')\n",
    "\n",
    "# Preprocesar el conjunto de datos\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['article'], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['highlights'], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"highlights\"])\n",
    "train_dataset = tokenized_datasets.shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Definir una función de evaluación ROUGE\n",
    "def compute_metrics(p):\n",
    "    metric = load_metric(\"rouge\")\n",
    "    predictions = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crear el objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo\n",
    "results = trainer.evaluate()\n",
    "print(\"ROUGE evaluation results:\", results)\n",
    "\n",
    "# Guardar el modelo y el tokenizador\n",
    "model.save_pretrained(\"./fine_tuned_summarization_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_summarization_model\")\n",
    "\n",
    "# Crear la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Definir la entrada del modelo\n",
    "class SummaryItem(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# Crear una función de resumen\n",
    "def summarize(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=50, min_length=25, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Crear un endpoint de resumen\n",
    "@app.post(\"/summarize/\")\n",
    "def summarize_text(item: SummaryItem):\n",
    "    try:\n",
    "        summary = summarize(item.text)\n",
    "        return {\"text\": item.text, \"summary\": summary}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Para correr el servidor, usa:\n",
    "# uvicorn script_name:app --reload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c696cfd",
   "metadata": {},
   "source": [
    "7 . Entrenamiento distribuido con PyTorch Lightning y DeepSpeed\n",
    "\n",
    "Este ejemplo muestra cómo utilizar PyTorch Lightning y DeepSpeed para entrenar modelos de manera distribuida y eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import deepspeed\n",
    "\n",
    "# Cargar el conjunto de datos y el tokenizador\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Preprocesar el conjunto de datos\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Crear un DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=16)\n",
    "\n",
    "# Definir el modelo de clasificación con BERT\n",
    "class BertClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "# Configurar el entrenador de PyTorch Lightning con DeepSpeed\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    precision=16,  # Para FP16\n",
    "    plugins=[deepspeed.DeepSpeedPlugin(config={\"train_batch_size\": 16})],\n",
    "    max_epochs=3\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = BertClassifier()\n",
    "trainer.fit(model, train_dataloader, eval_dataloader)\n",
    "\n",
    "# Guardar el modelo y el tokenizador\n",
    "model.model.save_pretrained(\"./deepspeed_bert\")\n",
    "tokenizer.save_pretrained(\"./deepspeed_bert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685fb8e1",
   "metadata": {},
   "source": [
    "8 . Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "RAG combina la generación de texto con la recuperación de documentos relevantes. Aquí se muestra cómo configurar y usar un modelo RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c1a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "\n",
    "# Cargar el tokenizador y el modelo\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever)\n",
    "\n",
    "# Texto de entrada\n",
    "input_text = \"What is the capital of France?\"\n",
    "\n",
    "# Tokenizar y generar\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "generated = model.generate(input_ids, num_beams=2, num_return_sequences=2)\n",
    "\n",
    "# Decodificar la salida\n",
    "generated_texts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n",
    "print(generated_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74335d",
   "metadata": {},
   "source": [
    "9 . GPT-3 en modo Zero-Shot\n",
    "\n",
    "En el modo Zero-Shot, utilizamos el modelo GPT-3 sin entrenamiento adicional para realizar tareas como la clasificación o generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Cargar el tokenizador y el modelo\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Texto de entrada para Zero-Shot\n",
    "input_text = \"Translate English to French: What time is it?\"\n",
    "\n",
    "# Tokenizar y generar\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs.input_ids, max_length=40)\n",
    "\n",
    "# Decodificar la salida\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781790ba",
   "metadata": {},
   "source": [
    "10 . GPT-3 en modo Few-Shot\n",
    "\n",
    "En el modo Few-Shot, proporcionamos algunos ejemplos de la tarea al modelo para guiar su generación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e039b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Cargar el tokenizador y el modelo\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Texto de entrada para Few-Shot\n",
    "input_text = \"\"\"\n",
    "Translate English to French:\n",
    "English: What time is it?\n",
    "French: Quelle heure est-il?\n",
    "\n",
    "English: How are you?\n",
    "French: Comment ça va?\n",
    "\n",
    "English: I am fine.\n",
    "French:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizar y generar\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs.input_ids, max_length=100)\n",
    "\n",
    "# Decodificar la salida\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696830cc",
   "metadata": {},
   "source": [
    "11 . Implementación completa de un pipeline de RAG\n",
    "\n",
    "Este ejemplo avanzado muestra cómo implementar un pipeline completo utilizando RAG, incluyendo la configuración del entorno, recuperación de documentos y generación de respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fe206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "import torch\n",
    "\n",
    "# Cargar el tokenizador, el recuperador y el modelo\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "# Configuración del entorno\n",
    "input_text = \"Explain the theory of relativity.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generar con RAG\n",
    "generated = model.generate(input_ids, num_return_sequences=1, num_beams=5)\n",
    "generated_texts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n",
    "print(\"Generated Texts:\", generated_texts)\n",
    "\n",
    "# Recuperar documentos relevantes\n",
    "docs = retriever(input_ids.numpy().flatten())\n",
    "retrieved_texts = docs['documents'][0]\n",
    "print(\"Retrieved Texts:\", retrieved_texts)\n",
    "\n",
    "# Generar respuestas basadas en los documentos recuperados\n",
    "context = \" \".join(retrieved_texts)\n",
    "context_input_ids = tokenizer(context, return_tensors=\"pt\").input_ids\n",
    "context_generated = model.generate(context_input_ids, num_return_sequences=1, num_beams=5)\n",
    "context_generated_texts = [tokenizer.decode(cg, skip_special_tokens=True) for cg in context_generated]\n",
    "print(\"Context-Based Generated Texts:\", context_generated_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920245d",
   "metadata": {},
   "source": [
    "12 . Comparación de GPT-3 en Modo Zero-Shot y Few-Shot\n",
    "\n",
    "Este ejemplo muestra cómo comparar el rendimiento del modelo GPT-3 en modos Zero-Shot y Few-Shot para una tarea específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Cargar el tokenizador y el modelo\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Modo Zero-Shot\n",
    "input_text_zero_shot = \"Translate English to French: Where is the library?\"\n",
    "inputs_zero_shot = tokenizer(input_text_zero_shot, return_tensors=\"pt\")\n",
    "outputs_zero_shot = model.generate(inputs_zero_shot.input_ids, max_length=50)\n",
    "generated_text_zero_shot = tokenizer.decode(outputs_zero_shot[0], skip_special_tokens=True)\n",
    "print(\"Zero-Shot Translation:\", generated_text_zero_shot)\n",
    "\n",
    "# Modo Few-Shot\n",
    "input_text_few_shot = \"\"\"\n",
    "Translate English to French:\n",
    "English: Where is the library?\n",
    "French: Où est la bibliothèque?\n",
    "\n",
    "English: How are you?\n",
    "French: Comment ça va?\n",
    "\n",
    "English: What time is it?\n",
    "French:\n",
    "\"\"\"\n",
    "inputs_few_shot = tokenizer(input_text_few_shot, return_tensors=\"pt\")\n",
    "outputs_few_shot = model.generate(inputs_few_shot.input_ids, max_length=100)\n",
    "generated_text_few_shot = tokenizer.decode(outputs_few_shot[0], skip_special_tokens=True)\n",
    "print(\"Few-Shot Translation:\", generated_text_few_shot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221f849",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 . Fine-Tuning y Evaluación de RoBERTa para Clasificación de Texto\n",
    "\n",
    "Objetivo: Realizar fine-tuning del modelo RoBERTa en un conjunto de datos específico y evaluar su rendimiento.\n",
    "\n",
    "Carga y preprocesamiento del conjunto de Datos:\n",
    "\n",
    "* Carga el conjunto de datos ag_news utilizando la biblioteca datasets.\n",
    "* Tokeniza el conjunto de datos utilizando RobertaTokenizer.\n",
    "\n",
    "Configuración y entrenamiento:\n",
    "\n",
    "* Configura el modelo RobertaForSequenceClassification para la tarea de clasificación de texto.\n",
    "* Configura los argumentos de entrenamiento utilizando TrainingArguments.\n",
    "* Entrena el modelo utilizando Trainer.\n",
    "\n",
    "Evaluación:\n",
    "\n",
    "- Evalúa el modelo en el conjunto de datos de prueba.\n",
    "- Calcula y muestra las métricas de precisión y F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc79637",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36055e8a",
   "metadata": {},
   "source": [
    "2 . Generación de texto con GPT-3 en Modo Few-Shot\n",
    "\n",
    "Objetivo: Usar GPT-3 en modo Few-Shot para generar texto basado en ejemplos proporcionados.\n",
    "\n",
    "Preparación de Prompt:\n",
    "\n",
    "* Define ejemplos de texto para guiar al modelo en la tarea de generación.\n",
    "\n",
    "Generación de texto:\n",
    "\n",
    "* Usa la API de OpenAI para generar texto basado en los ejemplos proporcionados.\n",
    "\n",
    "Evaluación de resultados:\n",
    "\n",
    "* Compara la calidad del texto generado con los ejemplos proporcionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82935378",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d337f",
   "metadata": {},
   "source": [
    "3 . : Traducción Automática con XLM y Evaluación BLEU\n",
    "\n",
    "Objetivo: Utilizar XLM para traducción automática y evaluar el rendimiento utilizando la métrica BLEU.\n",
    "\n",
    "Preparación del conjunto de datos:\n",
    "\n",
    "* Carga un conjunto de datos bilingüe para la traducción.\n",
    "\n",
    "Traducción automática:\n",
    "\n",
    "* Usa XLMTokenizer y XLMWithLMHeadModel para traducir texto.\n",
    "\n",
    "Evaluación:\n",
    "\n",
    "* Evalua las traducciones generadas utilizando la métrica BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d08d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202eac53",
   "metadata": {},
   "source": [
    "4 . Resumen de texto con T5 y evaluación ROUGE\n",
    "\n",
    "Objetivo: Utilizar T5 para resumen de texto y evaluar el rendimiento utilizando la métrica ROUGE.\n",
    "\n",
    "Carga y preprocesamiento del conjunto de datos:\n",
    "\n",
    "* Carga el conjunto de datos cnn_dailymail utilizando la biblioteca datasets.\n",
    "* Tokeniza el conjunto de datos utilizando T5Tokenizer.\n",
    "\n",
    "Generación de resúmenes:\n",
    "\n",
    "* Usa T5ForConditionalGeneration para generar resúmenes de texto.\n",
    "\n",
    "Evaluación:\n",
    "\n",
    "* Evalua los resúmenes generados utilizando la métrica ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# 1. Carga y Preprocesamiento del Conjunto de Datos\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train[:1%]')\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['article'], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['highlights'], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"highlights\"])\n",
    "train_dataset = tokenized_datasets.shuffle(seed=42).select(range(500))\n",
    "\n",
    "# 2. Configuración y Entrenamiento\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    compute_metrics=lambda p: load_metric(\"rouge\").compute(predictions=[tokenizer.decode(g, skip_special_tokens=True) for g in p.predictions], references=[[tokenizer.decode(g, skip_special_tokens=True)] for g in p.label_ids])\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# 3. Evaluación\n",
    "results = trainer.evaluate()\n",
    "print(\"ROUGE evaluation results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb627d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d1426",
   "metadata": {},
   "source": [
    "### Introducción a LLM, LangChain y LLaMA\n",
    "\n",
    "Los modelos de lenguaje de gran escala (LLM) han revolucionado el campo de la inteligencia artificial y el procesamiento del lenguaje natural (NLP). Con la capacidad de entender y generar texto de manera coherente y contextualmente precisa, los LLM están transformando la manera en que interactuamos con la tecnología. Dentro de este contexto, surgen herramientas y marcos como LangChain y LLaMA, que ofrecen capacidades avanzadas para el desarrollo y la implementación de aplicaciones basadas en LLM.\n",
    "\n",
    "\n",
    "Los LLM, como GPT-4 de OpenAI, BERT de Google y muchos otros, son redes neuronales profundas entrenadas en grandes volúmenes de datos textuales. Estos modelos pueden comprender y generar texto en múltiples idiomas, realizar tareas de traducción, responder preguntas, resumir textos, entre otras aplicaciones. La clave de su éxito radica en su capacidad para captar matices y contextos complejos en el lenguaje humano, gracias a técnicas de aprendizaje profundo y enormes cantidades de datos de entrenamiento.\n",
    "\n",
    "La arquitectura detrás de los LLM suele estar basada en transformers, una estructura de red neuronal que permite procesar secuencias de datos de manera más eficiente y efectiva que las arquitecturas anteriores, como las redes recurrentes. Los transformers utilizan mecanismos de atención que permiten a los modelos enfocarse en partes relevantes del texto al procesar información, mejorando así la comprensión y generación de texto.\n",
    "\n",
    "#### LangChain: Integración y expansión de capacidades LLM\n",
    "\n",
    "LangChain es un marco diseñado para facilitar la creación de aplicaciones basadas en LLM. Proporciona herramientas y bibliotecas que permiten a los desarrolladores integrar modelos de lenguaje en sus aplicaciones de manera más sencilla y efectiva. LangChain ofrece una serie de funcionalidades clave, incluyendo la gestión de diálogos, la integración con bases de datos, la ejecución de tareas específicas y la personalización de respuestas.\n",
    "\n",
    "Una de las características destacadas de LangChain es su capacidad para gestionar contextos prolongados y mantener coherencia en interacciones de varios turnos. Esto es particularmente útil en aplicaciones como chatbots y asistentes virtuales, donde es crucial mantener el contexto de la conversación a lo largo del tiempo. LangChain logra esto mediante el uso de técnicas avanzadas de manejo de estados y contextos, permitiendo que los LLM proporcionen respuestas coherentes y relevantes.\n",
    "\n",
    "Además, LangChain facilita la integración de modelos de lenguaje con otros sistemas y fuentes de datos. Por ejemplo, permite a los desarrolladores conectar LLM con bases de datos SQL y NoSQL, APIs externas, y sistemas de gestión de contenido, ampliando significativamente las capacidades de las aplicaciones basadas en LLM. Esto abre un abanico de posibilidades para crear soluciones personalizadas y adaptadas a necesidades específicas de diferentes industrias.\n",
    "\n",
    "#### LLaMA: Modelos de lenguaje de libre acceso\n",
    "\n",
    "[LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/), que significa \"Large Language Model Accessibility\", es un proyecto que busca democratizar el acceso a modelos de lenguaje avanzados. A diferencia de otros modelos que pueden tener restricciones de acceso debido a costos o limitaciones comerciales, LLaMA se centra en proporcionar modelos de lenguaje potentes que estén disponibles para una amplia audiencia, incluyendo investigadores, desarrolladores independientes y pequeñas empresas.\n",
    "\n",
    "El objetivo de LLaMA es reducir las barreras de entrada para el uso de LLM, permitiendo que más personas puedan experimentar con estas tecnologías y desarrollar aplicaciones innovadoras. LLaMA ofrece una serie de modelos preentrenados que los usuarios pueden descargar y utilizar en sus proyectos. Estos modelos están optimizados para funcionar en hardware accesible, lo que facilita su implementación en entornos con recursos limitados.\n",
    "\n",
    "LLaMA también se enfoca en proporcionar documentación y recursos educativos para ayudar a los usuarios a comprender y utilizar los modelos de lenguaje de manera efectiva. Esto incluye tutoriales, ejemplos de código, y guías de mejores prácticas, que son esenciales para maximizar el potencial de los LLM en aplicaciones prácticas.\n",
    "\n",
    "### Integración de LangChain y LLaMA\n",
    "\n",
    "La combinación de LangChain y LLaMA presenta una poderosa herramienta para el desarrollo de aplicaciones basadas en LLM. Mientras que LangChain ofrece un marco robusto para la integración y gestión de modelos de lenguaje, LLaMA proporciona acceso a modelos de lenguaje de alta calidad y accesibles. Juntos, estos recursos permiten a los desarrolladores crear aplicaciones sofisticadas que aprovechan al máximo las capacidades de los LLM.\n",
    "\n",
    "Una posible aplicación de esta integración podría ser el desarrollo de un asistente virtual personalizado para una pequeña empresa. Utilizando LangChain, el desarrollador puede integrar el asistente con el sistema de gestión de clientes de la empresa, permitiendo que el asistente acceda a información relevante y proporcione respuestas precisas y contextualmente apropiadas. Al mismo tiempo, utilizando un modelo LLaMA, el desarrollador puede asegurar que el asistente funcione de manera eficiente en hardware accesible, sin necesidad de inversiones significativas en infraestructura.\n",
    "\n",
    "Otra aplicación podría ser en el ámbito educativo, donde se pueden crear tutores virtuales que ayuden a los estudiantes a aprender nuevos conceptos y resolver dudas. Estos tutores pueden estar integrados con bases de datos de contenido educativo y proporcionar explicaciones detalladas y personalizadas basadas en el progreso y necesidades de cada estudiante.\n",
    "\n",
    "### Desafíos y Futuro de LLM, LangChain y LLaMA\n",
    "\n",
    "A pesar de las numerosas ventajas y aplicaciones de los LLM, LangChain y LLaMA, también existen desafíos significativos que deben abordarse. Uno de los principales retos es la ética y la responsabilidad en el uso de modelos de lenguaje. Los LLM tienen el potencial de generar contenido que puede ser perjudicial o engañoso si no se utilizan correctamente. Por lo tanto, es crucial desarrollar y seguir directrices éticas y mecanismos de control para asegurar el uso responsable de estas tecnologías.\n",
    "\n",
    "Otro desafío es la escalabilidad y eficiencia de los modelos de lenguaje. A medida que los LLM se vuelven más grandes y complejos, también aumentan los requisitos de computación y almacenamiento. Esto puede ser una barrera para su adopción en entornos con recursos limitados. Sin embargo, proyectos como LLaMA están trabajando para mitigar estos problemas proporcionando modelos optimizados y accesibles.\n",
    "\n",
    "En términos de futuro, se espera que la integración de LLM en aplicaciones continúe creciendo y evolucionando. Con la mejora continua en las arquitecturas de modelos y técnicas de entrenamiento, los LLM serán cada vez más capaces de comprender y generar texto de manera más precisa y contextualmente relevante. Además, herramientas como LangChain y LLaMA seguirán facilitando el desarrollo y la implementación de aplicaciones basadas en LLM, democratizando el acceso a estas poderosas tecnologías y permitiendo una innovación más amplia y diversificada.\n",
    "\n",
    "La combinación de LLM, LangChain y LLaMA representa una convergencia de tecnología y accesibilidad que tiene el potencial de transformar numerosos campos y aplicaciones. Al reducir las barreras de entrada y proporcionar herramientas avanzadas para el desarrollo, estas tecnologías están preparando el camino para una nueva era de aplicaciones inteligentes y contextualmente conscientes, beneficiando tanto a desarrolladores como a usuarios finales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
