{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08bbc71",
   "metadata": {},
   "source": [
    "## Modelos de languaje natural y HuggingFaces\n",
    "\n",
    "En este cuaderno, exploraremos varios modelos avanzados de lenguaje natural (NLP), incluidos GPT-1, GPT-2, GPT-3, BART, ELECTRA, XLNet, Transformer-XL, y métodos de difusión orientados a LLM. Cada sección incluye una breve explicación teórica y ejemplos de implementación en Python.\n",
    "\n",
    "#### GPT-1\n",
    "\n",
    "Generative Pre-trained Transformer 1 (GPT-1) es un modelo de lenguaje introducido por OpenAI en 2018. Utiliza el enfoque de aprendizaje no supervisado, preentrenando un modelo Transformer en una gran cantidad de texto y luego ajustándolo para tareas específicas de NLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'gpt2'  # Utilizamos GPT-2 como proxy para GPT-1\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate_text(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"El aprendizaje automático es\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72a0f4",
   "metadata": {},
   "source": [
    "**GPT-2**\n",
    "\n",
    "GPT-2 es una versión mejorada de GPT-1 con más parámetros y entrenada en una mayor cantidad de datos. Introducida por OpenAI en 2019, GPT-2 demostró capacidades avanzadas en la generación de texto coherente y relevante.\n",
    "\n",
    "**Implementación y ejemplos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"La inteligencia artificial tiene el potencial de\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73c895",
   "metadata": {},
   "source": [
    "#### GPT-3\n",
    "\n",
    "GPT-3, también de OpenAI, es una versión aún más avanzada con 175 mil millones de parámetros. Lanzado en 2020, GPT-3 puede realizar una amplia gama de tareas de NLP con mínima o ninguna modificación específica de la tarea.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT3Tokenizer, GPT3Model  # Asumiendo que existe una clase GPT-3 en transformers\n",
    "\n",
    "# GPT-3 no está disponible gratuitamente en transformers, se usa a través de la API de OpenAI\n",
    "\n",
    "api_key = 'your_openai_api_key'\n",
    "openai.api_key = api_key\n",
    "\n",
    "def generate_text_gpt3(prompt, max_length=50):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_length\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "prompt = \"Los avances en la inteligencia artificial incluyen\"\n",
    "generated_text = generate_text_gpt3(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885c40b",
   "metadata": {},
   "source": [
    "**BART**\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformers) es un modelo de Facebook AI que combina los enfoques de modelos auto-regresivos y auto-encoders. Es eficaz en tareas de generación y comprensión de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model_name = 'facebook/bart-large'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "def generate_text_bart(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"La ciencia de datos es\"\n",
    "generated_text = generate_text_bart(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065a876",
   "metadata": {},
   "source": [
    "**ELECTRA**\n",
    "\n",
    "ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) es un modelo de Google Research que introduce un nuevo enfoque de preentrenamiento más eficiente que BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForPreTraining\n",
    "\n",
    "model_name = 'google/electra-small-discriminator'\n",
    "tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
    "model = ElectraForPreTraining.from_pretrained(model_name).to(device)\n",
    "\n",
    "def predict_masked_word(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    discriminator_outputs = model(**inputs)\n",
    "    predictions = discriminator_outputs.logits.argmax(dim=-1)\n",
    "    return tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"El aprendizaje profundo es una rama de la [MASK]\"\n",
    "predicted_word = predict_masked_word(prompt)\n",
    "print(predicted_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c3800",
   "metadata": {},
   "source": [
    "**XLNet**\n",
    "\n",
    "XLNet es un modelo de Google AI que combina las ventajas de BERT y los modelos auto-regresivos. Introduce una permutación de tokens en su entrenamiento, mejorando la capacidad de capturar dependencias bidireccionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfc3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "\n",
    "model_name = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "model = XLNetLMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "prompt = \"El procesamiento de lenguaje natural\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e320e99",
   "metadata": {},
   "source": [
    "**Transformer-XL**\n",
    "\n",
    "Transformer-XL es una extensión del modelo Transformer que introduce la memoria segmentada recurrente, permitiendo a los modelos manejar dependencias a largo plazo de manera más efectiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b069f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n",
    "\n",
    "model_name = 'transfo-xl-wt103'\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(model_name)\n",
    "model = TransfoXLLMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "prompt = \"La inteligencia artificial\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bf273",
   "metadata": {},
   "source": [
    "### Métodos de difusión orientados a LLM\n",
    "\n",
    "Los métodos de difusión son una clase de modelos generativos que se inspiran en procesos de difusión física, como la propagación de calor o la difusión de partículas. En el contexto de LLM (Modelos de Lenguaje a Gran Escala), los métodos de difusión pueden ser utilizados para mejorar la calidad y coherencia de la generación de texto.\n",
    "\n",
    "Los modelos de difusión funcionan mediante un proceso de denoising (eliminación de ruido) iterativo. Inicialmente, se añade ruido a una entrada estructurada, y luego, el modelo aprende a revertir este proceso para recuperar la estructura original. Este enfoque puede ser utilizado para tareas de generación de texto donde se busca mantener una coherencia semántica y estilística a lo largo de la secuencia generada.\n",
    "\n",
    "\n",
    "A continuación, implementamos un modelo de difusión simple para generación de texto utilizando una arquitectura basada en Transformers. Primero, definimos las funciones para el proceso de adición de ruido y denoising:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Parámetros del modelo\n",
    "model_name = 'gpt2'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Cargar el modelo y el tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Función para añadir ruido al texto\n",
    "def add_noise(tokens, noise_level=0.1):\n",
    "    noisy_tokens = tokens.clone()\n",
    "    mask = torch.rand(tokens.shape) < noise_level\n",
    "    noisy_tokens[mask] = tokenizer.pad_token_id\n",
    "    return noisy_tokens\n",
    "\n",
    "# Función de denoising\n",
    "class DenoisingModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(DenoisingModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "# Entrenamiento del modelo de denoising\n",
    "def train_denoising_model(model, tokenizer, texts, epochs=5, noise_level=0.1):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors='pt').input_ids.to(device)\n",
    "            noisy_inputs = add_noise(inputs, noise_level)\n",
    "            outputs = model(noisy_inputs)\n",
    "            loss = criterion(outputs.view(-1, tokenizer.vocab_size), inputs.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(texts)}')\n",
    "\n",
    "# Datos de ejemplo y entrenamiento\n",
    "texts = [\n",
    "    \"La inteligencia artificial está revolucionando el mundo.\",\n",
    "    \"El aprendizaje automático es una subdisciplina de la inteligencia artificial.\",\n",
    "    \"Los modelos de lenguaje a gran escala tienen un impacto significativo en la tecnología moderna.\"\n",
    "]\n",
    "\n",
    "denoising_model = DenoisingModel(model)\n",
    "train_denoising_model(denoising_model, tokenizer, texts)\n",
    "\n",
    "# Generación de texto con el modelo de denoising\n",
    "def generate_text_with_denoising(model, tokenizer, prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "    noisy_inputs = add_noise(inputs)\n",
    "    denoised_outputs = model(noisy_inputs)\n",
    "    generated_text = tokenizer.decode(denoised_outputs.argmax(dim=-1).squeeze(), skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "prompt = \"La tecnología moderna\"\n",
    "generated_text = generate_text_with_denoising(denoising_model, tokenizer, prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc692c83",
   "metadata": {},
   "source": [
    "####  GPT-1\n",
    "\n",
    "**Ejercicio 1: Generación de texto con GPT-1**\n",
    "\n",
    "Utilizando el modelo GPT-1 (representado por GPT-2 en Hugging Face), genera un texto de al menos 100 palabras a partir de un prompt dado. Experimenta con diferentes prompts y observa cómo cambia la coherencia del texto generado.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Prompt de ejemplo\n",
    "prompt = \"En el futuro, la inteligencia artificial\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ff8e3",
   "metadata": {},
   "source": [
    "**Ejercicio 2:**  Ajuste fino de GPT-1 para una tarea específica \n",
    "\n",
    "Encuentra un conjunto de datos pequeño y ajusta finamente GPT-1 para una tarea específica (por ejemplo, generación de noticias, diálogos, etc.). Describe los pasos y resultados obtenidos.\n",
    "\n",
    "\n",
    "\n",
    "####  Este es un ejemplo de plantilla para el ajuste fino\n",
    "\n",
    "```\n",
    "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    " \n",
    "def preprocess(data):\n",
    "    return tokenizer(data['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcbbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f19f3",
   "metadata": {},
   "source": [
    "**Ejercicio 3:** Exploración de parámetros de generación\n",
    "\n",
    "Experimenta con diferentes parámetros de generación (como `max_length`, `temperature`, `top_k`, `top_p`) y analiza cómo afectan la calidad y diversidad del texto generado por GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_params(prompt, max_length=100, temperature=1.0, top_k=50, top_p=0.95):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length, temperature=temperature, top_k=top_k, top_p=top_p, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Prueba diferentes configuraciones de parámetros\n",
    "prompt = \"La revolución tecnológica\"\n",
    "generated_text = generate_text_with_params(prompt, max_length=100, temperature=0.7, top_k=30, top_p=0.8)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca955a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771927b",
   "metadata": {},
   "source": [
    "**Ejercicio 4:** Evaluación de GPT-2 en una tarea de resumen de textos\n",
    "\n",
    "Utiliza GPT-2 para generar resúmenes de textos largos. Evalúa la calidad de los resúmenes generados comparándolos con resúmenes humanos.\n",
    "\n",
    "####  Este es un ejemplo de plantilla para la generación de resúmenes\n",
    "\n",
    "```\n",
    "texts = [\n",
    "    \"El aprendizaje automático es una rama de la inteligencia artificial que se centra en el desarrollo de algoritmos que permiten a las máquinas aprender y mejorar a partir de la experiencia. ...\",\n",
    "    \"La inteligencia artificial ha avanzado significativamente en los últimos años, permitiendo aplicaciones en diversos campos como la medicina, el transporte y la robótica. ...\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    summary = generate_text_with_params(text, max_length=50, temperature=0.7, top_k=30, top_p=0.8)\n",
    "    print(f\"Texto original: {text}\")\n",
    "    print(f\"Resumen generado: {summary}\")\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c11c1",
   "metadata": {},
   "source": [
    "**Ejercicio 5:** Uso de la API de OpenAI para GPT-3\n",
    "\n",
    "Crea una cuenta en OpenAI y utiliza la API de GPT-3 para generar texto. Prueba diferentes prompts y analiza los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "api_key = 'your_openai_api_key'\n",
    "openai.api_key = api_key\n",
    "\n",
    "def generate_text_gpt3(prompt, max_length=50):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_length\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Prompt de ejemplo\n",
    "prompt = \"La inteligencia artificial en la medicina\"\n",
    "generated_text = generate_text_gpt3(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec8f86c",
   "metadata": {},
   "source": [
    "**Ejercicio 6:** Generación de resúmenes con BART\n",
    "\n",
    "Utiliza BART para generar resúmenes de artículos de noticias. Compara la calidad de los resúmenes generados con los de otros modelos como GPT-2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "def generate_summary(text, max_length=50):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=1024, truncation=True).to(device)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=max_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"El cambio climático es uno de los mayores desafíos de nuestro tiempo. Sus efectos se sienten en todo el mundo, con fenómenos meteorológicos extremos cada vez más frecuentes. ...\"\n",
    "summary = generate_summary(text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e19bf",
   "metadata": {},
   "source": [
    "**Ejercicio 7:** Traducción automática con BART\n",
    "\n",
    "Utiliza BART para realizar traducción automática entre dos idiomas. Evalúa la calidad de las traducciones generadas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32900ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f368713",
   "metadata": {},
   "source": [
    "**Ejercicio 8:** Clasificación de texto con ELECTRA\n",
    "\n",
    "Ajusta finamente ELECTRA para una tarea de clasificación de texto, como la detección de spam en correos electrónicos. Evalúa el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = 'google/electra-small-discriminator'\n",
    "tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
    "model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "# Cargar datos de ejemplo (detección de spam)\n",
    "dataset = load_dataset('sms_spam', split='train')\n",
    "\n",
    "# Preprocesamiento\n",
    "def preprocess(data):\n",
    "    return tokenizer(data['sms'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Evaluación del modelo\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0556af8f",
   "metadata": {},
   "source": [
    "**Ejercicio 9:** Generación de texto con ELECTRA\n",
    "\n",
    "Utiliza ELECTRA para generar texto a partir de un prompt dado. Compara la calidad del texto generado con el de otros modelos como GPT-2 y BART.\n",
    "\n",
    "####  Este es un ejemplo de plantilla para la generación de texto con ELECTRA (necesita ajuste fino)\n",
    "\n",
    "```\n",
    "prompt = \"La inteligencia artificial\"\n",
    "generated_text = generate_text_with_params(prompt, max_length=50, temperature=0.7, top_k=30, top_p=0.8)\n",
    "print(generated_text)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df2c102",
   "metadata": {},
   "source": [
    "**Ejercicio 10:** Completar Texto con XLNet\n",
    "\n",
    "Utiliza XLNet para completar un texto dado. Experimenta con diferentes prompts y analiza la coherencia del texto completado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "\n",
    "model_name = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "model = XLNetLMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def complete_text(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Prompt de ejemplo\n",
    "prompt = \"El futuro de la inteligencia artificial\"\n",
    "completed_text = complete_text(prompt)\n",
    "print(completed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac76f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566e43f",
   "metadata": {},
   "source": [
    "**Ejercicio 11:** Ajuste fino de XLNet para clasificación de sentimientos\n",
    "\n",
    "Ajusta finamente XLNet para una tarea de clasificación de sentimientos en un conjunto de datos de revisiones de productos. Evalúa el rendimiento del modelo.\n",
    "\n",
    "\n",
    "####  Este es un ejemplo de plantilla para la clasificación de sentimientos\n",
    "\n",
    "```\n",
    "from transformers import XLNetForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "dataset = load_dataset('imdb', split='train')\n",
    "\n",
    "def preprocess(data):\n",
    "    return tokenizer(data['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2edb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a62db",
   "metadata": {},
   "source": [
    "#### Ejercicio 12: Generación de texto a largo plazo con Transformer-XL\n",
    "\n",
    "Utiliza Transformer-XL para generar texto coherente a largo plazo a partir de un prompt dado. Compara la coherencia del texto generado con el de otros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52cc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n",
    "\n",
    "model_name = 'transfo-xl-wt103'\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(model_name)\n",
    "model = TransfoXLLMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def generate_long_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Prompt de ejemplo\n",
    "prompt = \"La historia de la inteligencia artificial comienza\"\n",
    "long_text = generate_long_text(prompt)\n",
    "print(long_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afd46a",
   "metadata": {},
   "source": [
    "**Ejercicio13:** Implementación de un modelo de difusión para generación de texto\n",
    "    \n",
    "Implementa un modelo de difusión simple para la generación de texto y evalúa su rendimiento en comparación con otros modelos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298996a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = 'gpt2'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def add_noise(tokens, noise_level=0.1):\n",
    "    noisy_tokens = tokens.clone()\n",
    "    mask = torch.rand(tokens.shape) < noise_level\n",
    "    noisy_tokens[mask] = tokenizer.pad_token_id\n",
    "    return noisy_tokens\n",
    "\n",
    "class DenoisingModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(DenoisingModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "def train_denoising_model(model, tokenizer, texts, epochs=5, noise_level=0.1):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors='pt').input_ids.to(device)\n",
    "            noisy_inputs = add_noise(inputs, noise_level)\n",
    "            outputs = model(noisy_inputs)\n",
    "            loss = criterion(outputs.view(-1, tokenizer.vocab_size), inputs.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(texts)}')\n",
    "\n",
    "texts = [\n",
    "    \"La inteligencia artificial está revolucionando el mundo.\",\n",
    "    \"El aprendizaje automático es una subdisciplina de la inteligencia artificial.\",\n",
    "    \"Los modelos de lenguaje a gran escala tienen un impacto significativo en la tecnología moderna.\"\n",
    "]\n",
    "\n",
    "denoising_model = DenoisingModel(model)\n",
    "train_denoising_model(denoising_model, tokenizer, texts)\n",
    "\n",
    "def generate_text_with_denoising(model, tokenizer, prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "    noisy_inputs = add_noise(inputs)\n",
    "    denoised_outputs = model(noisy_inputs)\n",
    "    generated_text = tokenizer.decode(denoised_outputs.argmax(dim=-1).squeeze(), skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "prompt = \"La tecnología moderna\"\n",
    "generated_text = generate_text_with_denoising(denoising_model, tokenizer, prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820283e",
   "metadata": {},
   "source": [
    "**Ejercicio 14:** Comparación de métodos de difusión\n",
    "\n",
    "Implementa varios métodos de difusión y compáralos en términos de calidad y coherencia del texto generado. Describe los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87618c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
