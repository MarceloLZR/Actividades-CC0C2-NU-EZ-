{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24210a13-913b-4d40-9b2c-ef86b7910b1d",
   "metadata": {},
   "source": [
    "### Palabras\n",
    "\n",
    "\n",
    "Dado que las palabras pueden tener diversas formas superficiales, y los morfemas son la unidad más pequeña que tiene significado, muchas tareas de procesamiento del lenguaje natural (NLP) comienzan con el análisis morfológico, es decir, dividiendo las palabras en sus componentes morfológicos. Este proceso es esencial para tareas como la lematización y la derivación, que son fundamentales en el preprocesamiento de datos para modelos de lenguaje grandes (LLM).\n",
    "\n",
    "Por ejemplo, palabras como *delighted*, *stepped* y *water* se analizarían morfológicamente para identificar sus raíces o lemas, así como sus prefijos y sufijos. Este tipo de análisis permite a los modelos de lenguaje comprender mejor las relaciones entre palabras, lo que mejora su capacidad para manejar variaciones morfológicas en la generación y comprensión de texto.\n",
    "\n",
    "---\n",
    "\n",
    "El corpus **Switchboard**, que contiene conversaciones telefónicas en inglés americano entre desconocidos, se recopiló a principios de la década de 1990. Este corpus incluye 2,430 conversaciones con una duración promedio de 6 minutos cada una, totalizando 240 horas de habla y alrededor de 3 millones de palabras. \n",
    "\n",
    "Los corpus de lenguaje hablado, como **Switchboard**, introducen otras complicaciones con respecto a la definición de palabras, que son importantes de considerar en el contexto del entrenamiento de modelos de lenguaje.\n",
    "\n",
    "Veamos una expresión del corpus **Switchboard**; una expresión es el correlato hablado de una oración:\n",
    "\n",
    "> I do uh main- mainly business data processing\n",
    "\n",
    "Esta expresión contiene dos tipos de disfluencias. La palabra interrumpida **main-** se llama un **fragmento**. Palabras como **uh** y **um** se conocen como **fillers**. ¿Deberíamos considerar estas palabras? Nuevamente, depende de la aplicación. Si estamos construyendo un sistema de transcripción de voz o un modelo de lenguaje que debe manejar lenguaje hablado, podríamos querer eventualmente eliminar las disfluencias. Sin embargo, en algunas ocasiones, como en el entrenamiento de LLMs para tareas de reconocimiento de voz o análisis de sentimientos, mantenemos estas disfluencias porque son útiles para predecir la próxima palabra o para capturar las pausas naturales en el discurso.\n",
    "\n",
    "Las disfluencias como **uh** o **um** también son útiles en el reconocimiento de voz para predecir la próxima palabra, ya que pueden indicar que el hablante está reiniciando una cláusula o idea. Por lo tanto, en el reconocimiento de voz y en modelos de lenguaje, estas disfluencias se tratan como palabras regulares. \n",
    "\n",
    "Además, dado que diferentes personas usan diferentes disfluencias, estas pueden ser una pista para la identificación del hablante, un aspecto que podría ser explotado en tareas de autenticación o personalización de modelos de lenguaje. De hecho, se ha demostrado que **uh** y **um** tienen significados diferentes, lo que podría influir en cómo un LLM modela el habla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2c64b",
   "metadata": {},
   "source": [
    "#### Definiciones: \n",
    "\n",
    "**Tipos de palabra** se refieren al número de palabras distintas en un corpus; si el conjunto de palabras en el vocabulario es **V**, el número de tipos corresponde al tamaño del vocabulario, denotado como **|V|**.\n",
    "\n",
    "**Instancias de palabra**, por otro lado, son el número total **N** de palabras que aparecen en el texto. Por ejemplo, si ignoramos la puntuación, la siguiente oración del corpus **Brown** tiene 14 tipos y 16 instancias:\n",
    "\n",
    "> They picnicked by the pool, then lay back on the grass and looked at the stars.\n",
    "\n",
    "¡Hay algunas preguntas que hacer! Por ejemplo, ¿deberíamos considerar una palabra con mayúscula inicial (como **They**) y una en minúscula (como **they**) como el mismo tipo de palabra? La respuesta depende de la tarea en cuestión. **They** y **they** podrían agruparse como el mismo tipo en tareas como el reconocimiento de voz, donde nos importa más la secuencia de palabras y menos su formato, mientras que en otras tareas, como el etiquetado de entidades nombradas (**named-entity tagging**), la capitalización es una característica útil que se conserva, ya que puede ayudar a identificar nombres propios y lugares.\n",
    "\n",
    "A veces, se desarrollan dos versiones de un modelo de **NLP** en particular: una que distingue entre mayúsculas y minúsculas, y otra que no lo hace. Esto es especialmente relevante en aplicaciones donde la precisión en el reconocimiento de nombres propios es crítica.\n",
    "\n",
    "Es útil también hacer una distinción adicional entre las **formas de palabra** y los **lemas**. \n",
    "\n",
    "Consideremos las formas inflexionadas como **cats** y **cat**. Estas dos palabras son diferentes **formas de palabra** pero comparten el mismo **lema**. Un **lema** es un conjunto de formas léxicas que comparten la misma raíz o **stem**, la misma **categoría gramatical**, y el mismo **sentido**. La **forma de palabra** es la versión completa inflexionada o derivada de la palabra. Así, **cat** y **cats** son formas de palabra distintas, pero tienen el mismo lema, que se representa como **cat**.\n",
    "\n",
    "Para lenguajes morfológicamente complejos como el árabe, la **lematización** es una tarea crucial. Sin embargo, para la mayoría de las tareas en inglés, trabajar con **formas de palabra (word forms)** suele ser suficiente, y cuando hablamos de palabras en este contexto, generalmente nos referimos a **formas de palabra**. Una de las situaciones en inglés donde los lemas son importantes es al medir el número de palabras en un diccionario. Las entradas de diccionario en negrita son una aproximación al número de lemas (aunque algunos lemas tienen múltiples formas en negrita). La edición de 1989 del Oxford English Dictionary tenía 615,000 entradas.\n",
    "\n",
    "Finalmente, es importante notar que, en la práctica, para muchas aplicaciones modernas de **NLP**, especialmente en el contexto de los modelos de lenguaje grandes (**LLM**), no usamos palabras como la unidad interna de representación. En cambio, las cadenas de entrada se tokenizan en **tokens**, que pueden ser palabras completas o partes de palabras. \n",
    "\n",
    "En **LLM** como GPT, BERT, o T5, estos tokens a menudo son subpalabras, generadas mediante técnicas como **Byte Pair Encoding (BPE)** o **WordPiece**. Esto permite a los modelos manejar vocabularios más manejables, capturando al mismo tiempo la estructura morfológica y semántica de las palabras.\n",
    "\n",
    "Por ejemplo, en un LLM, las palabras **cats** y **cat** pueden descomponerse en los tokens `cat` y `s`, permitiendo que el modelo aprenda las relaciones entre estas formas sin necesidad de representarlas por separado en su totalidad. Esto no solo mejora la eficiencia del modelo, sino que también permite una mejor generalización en tareas como la traducción automática, el análisis de sentimientos, y la generación de texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574555e2",
   "metadata": {},
   "source": [
    "### Corpus\n",
    "\n",
    "En NLP, un **corpus** es una colección extensa de textos o transcripciones de habla que se utiliza para desarrollar, entrenar y evaluar algoritmos y modelos de lenguaje. Los algoritmos de NLP son especialmente útiles cuando se aplican a múltiples idiomas, dado que existen 7,097 idiomas en el mundo, según el catálogo en línea **[Ethnologue](https://www.ethnologue.com/ethnoblog/welcome-21st-edition/)** (Simons y Fennig, 2018).\n",
    "\n",
    "Es crucial probar estos algoritmos en una variedad de idiomas, especialmente aquellos con diferentes propiedades lingüísticas. Sin embargo, existe una tendencia desafortunada a desarrollar y probar algoritmos de NLP principalmente en inglés. Incluso cuando se extienden a otros idiomas, suelen enfocarse en los idiomas oficiales de grandes naciones industrializadas como el chino, español, japonés y alemán, limitando así las herramientas a un reducido número de lenguas.\n",
    "\n",
    "Además, muchos idiomas tienen múltiples variedades, a menudo habladas en distintas regiones o por diversos grupos sociales. Por ejemplo, si un corpus incluye texto en inglés afroamericano (AAE o AAVE), las herramientas de NLP deben ser capaces de manejar las características lingüísticas específicas de esa variedad. \n",
    "\n",
    "En redes sociales como Twitter, es común encontrar construcciones como **iont** (equivalente a **I don’t** en inglés estándar) o **talmbout** (equivalente a **talking about**), que representan un desafío para la segmentación de palabras (Blodgett et al., 2016; Jones, 2015).\n",
    "\n",
    "Otro fenómeno lingüístico común es el **code switching**, donde los hablantes alternan entre varios idiomas en una misma conversación. Esto es frecuente en muchas partes del mundo. Por ejemplo:\n",
    "\n",
    "- \"Por primera vez veo a **@username** actually being hateful! it was beautiful:)\" [“For the first time I get to see @username actually being hateful! it was beautiful:)”].\n",
    "- \"**dost tha or ra- hega ... dont wory ... but dherya rakhe**\" [“he was and will remain a friend ... don’t worry ... but have faith”].\n",
    "\n",
    "Además de la diversidad de idiomas y dialectos, los corpus pueden incluir textos de diferentes géneros, como noticias, literatura de ficción y no ficción, artículos científicos o transcripciones de conversaciones cotidianas. También reflejan características demográficas de los autores, como edad, género, raza y clase socioeconómica, todas ellas influenciando las propiedades lingüísticas del texto.\n",
    "\n",
    "El tiempo es otro factor importante, ya que el lenguaje cambia con el tiempo. Algunos corpus contienen textos de diferentes periodos históricos, lo que permite estudiar la evolución del lenguaje.\n",
    "\n",
    "Dado que el lenguaje está profundamente contextualizado, es esencial considerar quién produjo el lenguaje, en qué contexto y con qué propósito al desarrollar modelos computacionales basados en corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d2974",
   "metadata": {},
   "source": [
    "#### Dos conceptos importantes\n",
    "\n",
    "**Datasheet** es un documento que proporciona detalles técnicos y metodológicos sobre un corpus, ofreciendo un marco estandarizado para describir las características, procesos y limitaciones del conjunto de datos. El propósito de una datasheet es garantizar la transparencia y facilitar la reproducibilidad de los resultados obtenidos con el corpus.\n",
    "\n",
    "**Data statement** es un documento que contextualiza un corpus dentro de un marco social, ético y lingüístico más amplio. Incluye información sobre las intenciones detrás de la creación del corpus, sus limitaciones éticas y sociales, y cómo deberían interpretarse los datos. Un data statement ayuda a los investigadores a entender el impacto potencial de los modelos entrenados con ese corpus en diferentes comunidades y contextos.\n",
    "\n",
    "Para conocer estos detalles, se recomienda que los creadores del corpus proporcionen una **datasheet** o una **data statement**. Estas especificaciones incluyen:\n",
    "\n",
    "- **Motivación**: Razón detrás de la recopilación del corpus, quién lo realizó y quién lo financió.\n",
    "- **Situación**: Contexto en el que se creó el texto, si era hablado o escrito, editado o espontáneo, monólogo o diálogo.\n",
    "- **Variedad de idioma**: Idioma y dialecto del corpus.\n",
    "- **Demografía del hablante**: Edad, género y otras características demográficas de los autores.\n",
    "- **Proceso de recopilación**: Tamaño del corpus, métodos de muestreo, consentimiento para la recopilación, preprocesamiento y metadatos disponibles.\n",
    "- **Proceso de anotación**: Tipo de anotaciones, demografía y capacitación de los anotadores.\n",
    "- **Distribución**: Restricciones de derechos de autor u otras limitaciones legales.\n",
    "\n",
    "\n",
    "Con todo esto, un corpus es una herramienta esencial en NLP para capturar la diversidad lingüística y asegurar que los modelos desarrollados sean efectivos y representativos de la variedad de usos del lenguaje en el mundo real. La inclusión de una datasheet o un data statement proporciona una comprensión más profunda del corpus y garantiza que se utilice de manera ética y transparente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657dd2ca",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "**1: Tokenización y cálculo de frecuencia de palabras**\n",
    "\n",
    "   **Descripción**: Usa herramientas de Unix para tokenizar un texto, normalizar las palabras a minúsculas, y calcular la frecuencia de cada palabra en un corpus.\n",
    "   \n",
    "   **Instrucciones**:\n",
    "   - Descarga un corpus en inglés (por ejemplo, una obra de Shakespeare o un archivo de texto grande).\n",
    "   - Utiliza los comandos de Unix mencionados (`tr`, `sort`, `uniq`) para tokenizar el texto y calcular la frecuencia de palabras.\n",
    "   - Genera una lista de las 10 palabras más frecuentes en el corpus.\n",
    "   \n",
    "   **Comando sugerido**:\n",
    "   ```bash\n",
    "   tr -sc 'A-Za-z' '\\n' < corpus.txt | tr A-Z a-z | sort | uniq -c | sort -n -r | head -10\n",
    "   ```\n",
    "   \n",
    "   **Resultados esperados**: Una lista de las palabras más frecuentes y sus recuentos.\n",
    "\n",
    "**2: Análisis comparativo de frecuencias en diferentes géneros**\n",
    "\n",
    "   **Descripción**: Compara las frecuencias de palabras en diferentes géneros de texto utilizando herramientas de Unix.\n",
    "   \n",
    "   **Instrucciones**:\n",
    "   - Selecciona dos textos de géneros distintos (por ejemplo, un artículo científico y una obra de teatro).\n",
    "   - Tokeniza ambos textos utilizando el proceso descrito anteriormente.\n",
    "   - Calcula la frecuencia de palabras en cada texto.\n",
    "   - Compara las 10 palabras más frecuentes en ambos textos.\n",
    "   \n",
    "   **Comando sugerido**:\n",
    "   ```bash\n",
    "   tr -sc 'A-Za-z' '\\n' < genre1.txt | tr A-Z a-z | sort | uniq -c | sort -n -r | head -10\n",
    "   tr -sc 'A-Za-z' '\\n' < genre2.txt | tr A-Z a-z | sort | uniq -c | sort -n -r | head -10\n",
    "   ```\n",
    "   \n",
    "   **Resultados esperados**: Una comparación de las frecuencias de palabras y un análisis de las diferencias según el género del texto.\n",
    "\n",
    "**3: Exploración de tokenización multilingüe**\n",
    "\n",
    "   **Descripción**: Investiga cómo las herramientas de Unix manejan la tokenización en diferentes idiomas.\n",
    "   \n",
    "   **Instrucciones**:\n",
    "   - Descarga un corpus en un idioma no inglés (por ejemplo, español o francés).\n",
    "   - Aplica los comandos de Unix para tokenizar y normalizar el texto.\n",
    "   - Observa y analiza cómo las herramientas manejan los caracteres especiales o acentuados.\n",
    "   \n",
    "   **Comando sugerido**:\n",
    "   ```bash\n",
    "   tr -sc 'A-Za-zÁÉÍÓÚáéíóúÑñ' '\\n' < corpus_spanish.txt | tr A-ZÁÉÍÓÚÑ a-záéíóúñ | sort | uniq -c | sort -n -r | head -10\n",
    "   ```\n",
    "   \n",
    "   **Resultados esperados**: Un análisis de cómo las herramientas de Unix tokenizan el texto en un idioma diferente y qué desafíos podrían presentarse.\n",
    "   \n",
    "   \n",
    "**4: Segmentación de oraciones en un corpus**\n",
    "\n",
    "   **Descripción**: Utiliza herramientas de Unix para realizar una segmentación básica de oraciones en un corpus.\n",
    "   \n",
    "   **Instrucciones**:\n",
    "   - Selecciona un corpus de texto.\n",
    "   - Escribe un comando que divida el texto en oraciones, considerando que cada punto seguido por un espacio (`.`) indica el final de una oración.\n",
    "   - Cuenta el número de oraciones en el texto.\n",
    "   \n",
    "   **Comando sugerido**:\n",
    "   ```bash\n",
    "   tr '.' '\\n' < corpus.txt | wc -l\n",
    "   ```\n",
    "   \n",
    "   **Resultados esperados**: El número total de oraciones en el corpus, junto con ejemplos de algunas oraciones tokenizadas.\n",
    "\n",
    "**5: Análisis de textos en diferentes periodos históricos**\n",
    "\n",
    "   **Descripción**: Analiza cómo cambia el uso de palabras en textos a lo largo del tiempo.\n",
    "   \n",
    "   **Instrucciones**:\n",
    "   - Selecciona textos de diferentes periodos históricos (por ejemplo, textos del siglo XVIII y textos contemporáneos).\n",
    "   - Tokeniza y normaliza ambos textos utilizando los comandos de Unix.\n",
    "   - Compara las palabras más frecuentes y analiza cómo ha cambiado el lenguaje a lo largo del tiempo.\n",
    "   \n",
    "   **Comando sugerido**:\n",
    "   ```bash\n",
    "   tr -sc 'A-Za-z' '\\n' < old_text.txt | tr A-Z a-z | sort | uniq -c | sort -n -r | head -10\n",
    "   tr -sc 'A-Za-z' '\\n' < modern_text.txt | tr A-Z a-z | sort | uniq -c | sort -n -r | head -10\n",
    "   ```\n",
    "   \n",
    "   **Resultados esperados**: Un análisis de las diferencias en el uso de palabras entre los textos de diferentes periodos históricos.\n",
    "\n",
    "Estos ejercicios ayudarán a los estudiantes a aplicar conceptos de normalización y tokenización de texto utilizando herramientas simples de Unix, y a explorar las diferencias en el procesamiento de textos en diferentes idiomas y contextos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5fb96",
   "metadata": {},
   "source": [
    "### Tokenización de palabras y subpalabras\n",
    "\n",
    "\n",
    "Existen dos tipos principales de algoritmos de tokenización en NLP: **tokenización Top-down** y **tokenización bottom-up**. En la tokenización top-down, se definen estándares y reglas para segmentar el texto, mientras que en la tokenización bottom-up, se utilizan estadísticas de secuencias de letras para dividir las palabras en subpalabras, que pueden ser palabras completas, partes de palabras o letras individuales.\n",
    "\n",
    "La **tokenización top-down** suele mantener la puntuación, números, y caracteres especiales, como en **m.p.h.**, **$45.55**, y URLs, tratándolos como tokens separados cuando es necesario. También maneja la expansión de contracciones clíticas, como convertir **doesn’t** en **does** y **n’t**. Este tipo de tokenización es común en estándares como el **Penn Treebank**, que separa la puntuación y los clíticos, pero mantiene juntas palabras con guiones.\n",
    "\n",
    "En la práctica, dado que la tokenización se ejecuta antes de cualquier otro procesamiento del lenguaje, necesita ser muy rápida. \n",
    "\n",
    "Para la tokenización de palabras, generalmente usamos algoritmos deterministas basados en expresiones regulares compiladas en **autómatas finitos** eficientes. Por ejemplo el codigo siguiente muestra una expresión regular básica que puede usarse para tokenizar el inglés con la función **nltk.regexp_tokenize** del **Natural Language Toolkit (NLTK)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# Texto de entrada\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "# Patrón para la tokenización\n",
    "pattern = r'''(?x)                   # activar verbose mode\n",
    "              (?:[A-Z]\\.)+           # abreviaturas, por ejemplo, U.S.A.\n",
    "            | \\w+(?:-\\w+)*           # palabras con guiones internos opcionales\n",
    "            | \\$?\\d+(?:\\.\\d+)?%?     # monedas, porcentajes, ej. $12.40, 82%\n",
    "            | \\.\\.\\.                 # elipsis\n",
    "            | [][.,;\"'?():-_`]       # tokens separados; incluye ], [\n",
    "            '''\n",
    "\n",
    "# Aplicando la tokenización usando el patrón\n",
    "tokens = nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "# Mostrando los tokens resultantes\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4beb4b5",
   "metadata": {},
   "source": [
    "Este código utiliza la biblioteca **nltk** (Natural Language Toolkit) para realizar la tokenización de un texto de entrada. La tokenización es el proceso de dividir un texto en unidades más pequeñas, llamadas **tokens**, que pueden ser palabras, números, símbolos o cualquier otra cosa que sea significativa en el contexto del análisis de texto.\n",
    "\n",
    "* Aquí se importan las bibliotecas necesarias. **nltk** es una biblioteca popular para el procesamiento de lenguaje natural, y **re** es el módulo de expresiones regulares de Python, que permite realizar operaciones avanzadas de búsqueda de patrones.\n",
    "\n",
    "* Este es el texto que se va a tokenizar: **'That U.S.A. poster-print costs $12.40...**\n",
    "\n",
    "* El patrón dado de esta expresión regular define cómo se deben identificar y separar los tokens. \n",
    "\n",
    "   - **`(?x)`**: Esto activa el **verbose mode**. En modo verbose, los espacios en blanco y los comentarios son ignorados en el patrón de expresión regular, lo que permite que el patrón se divida en varias líneas y se incluyan comentarios, mejorando la legibilidad.\n",
    "\n",
    "   - **`(?:[A-Z]\\.)+`**: Coincide con abreviaturas formadas por letras mayúsculas seguidas de un punto, como \"U.S.A.\".\n",
    "\n",
    "   - **`\\w+(?:-\\w+)*`**: Coincide con palabras que pueden contener guiones internos, como \"poster-print\".\n",
    "\n",
    "   - **`\\$?\\d+(?:\\.\\d+)?%?`**: Coincide con números que pueden representar monedas (como \"$12.40\") o porcentajes (como \"82%\").\n",
    "\n",
    "   - **`\\.\\.\\.`**: Coincide con una elipsis (\"...\").\n",
    "\n",
    "   - **`[][.,;\"'?():-_`]`**: Coincide con varios caracteres de puntuación y símbolos que se tratan como tokens individuales.\n",
    "   \n",
    "* Esta línea aplica la expresión regular definida en `pattern` para tokenizar el texto. La función `nltk.regexp_tokenize` realiza la tokenización basada en el patrón especificado `tokens = nltk.regexp_tokenize(text, pattern)`\n",
    "* Se imprimen los tokens resultantes de la tokenización. El resultado será una lista de tokens que han sido extraídos del texto original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7d729",
   "metadata": {},
   "source": [
    "**Ejercicio 1**: Modifica el patrón para reconocer direcciones de correo electrónico como un solo token. Por ejemplo, en el texto \"Contacta a john.doe@example.com\", el patrón debería tokenizar \"john.doe@example.com\" como un solo token.\n",
    "\n",
    "   ```python\n",
    "   text = 'Contacta a john.doe@example.com'\n",
    "   pattern = r'''(?x)\n",
    "                 (?:[A-Z]\\.)+\n",
    "               | \\w+(?:-\\w+)*           \n",
    "               | \\$?\\d+(?:\\.\\d+)?%?     \n",
    "               | \\.\\.\\.                 \n",
    "               | [][.,;\"'?():-_`]       \n",
    "               | [a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\n",
    "             '''\n",
    "   tokens = nltk.regexp_tokenize(text, pattern)\n",
    "   print(tokens)\n",
    "   ```\n",
    "\n",
    "**Ejercicio 2**: Amplía el patrón para capturar hashtags de redes sociales como un solo token. Por ejemplo, el texto \"El evento fue increíble #AmazingEvent\" debería tokenizar \"#AmazingEvent\" como un solo token.\n",
    "\n",
    "   ```python\n",
    "   text = 'El evento fue increíble #AmazingEvent'\n",
    "   pattern = r'''(?x)\n",
    "                 (?:[A-Z]\\.)+\n",
    "               | \\w+(?:-\\w+)*           \n",
    "               | \\$?\\d+(?:\\.\\d+)?%?     \n",
    "               | \\.\\.\\.                 \n",
    "               | [][.,;\"'?():-_`]       \n",
    "               | #[\\w-]+                # hashtags\n",
    "             '''\n",
    "   tokens = nltk.regexp_tokenize(text, pattern)\n",
    "   print(tokens)\n",
    "   ```\n",
    "\n",
    "**Ejercicio 3**: Crea un nuevo patrón que pueda tokenizar URLs como un solo token. \n",
    "Por ejemplo, en el texto \"Visita https://www.example.com para más detalles\", la URL debería ser un solo token.\n",
    "\n",
    "   ```python\n",
    "   text = 'Visita https://www.example.com para más detalles'\n",
    "   pattern = r'''(?x)\n",
    "                 (?:[A-Z]\\.)+\n",
    "               | \\w+(?:-\\w+)*           \n",
    "               | \\$?\\d+(?:\\.\\d+)?%?     \n",
    "               | \\.\\.\\.                 \n",
    "               | [][.,;\"'?():-_`]       \n",
    "               | https?://[a-zA-Z0-9./-]+  # URLs\n",
    "             '''\n",
    "   tokens = nltk.regexp_tokenize(text, pattern)\n",
    "   print(tokens)\n",
    "   ```\n",
    "  \n",
    "\n",
    "**Ejercicio 4**: Modifica el patrón para reconocer fechas en formato \"DD/MM/YYYY\" o \"MM-DD-YYYY\" como un solo token. Por ejemplo, en el texto \"La reunión es el 25/12/2024 o el 12-25-2024\", las fechas deben ser tokenizadas como tokens únicos.\n",
    "\n",
    "   ```python\n",
    "   text = 'La reunión es el 25/12/2024 o el 12-25-2024'\n",
    "   pattern = r'''(?x)\n",
    "                 (?:[A-Z]\\.)+\n",
    "               | \\w+(?:-\\w+)*           \n",
    "               | \\$?\\d+(?:\\.\\d+)?%?     \n",
    "               | \\.\\.\\.                 \n",
    "               | [][.,;\"'?():-_`]       \n",
    "               | \\d{2}/\\d{2}/\\d{4}      # fechas en formato DD/MM/YYYY\n",
    "               | \\d{2}-\\d{2}-\\d{4}      # fechas en formato MM-DD-YYYY\n",
    "             '''\n",
    "   tokens = nltk.regexp_tokenize(text, pattern)\n",
    "   print(tokens)\n",
    "   ```\n",
    "\n",
    "**Ejercicio 5**: Añade un patrón para capturar menciones de usuario en redes sociales (por ejemplo, @usuario) como un solo token. En el texto \"Gracias @user1 por tu ayuda, y @user2 también!\", las menciones deben ser tokenizadas como \"@user1\" y \"@user2\".\n",
    "\n",
    "   ```python\n",
    "   text = 'Gracias @user1 por tu ayuda, y @user2 también!'\n",
    "   pattern = r'''(?x)\n",
    "                 (?:[A-Z]\\.)+\n",
    "               | \\w+(?:-\\w+)*           \n",
    "               | \\$?\\d+(?:\\.\\d+)?%?     \n",
    "               | \\.\\.\\.                 \n",
    "               | [][.,;\"'?():-_`]       \n",
    "               | @[a-zA-Z0-9_]+         # menciones de usuario\n",
    "             '''\n",
    "   tokens = nltk.regexp_tokenize(text, pattern)\n",
    "   print(tokens)\n",
    "   ```\n",
    "\n",
    "**Ejercicio 6**: Modifica el patrón para reconocer números de teléfono en diferentes formatos (por ejemplo, \"(123) 456-7890\", \"123-456-7890\", \"123.456.7890\") como un solo token. En el texto \"Llama al (123) 456-7890 o al 123-456-7890\", los números de teléfono deben ser tokens únicos.\n",
    "\n",
    "   ```python\n",
    "   text = 'Llama al (123) 456-7890 o al 123-456-7890'\n",
    "   pattern = r'''(?x)\n",
    "                 (?:[A-Z]\\.)+\n",
    "               | \\w+(?:-\\w+)*           \n",
    "               | \\$?\\d+(?:\\.\\d+)?%?     \n",
    "               | \\.\\.\\.                 \n",
    "               | [][.,;\"'?():-_`]       \n",
    "               | \\(\\d{3}\\)\\s?\\d{3}-\\d{4}  # números de teléfono en formato (123) 456-7890\n",
    "               | \\d{3}[-.]\\d{3}[-.]\\d{4} # números de teléfono en formato 123-456-7890 o 123.456.7890\n",
    "             '''\n",
    "   tokens = nltk.regexp_tokenize(text, pattern)\n",
    "   print(tokens)\n",
    "   ```\n",
    "\n",
    "**Ejercicio 7**: Extiende el patrón para reconocer emoticonos simples como \":)\", \":(\", \":D\", \"XD\", etc., como un solo token. En el texto \"Hoy estoy muy feliz :D pero ayer estaba triste :(\", los emoticonos deben ser tokenizados correctamente.\n",
    "\n",
    "   ```python\n",
    "   text = 'Hoy estoy muy feliz :D pero ayer estaba triste :('\n",
    "   pattern = r'''(?x)\n",
    "                 (?:[A-Z]\\.)+\n",
    "               | \\w+(?:-\\w+)*           \n",
    "               | \\$?\\d+(?:\\.\\d+)?%?     \n",
    "               | \\.\\.\\.                 \n",
    "               | [][.,;\"'?():-_`]       \n",
    "               | [:;xX8]-?[)D(]          # emoticonos simples\n",
    "             '''\n",
    "   tokens = nltk.regexp_tokenize(text, pattern)\n",
    "   print(tokens)\n",
    "   ```\n",
    "\n",
    "**Ejercicio 8**: Crea un patrón que pueda tokenizar secuencias alfanuméricas complejas, como códigos de producto o identificadores únicos, que puedan contener letras, números y guiones. Por ejemplo, en el texto \"El código de producto es AB12-XY34-Z789\", cada código debe ser un token único.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b0ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6faab",
   "metadata": {},
   "source": [
    "La tokenización de palabras es más compleja en idiomas como el chino, japonés y tailandés, que no utilizan espacios para marcar los posibles límites de palabras. En chino, por ejemplo, las palabras están compuestas de caracteres (llamados **hanzi** en chino). \n",
    "\n",
    "Cada carácter generalmente representa una unidad de significado (**morfema**) y se pronuncia como una sola sílaba. Las palabras tienen una longitud promedio de alrededor de 2.4 caracteres. Sin embargo, decidir qué cuenta como una palabra en chino es complejo. Por ejemplo, considera la siguiente oración:\n",
    "\n",
    "```plaintext\n",
    "(2.21) 姚明进入总决赛\n",
    "yáo míng jìn rù zǒng jué sài\n",
    "“Yao Ming reaches the finals”\n",
    "```\n",
    "\n",
    "Esto podría tratarse como 3 palabras (segmentación según el **Chinese Treebank**):\n",
    "\n",
    "```plaintext\n",
    "(2.22) 姚明\n",
    "进入 总决赛\n",
    "Yao Ming reaches finals\n",
    "```\n",
    "\n",
    "o como 5 palabras (segmentación según la **Peking University**):\n",
    "\n",
    "```plaintext\n",
    "(2.23) 姚 明 进入 总 决赛\n",
    "Yao Ming reaches overall finals\n",
    "```\n",
    "\n",
    "Finalmente, es posible en chino simplemente ignorar las palabras por completo y usar caracteres como las unidades básicas, tratando la oración como una serie de 7 caracteres:\n",
    "\n",
    "```plaintext\n",
    "(2.24) 姚 明 进 入 总 决 赛\n",
    "Yao Ming enter enter overall decision game\n",
    "```\n",
    "\n",
    "De hecho, para la mayoría de las tareas de **NLP** en chino, resulta más eficiente utilizar caracteres en lugar de palabras como unidades de entrada, ya que los caracteres representan un nivel semántico adecuado para la mayoría de las aplicaciones. Además, los estándares de segmentación en palabras tienden a generar un vocabulario enorme con un gran número de palabras poco frecuentes.\n",
    "\n",
    "Sin embargo, en japonés y tailandés, el carácter es una unidad demasiado pequeña, por lo que se requieren algoritmos para la **segmentación de palabras**. Estos algoritmos también pueden ser útiles en chino en las raras situaciones en las que se necesitan límites de palabras en lugar de caracteres.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a11098",
   "metadata": {},
   "source": [
    "### Ejemplo de algoritmo de tokenización top-down\n",
    "\n",
    "El enfoque *top-down* para la tokenización implica comenzar con una estructura global o de alto nivel y descomponerla en partes más pequeñas o tokens. Este enfoque se utiliza a menudo en compiladores y analizadores sintácticos donde se tiene una visión general del lenguaje o de la estructura que se está analizando y luego se descompone en tokens.\n",
    "\n",
    "**Ejemplo**: Imagina que estás tokenizando una expresión matemática simple.\n",
    "\n",
    "**Expresión**: `\"3 + 5 * (10 - 2)\"`\n",
    "\n",
    "**Proceso de tokenización (top-down)**:\n",
    "1. **Inicio con la expresión completa**: Se reconoce que toda la cadena es una expresión matemática.\n",
    "2. **Descomposición en partes mayores**:\n",
    "   - `\"3\"` → Número.\n",
    "   - `\"+\"` → Operador.\n",
    "   - `\"5\"` → Número.\n",
    "   - `\"*\"` → Operador.\n",
    "   - `\"(\"` → Paréntesis de apertura.\n",
    "   - `\"10\"` → Número.\n",
    "   - `\"-\"` → Operador.\n",
    "   - `\"2\"` → Número.\n",
    "   - `\")\"` → Paréntesis de cierre.\n",
    "3. **Descomposición de cada parte en tokens**:\n",
    "   - `\"3\"` → Token de número.\n",
    "   - `\"+\"` → Token de operador.\n",
    "   - `\"5\"` → Token de número.\n",
    "   - `\"*\"` → Token de operador.\n",
    "   - `\"(\"` → Token de paréntesis de apertura.\n",
    "   - `\"10\"` → Token de número.\n",
    "   - `\"-\"` → Token de operador.\n",
    "   - `\"2\"` → Token de número.\n",
    "   - `\")\"` → Token de paréntesis de cierre.\n",
    "\n",
    "**Resultado**:\n",
    "- Tokens generados: `[3, +, 5, *, (, 10, -, 2, )]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708cc84",
   "metadata": {},
   "source": [
    "Supongamos que tenemos la siguiente oración en inglés:\n",
    "\n",
    "**Texto:** `\"The quick brown fox jumps over the lazy dog.\"`\n",
    "\n",
    "\n",
    "**Paso 1: Identificar frases o cláusulas**\n",
    "\n",
    "En un enfoque *top-down*, podríamos primero descomponer el texto en frases o cláusulas más grandes. Sin embargo, dado que esta oración no tiene múltiples cláusulas separadas por comas o conjunciones, consideraremos la oración completa como una unidad.\n",
    "\n",
    "**Resultado del paso 1:**\n",
    "\n",
    "```plaintext\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "\n",
    "**Paso 2: Descomponer en palabras**\n",
    "\n",
    "El siguiente paso es descomponer la oración en palabras. En un enfoque *top-down*, nos movemos desde la estructura más grande (la oración) hacia componentes más pequeños (las palabras).\n",
    "\n",
    "**Resultado del paso 2:**\n",
    "\n",
    "```plaintext\n",
    "[\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "```\n",
    "\n",
    "**Paso 3: Descomponer palabras en prefijos, raíces y sufijos\n",
    "\n",
    "Podríamos seguir descomponiendo cada palabra en prefijos, raíces y sufijos, si es necesario. Esto es especialmente útil en lenguajes con morfología compleja o en tareas de análisis morfológico.\n",
    "\n",
    "**Ejemplo de descomposición de palabras:**\n",
    "\n",
    "- `\"jumps\"` → `[\"jump\", \"s\"]` (raíz y sufijo)\n",
    "- `\"quick\"` → `[\"quick\"]` (palabra base)\n",
    "- `\"lazy\"` → `[\"lazy\"]` (palabra base)\n",
    "\n",
    "**Resultado del Paso 3:**\n",
    "\n",
    "```plaintext\n",
    "[\"The\", \"quick\", \"brown\", \"fox\", \"jump\", \"s\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "```\n",
    "\n",
    "**Descomponer en caracteres (opcional)**\n",
    "\n",
    "Si quisiéramos ir aún más lejos, podríamos descomponer cada palabra en caracteres individuales.\n",
    "\n",
    "**Ejemplo de descomposición en caracteres:**\n",
    "\n",
    "- `\"jump\"` → `[\"j\", \"u\", \"m\", \"p\"]`\n",
    "- `\"s\"` → `[\"s\"]`\n",
    "- `\"quick\"` → `[\"q\", \"u\", \"i\", \"c\", \"k\"]`\n",
    "\n",
    "**Resultado del paso 4:**\n",
    "\n",
    "```plaintext\n",
    "[\"T\", \"h\", \"e\", \"q\", \"u\", \"i\", \"c\", \"k\", \"b\", \"r\", \"o\", \"w\", \"n\", \"f\", \"o\", \"x\", \"j\", \"u\", \"m\", \"p\", \"s\", \"o\", \"v\", \"e\", \"r\", \"t\", \"h\", \"e\", \"l\", \"a\", \"z\", \"y\", \"d\", \"o\", \"g\"]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0dc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_top_down(text):\n",
    "    # Paso 1: Tokenización en palabras (top-level)\n",
    "    words = tokenize_sentence(text)\n",
    "    \n",
    "    # Paso 2: Descomposición de palabras en sub-palabras o caracteres\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        tokens.extend(tokenize_word(word))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    # Utilizamos una expresión regular simple para dividir la oración en palabras y puntuación\n",
    "    return re.findall(r'\\w+|[^\\w\\s]', sentence, re.UNICODE)\n",
    "\n",
    "def tokenize_word(word):\n",
    "    # Aquí podríamos implementar un tokenizador más sofisticado, pero por simplicidad\n",
    "    # lo dividimos en caracteres individuales\n",
    "    return list(word)\n",
    "\n",
    "# Ejemplo de uso\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = tokenize_top_down(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d617b53d",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "**1: Implementación de tokenización de arriba top-down**\n",
    "\n",
    "**Descripción**: Implementa un algoritmo de tokenización top-down utilizando expresiones regulares en Python.\n",
    "   \n",
    "   **Instrucciones**:\n",
    "   - Escribe un script en Python que tokenice un texto manteniendo la puntuación, los números, y las contracciones clíticas.\n",
    "   - Usa la función `nltk.regexp_tokenize` para aplicar un patrón de tokenización que maneje abreviaturas, palabras con guiones, monedas, y elipsis.\n",
    "   - Prueba tu tokenizador con un texto diverso.\n",
    "   \n",
    "   **Resultados esperados**: Una lista de tokens que separa las palabras, manteniendo los signos de puntuación y otros caracteres especiales como tokens separados.\n",
    "\n",
    "**2: Comparación de tokenización en idiomas diferentes**\n",
    "\n",
    "   **Descripción**: Compara cómo funciona la tokenización en inglés frente a otro idioma que no utiliza espacios como delimitadores de palabras (por ejemplo, chino o japonés).\n",
    "   \n",
    "   **Instrucciones**:\n",
    "   - Elige una oración en inglés y otra en chino o japonés.\n",
    "   - Aplica una tokenización de arriba hacia abajo al texto en inglés utilizando NLTK.\n",
    "   - Utiliza una librería específica para la tokenización en chino o japonés (por ejemplo, `jieba` para chino).\n",
    "   - Analiza las diferencias en los resultados y discute los desafíos que presenta la tokenización en cada idioma.\n",
    "   \n",
    "   **Resultados esperados**: Un informe comparativo que discuta las diferencias y desafíos en la tokenización entre ambos idiomas.\n",
    "\n",
    "**3: Tokenización de subpalabras**\n",
    "\n",
    "   **Descripción**: Implementa un algoritmo de tokenización top-down que divida las palabras en subpalabras.\n",
    "   \n",
    "   **Instrucciones**:\n",
    "   - Escribe un script en Python que utilice estadísticas simples de secuencias de letras para dividir palabras en subpalabras.\n",
    "   - Implementa un tokenizador básico que divida palabras compuestas por guiones y otras marcas internas, y que también separe letras individuales cuando no se formen palabras completas.\n",
    "   - Aplica tu tokenizador a una lista de palabras como **\"unbreakable\"**, **\"m.p.h.\"**, y **\"U.S.A.\"**.\n",
    "   \n",
    "   **Resultados esperados**: Una lista de subpalabras o letras individuales derivadas de las palabras de entrada.\n",
    "\n",
    "**4: Tokenización y segmentación en Chino**\n",
    "\n",
    "   **Descripción**: Experimenta con diferentes enfoques de tokenización en chino y analiza cuál es más efectivo para una tarea de NLP específica.\n",
    "   \n",
    "   **Instrucciones**:\n",
    "   - Utiliza la librería `jieba` para realizar una segmentación de palabras en chino.\n",
    "   - Aplica la segmentación de caracteres como alternativa.\n",
    "   - Compara los resultados de ambos métodos utilizando un corpus de texto chino.\n",
    "   - Evalúa cuál método es más apropiado para una tarea específica, como análisis de sentimientos o reconocimiento de entidades nombradas.\n",
    "   \n",
    "   **Resultados esperados**: Un análisis comparativo que determine cuándo es más eficaz utilizar caracteres en lugar de palabras como unidades de tokenización en chino.\n",
    "\n",
    "**5: Tokenización multilingüe**\n",
    "\n",
    "   **Descripción**: Desarrolla un tokenizador que pueda manejar múltiples idiomas, incluyendo aquellos que no utilizan espacios como delimitadores de palabras.\n",
    "   \n",
    "   **Instrucciones**:\n",
    "   - Implementa un tokenizador que combine enfoques de arriba hacia abajo y abajo hacia arriba.\n",
    "   - Asegúrate de que el tokenizador pueda manejar idiomas como inglés, chino, y japonés.\n",
    "   - Prueba tu tokenizador con un conjunto de textos multilingües.\n",
    "   \n",
    "   **Resultados esperados**: Un tokenizador versátil que funcione bien en diferentes idiomas y un análisis de su precisión en cada idioma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cb7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10373daa",
   "metadata": {},
   "source": [
    "### **Byte-Pair Encoding (BPE)**: Un algoritmo de tokenización bottom-up\n",
    "\n",
    "Hay otra opción para tokenizar texto, una que se usa más comúnmente en modelos de lenguaje grandes. En lugar de definir tokens como palabras (ya sea delimitadas por espacios o por algoritmos más complejos), o como caracteres (como en chino), podemos usar nuestros datos para determinar automáticamente cuáles deberían ser los tokens. Esto es especialmente útil para lidiar con palabras desconocidas, un problema importante en el procesamiento del lenguaje.\n",
    "\n",
    "Por ejemplo, si nuestro corpus de entrenamiento contiene las palabras **low**, **new**, **newer**, pero no **lower**, entonces si la palabra **lower** aparece en nuestro corpus de prueba, nuestro sistema no sabrá qué hacer con ella.\n",
    "\n",
    "Para lidiar con este problema de palabras desconocidas, los **tokenizadores** modernos inducen automáticamente conjuntos de tokens que incluyen tokens más pequeños que las palabras, llamados **subwords**. Las **subwords** pueden ser subcadenas arbitrarias o pueden ser unidades portadoras de significado como los morfemas **-est** o **-er**. \n",
    "\n",
    "(Un **morfema** es la unidad más pequeña portadora de significado de un idioma; por ejemplo, la palabra **unwashable** tiene los morfemas **un-**, **wash** y **-able**). En los esquemas de tokenización modernos, la mayoría de los tokens son palabras, pero algunos tokens son morfemas u otras **subwords** que ocurren con frecuencia, como **-er**. Cada palabra no vista, como **lower**, puede, por lo tanto, representarse mediante alguna secuencia de unidades **subword** conocidas, como **low** y **er**, o incluso como una secuencia de letras individuales si es necesario.\n",
    "\n",
    "La mayoría de los esquemas de tokenización tienen dos partes: un **token learner** (aprendiz de tokens) y un **token segmenter** (segmentador de tokens). El **token learner** toma un corpus de entrenamiento sin procesar (a veces más o menos pre-separado en palabras, por ejemplo, por espacios en blanco) e induce un vocabulario, un conjunto de tokens. El **token segmenter** toma una oración de prueba sin procesar y la segmenta en los tokens del vocabulario. Dos algoritmos son ampliamente utilizados: **[byte-pair encoding (BPE)](https://aclanthology.org/P16-1162/)** y **[unigram language modeling](https://doi.org/10.18653/v1/P18-1007)**. \n",
    "\n",
    "También hay una biblioteca llamada **[SentencePiece](https://github.com/google/sentencepiece)** que incluye implementaciones de ambos y a menudo se usa el nombre **SentencePiece** para simplemente referirse a la tokenización mediante **unigram language modeling**.\n",
    "\n",
    "#### **El algoritmo byte-pair encoding (BPE)**\n",
    "\n",
    "BPE es un algoritmo que comienza con un vocabulario inicial compuesto únicamente por caracteres individuales. A partir de ahí, examina un corpus de entrenamiento y busca los pares de caracteres más frecuentemente adyacentes. Luego, fusiona estos pares en un nuevo símbolo o token, que se agrega al vocabulario. Este proceso se repite hasta que se han realizado un número predefinido de fusiones, creando así un conjunto de tokens que puede incluir tanto palabras completas como subpalabras y morfemas.\n",
    "\n",
    "El **token learner** de **BPE** comienza con un vocabulario que es solo el conjunto de todos los caracteres individuales. Luego examina el corpus de entrenamiento, elige los dos símbolos que están más frecuentemente adyacentes (por ejemplo, **‘A’** y **‘B’**), agrega un nuevo símbolo fusionado **‘AB’** al vocabulario y reemplaza cada **‘A’** **‘B’** adyacente en el corpus con el nuevo **‘AB’**. Continúa contando y fusionando, creando nuevas cadenas de caracteres cada vez más largas, hasta que se han realizado **k** fusiones, creando **k** tokens nuevos; **k** es, por lo tanto, un parámetro del algoritmo. El vocabulario resultante consiste en el conjunto original de caracteres más **k** nuevos símbolos.\n",
    "\n",
    "El algoritmo generalmente se ejecuta dentro de las palabras, por lo que el corpus de entrada se separa primero por espacios en blanco para dar un conjunto de cadenas, cada una correspondiente a los caracteres de una palabra, más un símbolo especial de fin de palabra **_** y sus recuentos. Veamos su operación en el siguiente pequeño corpus de entrada de 18 tokens de palabras con recuentos para cada palabra (la palabra **low** aparece 5 veces, la palabra **newer** 6 veces, y así sucesivamente), que tendría un vocabulario inicial de 11 letras:\n",
    "\n",
    "```plaintext\n",
    "corpus\n",
    "5  l  o  w _\n",
    "2  l  o  w  e  s  t _\n",
    "6  n  e  w  e  r _\n",
    "3  w  i  d  e  r _\n",
    "2  n  e  w _\n",
    "\n",
    "vocabulary\n",
    "_, d, e, i, l, n, o, r, s, t, w\n",
    "```\n",
    "\n",
    "El algoritmo **BPE** primero cuenta todos los pares de símbolos adyacentes: el más frecuente es el par **e r** porque ocurre en **newer** (frecuencia de 6) y **wider** (frecuencia de 3) para un total de 9 ocurrencias. Luego, fusionamos estos símbolos, tratando **er** como un solo símbolo, y volvemos a contar:\n",
    "\n",
    "```plaintext\n",
    "corpus\n",
    "5  l  o  w _\n",
    "2  l  o  w  e  s  t _\n",
    "6  n  e  w  er _ \n",
    "3  w  i  d  er _ \n",
    "2  n  e  w\n",
    "\n",
    "vocabulary\n",
    "_, d, e, i, l, n, o, r, s, t, w, er\n",
    "```\n",
    "\n",
    "Ahora el par más frecuente es **er**, que ya ha sido fusionado; el siguiente par más frecuente sería diferente. (Nota: En el texto original, hay una pequeña confusión en la descripción de las fusiones posteriores. A continuación, se corrige para mayor claridad.)\n",
    "\n",
    "Luego, **n e** (con un recuento total de 8) se fusiona a **ne**:\n",
    "\n",
    "```plaintext\n",
    "corpus\n",
    "5  l  o  w _\n",
    "2  l  o  w  e  s  t _\n",
    "6  ne  w  er\n",
    "3  w  i  d  er\n",
    "2  ne  w\n",
    "\n",
    "vocabulary\n",
    "_, d, e, i, l, n, o, r, s, t, w, er, ne\n",
    "```\n",
    "\n",
    "Si continuamos, las siguientes fusiones son:\n",
    "\n",
    "| **Fusión**     | **Vocabulario actual**                                     |\n",
    "|----------------|------------------------------------------------------------|\n",
    "| **ne w**       | `_, d, e, i, l, n, o, r, s, t, w, er, ne, new`            |\n",
    "| **lo w**       | `_, d, e, i, l, n, o, r, s, t, w, er, ne, new, low`      |\n",
    "| **new er**     | `_, d, e, i, l, n, o, r, s, t, w, er, ne, new, low, newer`|\n",
    "\n",
    "Una vez que hemos aprendido nuestro vocabulario, el **token segmenter** se utiliza para tokenizar una oración de prueba. El **token segmenter** simplemente ejecuta las fusiones que hemos aprendido de los datos de entrenamiento en los datos de prueba. Las ejecuta de manera voraz, en el orden en que las aprendimos. (Por lo tanto, las frecuencias en los datos de prueba no juegan un papel, solo las frecuencias en los datos de entrenamiento). Entonces, primero segmentamos cada palabra de la oración de prueba en caracteres. Luego aplicamos la primera regla: reemplazar cada instancia de **er** en el corpus de prueba por **er**, y luego la segunda regla: reemplazar cada instancia de **ne** en el corpus de prueba por **ne**, y así sucesivamente. Al final, si el corpus de prueba contenía la secuencia de caracteres **n e w e r**, se tokenizaría como una palabra completa. Pero los caracteres de una nueva palabra (desconocida) como **l o w e r** se fusionarían en los dos tokens **low er**.\n",
    "\n",
    "Por supuesto, en configuraciones reales, **BPE** se ejecuta con muchos miles de fusiones en un corpus de entrada muy grande. El resultado es que la mayoría de las palabras se representarán como símbolos completos, y solo las palabras muy raras (y las palabras desconocidas) tendrán que ser representadas por sus partes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e83df",
   "metadata": {},
   "source": [
    "**Ejemplo 1: Descomposición de palabras en sub-palabras**\n",
    "\n",
    "Supongamos que tenemos un pequeño corpus con las siguientes palabras: `hello`, `hell`, y `help`. Queremos aplicar BPE para tokenizar estas palabras.\n",
    "\n",
    "\n",
    "Paso 1: Inicialización\n",
    "- Comenzamos con un vocabulario que contiene todos los caracteres únicos:\n",
    "  - `h`, `e`, `l`, `o`, `p`\n",
    "\n",
    "\n",
    "Paso 2: Tokenización inicial\n",
    "- Dividimos las palabras en caracteres individuales:\n",
    "  - `h e l l o`\n",
    "  - `h e l l`\n",
    "  - `h e l p`\n",
    "\n",
    "Paso 3: Encontrar el par más frecuente\n",
    "- Contamos la frecuencia de cada par de caracteres consecutivos:\n",
    "  - `he`, `el`, `ll`, `lo`, `lp`\n",
    "\n",
    "- Supongamos que el par `ll` es el más frecuente.\n",
    "\n",
    "Paso 4: Combinar el par frecuente\n",
    "- Combinamos `ll` en un solo token:\n",
    "  - `h e ll o`\n",
    "  - `h e ll`\n",
    "  - `h e l p`\n",
    "\n",
    "Paso 5: Repetir el proceso\n",
    "- Contamos de nuevo la frecuencia de los pares:\n",
    "  - `he`, `ell`, `lo`, `lp`\n",
    "\n",
    "- Supongamos que `he` es el siguiente par más frecuente.\n",
    "\n",
    "- Combinamos `he` en un solo token:\n",
    "  - `he ll o`\n",
    "  - `he ll`\n",
    "  - `he l p`\n",
    "\n",
    "Paso 6: Vocabulario final\n",
    "- Después de algunas iteraciones más, el vocabulario podría contener los tokens `he`, `ll`, `o`, `p`, `l`, etc.\n",
    "\n",
    "- Las palabras finales tokenizadas serían:\n",
    "  - `he ll o`\n",
    "  - `he ll`\n",
    "  - `he l p`\n",
    "\n",
    "\n",
    "**Ejemplo 2: Aplicación a una oración completa**\n",
    "\n",
    "Supongamos que queremos aplicar BPE a la oración \"I am learning BPE\".\n",
    "\n",
    "Paso 1: Inicialización\n",
    "- Empezamos con los caracteres individuales:\n",
    "  - `I`, `a`, `m`, `l`, `e`, `r`, `n`, `i`, `n`, `g`, `B`, `P`, `E`\n",
    "\n",
    "Paso 2: Tokenización inicial\n",
    "- Dividimos la oración en caracteres individuales:\n",
    "  - `I a m l e a r n i n g B P E`\n",
    "\n",
    "Paso 3: Encontrar el par más frecuente\n",
    "- Contamos la frecuencia de los pares:\n",
    "  - `le`, `ar`, `rn`, `ni`, `in`, etc.\n",
    "\n",
    "- Supongamos que `in` es el par más frecuente.\n",
    "\n",
    "Paso 4: Combinar el par frecuente\n",
    "- Combinamos `in` en un solo token:\n",
    "  - `I a m l e ar n in g B P E`\n",
    "\n",
    "Paso 5: Continuar el proceso\n",
    "- Seguimos encontrando y combinando pares frecuentes:\n",
    "  - `ar`, `le`, `ing`, etc.\n",
    "\n",
    "- Después de varias iteraciones, podríamos tener tokens como `learning`, `I`, `am`, `BPE`.\n",
    "\n",
    "Paso 6: Tokenización final\n",
    "- La oración final podría ser tokenizada como:\n",
    "  - `I am learning BPE`\n",
    "\n",
    "- En este caso, el token `learning` podría estar compuesto de varias sub-palabras, dependiendo de la granularidad deseada.\n",
    "\n",
    "**Ejemplo 3: Manejo de palabras desconocidas**\n",
    "\n",
    "Supongamos que ya hemos aplicado BPE a un corpus y tenemos un vocabulario que incluye los tokens `th`, `e`, `at`, `cat`, pero no incluye la palabra `caterpillar`.\n",
    "\n",
    "Paso 1: Descomponer la palabra desconocida\n",
    "- Queremos tokenizar `caterpillar` utilizando nuestro vocabulario existente.\n",
    "\n",
    "- Podemos descomponer `caterpillar` en tokens conocidos:\n",
    "  - `cat`, `er`, `p`, `ill`, `ar`\n",
    "\n",
    "- Estos tokens pueden estar presentes en el vocabulario o ser generados como sub-palabras utilizando las reglas de BPE.\n",
    "\n",
    "Paso 2: Tokenización\n",
    "- La palabra `caterpillar` podría ser tokenizada como:\n",
    "  - `cat er p ill ar`\n",
    "\n",
    "- Esto permite que incluso las palabras que no estaban presentes en el corpus original sean representadas de manera eficiente.\n",
    "\n",
    "**Ejemplo 4: Aplicación a un texto más largo**\n",
    "\n",
    "Supongamos que tenemos el siguiente texto: `\"Byte Pair Encoding is useful for compressing data\"`.\n",
    "\n",
    "Paso 1: Inicialización\n",
    "- Empezamos con los caracteres individuales:\n",
    "  - `B`, `y`, `t`, `e`, `P`, `a`, `i`, `r`, `E`, `n`, `c`, `o`, `d`, `i`, `n`, `g`, `s`, `u`, `f`, `l`, `r`, `o`, `m`, `p`, `r`, `e`, `s`, `s`, `i`, `n`, `g`, `d`, `a`, `t`, `a`\n",
    "\n",
    "Paso 2: Tokenización inicial\n",
    "- Dividimos el texto en caracteres individuales.\n",
    "\n",
    "Paso 3: Encontrar el par más frecuente\n",
    "- Contamos la frecuencia de los pares.\n",
    "\n",
    "- Supongamos que `in` es el más frecuente.\n",
    "\n",
    "Paso 4: Combinar el par frecuente\n",
    "- Combinamos `in` en un solo token.\n",
    "\n",
    "- Seguimos este proceso para otros pares frecuentes como `en`, `co`, `ss`, `da`.\n",
    "\n",
    "Paso 5: Construcción del vocabulario\n",
    "- El vocabulario final podría incluir tokens como `Byte`, `Pair`, `Encoding`, `is`, `use`, `ful`, `for`, `comp`, `ress`, `ing`, `data`.\n",
    "\n",
    "Paso 6: Tokenización Final\n",
    "- La oración final podría ser tokenizada de manera eficiente utilizando el vocabulario generado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f8349",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class BytePairEncoding:\n",
    "    def __init__(self, num_merges):\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Inicializa el vocabulario con todos los caracteres únicos en el corpus\n",
    "        self.vocab = set()\n",
    "        for word in corpus:\n",
    "            for char in word:\n",
    "                self.vocab.add(char)\n",
    "\n",
    "        self.vocab = {char: char for char in self.vocab}  # Diccionario con los tokens iniciales\n",
    "        \n",
    "        print(f\"Vocabulario inicial: {self.vocab}\")\n",
    "        \n",
    "        for merge_step in range(1, self.num_merges + 1):\n",
    "            pairs = self.get_pair_frequencies(corpus)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Encuentra el par con la frecuencia más alta\n",
    "            most_frequent_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Concatenar el par más frecuente para formar un nuevo token\n",
    "            new_token = ''.join(most_frequent_pair)\n",
    "            \n",
    "            # Actualiza el vocabulario y guarda la fusión\n",
    "            self.vocab[new_token] = new_token\n",
    "            self.merges.append(most_frequent_pair)\n",
    "            \n",
    "            # Muestra el estado actual del vocabulario y el corpus\n",
    "            print(f\"\\nPaso {merge_step}:\")\n",
    "            print(f\"Par más frecuente: {most_frequent_pair}\")\n",
    "            print(f\"Nuevo token: {new_token}\")\n",
    "            \n",
    "            # Reemplaza cada aparición del par en las cadenas\n",
    "            corpus = self.replace_pairs_in_corpus(corpus, most_frequent_pair, new_token)\n",
    "            \n",
    "            print(f\"Corpus actualizado: {corpus}\")\n",
    "            print(f\"Vocabulario actualizado: {self.vocab}\")\n",
    "    \n",
    "    def get_pair_frequencies(self, corpus):\n",
    "        pairs = {}\n",
    "        for word in corpus:\n",
    "            tokens = list(word)\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                if pair in pairs:\n",
    "                    pairs[pair] += 1\n",
    "                else:\n",
    "                    pairs[pair] = 1\n",
    "        return pairs\n",
    "\n",
    "    def replace_pairs_in_corpus(self, corpus, pair, new_token):\n",
    "        new_corpus = []\n",
    "        for word in corpus:\n",
    "            new_word = word.replace(''.join(pair), new_token)\n",
    "            new_corpus.append(new_word)\n",
    "        return new_corpus\n",
    "\n",
    "    def tokenize(self, word):\n",
    "        tokens = list(word)\n",
    "        for merge in self.merges:\n",
    "            new_token = ''.join(merge)\n",
    "            merged_word = ''.join(tokens)\n",
    "            if new_token in merged_word:\n",
    "                tokens = merged_word.replace(new_token, new_token).split(new_token)\n",
    "                tokens = [token if token else new_token for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocab\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = [\"low\", \"lowest\", \"new\", \"wider\"]\n",
    "num_merges = 10  # Número de combinaciones deseadas\n",
    "\n",
    "bpe = BytePairEncoding(num_merges)\n",
    "bpe.train(corpus)\n",
    "vocab = bpe.get_vocabulary()\n",
    "\n",
    "print(\"\\nVocabulario final generado:\")\n",
    "print(vocab)\n",
    "\n",
    "# Tokenización de una palabra nueva\n",
    "word_to_tokenize = \"lower\"\n",
    "tokens = bpe.tokenize(word_to_tokenize)\n",
    "print(f\"\\nTokenización de '{word_to_tokenize}': {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230f19a",
   "metadata": {},
   "source": [
    "### Variantes del BPE\n",
    "\n",
    "1. **[Subword regularization](https://arxiv.org/abs/1804.10959)**\n",
    "   - **Descripción**: Subword Regularization es una técnica que introduce aleatoriedad en la tokenización para mejorar la robustez del modelo. En lugar de generar una única secuencia de sub-palabras para una palabra, esta técnica genera múltiples posibles segmentaciones de sub-palabras durante el entrenamiento. Esto mejora la capacidad del modelo para manejar variaciones y reduce la dependencia en una única tokenización.\n",
    "   - **Aplicación**: Este método es útil en situaciones donde las palabras pueden tener múltiples formas o en idiomas con morfología compleja.\n",
    "   - **Implementación**: En lugar de siempre elegir la segmentación con la mayor frecuencia, se elige de forma aleatoria entre las opciones disponibles, ponderando según la probabilidad de cada segmentación.\n",
    "   \n",
    "2. **[BPE-Dropout](https://arxiv.org/abs/1910.13267)**\n",
    "   - **Descripción**: BPE-Dropout es una técnica que introduce aleatoriedad durante el entrenamiento de BPE. En lugar de siempre aplicar la regla de combinación más frecuente, se omite aleatoriamente ciertas reglas durante la construcción del vocabulario.\n",
    "   - **Ventaja**: Esta técnica promueve la diversidad en la tokenización y reduce el sobreajuste del modelo a una tokenización específica, lo que puede mejorar el rendimiento en situaciones de prueba con palabras no vistas.\n",
    "   - **Aplicación**: Es útil para mejorar la generalización de modelos de lenguaje, especialmente en casos con corpus de entrenamiento limitado.\n",
    "\n",
    "3. **[Unigram language model](https://huggingface.co/learn/nlp-course/en/chapter6/7)**\n",
    "   - **Descripción**: En lugar de usar BPE, donde los pares de tokens más frecuentes se combinan de manera iterativa, el modelo de lenguaje Unigram es un enfoque basado en la probabilidad. En este enfoque, se construye un modelo de lenguaje unigram (donde cada token es independiente de los demás), y se realiza una selección basada en la probabilidad para determinar qué tokens mantener y cuáles eliminar durante la tokenización.\n",
    "   - **Ventaja**: Este método es más flexible en cuanto a la tokenización, ya que permite mantener múltiples segmentaciones posibles, y se ajusta bien a diferentes idiomas con complejidades morfológicas.\n",
    "   - **Aplicación**: Es utilizado en modelos como SentencePiece, que implementa tanto BPE como Unigram para la tokenización.\n",
    "\n",
    "4. **BBPE (Bilateral Byte Pair Encoding)**\n",
    "   - **Descripción**: BBPE es una variante del BPE que considera tanto la frecuencia de los tokens como la regularidad de la estructura de sub-palabras. Esta técnica busca evitar la creación de sub-palabras que no sean lingüísticamente significativas o que sean demasiado raras.\n",
    "   - **Proceso**: BBPE introduce un criterio de regularización que penaliza la fusión de pares de tokens si estos no contribuyen a una representación más coherente del lenguaje.\n",
    "   - **Aplicación**: Es especialmente útil para reducir la fragmentación en palabras largas o compuestas.\n",
    "\n",
    "5. **[Scaffold-BPE](https://arxiv.org/html/2404.17808v1)**\n",
    "    - **Descripción**: Incorpora un mecanismo dinámico de eliminación de tokens de andamiaje mediante modificaciones sin parámetros, de bajo costo computacional y fáciles de implementar al BPE original.\n",
    "   - **Proceso**: Usa un mecanismo de exclusión de Scaffold Tokens de baja frecuencia de las representaciones de tokens para los textos dados, mitigando así el problema del desequilibrio de frecuencia y facilitando el entrenamiento del modelo.\n",
    "   - **Aplicación**: Es especialmente útil para tareas de modelado de lenguaje y de traducción automática.\n",
    "\n",
    "6. **[Multi-lingual BPE](https://aclanthology.org/P19-1341/)**\n",
    "   - **Descripción**: Multi-lingual BPE es una extensión del BPE que se aplica a múltiples idiomas simultáneamente. El vocabulario se construye a partir de corpus multilingües y las reglas de combinación se aplican de manera que maximicen la reutilización de sub-palabras comunes entre diferentes idiomas.\n",
    "   - **Ventaja**: Promueve la transferencia de conocimiento entre idiomas y es útil en modelos multilingües.\n",
    "   - **Aplicación**: Utilizado en sistemas de traducción automática multilingüe o en modelos de lenguaje que abarcan varios idiomas.\n",
    "\n",
    "7. **Adaptive BPE**\n",
    "   - **Descripción**: En Adaptive BPE, la segmentación de palabras se adapta dinámicamente durante el entrenamiento de un modelo, en lugar de utilizar un vocabulario estático. Esto permite al modelo ajustar la granularidad de la tokenización según la complejidad del contexto.\n",
    "   - **Ventaja**: Mejora la capacidad del modelo para manejar palabras raras o complejas y permite una segmentación más fina cuando es necesario.\n",
    "   - **Aplicación**: Este enfoque es ideal para modelos que requieren un alto nivel de precisión en la tokenización, como los utilizados en tareas de traducción automática.\n",
    "\n",
    "8. **BPE con Vocabulary Restriction**\n",
    "   - **Descripción**: En esta variante, se aplica una restricción de vocabulario donde solo se permite que ciertas combinaciones de tokens se realicen si no exceden un tamaño de vocabulario predeterminado.\n",
    "   - **Ventaja**: Controla el crecimiento del vocabulario y previene la generación de demasiados tokens, lo que puede ser útil en situaciones donde el tamaño del vocabulario debe mantenerse limitado.\n",
    "   - **Aplicación**: Utilizado en sistemas donde los recursos de memoria o procesamiento son limitados.\n",
    "\n",
    "9. **BPE con hierarchical merging**\n",
    "   - **Descripción**: En lugar de combinar los pares de tokens más frecuentes en una sola fase, la combinación se realiza en múltiples fases, siguiendo una estructura jerárquica. Este enfoque permite una tokenización que respeta la estructura morfológica o gramatical del idioma.\n",
    "   - **Ventaja**: Mejor preservación de la semántica y la morfología del lenguaje durante la tokenización.\n",
    "   - **Aplicación**: Especialmente útil en idiomas con estructuras morfológicas complejas, como el turco o el finés.\n",
    "\n",
    "\n",
    "Se podría implementar un flujo básico de Scaffold-BPE en Python:\n",
    "\n",
    "1. Entrenamiento de un modelo BPE estándar.\n",
    "2. Identificación de scaffold tokens.\n",
    "3. Eliminación de scaffold tokens.\n",
    "4. Reentrenamiento del modelo BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0dcf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "class ScaffoldBPE:\n",
    "    def __init__(self, num_merges):\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        self.scaffold_tokens = set()\n",
    "\n",
    "    def train_bpe(self, corpus):\n",
    "        # Paso 1: Entrenar un modelo BPE estándar\n",
    "        self.vocab = set()\n",
    "        for word in corpus:\n",
    "            for char in word:\n",
    "                self.vocab.add(char)\n",
    "\n",
    "        self.vocab = {char: char for char in self.vocab}  # Diccionario con los tokens iniciales\n",
    "        \n",
    "        print(f\"Vocabulario inicial: {self.vocab}\")\n",
    "        \n",
    "        for merge_step in range(1, self.num_merges + 1):\n",
    "            pairs = self.get_pair_frequencies(corpus)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Encuentra el par con la frecuencia más alta\n",
    "            most_frequent_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Concatenar el par más frecuente para formar un nuevo token\n",
    "            new_token = ''.join(most_frequent_pair)\n",
    "            \n",
    "            # Actualiza el vocabulario y guarda la fusión\n",
    "            self.vocab[new_token] = new_token\n",
    "            self.merges.append(most_frequent_pair)\n",
    "            \n",
    "            # Muestra el estado actual del vocabulario y el corpus\n",
    "            print(f\"\\nPaso {merge_step}:\")\n",
    "            print(f\"Par más frecuente: {most_frequent_pair}\")\n",
    "            print(f\"Nuevo token: {new_token}\")\n",
    "            \n",
    "            # Reemplaza cada aparición del par en las cadenas\n",
    "            corpus = self.replace_pairs_in_corpus(corpus, most_frequent_pair, new_token)\n",
    "            \n",
    "            print(f\"Corpus actualizado: {corpus}\")\n",
    "            print(f\"Vocabulario actualizado: {self.vocab}\")\n",
    "    \n",
    "    def get_pair_frequencies(self, corpus):\n",
    "        pairs = {}\n",
    "        for word in corpus:\n",
    "            tokens = list(word)\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                if pair in pairs:\n",
    "                    pairs[pair] += 1\n",
    "                else:\n",
    "                    pairs[pair] = 1\n",
    "        return pairs\n",
    "\n",
    "    def replace_pairs_in_corpus(self, corpus, pair, new_token):\n",
    "        new_corpus = []\n",
    "        for word in corpus:\n",
    "            new_word = word.replace(''.join(pair), new_token)\n",
    "            new_corpus.append(new_word)\n",
    "        return new_corpus\n",
    "\n",
    "    def identify_scaffold_tokens(self, token_label_pairs):\n",
    "        # Paso 2: Identificación de scaffold tokens utilizando la información mutua\n",
    "        token_frequencies = Counter([token for tokens, _ in token_label_pairs for token in tokens])\n",
    "        token_label_matrix = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for tokens, label in token_label_pairs:\n",
    "            for token in tokens:\n",
    "                token_label_matrix[token][label] += 1\n",
    "\n",
    "        # Calcula la información mutua entre cada token y las etiquetas\n",
    "        mutual_information = {}\n",
    "        total_samples = len(token_label_pairs)\n",
    "        for token in token_frequencies:\n",
    "            token_prob = token_frequencies[token] / total_samples\n",
    "            mi = 0\n",
    "            for label in token_label_matrix[token]:\n",
    "                joint_prob = token_label_matrix[token][label] / total_samples\n",
    "                label_prob = sum(token_label_matrix[token].values()) / total_samples\n",
    "                mi += joint_prob * np.log2(joint_prob / (token_prob * label_prob))\n",
    "            mutual_information[token] = mi\n",
    "        \n",
    "        # Identifica tokens con baja información mutua como scaffold tokens\n",
    "        self.scaffold_tokens = {token for token, mi in mutual_information.items() if mi < 0.1}  # Umbral arbitrario\n",
    "        print(f\"\\nScaffold tokens identificados: {self.scaffold_tokens}\")\n",
    "\n",
    "    def retrain_bpe(self, corpus):\n",
    "        # Paso 3: Eliminar scaffold tokens\n",
    "        for scaffold_token in self.scaffold_tokens:\n",
    "            if scaffold_token in self.vocab:\n",
    "                del self.vocab[scaffold_token]\n",
    "                self.merges = [merge for merge in self.merges if ''.join(merge) != scaffold_token]\n",
    "        \n",
    "        # Reentrenar BPE con el vocabulario reducido\n",
    "        self.train_bpe(corpus)\n",
    "    \n",
    "    def tokenize(self, word):\n",
    "        tokens = list(word)\n",
    "        for merge in self.merges:\n",
    "            new_token = ''.join(merge)\n",
    "            merged_word = ''.join(tokens)\n",
    "            if new_token in merged_word:\n",
    "                tokens = merged_word.replace(new_token, new_token).split(new_token)\n",
    "                tokens = [token if token else new_token for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = [\"low\", \"lowest\", \"new\", \"wider\"]\n",
    "num_merges = 10  # Número de combinaciones deseadas\n",
    "\n",
    "bpe = ScaffoldBPE(num_merges)\n",
    "bpe.train_bpe(corpus)\n",
    "\n",
    "# Paso 2: Identificación de scaffold tokens\n",
    "# Aquí asumimos que tenemos una lista de pares token-etiqueta\n",
    "# Ejemplo: [([\"low\"], \"positive\"), ([\"lowest\"], \"positive\"), ([\"new\"], \"neutral\")]\n",
    "token_label_pairs = [\n",
    "    (bpe.tokenize(\"low\"), \"positive\"),\n",
    "    (bpe.tokenize(\"lowest\"), \"positive\"),\n",
    "    (bpe.tokenize(\"new\"), \"neutral\"),\n",
    "    (bpe.tokenize(\"wider\"), \"neutral\")\n",
    "]\n",
    "\n",
    "bpe.identify_scaffold_tokens(token_label_pairs)\n",
    "\n",
    "# Paso 3 y 4: Reentrenar BPE eliminando scaffold tokens\n",
    "bpe.retrain_bpe(corpus)\n",
    "\n",
    "# Tokenización de una palabra nueva\n",
    "word_to_tokenize = \"lower\"\n",
    "tokens = bpe.tokenize(word_to_tokenize)\n",
    "print(f\"\\nTokenización de '{word_to_tokenize}': {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968201e8",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "**Limpieza y tokenización de texto con expresiones regulares y SentencePiece**\n",
    "\n",
    "1. **Objetivo**: Implementa una función en Python que limpie un texto utilizando expresiones regulares para eliminar caracteres especiales, números y palabras de menos de 3 caracteres. Luego, usa SentencePiece para tokenizar el texto limpio.\n",
    "2. **Pasos**:\n",
    "    - Define un conjunto de expresiones regulares para limpiar el texto.\n",
    "    - Limpia el texto de un corpus dado.\n",
    "    - Entrena un modelo de tokenización usando SentencePiece con el texto limpio.\n",
    "    - Tokeniza un nuevo texto utilizando el modelo entrenado.\n",
    "\n",
    "3. **Puntos clave**:\n",
    "    - Utiliza `re.sub` para aplicar las expresiones regulares.\n",
    "    - Configura SentencePiece para realizar la tokenización basada en sub-palabras.\n",
    "    - Evalúa cómo la limpieza afecta la tokenización final.\n",
    "\n",
    "4. **Referencia**:\n",
    "   - [SentencePiece Documentation](https://github.com/google/sentencepiece)\n",
    "   - [Python `re` Module Documentation](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "\n",
    "**Tokenización con BPE y subword regularization**\n",
    "1. **Objetivo**: Implementa un sistema de tokenización utilizando BPE y Subword Regularization con SentencePiece. Entrena el modelo en un corpus y realiza la tokenización en textos nuevos, aplicando regularización en la segmentación de sub-palabras.\n",
    "2. **Pasos**:\n",
    "    - Preprocesa un corpus de texto (puede ser de reseñas de productos o noticias) utilizando expresiones regulares.\n",
    "    - Usa SentencePiece para entrenar un modelo de tokenización con BPE.\n",
    "    - Implementa Subword Regularization durante la tokenización de nuevos textos.\n",
    "    - Compara la tokenización con y sin Subword Regularization.\n",
    "\n",
    "3. **Puntos clave**:\n",
    "    - Utiliza `spm.SentencePieceTrainer` para entrenar el modelo.\n",
    "    - Aplica regularización usando el parámetro `enable_sampling=True` en SentencePiece.\n",
    "    - Analiza cómo afecta la regularización la diversidad en la tokenización de textos similares.\n",
    "\n",
    "4. **Referencia**:\n",
    "   - [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/abs/1804.10959)\n",
    "   - [SentencePiece Implementation](https://github.com/google/sentencepiece)\n",
    "\n",
    "**Análisis de la eficiencia de BPE-Dropout en un corpus multilingüe**\n",
    "1. **Objetivo**: Entrena modelos de tokenización usando BPE estándar y BPE-Dropout en un corpus multilingüe. Compara los resultados en términos de la diversidad de tokens generados y la robustez del modelo en diferentes idiomas.\n",
    "2. **Pasos**:\n",
    "    - Preprocesa un corpus multilingüe usando expresiones regulares específicas para cada idioma.\n",
    "    - Entrena un modelo de tokenización usando SentencePiece con BPE.\n",
    "    - Implementa BPE-Dropout durante el entrenamiento y la tokenización.\n",
    "    - Tokeniza un conjunto de pruebas multilingüe y compara la diversidad de tokens generados.\n",
    "\n",
    "3. **Puntos clave**:\n",
    "    - Configura SentencePiece para manejar múltiples idiomas en un solo modelo.\n",
    "    - Aplica BPE-Dropout utilizando variaciones en el parámetro de probabilidad de muestreo.\n",
    "    - Evalúa la efectividad de BPE-Dropout en términos de reducción del sobreajuste y manejo de idiomas con estructuras morfológicas complejas.\n",
    "\n",
    "4. **Referencia**:\n",
    "   - [BPE-Dropout: Simple and Effective Subword Regularization](https://arxiv.org/abs/1910.13267)\n",
    "   - [GitHub - google/sentencepiece: Unsupervised text tokenizer for Neural Network-based text generation](https://github.com/google/sentencepiece)\n",
    "\n",
    "**Optimización de la segmentación de texto con Unigram Language Model**\n",
    "\n",
    "1. **Objetivo**: Implementa un modelo de tokenización utilizando el Unigram Language Model de SentencePiece. Realiza la tokenización en un corpus específico y evalúa la calidad de la segmentación en términos de la representación semántica.\n",
    "2. **Pasos**:\n",
    "    - Preprocesa un corpus de texto utilizando expresiones regulares para normalizarlo.\n",
    "    - Entrena un modelo de tokenización utilizando el Unigram Language Model con SentencePiece.\n",
    "    - Tokeniza un conjunto de pruebas y analiza las representaciones de sub-palabras generadas.\n",
    "    - Compara las representaciones semánticas con las generadas por BPE.\n",
    "\n",
    "3. **Puntos clave**:\n",
    "    - Utiliza la función de SentencePiece para entrenar el Unigram Language Model.\n",
    "    - Evalúa la calidad de las representaciones en tareas de clasificación de texto o análisis semántico.\n",
    "    - Considera cómo el Unigram Language Model maneja las palabras raras y el out-of-vocabulary.\n",
    "\n",
    "4. **Referencia**:\n",
    "   - [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/abs/1804.10959)\n",
    "   - [SentencePiece Unigram Language Model](https://github.com/google/sentencepiece)\n",
    "\n",
    "\n",
    "**Construcción de un modelo de corrección ortográfica usando BPE y expresiones regulares**\n",
    "\n",
    "1. **Objetivo**: Implementa un sistema de corrección ortográfica que utiliza expresiones regulares para detectar errores comunes y SentencePiece con BPE para corregirlos. Entrena el modelo en un corpus y aplica las correcciones en textos con errores.\n",
    "2. **Pasos**:\n",
    "    - Usa expresiones regulares para identificar patrones comunes de errores ortográficos (e.g., repetición de letras, errores tipográficos).\n",
    "    - Entrena un modelo de BPE con SentencePiece en un corpus limpio.\n",
    "    - Aplica el modelo para tokenizar y corregir textos con errores.\n",
    "    - Evalúa la efectividad del sistema comparando los textos corregidos con versiones sin errores.\n",
    "\n",
    "3. **Puntos clave**:\n",
    "    - Define las expresiones regulares para capturar errores ortográficos.\n",
    "    - Entrena y utiliza el modelo de BPE para generar sugerencias de corrección basadas en sub-palabras.\n",
    "    - Implementa una función para validar las correcciones.\n",
    "\n",
    "4. **Referencia**:\n",
    "   - [Python Regular Expressions](https://docs.python.org/3/library/re.html)\n",
    "   - [SentencePiece BPE Implementation](https://github.com/google/sentencepiece)\n",
    "\n",
    "\n",
    "**Generación de textos creativos con BPE y subword regularization**\n",
    "\n",
    "1. **Objetivo**: Crea un modelo de generación de texto creativo (e.g., poesía o prosa) utilizando SentencePiece para la tokenización con BPE y Subword Regularization. Entrena el modelo en un corpus literario y genera nuevos textos creativos.\n",
    "2. **Pasos**:\n",
    "    - Preprocesa un corpus literario utilizando expresiones regulares para eliminar ruido.\n",
    "    - Entrena un modelo de BPE con SentencePiece.\n",
    "    - Utiliza Subword Regularization durante la generación de texto para crear variaciones creativas en la salida.\n",
    "    - Genera varios textos y compara la creatividad y fluidez con y sin Subword Regularization.\n",
    "\n",
    "3. **Puntos clave**:\n",
    "    - Implementa BPE para tokenizar un corpus literario.\n",
    "    - Aplica Subword Regularization durante la generación de texto para incrementar la diversidad creativa.\n",
    "    - Evalúa la calidad literaria de las salidas generadas.\n",
    "\n",
    "4. **Referencia**:\n",
    "   - [Subword Regularization in SentencePiece](https://github.com/google/sentencepiece)\n",
    "\n",
    "**Desarrollo de un sistema de resumen de textos basado en BPE y Unigram Language Model**\n",
    "\n",
    "1. **Objetivo**: Implementa un sistema de resumen automático de textos que use BPE y Unigram Language Model para tokenizar el texto antes de aplicar un algoritmo de resumen basado en la frecuencia de sub-palabras.\n",
    "2. **Pasos**:\n",
    "    - Preprocesa un corpus de artículos utilizando expresiones regulares para limpiar el texto.\n",
    "    - Entrena un modelo de BPE con SentencePiece y otro con Unigram Language Model.\n",
    "    - Implementa un algoritmo de resumen que identifique las sub-palabras más frecuentes para generar un resumen.\n",
    "    - Compara la efectividad de los resúmenes generados por BPE y Unigram.\n",
    "\n",
    "3. **Puntos clave**:\n",
    "    - Preprocesa el texto eliminando ruido y caracteres innecesarios.\n",
    "    - Utiliza SentencePiece para entrenar modelos de BPE y Unigram.\n",
    "    - Implementa un algoritmo simple de resumen basado en la frecuencia de tokens.\n",
    "\n",
    "4. **Referencia**:\n",
    "   - [Unigram Language Model in SentencePiece](https://github.com/google/sentencepiece)\n",
    "   - [Text Summarization Techniques](https://towardsdatascience.com/extractive-summarization-in-python-tf-idf-textrank-bert-gensim-textrank-vs-lsa-vs-lda-8f6e2b49f3b9)\n",
    "\n",
    "**Sistema de clasificación de opiniones usando tokenización con BPE y expresiones regulares**\n",
    "\n",
    "1. **Objetivo**: Desarrolla un sistema que clasifique opiniones (positivas o negativas) utilizando BPE para la tokenización y expresiones regulares para normalizar el texto. Entrena el modelo en un corpus de reseñas de productos.\n",
    "2. **Pasos**:\n",
    "    - Preprocesa el corpus de reseñas usando expresiones regulares para eliminar palabras innecesarias y normalizar el texto.\n",
    "    - Entrena un modelo de tokenización usando BPE con SentencePiece.\n",
    "    - Tokeniza el texto preprocesado y entrena un clasificador de opiniones.\n",
    "    - Evalúa el rendimiento del clasificador comparando diferentes configuraciones de BPE (con y sin regularización de sub-palabras).\n",
    "\n",
    "3. **Puntos clave**:\n",
    "    - Preprocesa y normaliza el texto usando expresiones regulares.\n",
    "    - Aplica BPE para tokenizar las reseñas y entrena un modelo de clasificación.\n",
    "    - Evalúa la precisión del clasificador y compara las configuraciones de tokenización.\n",
    "\n",
    "4. **Referencia**:\n",
    "   - [Regular Expressions in NLP](https://towardsdatascience.com/nlp-text-preprocessing-with-regular-expressions-1fd4f8f4c44f)\n",
    "   - [SentencePiece and BPE for Text Classification](https://github.com/google/sentencepiece)\n",
    "\n",
    "**Tokenización y clasificación multilingüe con multi-lingual BPE**\n",
    "\n",
    "1. **Objetivo**: Implementa un sistema de clasificación de textos que soporte múltiples idiomas utilizando Multi-lingual BPE para tokenización. Entrena el sistema en un corpus multilingüe y evalúa su rendimiento en tareas de clasificación de textos en varios idiomas.\n",
    "2. **Pasos**:\n",
    "    - Preprocesa un corpus multilingüe utilizando expresiones regulares específicas para cada idioma.\n",
    "    - Entrena un modelo de tokenización multilingüe utilizando BPE con SentencePiece.\n",
    "    - Tokeniza y clasifica textos en diferentes idiomas, evaluando la precisión del modelo en cada idioma.\n",
    "    - Compara la efectividad del modelo multilingüe con un modelo entrenado en un solo idioma.\n",
    "\n",
    "3. **Puntos clave**:\n",
    "    - Aplica expresiones regulares para manejar las peculiaridades lingüísticas en diferentes idiomas.\n",
    "    - Entrena un modelo BPE que puede manejar múltiples idiomas simultáneamente.\n",
    "    - Evalúa cómo el modelo maneja textos en diferentes idiomas y analiza los resultados.\n",
    "\n",
    "4. **Referencia**:\n",
    "   - [Multi-lingual NLP with SentencePiece](https://github.com/google/sentencepiece)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f646d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc555e",
   "metadata": {},
   "source": [
    "### Proyectos\n",
    "\n",
    "\n",
    "1. **Proyecto de análisis de sentimientos con expresiones regulares y tokenización**\n",
    "   - **Descripción**: Crea un sistema de análisis de sentimientos que utilice expresiones regulares para limpiar y normalizar el texto antes de aplicar tokenización básica. Este sistema podría identificar patrones comunes, como emoticonos, abreviaciones y texto repetido, para mejorar la precisión del análisis de sentimientos.\n",
    "   - **Tecnologías**: Python (re), NLTK o SpaCy para tokenización.\n",
    "   - **Objetivo**: Mejorar la precisión del análisis de sentimientos en redes sociales o reseñas mediante la normalización previa del texto usando expresiones regulares.\n",
    "\n",
    "2. **Tokenización avanzada usando BPE y BPE-Dropout en traducción automática**\n",
    "   - **Descripción**: Implementa un modelo de traducción automática utilizando BPE para la tokenización de texto. Luego, compara los resultados de BPE estándar con BPE-Dropout para evaluar la mejora en la robustez y precisión del modelo, especialmente en textos con ruido o palabras no vistas.\n",
    "   - **Tecnologías**: SentencePiece, PyTorch, o TensorFlow para construir el modelo de traducción.\n",
    "   - **Objetivo**: Evaluar el impacto de BPE-Dropout en la mejora de las puntuaciones BLEU en tareas de traducción automática.\n",
    "\n",
    "3. **Sistema multilingüe de clasificación de texto con multi-lingual BPE**\n",
    "   - **Descripción**: Desarrolla un sistema de clasificación de texto que utilice Multi-lingual BPE para tokenizar textos en múltiples idiomas. Implementa un modelo de clasificación que pueda trabajar de manera eficiente con entradas en diferentes idiomas, aprovechando la reutilización de sub-palabras comunes.\n",
    "   - **Tecnologías**: SentencePiece, Scikit-learn, o Transformers de Hugging Face.\n",
    "   - **Objetivo**: Crear un sistema de clasificación que maneje múltiples idiomas eficientemente y evalúe su rendimiento en tareas de clasificación de noticias o detección de spam.\n",
    "\n",
    "4. **Modelo de lenguaje basado en subword regularization y unigram language model**\n",
    "   - **Descripción**: Implementa un modelo de lenguaje utilizando Subword Regularization y Unigram Language Model para manejar la tokenización del texto. Este proyecto se puede centrar en la generación de texto o en la mejora de la comprensión de lenguaje natural en aplicaciones como chatbots.\n",
    "   - **Tecnologías**: SentencePiece, TensorFlow, o PyTorch para el desarrollo del modelo.\n",
    "   - **Objetivo**: Mejorar la robustez y flexibilidad del modelo de lenguaje en la generación de texto, comparando el rendimiento con modelos que utilizan solo tokenización tradicional.\n",
    "\n",
    "5. **Optimización de modelos de traducción con adaptive BPE**\n",
    "   - **Descripción**: Implementa un modelo de traducción automática que utiliza Adaptive BPE para ajustar dinámicamente la segmentación de sub-palabras durante el entrenamiento. Evalúa cómo este enfoque mejora la calidad de la traducción en comparación con un enfoque BPE estático.\n",
    "   - **Tecnologías**: SentencePiece, Transformers de Hugging Face.\n",
    "   - **Objetivo**: Evaluar la efectividad de Adaptive BPE en la mejora de la traducción automática en comparación con métodos tradicionales de tokenización.\n",
    "\n",
    "6. **Generación de texto multilingüe utilizando unigram language model y multi-lingual BPE**\n",
    "   - **Descripción**: Desarrolla un modelo de generación de texto que pueda manejar múltiples idiomas simultáneamente utilizando Unigram Language Model y Multi-lingual BPE para la tokenización. Esto podría aplicarse a la generación de descripciones de productos, donde el texto necesita ser generado en varios idiomas.\n",
    "   - **Tecnologías**: SentencePiece, PyTorch, GPT-3 o modelos similares.\n",
    "   - **Objetivo**: Crear un sistema que genere texto coherente y adecuado en varios idiomas, evaluando la calidad y fluidez de las salidas generadas.\n",
    "\n",
    "7. **Comparativa de métodos de tokenización en tareas de clasificación de textos**\n",
    "   - **Descripción**: Realiza un estudio comparativo entre diferentes métodos de tokenización, como BPE, Unigram Language Model, y sus variantes como BPE-Dropout y Subword Regularization, en una tarea de clasificación de texto.\n",
    "   - **Tecnologías**: SentencePiece, Scikit-learn, o Transformers de Hugging Face.\n",
    "   - **Objetivo**: Determinar cuál método de tokenización proporciona los mejores resultados en términos de precisión y eficiencia en una tarea de clasificación de textos en un corpus específico.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720a547",
   "metadata": {},
   "source": [
    " **Referencia:**[SentencePiece](https://github.com/google/sentencepiece): Un tokenizador y detokenizador de texto no supervisado, diseñado principalmente para sistemas de generación de texto basados en redes neuronales, donde el tamaño del vocabulario se determina previamente al entrenamiento del modelo neuronal. \n",
    "SentencePiece implementa unidades de sub-palabras (por ejemplo, codificación por pares de bytes (BPE) ) y el modelo de lenguaje unigram, con la extensión de entrenamiento directo a partir de oraciones sin procesar. \n",
    "\n",
    "SentencePiece nos permite crear un sistema completamente de extremo a extremo que no depende de un pre/procesamiento específico del idioma."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
