{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A5xOQz5emHC"
   },
   "source": [
    "## ¿Qué es el procesamiento del lenguaje natural?  \n",
    "\n",
    "Wikipedia:\n",
    "\n",
    "\n",
    "\n",
    " El procesamiento del lenguaje natural (NLP) es un subcampo de la lingüística, la informática, la ingeniería de la información y la inteligencia artificial que se ocupa de las interacciones entre las computadoras y los lenguajes humanos (naturales), en particular, cómo programar las computadoras para procesar y analizar grandes cantidades de datos del lenguaje natural. \n",
    " Los desafíos en el procesamiento del lenguaje natural con frecuencia implican el reconocimiento del habla, la comprensión del lenguaje natural y la generación del lenguaje natural.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfNVhld3gK7F"
   },
   "source": [
    "### Aplicaciones populares\n",
    "\n",
    " - Traducción automática \n",
    " - Reconocimiento de voz \n",
    " - Respuestas a preguntas (question answering)  \n",
    " - Resumen de texto\n",
    " - Chatbots \n",
    " - Texto a voz y voz a texto\n",
    " - Voicebots\n",
    " - Generación de texto y audio \n",
    " - Análisis de los sentimientos \n",
    " - Extracción de información  \n",
    "\n",
    " Enlace: https://paperswithcode.com/area/natural-language-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puntos de inflexión \n",
    "\n",
    " \n",
    "El NLP  hizo olas a partir de 2014 con el lanzamiento de Amazon Alexa, un renovado Apple Siri, Google Assistant y Microsoft Cortana. Google también lanzó una versión muy mejorada de Google Translate en 2016, y ahora los chatbots y voicebots son mucho más comunes. Dicho esto, no fue hasta 2018 que NLP tuvo su propio momento ImageNet con el lanzamiento de grandes modelos de lenguaje preentrenados entrenados con la arquitectura Transformer; el más notable de estos fue  BERT de Google, que se lanzó en noviembre de 2018. \n",
    "\n",
    "\n",
    "En 2019, los modelos generativos como GPT-2 de OpenAI causaron sensación, generando nuevo contenido sobre la marcha basado en contenido anterior, una hazaña que antes era insuperable. En 2020, OpenAI lanzó una versión aún más grande e impresionante, GPT-3, basada en sus éxitos anteriores. \n",
    "\n",
    "\n",
    "De cara al 2023 y más allá, la NLP ya no es un subcampo experimental de la IA. Junto con la visión por computadora, NLP ahora está preparado para tener muchas aplicaciones de base amplia en la empresa.  \n",
    "\n",
    "Enlace: https://medium.com/@smdtechnosol/the-success-of-nlp-natural-language-processing-in-2024-c123d8341733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxiEHG4rhHwN"
   },
   "source": [
    "### Introducción al preprocesamiento de texto\n",
    "\n",
    "Dado que, el texto es la forma más no estructurada de todos los datos disponibles, hay varios tipos de ruido en él y los datos no son fácilmente analizables sin ningún procesamiento previo. Todo el proceso de limpieza y estandarización del texto, que lo hace sin ruido y listo para el análisis, se conoce como preprocesamiento de texto.\n",
    "\n",
    "Se compone principalmente de tres pasos:\n",
    "\n",
    " *  Eliminación de ruido\n",
    " *  Normalización del léxico\n",
    " *  Estandarización de objetos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de ruido\n",
    "\n",
    "Cualquier fragmento de texto que no sea relevante para el contexto de los datos y la salida final se puede especificar como ruido.\n",
    "\n",
    "Por ejemplo, `stopwords`  (palabras de uso común de un idioma, es, am, the, of, in, etc), URL o enlaces, entidades de medios sociales (menciones, hashtags), puntuaciones y palabras específicas de la industria. Este paso trata sobre la eliminación de todos los tipos de entidades ruidosas presentes en el texto.\n",
    "\n",
    "Un enfoque general para la eliminación de ruido es preparar un diccionario de entidades ruidosas e iterar el objeto de texto con tokens (o con palabras), eliminando esos tokens que están presentes en el diccionario de ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "oUg9GnzOhBFr",
    "outputId": "68a3ea02-82f5-49f4-827b-adade8ecb091"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WordEmbeddingsformofrepresent'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Simple código para remover palabras ruidosas de texto\n",
    "\n",
    "lista_ruido = [\"is\", \"a\", \"of,\"\"this\", \"...\"]\n",
    "def remove_ruido(input_text):\n",
    "  ## Completar\n",
    "\n",
    "\n",
    "#remove_ruido(\"Word Embeddings is a form of represent ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlR_gFI4kvhJ"
   },
   "source": [
    "Otro enfoque es utilizar las expresiones regulares al tratar con patrones especiales de ruido.  El siguiente código de Python elimina un patrón de expresiones regulares de un texto de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Dp1N8ftsk7jd",
    "outputId": "f4228271-062b-4d5c-aa53-a080d07543d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Word Embeddings is a  of represent ...*'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_regex(input_text, regex_pattern):\n",
    "  urls = re.finditer(regex_pattern, input_text)\n",
    "  for i in urls:\n",
    "    input_text = re.sub(i.group().strip(), '', input_text)\n",
    "  return input_text\n",
    "\n",
    "regex_pattern = '#[\\w]*'\n",
    "remove_regex(\"Word Embeddings is a #form of represent ...*\", regex_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOZJZy8VmsJr"
   },
   "source": [
    "#### Normalización del léxico\n",
    "\n",
    "Otro tipo de ruido textual es acerca de las múltiples representaciones exhibidas por una sola palabra.\n",
    "\n",
    "Por ejemplo, `jugar`, `jugador`, `jugar`, `jugar` y `jugar` son las diferentes variaciones de la palabra - `jugar`, aunque significan cosas diferentes, contextualmente todas son similares. El paso convierte todas las disparidades de una palabra en su forma normalizada (también conocida como `lema``). \n",
    "\n",
    "\n",
    "Las prácticas de normalización del léxico más comunes son:\n",
    "\n",
    "* `Stemming`:  es un proceso rudimentario basado en reglas para eliminar los sufijos (\"ing\", \"ly\", \"es\", \"s\", etc.) de una palabra (para el idioma inglés por ejemplo).\n",
    "* `Lemmatización`:  es un procedimiento organizado y paso a paso para obtener la forma de la raíz de la palabra haciendo hace uso de vocabulario (importancia de diccionario de palabras) y del análisis morfológico (estructura de palabras y relaciones gramaticales).\n",
    "\n",
    "A continuación se realizan estos procesos utilizando la popular biblioteca de python: NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "S6jJZO_SmZd4",
    "outputId": "1f861401-4ed4-4603-9f90-26671645676b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/clara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/clara/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'analyze'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"analyzing\"\n",
    "lem.lemmatize(word, \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Dqv8xMrVprM8",
    "outputId": "fd78e187-8d8a-434e-926d-7ed67a49bec0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'analyz'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lAIUVBtrRGc"
   },
   "source": [
    "#### Estandarización de objetos\n",
    "\n",
    "Los datos de texto a menudo contienen palabras o frases que no están presentes en ningún diccionario léxico estándar. Estas piezas no son reconocidas por los motores de búsqueda y los modelos.\n",
    "\n",
    "Algunos de los ejemplos son: acrónimos, hashtags con palabras adjuntas y slangs coloquiales. Con la ayuda de expresiones regulares y diccionarios de datos preparados manualmente, este tipo de ruido puede solucionarse, el siguiente código utiliza un método de búsqueda en el diccionario para reemplazar el slangs de lasredes sociales de un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "t46DEB3hqPqp",
    "outputId": "9fa396d5-0aa1-438f-a7ce-444a1f617322"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retweet'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {'rt': 'Retweet', 'dm':'direct message', 'awsm': \"awesome\",\n",
    "               'luv': 'love'}\n",
    "\n",
    "def lookup_words(input_text):\n",
    "  words = input_text.split()\n",
    "  new_words = []\n",
    "  for word in words:\n",
    "    if word.lower() in lookup_dict:\n",
    "      word = lookup_dict[word.lower()]\n",
    "    new_words.append(word)\n",
    "    new_text = \" \".join(new_words)\n",
    "    return new_text\n",
    "\n",
    "lookup_words(\"RT this is a retweeted tweet by awsm C-Lara\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0yKQASXuysv"
   },
   "source": [
    "Además de los tres pasos analizados hasta ahora, otros tipos de preprocesamiento de texto incluyen codificación-decodificación de ruido, corrector gramatical y corrección ortográfica, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cOVOGgPvAAs"
   },
   "source": [
    "### Texto  a características \n",
    "\n",
    "Para analizar los datos preprocesados, es necesario convertirlos en características. Dependiendo del uso, las características del texto se pueden construir utilizando técnicas variadas: análisis sintáctico, entidades, N-grams, características basadas en palabras, características estadísticas e embeddings  de palabras.\n",
    "\n",
    "- Análisis sintáctico: el análisis sintáctico involucra el análisis de las palabras en la oración y su disposición de manera que muestra las relaciones entre las palabras. La gramática de dependencia y las etiquetas de categorías gramaticales  son los atributos importantes de las sintácticas de texto.\n",
    "\n",
    "- Árboles de dependencia: Las oraciones se componen de algunas palabras unidas entre sí. La relación entre las palabras en una oración está determinada por la gramática básica de la dependencia. La gramática de dependencia es una clase de análisis de texto sintáctico que trata con relaciones binarias asimétricas (etiquetadas) entre dos elementos léxicos (palabras). Cada relación puede representarse en forma de triplete (relation, governor, dependent). Este tipo de árbol, cuando se analiza de forma recursiva de manera descendente, proporciona tripletas de relación gramatical como salida que se pueden usar como características para muchos problemas de NLP como el análisis de sentimientos, la identificación de actores y entidades y la clasificación de texto.\n",
    "\n",
    "\n",
    "- Etiquetado de categorías gramaticales: Aparte de las relaciones gramaticales, cada palabra en una oración también está asociada con una categoría gramatical (POS) (sustantivos, verbos, adjetivos, adverbios, etc.). Las etiquetas POS definen el uso y la función de una palabra en la oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_EW0iMns7BK",
    "outputId": "dc1fa2d5-7cd4-4b84-d182-3a5d275f0ac2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/clara/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/clara/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " (\"'m\", 'VBP'),\n",
       " ('learning', 'VBG'),\n",
       " ('Natural', 'NNP'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Deep', 'NNP'),\n",
       " ('Learning', 'NNP'),\n",
       " ('on', 'IN'),\n",
       " ('Github', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Analytics', 'NNP'),\n",
       " ('Vidhya', 'NNP')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Completar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenización\n",
    "\n",
    "La tokenización es el proceso de dividir el texto en unidades significativas mínimas, como palabras, signos de puntuación, símbolos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Corpus de ejemplo\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "## Completa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-XDUx1_4fGw"
   },
   "source": [
    "El etiquetado gramatical se usa para muchos propósitos importantes en NLP:\n",
    "\n",
    "1. Desambiguación del sentido de la palabra: algunas palabras del lenguaje tienen múltiples significados según su uso. Por ejemplo, en las dos oraciones siguientes:\n",
    "\n",
    "* `\" Please book my flight for Delhi\"``\n",
    "\n",
    "* `\"I am going to read this book in the flight\"``\n",
    "\n",
    "\"Book\" se utiliza con un contexto diferente, sin embargo, la etiqueta de categoría gramatical para ambos casos es diferente. En la oración inicial , la palabra \"book\" se usa como verbo, mientras que en la segunda se usa como un sustantivo. \n",
    "\n",
    "Lectura: https://medium.com/aimonks/word-sense-disambiguation-resolving-ambiguity-in-natural-language-processing-3986a83d41fa \n",
    "\n",
    "2. Mejora de las características basadas en palabras: un modelo de aprendizaje podría aprender diferentes contextos de una palabra cuando se usan palabras como características, sin embargo, si la parte de la etiqueta de catagoría gramatical está vinculada a ellas, el contexto se conserva, por lo que se crean características sólidas. Por ejemplo:\n",
    "\n",
    "    * Oración - `\"book my flight, I will read this book\"``\n",
    "\n",
    "    * Tokens – (\"book\", 2), (\"my\", 1), (\"flight\", 1), (\"I\", 1), (\"will\", 1), (\"read\", 1), (\"this\", 1)\n",
    "\n",
    "   * Tokens with POS – (\"book_VB\", 1), (\"my_PRP$\", 1), (\"flight_NN\", 1), (\"I_PRP\", 1), (\"will_MD\", 1), (\"read_VB\", 1), (\"this_DT\", 1), (\"book_NN\", 1).\n",
    "\n",
    "3. Normalización y lematización: las etiquetas POS son la base del proceso de lematización para convertir una palabra a su forma básica (lema).\n",
    "\n",
    "4. Eliminación de stopwords suficientes: las etiquetas P OS también son útiles para la eliminación eficiente de stopwords.\n",
    "\n",
    "      Por ejemplo, hay algunas etiquetas que siempre definen las palabras de baja frecuencia/menos importantes de un lenguaje. Por ejemplo: (IN - “within”, “upon”, “except”), (CD -\"one\",\"two\", \"hundred\" ), (MD - \"may\", \"mu st\", etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas gramaticales:\n",
      "The : DT\n",
      "quick : JJ\n",
      "brown : NN\n",
      "fox : NN\n",
      "jumps : VBZ\n",
      "over : IN\n",
      "the : DT\n",
      "lazy : JJ\n",
      "dog : NN\n",
      ". : .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Oración de ejemplo\n",
    "#oracion = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fragmentación (chunking)\n",
    "\n",
    " La fragmentación implica la combinación de tokens relacionados en un solo token, la creación de grupos de sustantivos relacionados, grupos de verbos relacionados, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT black/JJ cat/NN)\n",
      "  sat/VBD\n",
      "  (PP on/IN (NP the/DT mat/NN))\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Completa\n",
    "\n",
    "#grammar = r\"\"\"\n",
    "#    NP: {<DT|JJ|NN.*>+}          # Frase nominal\n",
    "#    PP: {<IN><NP>}                # Frase preposicional\n",
    "#    VP: {<VB.*><NP|PP|CLAUSE>+$}  # Frase verbal\n",
    "#    CLAUSE: {<NP><VP>}            # Cláusula\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_KtCtRj6qJ3"
   },
   "source": [
    "### Extracción de entidades\n",
    "\n",
    "Las entidades se definen como las partes  más importantes de una oración: frases nominales, frases verbales o ambas. Los algoritmos de detección de entidades generalmente son modelos conjuntos de análisis basados en reglas,  diccionarios de búsqueda, etiquetado pos y análisis de dependencias. La aplicabilidad de la detección de entidades se puede ver en los robots de chat automatizados, los analizadores de contenido y la información del consumidor.\n",
    "\n",
    "El modelado de tópicos y el reconocimiento de entidades con nombre son los dos métodos de detección de entidades clave en el NLP.\n",
    "\n",
    "\n",
    "#####  Reconocimiento de entidad nombrada (NER)\n",
    "\n",
    "El proceso de detección de las entidades nombradas, tales como nombres de personas, nombres de ubicaciones, nombres de compañías, etc. desde el texto se llama `NER``.\n",
    "\n",
    "Por ejemplo :\n",
    "\n",
    "Sentencia: Sergey Brin, el gerente de Google Inc. está caminando por las calles de Nueva York.\n",
    "\n",
    "Entidades nombradas - (\"persona\": \"Sergey Brin\"), (\"org\": \"Google Inc.\"), (\"ubicación\": \"Nueva York\")\n",
    "\n",
    "Un modelo típico de NER consiste en tres bloques:\n",
    "\n",
    "* Identificación de frases de nombre: este paso trata de extraer todas las frases de nombres de un texto mediante el análisis de dependencia y el etiquetado de categorías gramaticales.\n",
    "\n",
    "* Clasificación de frases: este es el paso de clasificación en el que todas las frases nominales extraídas se clasifican en categorías respectivas (ubicaciones, nombres, etc.). La API de Google Maps proporciona un buen camino para eliminar la ambigüedad de las ubicaciones. Luego, las bases de datos abiertas de dbpedia y wikipedia se pueden usar para identificar nombres de personas o nombres de compañías. Aparte de esto, uno puede anotar las tablas de búsqueda y los diccionarios combinando información de diferentes fuentes.\n",
    "\n",
    "* Desambiguación de entidades: a veces es posible que las entidades se clasifiquen incorrectamente, por lo que es útil crear una capa de validación sobre los resultados. El uso de grafos de conocimiento puede ser explotado para este propósito. Los grafos de conocimiento popular son: Google Knowledge Graph, IBM Watson y Wikipedia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Oración de ejemplo\n",
    "sentence = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in 1976. Its headquarters is located in Cupertino, California.\"\n",
    "\n",
    "# Tokenización de la oración\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Etiquetado gramatical (POS tagging) de los tokens\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Reconocimiento de entidades nombradas (NER) utilizando ne_chunk\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "# Función para extraer las entidades nombradas reconocidas\n",
    "def extract_named_entities(tree):\n",
    "   pass \n",
    "# Extracción de las entidades nombradas reconocidas\n",
    "named_entities = extract_named_entities(named_entities)\n",
    "\n",
    "# Impresión de las entidades nombradas reconocidas\n",
    "print(\"Entidades nombradas reconocidas:\")\n",
    "for entity in named_entities:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelado de tópicos\n",
    "\n",
    "El modelado de tópicos es un proceso de identificación automática de los tópicos presentes en un corpus de texto, deriva los patrones ocultos entre las palabras en el corpus de una manera no supervisada. Los temas se definen como \"un patrón repetitivo de términos co-ocurrentes en un corpus\".\n",
    "Un buen modelo de tema resulta en:  \"salud\", \"médico\", \"paciente\", \"hospital\" para un tópico - Salud, y \"granja\", \"cultivos\", \"trigo\" para un tema - \"Agricultura\".\n",
    "\n",
    "Latent Dirichlet  Allocation(LDA) es una de las técnica de modelado de temas más popular, pero no es la única!. Ver: https://www.leewayhertz.com/topic-modeling-in-nlp/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Eeu-FLPCRkZ",
    "outputId": "56de5101-0e22-4105-b2ee-426f4ace0f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.061*\"driving\" + 0.061*\"my\" + 0.061*\"blood\" + 0.061*\"Doctors\" + 0.061*\"that\" + 0.061*\"and\" + 0.061*\"cause\" + 0.061*\"stress\" + 0.061*\"preassure.\" + 0.061*\"increased\"'), (1, '0.058*\"to\" + 0.058*\"My\" + 0.058*\"sister\" + 0.058*\"father\" + 0.058*\"my\" + 0.033*\"likes\" + 0.033*\"but\" + 0.033*\"consume.\" + 0.033*\"not\" + 0.033*\"Sugar\"'), (2, '0.030*\"driving\" + 0.030*\"my\" + 0.030*\"lot\" + 0.030*\"practice\" + 0.030*\"of\" + 0.030*\"dance\" + 0.030*\"around\" + 0.030*\"aa\" + 0.030*\"the\" + 0.030*\"spends\"')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father\"\n",
    "doc2 = \"My father spends aa lot of time driving my sister around the dance practice\"\n",
    "doc3 = \"Doctors suggest that driving my cause increased stress and blood preassure.\"\n",
    "\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creando el diccionario de términos de nuestro corpus, donde cada término único tiene asignado un índice\n",
    "diccionario = corpora.Dictionary(doc_clean)\n",
    "# Convertir la lista de documentos (corpus) en la Matriz de Términos del Documento utilizando el diccionario preparado anteriormente\n",
    "doc_term_matrix = [diccionario.doc2bow(doc) for doc in doc_clean]\n",
    "# Creando el objeto para el modelo LDA, usando la libreria gensim\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Corriendo y entrenando el modelo LDA en la Matriz de Términos de Documentos\n",
    "\n",
    "ldamodel = LDA(doc_term_matrix, num_topics =3, id2word =diccionario, passes = 50)\n",
    "\n",
    "# Resultados\n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: was\n",
      "Label: VBD\n",
      "Entity Link: be\n",
      "\n",
      "Entity: born\n",
      "Label: VBN\n",
      "Entity Link: bear\n",
      "\n",
      "Entity: Hawaii\n",
      "Label: NNP\n",
      "Entity Link: hawaii\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Definición de la oración de ejemplo\n",
    "sentence = \"Barack Obama was born in Hawaii.\"\n",
    "\n",
    "# Tokenización de la oración\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Etiquetado gramatical (POS tagging) de los tokens\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Mapeo de etiquetas POS de NLTK a las etiquetas POS de WordNet\n",
    "def map_pos(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Procesamiento de las entidades reconocidas\n",
    "for word, pos_tag in pos_tags:\n",
    "    # Mapeo de la etiqueta POS a la correspondiente en WordNet\n",
    "    wn_pos = map_pos(pos_tag)\n",
    "    if wn_pos:\n",
    "        # Búsqueda de sinónimos en WordNet\n",
    "        synsets = wn.synsets(word, pos=wn_pos)\n",
    "        if synsets:\n",
    "            # Seleccionar el primer synset como representante del concepto\n",
    "            entity = synsets[0].name().split('.')[0]\n",
    "            print(\"Entity:\", word)\n",
    "            print(\"Label:\", pos_tag)\n",
    "            print(\"Entity Link:\", entity)\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omZdA-p7Hpw3"
   },
   "source": [
    "#### N-gramas como características\n",
    "\n",
    "Una combinación de N palabras juntas se llama N-Gramas. Los N-gramas (N> 1) son generalmente más informativos en comparación con las palabras (unigramas) como características. Además, los bigrams (N = 2) se consideran como las características más importantes de todas las demás. El siguiente código genera bigramas de un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrIhrkSXFaKC",
    "outputId": "098b1daf-f9e4-4296-8739-06ba5e046850"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Assigns', 'the'],\n",
       " ['the', 'POS'],\n",
       " ['POS', 'tag'],\n",
       " ['tag', 'the'],\n",
       " ['the', 'most'],\n",
       " ['most', 'frequently'],\n",
       " ['frequently', 'occurring'],\n",
       " ['occurring', 'with'],\n",
       " ['with', 'a'],\n",
       " ['a', 'word'],\n",
       " ['word', 'in'],\n",
       " ['in', 'the'],\n",
       " ['the', 'training'],\n",
       " ['training', 'corpus']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(texto, n):\n",
    "  words = texto.split()\n",
    "  output = []\n",
    "  for i in range(len(words) -n +1 ):\n",
    "    output.append(words[i:i + n])\n",
    "\n",
    "  return output\n",
    "\n",
    "generate_ngrams(\"Assigns the POS tag the most frequently occurring with a word in the training corpus\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSttRoo_LAWJ"
   },
   "source": [
    "### Estadisticas de  características\n",
    "\n",
    "Los datos de texto también se pueden cuantificar directamente en números usando varias técnicas descritas a continuación\n",
    "\n",
    "\n",
    "####  Frecuencia de términos-Frecuencia de documento inversa (TF - IDF)\n",
    "\n",
    "TF-IDF es un modelo ponderado comúnmente utilizado para problemas de recuperación de información. Su objetivo es convertir los documentos de texto en modelos vectoriales en función de la aparición de palabras en los documentos sin tener en cuenta el orden exacto. \n",
    "\n",
    "La fórmula general para calcular TF-IDF es: \n",
    "\n",
    "$$TF-IDF(t,d,D)=TF(t,d)\\times IDF(t,D)$$ \n",
    "\n",
    "Donde: \n",
    "\n",
    "- `t`  es el término. \n",
    "- `d` es el documento. \n",
    "- `D` es el conjunto de documentos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTnag8OeKyfX",
    "outputId": "1f40b319-b918-4f9d-ff00-c57875e4f51e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/clara/anaconda3/lib/python3.7/site-packages (0.24.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/clara/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/clara/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/clara/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/clara/anaconda3/lib/python3.7/site-packages (from scikit-learn) (2.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "  (0, 1)\t0.2805198619076071\n",
      "  (0, 10)\t0.36122039714507487\n",
      "  (0, 16)\t0.36122039714507487\n",
      "  (0, 14)\t0.36122039714507487\n",
      "  (0, 3)\t0.2805198619076071\n",
      "  (0, 7)\t0.47496141327993013\n",
      "  (0, 11)\t0.47496141327993013\n",
      "  (1, 1)\t0.2805198619076071\n",
      "  (1, 10)\t0.36122039714507487\n",
      "  (1, 16)\t0.36122039714507487\n",
      "  (1, 14)\t0.36122039714507487\n",
      "  (1, 3)\t0.2805198619076071\n",
      "  (1, 0)\t0.47496141327993013\n",
      "  (1, 12)\t0.47496141327993013\n",
      "  (2, 1)\t0.1298209308406929\n",
      "  (2, 3)\t0.1298209308406929\n",
      "  (2, 6)\t0.21980594303058684\n",
      "  (2, 15)\t0.6594178290917605\n",
      "  (2, 5)\t0.21980594303058684\n",
      "  (2, 9)\t0.4396118860611737\n",
      "  (2, 2)\t0.21980594303058684\n",
      "  (2, 17)\t0.21980594303058684\n",
      "  (2, 8)\t0.21980594303058684\n",
      "  (2, 4)\t0.21980594303058684\n",
      "  (2, 13)\t0.21980594303058684\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = [\"Document retrieval using TF-IDF matching score\", \"Document retrieval using TF-IDF cosine similarity\",\n",
    "         \"IDF is the inverse of the document frequency which measures the informativeness of term.\"]\n",
    "\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QSK38tIP7Qo"
   },
   "source": [
    "El modelo crea un diccionario de vocabulario y asigna un índice a cada palabra. Cada fila en la salida contiene una tupla `(i, j)`` y un valor tf-idf de palabra en el índice j en el documento i.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSJKbAn0RMJn"
   },
   "source": [
    "### Embeddings  de palabras (vectores de texto)\n",
    "\n",
    "El embeddings de palabras es la forma moderna de representar palabras como vectores. El objetivo del embeddings  de palabras es redefinir las características de las palabras de alta dimensión en vectores de características de baja dimensión, preservando la similitud contextual en el corpus. Se utilizan ampliamente en modelos de aprendizaje profundo, como las redes neuronales convolucionales y las redes neuronales recurrentes.\n",
    "\n",
    "Word2Vec y GloVe son los dos modelos populares para crear embedding de texto de un texto. Estos modelos toman un cuerpo de texto como entrada y producen los vectores de palabras como salida.\n",
    "\n",
    "**El modelo de Word2Vec se compone de un módulo de preprocesamiento, un modelo de red neuronal superficial llamado Continuous Bag of Words y otro modelo de red neuronal superficial llamado skip-gram**. Estos modelos son ampliamente utilizados para todos los demás problemas del NLP.\n",
    "\n",
    "Lectura: https://www.ruder.io/word-embeddings-1/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgBbLzPxOoFI",
    "outputId": "2ce9a90f-75e6-48d8-b5d9-4aba1cffb41e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.023671668"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['cesar', 'science', 'data', 'analytics'], ['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# Entrenamiento del modelo en el corpus\n",
    "model = Word2Vec(sentences, min_count =1)\n",
    "model.wv.similarity('data','science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USKyzaCIcIkG"
   },
   "source": [
    "Se pueden usar como vectores de características para un modelo ML, para medir la similitud del texto utilizando técnicas de similitud de coseno, clustering de palabras y técnicas de clasificación de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnbyczR0ceeg"
   },
   "source": [
    "## Tareas elementales importantes del NLP\n",
    "\n",
    "Mostramos diferentes casos de uso y problemas en el campo del procesamiento del lenguaje natural.\n",
    "\n",
    "### Clasificación de texto\n",
    "\n",
    "La clasificación de textos es uno de los problemas clásicos del NLP. Entre los ejemplos notorios se incluyen: Identificación de correo electrónico no deseado, clasificación de temas de noticias, clasificación de opiniones y organización de páginas web por motores de búsqueda.\n",
    "\n",
    "La clasificación de texto,  se define como una técnica para clasificar sistemáticamente un objeto de texto (documento u oración) en una de las categorías fijas. Es realmente útil cuando la cantidad de datos es demasiado grande, especialmente para fines de organización, filtrado de información y almacenamiento.\n",
    "\n",
    "Un clasificador típico de lenguaje natural consta de dos partes: (a) Entrenamiento (b) Predicción. En primer lugar la entrada de texto se procesa y se crean características. Los modelos de aprendizaje automático aprenden estas características y se utilizan para predecir el nuevo texto.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7gvSnb7Y-k0",
    "outputId": "77cb5645-eccf-4cd0-ceef-021761e10664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /home/clara/anaconda3/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/clara/anaconda3/lib/python3.7/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in /home/clara/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: click in /home/clara/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/clara/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /home/clara/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.1.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/clara/anaconda3/lib/python3.7/site-packages (from click->nltk>=3.1->textblob) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/clara/anaconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk>=3.1->textblob) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/clara/anaconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk>=3.1->textblob) (3.11.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Código que usa un clasificador de Bayes que usa la biblioteca de  textblob (construida sobre NLTK)\n",
    "\n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "\n",
    "corpus_entrenamiento = [\n",
    "                       ('I am exhausted of this work.', 'Class_B'),\n",
    "                       (\"I can't cooperate with this\", 'Class_B'),\n",
    "                       ('He is my badest enemy!', 'Class_B'),\n",
    "                       ('My management is poor.', 'Class_B'),\n",
    "                       ('I love this burger.', 'Class_A'),\n",
    "                       ('This is an brilliant place!', 'Class_A'),\n",
    "                       ('I feel very good about these dates.', 'Class_A'),\n",
    "                       ('This is my best work.', 'Class_A'),\n",
    "                       (\"What an awesome view\", 'Class_A'),\n",
    "                       ('I do not like this dish', 'Class_B')]\n",
    "corpus_prueba = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'),\n",
    "                (\"I feel brilliant!\", 'Class_A'),\n",
    "                ('Gary is a friend of mine.', 'Class_A'),\n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'),\n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4j-PWAPMfHx4",
    "outputId": "977752db-9d5b-447e-873b-b2197d8fef83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n"
     ]
    }
   ],
   "source": [
    "model = NBC(corpus_entrenamiento)\n",
    "print(model.classify(\"Comentarios\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2jZNi-AfUpL",
    "outputId": "f311ca50-8266-41c2-b80f-2e559ee77959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_B\n"
     ]
    }
   ],
   "source": [
    "print(model.classify(\"I don't like their computer.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGNDcZ0Bffcp",
    "outputId": "3f25c7e1-1ef9-440f-ea1d-3427904857fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(model.accuracy(corpus_prueba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbfRn9rLfvBl"
   },
   "source": [
    "Scikit Learn proporciona un framework pipeline para clasificación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BuqIxX0f54B",
    "outputId": "8b3692f7-0963-45b4-c22a-e7edf53e9004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class_A       0.50      0.67      0.57         3\n",
      "     Class_B       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.50         6\n",
      "   macro avg       0.50      0.50      0.49         6\n",
      "weighted avg       0.50      0.50      0.49         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "# preparando datos para el modelo SVM (usando el corpus_entrenamiento, corpus_prueba desde el ejemplo anterior)\n",
    "\n",
    "datos_entrenamiento = []\n",
    "etiquetas_entrenamiento = []\n",
    "\n",
    "for fila in corpus_entrenamiento:\n",
    "  datos_entrenamiento.append(fila[0])\n",
    "  etiquetas_entrenamiento.append(fila[1])\n",
    "\n",
    "\n",
    "datos_prueba =[]\n",
    "etiquetas_pruebas = []\n",
    "for fila in corpus_prueba:\n",
    "  datos_prueba.append(fila[0])\n",
    "  etiquetas_pruebas.append(fila[1])\n",
    "\n",
    "# Creamos vectores caracteristicas\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "\n",
    "# Entrenamiento los vectores caracteristicas\n",
    "\n",
    "vectores_entrenamiento = vectorizer.fit_transform(datos_entrenamiento)\n",
    "\n",
    "# Aplicar el modelo sobre el conjunto de pruebas\n",
    "\n",
    "vectores_prueba =vectorizer.transform(datos_prueba)\n",
    "\n",
    "# Desempeño de la clasificacion de SVM kernel=linear\n",
    "\n",
    "model = svm.SVC(kernel=\"linear\")\n",
    "model.fit(vectores_entrenamiento, etiquetas_entrenamiento)\n",
    "prediction = model.predict(vectores_prueba)\n",
    "\n",
    "print (classification_report(etiquetas_pruebas, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH0BPG-Zd5x9"
   },
   "source": [
    "El modelo de clasificación de texto depende en gran medida de la calidad y cantidad de las características, mientras que al aplicar cualquier modelo de aprendizaje automático, siempre es una buena práctica incluir más y más datos de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQBW-a_nm4IF"
   },
   "source": [
    "#### Coincidencia de texto/ similitud\n",
    "\n",
    "Una de las áreas importantes del NLP es la coincidencia de objetos de texto para encontrar similitudes. Las aplicaciones importantes de la coincidencia de texto incluyen la corrección automática de la ortografía, la deduplicación de datos y el análisis del genoma, etc.\n",
    "\n",
    "Una serie de técnicas de coincidencia de texto están disponibles según el requerimiento.\n",
    "\n",
    "*  Distancia de Levenshtein: la distancia de Levenshtein entre dos cadenas se define como el número mínimo de ediciones necesarias para transformar una cadena en otra, con las operaciones de edición permitidas que son la inserción, eliminación o sustitución de un solo carácter. A continuación se muestra la implementación para cálculos de memoria eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhPthSwieBjF",
    "outputId": "59452a12-a80a-4ca2-c3c3-fa35b67b0c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def levenshtein(s1,s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1\n",
    "    distancias = range(len(s1) + 1)\n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistancias = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistancias.append(distancias[index1])\n",
    "            else:\n",
    "                 newDistancias.append(1 + min((distancias[index1], distancias[index1+1], newDistancias[-1])))\n",
    "        distancias = newDistancias\n",
    "    return distancias[-1]\n",
    "\n",
    "print(levenshtein(\"analyze\",\"analyse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cot3-3V2oYMW"
   },
   "source": [
    "* Coincidencia fonética: un algoritmo de coincidencia fonética toma una palabra clave como entrada (nombre de una persona, nombre de la ubicación, etc.) y produce una cadena de caracteres que identifica un conjunto de palabras que son (aproximadamente) fonéticamente similares. Es muy útil para buscar grandes corpuses de texto, corregir errores de ortografía y hacer coincidir nombres relevantes. Soundex y Metaphone son dos algoritmos fonéticos principales utilizados para este propósito. Revisar: https://medium.com/@ievgenii.shulitskyi/phonetic-matching-algorithms-50165e684526 \n",
    "\n",
    "*  Coincidencia de cadenas flexible: un completo sistema de correspondencia de texto incluye diferentes algoritmos agrupados para calcular una variedad de variaciones de texto. Las expresiones regulares son realmente útiles para este propósito también. Otras técnicas comunes incluyen: coincidencia exacta de cadenas, correspondencia lematizada y correspondencia compacta (se ocupa de los espacios, puntuación, slangs, etc.).\n",
    "\n",
    "* Similitud de coseno - Cuando el texto se representa como notación vectorial, también se puede aplicar una similitud de coseno general para medir la similitud vectorizada. El siguiente código convierte un texto en vectores (usando el término frecuencia) y aplica la similitud de coseno para proporcionar cercanía entre dos textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xA7Gx-9aoeWa",
    "outputId": "397aacae-a20d-4e37-e236-63b4e6e63815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1543033499620919\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "def coseno_(vec1, vec2):\n",
    "    comun = set(vec1.keys())& set(vec2.keys())\n",
    "    numerador = sum([vec1[x]*vec2[x] for x in comun])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominador = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominador:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerador)/ denominador\n",
    "\n",
    "def texto_a_vector(text):\n",
    "    words = text.split()\n",
    "    return Counter(words)\n",
    "\n",
    "text1 = 'This is an works of NLTK'\n",
    "text2 = 'The work is about natural language processing'\n",
    "\n",
    "vector1 = texto_a_vector(text1)\n",
    "vector2 = texto_a_vector(text2)\n",
    "coseno = coseno_(vector1, vector2)\n",
    "\n",
    "print(coseno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn86lKikwdNu"
   },
   "source": [
    "#### Resolución de la coreferencia\n",
    "\n",
    "La resolución de la coreferencia  es un proceso de búsqueda de vínculos relacionales entre las palabras (o frases) dentro de las oraciones. Considere una oración de ejemplo: \"Donald went to John’s office to see the new table. He looked at it for an hour\".\n",
    "\n",
    "Los humanos pueden darse cuenta rápidamente de que \"él\" denota a Donald (y no a John), y que \"esto\" denota la mesa (y no la oficina de John). La resolución de referencia es el componente de NLP  que hace este trabajo automáticamente. Se utiliza para resumir documentos, responder preguntas y extraer información. Stanford CoreNLP proporciona una envoltura de python para fines comerciales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5xlx1-gxW-W"
   },
   "source": [
    "#### Ejercicio\n",
    "\n",
    "Explora código en python y NLTK de las siguientes tareas de NLP:\n",
    "\n",
    "* Resumenes de texto: dado un artículo de texto o párrafo, resumirlo  automáticamente para producir las oraciones más importantes y relevantes ordenadas\n",
    "* Traducción automática: traduce automáticamente el texto de un idioma humano a otro mediante el cuidado de la gramática, la semántica y la información sobre el mundo real, etc.\n",
    "*  Generación y comprensión del lenguaje natural: la conversión de información de bases de datos informáticas o intenciones semánticas a lenguaje humano legible se denomina generación de lenguaje. La conversión de fragmentos de texto en estructuras más lógicas que son más fáciles de manipular por los programas de computadora se denomina comprensión del lenguaje.\n",
    "*  Reconocimiento óptico de caracteres: dada una imagen que representa un texto impreso, determine el texto correspondiente.\n",
    "* Documento a información: implica el análisis de datos textuales presentes en documentos (sitios web, archivos, pdf e imágenes) en un formato analizable y limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMzIQetQ2T8D"
   },
   "source": [
    "## Librerías importantes para NLP (python)\n",
    "\n",
    "* Scikit-learn: Aprendizaje automático en Python\n",
    "* Natural Language Toolkit (NLTK): el conjunto de herramientas completo para todas las técnicas de NLP.\n",
    "* spaCy - Paquete industrial de NLP con Python y Cython.\n",
    "* HuggingFaces es una comunidad y una plataforma que proporciona herramientas y recursos de vanguardia en NLP, facilitando el desarrollo y la implementación de soluciones basadas en inteligencia artificial en una amplia variedad de aplicaciones y dominios.\n",
    "* fast.ai, es una plataforma integral que proporciona herramientas, recursos educativos y una comunidad activa para hacer que el aprendizaje profundo sea más accesible y práctico para todos, con el objetivo de democratizar la inteligencia artificial y fomentar la innovación en este campo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dx4MDwVK3pSY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
