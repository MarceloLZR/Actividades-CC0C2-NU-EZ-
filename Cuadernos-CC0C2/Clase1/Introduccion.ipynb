{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A5xOQz5emHC"
   },
   "source": [
    "## ¿Qué es el procesamiento del lenguaje natural?  \n",
    "\n",
    "Los lenguajes son una parte crucial de la inteligencia humana e importantes para la comunicación humana. Al investigar la comprensión automática y la generación de lenguajes humanos, el procesamiento del lenguaje natural (NLP) ha sido un subcampo central de la investigación en inteligencia artificial. Desde la década de 1950, la tecnología de NLP ha recibido una atención continua por parte de la investigación y se han logrado grandes avances. Hoy en día, la tecnología NLP se está convirtiendo en una parte indispensable de nuestro negocio y de nuestra vida diaria. Por ejemplo, los motores de búsqueda procesan automáticamente billones de documentos a través de Internet, obtienen conocimientos de ellos y responden a las consultas de los usuarios basándose en su comprensión. Los minoristas en línea procesan millones de descripciones de productos y comentarios de usuarios para recomendar el producto más adecuado según la búsqueda de un usuario. Los sistemas de diálogo automático y los sistemas de traducción son cada vez más utilizados para facilitar la comunicación. En los negocios, los motores de análisis de texto han estado reemplazando el trabajo manual en el análisis de grandes cantidades de documentos para una mejor toma de decisiones. \n",
    "\n",
    "Este progreso ha sido impulsado en gran medida por avances en las técnicas de aprendizaje automático, que contribuyen a los algoritmos fundamentales y los modelos fundamentales de la NLP.\n",
    "\n",
    "En el sentido más amplio, el NLP se refiere al estudio del procesamiento o síntesis automáticos de los lenguajes humanos. Esto puede variar desde simples algoritmos de coincidencia de patrones de cadenas utilizando expresiones regulares hasta sistemas inteligentes sofisticados que usan redes neuronales artificiales para traducir entre diferentes lenguajes. \n",
    "\n",
    "El NLP es un área de investigación interdisciplinaria. Algunos trabajos se sitúan entre la lingüística y la informática, investigando métodos computacionales para modelar lenguajes y abordar preguntas lingüísticas. Algunos trabajos se originan desde una perspectiva de inteligencia artificial, con el objetivo de dotar a los sistemas inteligentes con capacidades de lenguaje humano. Algunos trabajos están más orientados a la ciencia de datos, considerando el procesamiento automático de datos de texto a gran escala para extraer conocimiento estructurado útil. También hay trabajos relacionados con la psicología, la ciencia cognitiva y la ciencia neural. Veremos el espectro de tareas investigadas en la investigación de NLP con más detalles sobre tareas representativas luego. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto de inflexión \n",
    "\n",
    "Desde finales de la década de 1980, los métodos estadísticos y el aprendizaje automático gradualmente reemplazaron los métodos basados en reglas tanto en la literatura de investigación como en la industria. La idea es que los algoritmos aprendan las distribuciones estadísticas de patrones lingüísticos a partir de datos y los utilicen para tomar decisiones. Por ejemplo, si un verbo es seguido por un sustantivo más frecuentemente que por otro verbo en los datos, entonces asignamos una mayor probabilidad al “sustantivo” cuando vemos una palabra desconocida o ambigua después de un verbo. Este método resultó ser mucho más efectivo para tratar con las ambigüedades prevalentes en los lenguajes en comparación con la creación de reglas codificadas de forma rígida. Como resultado, hubo un resurgimiento de la investigación de NLP. \n",
    "\n",
    "Desde finales de la década de 2000, el aprendizaje profundo ha surgido para superar los métodos estadísticos tradicionales como el enfoque dominante. La idea es entrenar redes neuronales artificiales de múltiples capas apiladas, las cuales tienen el poder de aprender funciones complejas arbitrariamente. El aprendizaje profundo ha demostrado empíricamente resultados más fuertes en comparación con los métodos estadísticos tradicionales para una amplia gama de tareas de NLP. Tales modelos pueden ser casi libres de características lingüísticas, dependiendo completamente de las redes neuronales para aprender las asociaciones subyacentes entre entradas y salidas. Esta forma de aprendizaje automático también se conoce como aprendizaje de extremo a extremo. \n",
    "\n",
    "La tendencia de investigación ha debilitado aún más la influencia de la lingüística en la investigación de NLP, pero ha nutrido más la investigación de tareas integrales y aplicaciones para usuarios. A pesar del fuerte poder de las representaciones neuronales, la falta de patrones lingüísticos puede hacer que los modelos sean menos interpretables o visualizables. Los métodos estadísticos discretos tradicionales y los enfoques actuales de aprendizaje profundo están profundamente conectados, con los mismos principios de aprendizaje automático subyacentes y técnicas de optimización. Sin embargo, cada método ofrece ventajas y limitaciones únicas desde perspectivas científicas y de ingeniería.  \n",
    "\n",
    "El NLP  hizo olas a partir de 2014 con el lanzamiento de Amazon Alexa, un renovado Apple Siri, Google Assistant y Microsoft Cortana. Google también lanzó una versión muy mejorada de Google Translate en 2016, y ahora los chatbots y voicebots son mucho más comunes. Dicho esto, no fue hasta 2018 que NLP tuvo su propio momento ImageNet con el lanzamiento de grandes modelos de lenguaje preentrenados entrenados con la arquitectura Transformer; el más notable de estos fue  BERT de Google, que se lanzó en noviembre de 2018. \n",
    "En 2019, los modelos generativos como GPT-2 de OpenAI causaron sensación, generando nuevo contenido sobre la marcha basado en contenido anterior, una hazaña que antes era insuperable. En 2020, OpenAI lanzó una versión aún más grande e impresionante, GPT-3, basada en sus éxitos anteriores. \n",
    "De cara al 2023 y más allá, la NLP ya no es un subcampo experimental de la IA. Junto con la visión por computadora, NLP ahora está preparado para tener muchas aplicaciones de base amplia en la empresa.  \n",
    "\n",
    "Enlace: https://medium.com/@smdtechnosol/the-success-of-nlp-natural-language-processing-in-2024-c123d8341733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxiEHG4rhHwN"
   },
   "source": [
    "### Introducción al preprocesamiento de texto\n",
    "\n",
    "Dado que, el texto es la forma más no estructurada de todos los datos disponibles, hay varios tipos de ruido en él y los datos no son fácilmente analizables sin ningún procesamiento previo. Todo el proceso de limpieza y estandarización del texto, que lo hace sin ruido y listo para el análisis, se conoce como preprocesamiento de texto.\n",
    "\n",
    "Se compone principalmente de tres pasos:\n",
    "\n",
    " *  Eliminación de ruido\n",
    " *  Normalización del léxico\n",
    " *  Estandarización de objetos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de ruido\n",
    "\n",
    "Cualquier fragmento de texto que no sea relevante para el contexto de los datos y la salida final se puede especificar como ruido.\n",
    "\n",
    "Por ejemplo, `stopwords`  (palabras de uso común de un idioma, es, am, the, of, in, etc), URL o enlaces, entidades de medios sociales (menciones, hashtags), puntuaciones y palabras específicas de la industria. Este paso trata sobre la eliminación de todos los tipos de entidades ruidosas presentes en el texto.\n",
    "\n",
    "Un enfoque general para la eliminación de ruido es preparar un diccionario de entidades ruidosas e iterar el objeto de texto con tokens (o con palabras), eliminando esos tokens que están presentes en el diccionario de ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "oUg9GnzOhBFr",
    "outputId": "68a3ea02-82f5-49f4-827b-adade8ecb091"
   },
   "outputs": [],
   "source": [
    "## Simple código para remover palabras ruidosas de texto\n",
    "\n",
    "lista_ruido = [\"is\", \"a\", \"of,\"\"this\", \"...\"]\n",
    "def remove_ruido(input_text):\n",
    "    words =input_text.split()\n",
    "    noise_free_words = [word for word in words if word not in lista_ruido]\n",
    "    noise_free_text = \"\".join(noise_free_words)\n",
    "    return noise_free_text\n",
    "\n",
    "remove_ruido(\"Word Embeddings is a form of represent ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlR_gFI4kvhJ"
   },
   "source": [
    "Otro enfoque es utilizar las expresiones regulares al tratar con patrones especiales de ruido.  El siguiente código de Python elimina un patrón de expresiones regulares de un texto de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Dp1N8ftsk7jd",
    "outputId": "f4228271-062b-4d5c-aa53-a080d07543d2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_regex(input_text, regex_pattern):\n",
    "  urls = re.finditer(regex_pattern, input_text)\n",
    "  for i in urls:\n",
    "    input_text = re.sub(i.group().strip(), '', input_text)\n",
    "  return input_text\n",
    "\n",
    "regex_pattern = '#[\\w]*'\n",
    "remove_regex(\"Word Embeddings is a #form of represent ...*\", regex_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOZJZy8VmsJr"
   },
   "source": [
    "#### Normalización del léxico\n",
    "\n",
    "Otro tipo de ruido textual es acerca de las múltiples representaciones exhibidas por una sola palabra.\n",
    "\n",
    "Por ejemplo, `play`, `played`, `playing`, son diferentes variaciones de la palabra `play`, aunque significan cosas diferentes, contextualmente todas son similares. El paso convierte todas las disparidades de una palabra en su forma normalizada (también conocida como `lema`). \n",
    "\n",
    "\n",
    "Las prácticas de normalización del léxico más comunes son:\n",
    "\n",
    "* `Stemming`:  es un proceso rudimentario basado en reglas para eliminar los sufijos (`ing`, `ly`, `es`, `s`, etc.) de una palabra (para el idioma inglés por ejemplo).\n",
    "* `Lemmatización`:  es un procedimiento organizado y paso a paso para obtener la forma de la raíz de la palabra haciendo hace uso de vocabulario (importancia de diccionario de palabras) y del análisis morfológico (estructura de palabras y relaciones gramaticales).\n",
    "\n",
    "A continuación se realizan estos procesos utilizando la popular biblioteca de python: NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "S6jJZO_SmZd4",
    "outputId": "1f861401-4ed4-4603-9f90-26671645676b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"analyzing\"\n",
    "lem.lemmatize(word, \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Dqv8xMrVprM8",
    "outputId": "fd78e187-8d8a-434e-926d-7ed67a49bec0"
   },
   "outputs": [],
   "source": [
    "stem.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo más completo que aplique tanto la lematización como el stemming de palabra a ese texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Descargar recursos lingüísticos necesarios\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Crear instancias de lematizador y stemming de palabras\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"Scientists are analyzing various aspects of the data to draw meaningful conclusions.\"\n",
    "\n",
    "# Tokenización del texto en palabras\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Lematización y stemming de palabras\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, 'v') for word in tokens]\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Texto original:\", text)\n",
    "print(\"Palabras lematizadas:\", lemmatized_words)\n",
    "print(\"Palabras con stemming:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregamos un ejemplo que incluya lematización y stemming de palabras para ambos idiomas, inglés y español. Para ello,se utiliza NLTK con `WordNet` para inglés y un recurso llamado `omw` para español. Aquí está el código:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "# Crear instancias de lematizador y truncador de palabras para inglés\n",
    "lemmatizer_en = WordNetLemmatizer()\n",
    "stemmer_en = SnowballStemmer('english')\n",
    "\n",
    "# Crear instancias de lematizador y truncador de palabras para español\n",
    "lemmatizer_es = nltk.WordNetLemmatizer()\n",
    "stemmer_es = SnowballStemmer('spanish')\n",
    "\n",
    "# Texto de ejemplo en inglés\n",
    "text_en = \"Scientists are analyzing various aspects of the data to draw meaningful conclusions.\"\n",
    "\n",
    "# Tokenización del texto en palabras para inglés\n",
    "tokens_en = nltk.word_tokenize(text_en)\n",
    "\n",
    "# Lematización y truncamiento de palabras para inglés\n",
    "lemmatized_words_en = [lemmatizer_en.lemmatize(word, 'v') for word in tokens_en]\n",
    "stemmed_words_en = [stemmer_en.stem(word) for word in tokens_en]\n",
    "\n",
    "# Texto de ejemplo en español\n",
    "text_es = \"Los científicos están analizando diversos aspectos de los datos para sacar conclusiones significativas.\"\n",
    "\n",
    "# Tokenización del texto en palabras para español\n",
    "tokens_es = nltk.word_tokenize(text_es)\n",
    "\n",
    "# Lematización y truncamiento de palabras para español\n",
    "lemmatized_words_es = [lemmatizer_es.lemmatize(word, 'v') for word in tokens_es]\n",
    "stemmed_words_es = [stemmer_es.stem(word) for word in tokens_es]\n",
    "\n",
    "# Imprimir resultados para inglés\n",
    "print(\"Texto original en inglés:\", text_en)\n",
    "print(\"Palabras lematizadas en inglés:\", lemmatized_words_en)\n",
    "print(\"Palabras con stemming en inglés:\", stemmed_words_en)\n",
    "print()\n",
    "\n",
    "# Imprimir resultados para español\n",
    "print(\"Texto original en español:\", text_es)\n",
    "print(\"Palabras lematizadas en español:\", lemmatized_words_es)\n",
    "print(\"Palabras con stemming en español:\", stemmed_words_es)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lAIUVBtrRGc"
   },
   "source": [
    "#### Estandarización de objetos\n",
    "\n",
    "Los datos de texto a menudo contienen palabras o frases que no están presentes en ningún diccionario léxico estándar. Estas piezas no son reconocidas por los motores de búsqueda y los modelos.\n",
    "\n",
    "Algunos de los ejemplos son: acrónimos, hashtags con palabras adjuntas y slangs coloquiales. Con la ayuda de expresiones regulares y diccionarios de datos preparados manualmente, este tipo de ruido puede solucionarse, el siguiente código utiliza un método de búsqueda en el diccionario para reemplazar el slangs de las redes sociales de un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "t46DEB3hqPqp",
    "outputId": "9fa396d5-0aa1-438f-a7ce-444a1f617322"
   },
   "outputs": [],
   "source": [
    "lookup_dict = {'rt': 'Retweet', 'dm':'direct message', 'awsm': \"awesome\",\n",
    "               'luv': 'love'}\n",
    "\n",
    "def lookup_words(input_text):\n",
    "  words = input_text.split()\n",
    "  new_words = []\n",
    "  for word in words:\n",
    "    if word.lower() in lookup_dict:\n",
    "      word = lookup_dict[word.lower()]\n",
    "    new_words.append(word)\n",
    "    new_text = \" \".join(new_words)\n",
    "    return new_text\n",
    "\n",
    "lookup_words(\"RT this is a retweeted tweet by awsm C-Lara\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver otro ejemplo de cómo podrías utilizar expresiones regulares junto con NLTK (Natural Language Toolkit) para explicar el concepto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Definir una función para eliminar acrónimos\n",
    "def eliminar_acronimos(texto):\n",
    "    return re.sub(r'\\b[A-Z]{2,}\\b', '', texto)\n",
    "\n",
    "# Definir una función para eliminar hashtags con palabras adjuntas\n",
    "def eliminar_hashtags(texto):\n",
    "    return re.sub(r'#\\w+\\b', '', texto)\n",
    "\n",
    "# Definir una lista de slangs coloquiales para eliminar\n",
    "slangs = ['lol', 'rofl', 'omg', 'btw']  # Agrega más slangs según sea necesario\n",
    "\n",
    "# Definir una función para eliminar slangs coloquiales\n",
    "def eliminar_slangs(texto):\n",
    "    return ' '.join(word for word in word_tokenize(texto) if word.lower() not in slangs)\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"LOL, #Python is so #awesome! BTW, OMG I'm ROFL! This is incredible!\"\n",
    "\n",
    "# Eliminar acrónimos\n",
    "texto_sin_acronimos = eliminar_acronimos(texto)\n",
    "\n",
    "# Eliminar hashtags con palabras adjuntas\n",
    "texto_sin_hashtags = eliminar_hashtags(texto_sin_acronimos)\n",
    "\n",
    "# Eliminar slangs coloquiales\n",
    "texto_final = eliminar_slangs(texto_sin_hashtags)\n",
    "\n",
    "print(\"Texto original:\", texto)\n",
    "print(\"Texto procesado:\", texto_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0yKQASXuysv"
   },
   "source": [
    "Además de los tres pasos analizados hasta ahora, otros tipos de preprocesamiento de texto incluyen codificación-decodificación de ruido, corrector gramatical y corrección ortográfica, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenización\n",
    "\n",
    "La tokenización es el proceso de dividir el texto en unidades significativas mínimas, como palabras, signos de puntuación, símbolos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Corpus de ejemplo\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Tokenización del corpus\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "# Crear una lista de todas las palabras únicas en el corpus\n",
    "all_words = sorted(set(word for doc in tokenized_corpus for word in doc))\n",
    "\n",
    "# Crear un diccionario que mapea cada palabra única a un índice único\n",
    "word_index = {word: i for i, word in enumerate(all_words)}\n",
    "\n",
    "# Calcular la frecuencia de los tokens en el corpus\n",
    "freq_dist = FreqDist(word for doc in tokenized_corpus for word in doc)\n",
    "\n",
    "# Crear una matriz dispersa que represente la frecuencia de los tokens en cada documento\n",
    "num_docs = len(corpus)\n",
    "vocab_size = len(all_words)\n",
    "sparse_matrix = lil_matrix((num_docs, vocab_size))\n",
    "for i, doc in enumerate(tokenized_corpus):\n",
    "    for word in doc:\n",
    "        word_idx = word_index[word]\n",
    "        sparse_matrix[i, word_idx] += 1\n",
    "\n",
    "print(\"Matriz dispersa que representa la frecuencia de los tokens en cada documento:\")\n",
    "print(sparse_matrix.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentario:** La dispersión o `sparsity` en el contexto de análisis de texto se refiere a la proporción de celdas vacías o nulas en una matriz que representa datos. Es una medida de cuántos elementos de la matriz son cero en comparación con el total de elementos de la matriz.\n",
    "\n",
    "En el contexto de tokenización y análisis de texto, se refiere a cuán dispersos están los términos (tokens) en el corpus de documentos. Si un corpus tiene una alta dispersión, significa que los términos en el vocabulario no están muy `distribuidos` en los documentos y, por lo tanto, cada documento contiene solo una fracción de los términos del vocabulario. Por otro lado, si un corpus tiene una baja dispersión, significa que los términos están más uniformemente distribuidos entre los documentos.\n",
    "\n",
    "La dispersión es importante en el procesamiento de lenguaje natural y el análisis de texto porque puede afectar la eficacia de los algoritmos de modelado de tópicos, clasificación de documentos y otras tareas. Una matriz densa (baja dispersión) puede ser costosa en términos de almacenamiento y cálculos, mientras que una matriz dispersa puede ser más eficiente en estos aspectos. Sin embargo, también puede presentar desafíos en términos de manejo de datos dispersos y selección de características en algunos algoritmos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto  a características \n",
    "\n",
    "Para analizar los datos preprocesados, es necesario convertirlos en características. Dependiendo del uso, las características del texto se pueden construir utilizando técnicas variadas: análisis sintáctico, entidades, N-grams, características basadas en palabras, características estadísticas e embeddings de palabras.\n",
    "\n",
    "El análisis sintáctico involucra el análisis de las palabras en la oración y su disposición de manera que muestra las relaciones entre las palabras. La gramática de dependencia y las etiquetas de categorías gramaticales  son los atributos importantes de las sintácticas de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det cat\n",
      "cat nsubj sat\n",
      "sat ROOT sat\n",
      "on prep sat\n",
      "the det mat\n",
      "mat pobj on\n",
      ". punct sat\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"bfaf0f7b390649f183a2349a0ac4af3c-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">cat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">sat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">mat.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bfaf0f7b390649f183a2349a0ac4af3c-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bfaf0f7b390649f183a2349a0ac4af3c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bfaf0f7b390649f183a2349a0ac4af3c-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bfaf0f7b390649f183a2349a0ac4af3c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bfaf0f7b390649f183a2349a0ac4af3c-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bfaf0f7b390649f183a2349a0ac4af3c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bfaf0f7b390649f183a2349a0ac4af3c-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bfaf0f7b390649f183a2349a0ac4af3c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bfaf0f7b390649f183a2349a0ac4af3c-0-4\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 925.0,2.0 925.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bfaf0f7b390649f183a2349a0ac4af3c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,179.0 L933.0,167.0 917.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de idioma en inglés de spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Oración de ejemplo\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# Procesar la oración con spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Imprimir el árbol de dependencias sintácticas\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text)\n",
    "\n",
    "# Dibujar el árbol de dependencias sintácticas\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Árboles de dependencia: Las oraciones se componen de algunas palabras unidas entre sí. La relación entre las palabras en una oración está determinada por la gramática básica de la dependencia. La gramática de dependencia es una clase de análisis de texto sintáctico que trata con relaciones binarias asimétricas (etiquetadas) entre dos elementos léxicos (palabras). Cada relación puede representarse en forma de triplete (`relation`, `governor`, `dependent`). \n",
    "\n",
    "    Este tipo de árbol, cuando se analiza de forma recursiva de manera descendente, proporciona tripletas de relación gramatical como salida que se pueden usar como características para muchos problemas de NLP como el análisis de sentimientos, la identificación de actores y entidades y la clasificación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f0a62d9d04044d169ccce60b69dcf109-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">quick</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">brown</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">fox</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">jumps</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">lazy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">dog.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a62d9d04044d169ccce60b69dcf109-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a62d9d04044d169ccce60b69dcf109-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a62d9d04044d169ccce60b69dcf109-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a62d9d04044d169ccce60b69dcf109-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a62d9d04044d169ccce60b69dcf109-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a62d9d04044d169ccce60b69dcf109-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a62d9d04044d169ccce60b69dcf109-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a62d9d04044d169ccce60b69dcf109-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a62d9d04044d169ccce60b69dcf109-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a62d9d04044d169ccce60b69dcf109-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a62d9d04044d169ccce60b69dcf109-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a62d9d04044d169ccce60b69dcf109-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a62d9d04044d169ccce60b69dcf109-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a62d9d04044d169ccce60b69dcf109-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f0a62d9d04044d169ccce60b69dcf109-0-7\" stroke-width=\"2px\" d=\"M945,264.5 C945,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f0a62d9d04044d169ccce60b69dcf109-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Cargar el modelo de idioma en inglés de spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Oración de ejemplo\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Procesar la oración con spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Visualizar el árbol de dependencias sintácticas\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código utiliza spaCy para procesar una oración de ejemplo y luego utiliza la función `displacy.render()` para visualizar el árbol de dependencias sintácticas de manera gráfica en un formato interactivo.\n",
    "\n",
    "También puedes obtener una representación textual del árbol de dependencias sintácticas iterando sobre los tokens del documento procesado y accediendo a la relación de dependencia sintáctica y la cabecera de cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det fox\n",
      "quick amod fox\n",
      "brown amod fox\n",
      "fox nsubj jumps\n",
      "jumps ROOT jumps\n",
      "over prep jumps\n",
      "the det dog\n",
      "lazy amod dog\n",
      "dog pobj over\n",
      ". punct jumps\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el árbol de dependencias sintácticas de manera textual\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etiquetado de categorías gramaticales: Aparte de las relaciones gramaticales, cada palabra en una oración también está asociada con una categoría gramatical (POS) (sustantivos, verbos, adjetivos, adverbios, etc.). Las etiquetas POS definen el uso y la función de una palabra en la oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_EW0iMns7BK",
    "outputId": "dc1fa2d5-7cd4-4b84-d182-3a5d275f0ac2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "texto = \"I'm learning Natural Language Processing and Deep Learning on Github␣ and CC0C2\"\n",
    "tokens = word_tokenize(texto)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-XDUx1_4fGw"
   },
   "source": [
    "El etiquetado gramatical se usa para muchos propósitos importantes en NLP:\n",
    "\n",
    "1. Desambiguación del sentido de la palabra: algunas palabras del lenguaje tienen múltiples significados según su uso. Por ejemplo, en las dos oraciones siguientes:\n",
    "\n",
    "* \" Please book my flight for Delhi\"`\n",
    "\n",
    "* \"I am going to read this book in the flight\"\n",
    "\n",
    "\"Book\" se utiliza con un contexto diferente, sin embargo, la etiqueta de categoría gramatical para ambos casos es diferente. En la oración inicial , la palabra \"book\" se usa como verbo, mientras que en la segunda se usa como un sustantivo. \n",
    "\n",
    "Lectura: https://medium.com/aimonks/word-sense-disambiguation-resolving-ambiguity-in-natural-language-processing-3986a83d41fa \n",
    "\n",
    "2. Mejora de las características basadas en palabras: un modelo de aprendizaje podría aprender diferentes contextos de una palabra cuando se usan palabras como características, sin embargo, si la parte de la etiqueta de catagoría gramatical está vinculada a ellas, el contexto se conserva, por lo que se crean características sólidas. Por ejemplo:\n",
    "\n",
    "    * Oración - `\"book my flight, I will read this book\"``\n",
    "\n",
    "    * Tokens – (\"book\", 2), (\"my\", 1), (\"flight\", 1), (\"I\", 1), (\"will\", 1), (\"read\", 1), (\"this\", 1)\n",
    "\n",
    "   * Tokens with POS – (\"book_VB\", 1), (\"my_PRP$\", 1), (\"flight_NN\", 1), (\"I_PRP\", 1), (\"will_MD\", 1), (\"read_VB\", 1), (\"this_DT\", 1), (\"book_NN\", 1).\n",
    "\n",
    "3. Normalización y lematización: las etiquetas POS son la base del proceso de lematización para convertir una palabra a su forma básica (lema).\n",
    "\n",
    "4. Eliminación de stopwords suficientes: las etiquetas P OS también son útiles para la eliminación eficiente de stopwords.\n",
    "\n",
    "      Por ejemplo, hay algunas etiquetas que siempre definen las palabras de baja frecuencia/menos importantes de un lenguaje. Por ejemplo: (IN - “within”, “upon”, “except”), (CD -\"one\",\"two\", \"hundred\" ), (MD - \"may\", \"mu st\", etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas gramaticales:\n",
      "The : DT\n",
      "quick : JJ\n",
      "brown : NN\n",
      "fox : NN\n",
      "jumps : VBZ\n",
      "over : IN\n",
      "the : DT\n",
      "lazy : JJ\n",
      "dog : NN\n",
      ". : .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Oración de ejemplo\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenización de la oración\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Etiquetado gramatical\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Imprimir etiquetas gramaticales\n",
    "print(\"Etiquetas gramaticales:\")\n",
    "for token, pos_tag in pos_tags:\n",
    "    print(token, \":\", pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fragmentación (chunking)\n",
    "\n",
    " La fragmentación implica la combinación de tokens relacionados en un solo token, la creación de grupos de sustantivos relacionados, grupos de verbos relacionados, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT black/JJ cat/NN)\n",
      "  sat/VBD\n",
      "  (PP on/IN (NP the/DT mat/NN))\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Oración de ejemplo\n",
    "sentence = \"The black cat sat on the mat.\"\n",
    "\n",
    "# Tokenización de la oración\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Etiquetado gramatical (POS tagging) de los tokens\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Definición de la gramática para chunking\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT|JJ|NN.*>+}          # Frase nominal\n",
    "    PP: {<IN><NP>}                # Frase preposicional\n",
    "    VP: {<VB.*><NP|PP|CLAUSE>+$}  # Frase verbal\n",
    "    CLAUSE: {<NP><VP>}            # Cláusula\n",
    "\"\"\"\n",
    "\n",
    "# Creación del analizador sintáctico basado en la gramática definida\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "# Aplicación del chunking a la lista de etiquetas POS\n",
    "chunked_sentence = chunk_parser.parse(pos_tags)\n",
    "\n",
    "# Impresión del resultado del chunking\n",
    "print(chunked_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo se realiza el etiquetado gramatical (POS tagging) de los tokens además de que se define una gramática en forma de expresiones regulares para el chunking. En este caso, la gramática define patrones para frases nominales (NP), frases preposicionales (PP), frases verbales (VP) y cláusulas (CLAUSE). Se crea un analizador sintáctico (chunk_parser) basado en la gramática definida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_KtCtRj6qJ3"
   },
   "source": [
    "### Extracción de entidades\n",
    "\n",
    "Las entidades se definen como las partes  más importantes de una oración: frases nominales, frases verbales o ambas. Los algoritmos de detección de entidades generalmente son modelos conjuntos de análisis basados en reglas,  diccionarios de búsqueda, etiquetado pos y análisis de dependencias. La aplicabilidad de la detección de entidades se puede ver en los robots de chat automatizados, los analizadores de contenido y la información del consumidor.\n",
    "\n",
    "El modelado de tópicos y el reconocimiento de entidades con nombre son los dos métodos de detección de entidades clave en el NLP.\n",
    "\n",
    "\n",
    "#####  Reconocimiento de entidad nombrada (NER)\n",
    "\n",
    "El proceso de detección de las entidades nombradas, tales como nombres de personas, nombres de ubicaciones, nombres de compañías, etc. desde el texto se llama `NER``.\n",
    "\n",
    "Por ejemplo :\n",
    "\n",
    "Sentencia: Sergey Brin, el gerente de Google Inc. está caminando por las calles de Nueva York.\n",
    "\n",
    "Entidades nombradas - (\"persona\": \"Sergey Brin\"), (\"org\": \"Google Inc.\"), (\"ubicación\": \"Nueva York\")\n",
    "\n",
    "Un modelo típico de NER consiste en tres bloques:\n",
    "\n",
    "* Identificación de frases de nombre: este paso trata de extraer todas las frases de nombres de un texto mediante el análisis de dependencia y el etiquetado de categorías gramaticales.\n",
    "\n",
    "* Clasificación de frases: este es el paso de clasificación en el que todas las frases nominales extraídas se clasifican en categorías respectivas (ubicaciones, nombres, etc.). La API de Google Maps proporciona un buen camino para eliminar la ambigüedad de las ubicaciones. Luego, las bases de datos abiertas de dbpedia y wikipedia se pueden usar para identificar nombres de personas o nombres de compañías. Aparte de esto, uno puede anotar las tablas de búsqueda y los diccionarios combinando información de diferentes fuentes.\n",
    "\n",
    "* Desambiguación de entidades: a veces es posible que las entidades se clasifiquen incorrectamente, por lo que es útil crear una capa de validación sobre los resultados. El uso de grafos de conocimiento puede ser explotado para este propósito. Los grafos de conocimiento popular son: Google Knowledge Graph, IBM Watson y Wikipedia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades nombradas reconocidas: \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Oración de ejemplo\n",
    "sentence = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in 1976. Its headquarters is located in Cupertino, California.\"\n",
    "\n",
    "# Tokenización de la oración\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Etiquetado gramatical (POS tagging) de los tokens\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Reconocimiento de entidades nombradas (NER) utilizando ne_chunk\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "# Función para extraer las entidades nombradas reconocidas\n",
    "def extract_named_entities(tree):\n",
    "    named_entities = []\n",
    "    if hasattr(tree, 'label') and tree.label():\n",
    "        if tree.label() == 'NE':\n",
    "            named_entities.append(' '.join([child[0] for child in tree]))\n",
    "        else:\n",
    "            for child in tree:\n",
    "                named_entities.extend(extract_named_entities(child))\n",
    "    return named_entities\n",
    "\n",
    "# Extracción de las entidades nombradas reconocidas\n",
    "named_entities = extract_named_entities(named_entities)\n",
    "\n",
    "# Impresión de las entidades nombradas reconocidas en una sola línea separada por comas\n",
    "print(\"Entidades nombradas reconocidas:\", \", \".join(named_entities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes realizar un enfoque simplificado de la vinculación de entidades utilizando NLTK junto con una base de conocimiento predefinida, como WordNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: was\n",
      "Label: VBD\n",
      "Entity Link: be\n",
      "\n",
      "Entity: born\n",
      "Label: VBN\n",
      "Entity Link: bear\n",
      "\n",
      "Entity: Hawaii\n",
      "Label: NNP\n",
      "Entity Link: hawaii\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Definición de la oración de ejemplo\n",
    "sentence = \"Barack Obama was born in Hawaii.\"\n",
    "\n",
    "# Tokenización de la oración\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Etiquetado gramatical (POS tagging) de los tokens\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Mapeo de etiquetas POS de NLTK a las etiquetas POS de WordNet\n",
    "def map_pos(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Procesamiento de las entidades reconocidas\n",
    "for word, pos_tag in pos_tags:\n",
    "    # Mapeo de la etiqueta POS a la correspondiente en WordNet\n",
    "    wn_pos = map_pos(pos_tag)\n",
    "    if wn_pos:\n",
    "        # Búsqueda de sinónimos en WordNet\n",
    "        synsets = wn.synsets(word, pos=wn_pos)\n",
    "        if synsets:\n",
    "            # Seleccionar el primer synset como representante del concepto\n",
    "            entity = synsets[0].name().split('.')[0]\n",
    "            print(\"Entity:\", word)\n",
    "            print(\"Label:\", pos_tag)\n",
    "            print(\"Entity Link:\", entity)\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, cada palabra en la oración se busca en WordNet utilizando su etiqueta POS correspondiente. Si se encuentra un synset en WordNet, se toma el primer synset como representante del concepto y se muestra como el enlace de entidad. Ten en cuenta que este enfoque es muy básico y no captura la complejidad de la vinculación de entidades en su totalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelado de tópicos\n",
    "\n",
    "El modelado de tópicos es un proceso de identificación automática de los tópicos presentes en un corpus de texto, deriva los patrones ocultos entre las palabras en el corpus de una manera no supervisada. Los temas se definen como \"un patrón repetitivo de términos co-ocurrentes en un corpus\".\n",
    "\n",
    "Latent Dirichlet  Allocation(LDA) es una de las técnica de modelado de temas más popular, pero no es la única!. Ver: https://www.leewayhertz.com/topic-modeling-in-nlp/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Eeu-FLPCRkZ",
    "outputId": "56de5101-0e22-4105-b2ee-426f4ace0f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.030*\"my\" + 0.030*\"My\" + 0.030*\"father\" + 0.030*\"sister\" + 0.030*\"suggest\" + 0.030*\"cause\" + 0.030*\"and\" + 0.030*\"preassure.\" + 0.030*\"blood\" + 0.030*\"increased\"'), (1, '0.065*\"my\" + 0.065*\"driving\" + 0.037*\"time\" + 0.037*\"practice\" + 0.037*\"around\" + 0.037*\"aa\" + 0.037*\"spends\" + 0.037*\"the\" + 0.037*\"of\" + 0.037*\"dance\"'), (2, '0.090*\"to\" + 0.051*\"sister\" + 0.051*\"My\" + 0.051*\"father\" + 0.051*\"my\" + 0.051*\"is\" + 0.051*\"bad\" + 0.051*\"sugar,\" + 0.051*\"Sugar\" + 0.051*\"but\"')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father\"\n",
    "doc2 = \"My father spends aa lot of time driving my sister around the dance practice\"\n",
    "doc3 = \"Doctors suggest that driving my cause increased stress and blood preassure.\"\n",
    "\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creando el diccionario de términos de nuestro corpus, donde cada término único tiene asignado un índice\n",
    "diccionario = corpora.Dictionary(doc_clean)\n",
    "# Convertir la lista de documentos (corpus) en la Matriz de Términos del Documento utilizando el diccionario preparado anteriormente\n",
    "doc_term_matrix = [diccionario.doc2bow(doc) for doc in doc_clean]\n",
    "# Creando el objeto para el modelo LDA, usando la libreria gensim\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Corriendo y entrenando el modelo LDA en la Matriz de Términos de Documentos\n",
    "\n",
    "ldamodel = LDA(doc_term_matrix, num_topics =3, id2word =diccionario, passes = 50)\n",
    "\n",
    "# Resultados\n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que estás viendo es la representación de los temas encontrados por el modelo LDA (Latent Dirichlet Allocation) después de ser entrenado con los documentos proporcionados.\n",
    "\n",
    "Cada tema está representado como una tupla que contiene dos elementos:\n",
    "\n",
    "El primer elemento de la tupla es un identificador único para el tema.\n",
    "El segundo elemento de la tupla es una cadena que representa las palabras más relevantes asociadas con ese tema, junto con sus probabilidades de pertenecer al tema.\n",
    "Por ejemplo, para el primer tema, el identificador es 0, y las palabras más relevantes y sus respectivas probabilidades son:\n",
    "\n",
    "\n",
    "```\n",
    "\"driving\": 0.061\n",
    "\"my\": 0.061\n",
    "\"blood\": 0.061\n",
    "\"Doctors\": 0.061\n",
    "\"that\": 0.061\n",
    "\"and\": 0.061\n",
    "\"cause\": 0.061\n",
    "\"stress\": 0.061\n",
    "\"preassure.\": 0.061\n",
    "\"increased\": 0.061\n",
    "```\n",
    "\n",
    "Esto significa que el tema está relacionado con conceptos como \"driving\", \"Doctors\", \"estrés\",  etc., y que estas palabras tienen una alta probabilidad de pertenecer a este tema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omZdA-p7Hpw3"
   },
   "source": [
    "#### N-gramas como características\n",
    "\n",
    "Una combinación de N palabras juntas se llama N-Gramas. Los N-gramas (N> 1) son generalmente más informativos en comparación con las palabras (unigramas) como características. Además, los bigrams (N = 2) se consideran como las características más importantes de todas las demás. El siguiente código genera bigramas de un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrIhrkSXFaKC",
    "outputId": "098b1daf-f9e4-4296-8739-06ba5e046850"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Assigns', 'the'],\n",
       " ['the', 'POS'],\n",
       " ['POS', 'tag'],\n",
       " ['tag', 'the'],\n",
       " ['the', 'most'],\n",
       " ['most', 'frequently'],\n",
       " ['frequently', 'occurring'],\n",
       " ['occurring', 'with'],\n",
       " ['with', 'a'],\n",
       " ['a', 'word'],\n",
       " ['word', 'in'],\n",
       " ['in', 'the'],\n",
       " ['the', 'training'],\n",
       " ['training', 'corpus']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(texto, n):\n",
    "  words = texto.split()\n",
    "  output = []\n",
    "  for i in range(len(words) -n +1 ):\n",
    "    output.append(words[i:i + n])\n",
    "\n",
    "  return output\n",
    "\n",
    "generate_ngrams(\"Assigns the POS tag the most frequently occurring with a word in the training corpus\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSttRoo_LAWJ"
   },
   "source": [
    "### Estadisticas de  características\n",
    "\n",
    "Los datos de texto también se pueden cuantificar directamente en números usando varias técnicas descritas a continuación\n",
    "\n",
    "\n",
    "####  Frecuencia de términos-Frecuencia de documento inversa (TF-IDF)\n",
    "\n",
    "TF-IDF es un modelo ponderado comúnmente utilizado para problemas de recuperación de información. Su objetivo es convertir los documentos de texto en modelos vectoriales en función de la aparición de palabras en los documentos sin tener en cuenta el orden exacto. \n",
    "\n",
    "La fórmula general para calcular TF-IDF es: \n",
    "\n",
    "$$TF-IDF(t,d,D)=TF(t,d)\\times IDF(t,D)$$ \n",
    "\n",
    "Donde: \n",
    "\n",
    "- `t`  es el término. \n",
    "- `d` es el documento. \n",
    "- `D` es el conjunto de documentos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTnag8OeKyfX",
    "outputId": "1f40b319-b918-4f9d-ff00-c57875e4f51e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.2805198619076071\n",
      "  (0, 10)\t0.36122039714507487\n",
      "  (0, 16)\t0.36122039714507487\n",
      "  (0, 14)\t0.36122039714507487\n",
      "  (0, 3)\t0.2805198619076071\n",
      "  (0, 7)\t0.47496141327993013\n",
      "  (0, 11)\t0.47496141327993013\n",
      "  (1, 1)\t0.2805198619076071\n",
      "  (1, 10)\t0.36122039714507487\n",
      "  (1, 16)\t0.36122039714507487\n",
      "  (1, 14)\t0.36122039714507487\n",
      "  (1, 3)\t0.2805198619076071\n",
      "  (1, 0)\t0.47496141327993013\n",
      "  (1, 12)\t0.47496141327993013\n",
      "  (2, 1)\t0.1298209308406929\n",
      "  (2, 3)\t0.1298209308406929\n",
      "  (2, 6)\t0.21980594303058684\n",
      "  (2, 15)\t0.6594178290917605\n",
      "  (2, 5)\t0.21980594303058684\n",
      "  (2, 9)\t0.4396118860611737\n",
      "  (2, 2)\t0.21980594303058684\n",
      "  (2, 17)\t0.21980594303058684\n",
      "  (2, 8)\t0.21980594303058684\n",
      "  (2, 4)\t0.21980594303058684\n",
      "  (2, 13)\t0.21980594303058684\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = [\"Document retrieval using TF-IDF matching score\", \"Document retrieval using TF-IDF cosine similarity\",\n",
    "         \"IDF is the inverse of the document frequency which measures the informativeness of term.\"]\n",
    "\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QSK38tIP7Qo"
   },
   "source": [
    "El modelo crea un diccionario de vocabulario y asigna un índice a cada palabra. Cada fila en la salida contiene una tupla `(i, j)`` y un valor tf-idf de palabra en el índice j en el documento i.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSJKbAn0RMJn"
   },
   "source": [
    "### Embeddings  de palabras (vectores de texto)\n",
    "\n",
    "El embeddings de palabras es la forma moderna de representar palabras como vectores. El objetivo del embeddings  de palabras es redefinir las características de las palabras de alta dimensión en vectores de características de baja dimensión, preservando la similitud contextual en el corpus. Se utilizan ampliamente en modelos de aprendizaje profundo, como las redes neuronales convolucionales y las redes neuronales recurrentes.\n",
    "\n",
    "Word2Vec y GloVe son los dos modelos populares para crear embedding de texto de un texto. Estos modelos toman un cuerpo de texto como entrada y producen los vectores de palabras como salida.\n",
    "\n",
    "**El modelo de Word2Vec se compone de un módulo de preprocesamiento, un modelo de red neuronal superficial llamado Continuous Bag of Words y otro modelo de red neuronal superficial llamado skip-gram**. Estos modelos son ampliamente utilizados para todos los demás problemas del NLP.\n",
    "\n",
    "Lectura: https://www.ruder.io/word-embeddings-1/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgBbLzPxOoFI",
    "outputId": "2ce9a90f-75e6-48d8-b5d9-4aba1cffb41e"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['cesar', 'science', 'data', 'analytics'], ['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# Entrenamiento del modelo en el corpus\n",
    "model = Word2Vec(sentences, min_count =1)\n",
    "model.wv.similarity('data','science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USKyzaCIcIkG"
   },
   "source": [
    "Se pueden usar como vectores de características para un modelo ML, para medir la similitud del texto utilizando técnicas de similitud de coseno, clustering de palabras y técnicas de clasificación de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnbyczR0ceeg"
   },
   "source": [
    "## Tareas elementales importantes del NLP\n",
    "\n",
    "Mostramos diferentes casos de uso y problemas en el campo del procesamiento del lenguaje natural.\n",
    "\n",
    "### Clasificación de texto\n",
    "\n",
    "La clasificación de textos es uno de los problemas clásicos del NLP. Entre los ejemplos notorios se incluyen: Identificación de correo electrónico no deseado, clasificación de temas de noticias, clasificación de opiniones y organización de páginas web por motores de búsqueda.\n",
    "\n",
    "La clasificación de texto,  se define como una técnica para clasificar sistemáticamente un objeto de texto (documento u oración) en una de las categorías fijas. Es realmente útil cuando la cantidad de datos es demasiado grande, especialmente para fines de organización, filtrado de información y almacenamiento.\n",
    "\n",
    "Un clasificador típico de lenguaje natural consta de dos partes: (a) Entrenamiento (b) Predicción. En primer lugar la entrada de texto se procesa y se crean características. Los modelos de aprendizaje automático aprenden estas características y se utilizan para predecir el nuevo texto.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7gvSnb7Y-k0",
    "outputId": "77cb5645-eccf-4cd0-ceef-021761e10664"
   },
   "outputs": [],
   "source": [
    "## Código que usa un clasificador de Bayes que usa la biblioteca de  textblob (construida sobre NLTK)\n",
    "\n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "\n",
    "corpus_entrenamiento = [\n",
    "                       ('I am exhausted of this work.', 'Class_B'),\n",
    "                       (\"I can't cooperate with this\", 'Class_B'),\n",
    "                       ('He is my badest enemy!', 'Class_B'),\n",
    "                       ('My management is poor.', 'Class_B'),\n",
    "                       ('I love this burger.', 'Class_A'),\n",
    "                       ('This is an brilliant place!', 'Class_A'),\n",
    "                       ('I feel very good about these dates.', 'Class_A'),\n",
    "                       ('This is my best work.', 'Class_A'),\n",
    "                       (\"What an awesome view\", 'Class_A'),\n",
    "                       ('I do not like this dish', 'Class_B')]\n",
    "corpus_prueba = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'),\n",
    "                (\"I feel brilliant!\", 'Class_A'),\n",
    "                ('Gary is a friend of mine.', 'Class_A'),\n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'),\n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4j-PWAPMfHx4",
    "outputId": "977752db-9d5b-447e-873b-b2197d8fef83"
   },
   "outputs": [],
   "source": [
    "model = NBC(corpus_entrenamiento)\n",
    "print(model.classify(\"Comentarios\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2jZNi-AfUpL",
    "outputId": "f311ca50-8266-41c2-b80f-2e559ee77959"
   },
   "outputs": [],
   "source": [
    "print(model.classify(\"I don't like their computer.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGNDcZ0Bffcp",
    "outputId": "3f25c7e1-1ef9-440f-ea1d-3427904857fd"
   },
   "outputs": [],
   "source": [
    "print(model.accuracy(corpus_prueba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbfRn9rLfvBl"
   },
   "source": [
    "Scikit Learn proporciona un framework pipeline para clasificación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BuqIxX0f54B",
    "outputId": "8b3692f7-0963-45b4-c22a-e7edf53e9004"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "# preparando datos para el modelo SVM (usando el corpus_entrenamiento, corpus_prueba desde el ejemplo anterior)\n",
    "\n",
    "datos_entrenamiento = []\n",
    "etiquetas_entrenamiento = []\n",
    "\n",
    "for fila in corpus_entrenamiento:\n",
    "  datos_entrenamiento.append(fila[0])\n",
    "  etiquetas_entrenamiento.append(fila[1])\n",
    "\n",
    "\n",
    "datos_prueba =[]\n",
    "etiquetas_pruebas = []\n",
    "for fila in corpus_prueba:\n",
    "  datos_prueba.append(fila[0])\n",
    "  etiquetas_pruebas.append(fila[1])\n",
    "\n",
    "# Creamos vectores caracteristicas\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "\n",
    "# Entrenamiento los vectores caracteristicas\n",
    "\n",
    "vectores_entrenamiento = vectorizer.fit_transform(datos_entrenamiento)\n",
    "\n",
    "# Aplicar el modelo sobre el conjunto de pruebas\n",
    "\n",
    "vectores_prueba =vectorizer.transform(datos_prueba)\n",
    "\n",
    "# Desempeño de la clasificacion de SVM kernel=linear\n",
    "\n",
    "model = svm.SVC(kernel=\"linear\")\n",
    "model.fit(vectores_entrenamiento, etiquetas_entrenamiento)\n",
    "prediction = model.predict(vectores_prueba)\n",
    "\n",
    "print (classification_report(etiquetas_pruebas, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH0BPG-Zd5x9"
   },
   "source": [
    "El modelo de clasificación de texto depende en gran medida de la calidad y cantidad de las características, mientras que al aplicar cualquier modelo de aprendizaje automático, siempre es una buena práctica incluir más y más datos de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQBW-a_nm4IF"
   },
   "source": [
    "#### Coincidencia de texto/ similitud\n",
    "\n",
    "Una de las áreas importantes del NLP es la coincidencia de objetos de texto para encontrar similitudes. Las aplicaciones importantes de la coincidencia de texto incluyen la corrección automática de la ortografía, la deduplicación de datos y el análisis del genoma, etc.\n",
    "\n",
    "Una serie de técnicas de coincidencia de texto están disponibles según el requerimiento.\n",
    "\n",
    "*  Distancia de Levenshtein: la distancia de Levenshtein entre dos cadenas se define como el número mínimo de ediciones necesarias para transformar una cadena en otra, con las operaciones de edición permitidas que son la inserción, eliminación o sustitución de un solo carácter. A continuación se muestra la implementación para cálculos de memoria eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhPthSwieBjF",
    "outputId": "59452a12-a80a-4ca2-c3c3-fa35b67b0c6f"
   },
   "outputs": [],
   "source": [
    "def levenshtein(s1,s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1\n",
    "    distancias = range(len(s1) + 1)\n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistancias = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistancias.append(distancias[index1])\n",
    "            else:\n",
    "                 newDistancias.append(1 + min((distancias[index1], distancias[index1+1], newDistancias[-1])))\n",
    "        distancias = newDistancias\n",
    "    return distancias[-1]\n",
    "\n",
    "print(levenshtein(\"analyze\",\"analyse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cot3-3V2oYMW"
   },
   "source": [
    "* Coincidencia fonética: un algoritmo de coincidencia fonética toma una palabra clave como entrada (nombre de una persona, nombre de la ubicación, etc.) y produce una cadena de caracteres que identifica un conjunto de palabras que son (aproximadamente) fonéticamente similares. Es muy útil para buscar grandes corpuses de texto, corregir errores de ortografía y hacer coincidir nombres relevantes. Soundex y Metaphone son dos algoritmos fonéticos principales utilizados para este propósito. Revisar: https://medium.com/@ievgenii.shulitskyi/phonetic-matching-algorithms-50165e684526 \n",
    "\n",
    "*  Coincidencia de cadenas flexible: un completo sistema de correspondencia de texto incluye diferentes algoritmos agrupados para calcular una variedad de variaciones de texto. Las expresiones regulares son realmente útiles para este propósito también. Otras técnicas comunes incluyen: coincidencia exacta de cadenas, correspondencia lematizada y correspondencia compacta (se ocupa de los espacios, puntuación, slangs, etc.).\n",
    "\n",
    "* Similitud de coseno - Cuando el texto se representa como notación vectorial, también se puede aplicar una similitud de coseno general para medir la similitud vectorizada. El siguiente código convierte un texto en vectores (usando el término frecuencia) y aplica la similitud de coseno para proporcionar cercanía entre dos textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xA7Gx-9aoeWa",
    "outputId": "397aacae-a20d-4e37-e236-63b4e6e63815"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "def coseno_(vec1, vec2):\n",
    "    comun = set(vec1.keys())& set(vec2.keys())\n",
    "    numerador = sum([vec1[x]*vec2[x] for x in comun])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominador = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominador:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerador)/ denominador\n",
    "\n",
    "def texto_a_vector(text):\n",
    "    words = text.split()\n",
    "    return Counter(words)\n",
    "\n",
    "text1 = 'This is an works of NLTK'\n",
    "text2 = 'The work is about natural language processing'\n",
    "\n",
    "vector1 = texto_a_vector(text1)\n",
    "vector2 = texto_a_vector(text2)\n",
    "coseno = coseno_(vector1, vector2)\n",
    "\n",
    "print(coseno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn86lKikwdNu"
   },
   "source": [
    "#### Resolución de la coreferencia\n",
    "\n",
    "La resolución de la coreferencia  es un proceso de búsqueda de vínculos relacionales entre las palabras (o frases) dentro de las oraciones. Considere una oración de ejemplo: \"Donald went to John’s office to see the new table. He looked at it for an hour\".\n",
    "\n",
    "Los humanos pueden darse cuenta rápidamente de que \"él\" denota a Donald (y no a John), y que \"esto\" denota la mesa (y no la oficina de John). La resolución de referencia es el componente de NLP  que hace este trabajo automáticamente. Se utiliza para resumir documentos, responder preguntas y extraer información. Stanford CoreNLP proporciona una envoltura de python para fines comerciales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5xlx1-gxW-W"
   },
   "source": [
    "#### Ejercicio\n",
    "\n",
    "Explora código en python y NLTK de las siguientes tareas de NLP:\n",
    "\n",
    "* Resumenes de texto: dado un artículo de texto o párrafo, resumirlo  automáticamente para producir las oraciones más importantes y relevantes ordenadas\n",
    "* Traducción automática: traduce automáticamente el texto de un idioma humano a otro mediante el cuidado de la gramática, la semántica y la información sobre el mundo real, etc.\n",
    "*  Generación y comprensión del lenguaje natural: la conversión de información de bases de datos informáticas o intenciones semánticas a lenguaje humano legible se denomina generación de lenguaje. La conversión de fragmentos de texto en estructuras más lógicas que son más fáciles de manipular por los programas de computadora se denomina comprensión del lenguaje.\n",
    "*  Reconocimiento óptico de caracteres: dada una imagen que representa un texto impreso, determine el texto correspondiente.\n",
    "* Documento a información: implica el análisis de datos textuales presentes en documentos (sitios web, archivos, pdf e imágenes) en un formato analizable y limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMzIQetQ2T8D"
   },
   "source": [
    "## Librerías importantes para NLP (python)\n",
    "\n",
    "* Scikit-learn: Aprendizaje automático en Python\n",
    "* Natural Language Toolkit (NLTK): el conjunto de herramientas completo para todas las técnicas de NLP.\n",
    "* spaCy - Paquete industrial de NLP con Python y Cython.\n",
    "* HuggingFaces es una comunidad y una plataforma que proporciona herramientas y recursos de vanguardia en NLP, facilitando el desarrollo y la implementación de soluciones basadas en inteligencia artificial en una amplia variedad de aplicaciones y dominios.\n",
    "* fast.ai, es una plataforma integral que proporciona herramientas, recursos educativos y una comunidad activa para hacer que el aprendizaje profundo sea más accesible y práctico para todos, con el objetivo de democratizar la inteligencia artificial y fomentar la innovación en este campo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dx4MDwVK3pSY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
