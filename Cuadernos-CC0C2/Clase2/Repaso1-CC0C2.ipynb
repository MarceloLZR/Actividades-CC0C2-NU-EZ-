{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b94aa1e",
   "metadata": {},
   "source": [
    "### Repaso de ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af05b4",
   "metadata": {},
   "source": [
    "**Ejercicio 1: Expresiones regulares - Captura de grupos y Lookahead**\n",
    "\n",
    "Implementa una función extract_emails que extraiga correos electrónicos de un texto, validando que los correos sean seguidos por un punto y no por una coma (lookahead positivo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd93047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_emails(text: str):\n",
    "    # Completa la expresión regular para capturar los correos con lookahead positivo\n",
    "    return re.findall(r\"\", text)\n",
    "\n",
    "# Pruebas\n",
    "assert extract_emails(\"Contacta a example@mail.com. o sales@company.org.\") == [\"example@mail.com\", \"sales@company.org\"]\n",
    "assert extract_emails(\"Envíalo a hr@company.net, por favor.\") == []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d27d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6e1c0",
   "metadata": {},
   "source": [
    "**Ejercicio 2 : Tokenización de subpalabras con Byte-Pair Encoding (BPE)**\n",
    "\n",
    "Desarrolla una función bpe_tokenize que utilice el algoritmo BPE para tokenizar una lista de palabras basada en fusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78345b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_tokenize(vocab: dict, num_merges: int):\n",
    "    # Completa el algoritmo BPE para tokenización\n",
    "    pass\n",
    "\n",
    "# Pruebas\n",
    "vocab = {\"lowest\": 5, \"newest\": 6, \"widest\": 3}\n",
    "assert bpe_tokenize(vocab, 2) == [\"low\", \"newest\", \"widest\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b626fd",
   "metadata": {},
   "source": [
    "**Ejercicio 3: Lemmatización y Stemming**\n",
    "\n",
    "Crea una función normalize_words que aplique tanto lematización como stemming a una lista de palabras y devuelva los resultados normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "def normalize_words(words: list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    # completa...\n",
    "\n",
    "# Pruebas\n",
    "words = [\"running\", \"jumps\", \"eaten\"]\n",
    "lemmatized, stemmed = normalize_words(words)\n",
    "assert lemmatized == [\"running\", \"jump\", \"eaten\"]\n",
    "assert stemmed == [\"run\", \"jump\", \"eaten\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f60a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11e53d",
   "metadata": {},
   "source": [
    "**Ejercicio 4: Algoritmo de N-gramas con Máxima Verosimilitud (MLE)**\n",
    "\n",
    "Implementa la clase NgramModelMLE que calcule la probabilidad de una secuencia usando máxima verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModelMLE:\n",
    "    def __init__(self, n: int, corpus: list):\n",
    "        self.n = n\n",
    "        self.corpus = corpus\n",
    "        self.ngram_counts = {}\n",
    "        self._train()\n",
    "\n",
    "    def _train(self):\n",
    "        # Completa el entrenamiento del modelo N-gram\n",
    "        pass\n",
    "\n",
    "    def calculate_probability(self, sequence: list):\n",
    "        # Calcula la probabilidad usando MLE\n",
    "        pass\n",
    "\n",
    "# Pruebas\n",
    "corpus = [\"el gato come pescado\", \"el perro come hueso\"]\n",
    "ngram_model = NgramModelMLE(2, corpus)\n",
    "assert abs(ngram_model.calculate_probability([\"el\", \"gato\"])) - 0.5 < 0.01\n",
    "assert abs(ngram_model.calculate_probability([\"el\", \"perro\"])) - 0.5 < 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f13c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ecabdd",
   "metadata": {},
   "source": [
    "**Ejercicio 5: Evaluación de modelos de lenguaje con Perplexity**\n",
    "\n",
    "Desarrolla una función calculate_perplexity que evalúe un modelo n-grama calculando la perplexity para un conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72811fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(ngram_model, test_data: list):\n",
    "    perplexity = 0\n",
    "    total_words = 0\n",
    "    # completa...\n",
    "\n",
    "# Pruebas\n",
    "test_data = [\"el gato come pescado\", \"el perro ladra fuerte\"]\n",
    "ngram_model = NgramModelMLE(2, test_data)\n",
    "assert calculate_perplexity(ngram_model, test_data) < 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf95ec9",
   "metadata": {},
   "source": [
    "**Ejercicio 6: Sampling de frases desde un modelo de lenguaje**\n",
    "\n",
    "Implementa una función generate_sentence que genere frases desde un modelo n-grama mediante sampling probabilístico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cec1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence(ngram_model, max_length: int):\n",
    "    sentence = []\n",
    "    word = \"<START>\"\n",
    "\n",
    "    while word != \"<END>\" and len(sentence) < max_length:\n",
    "        # completa.....\n",
    "\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "# Pruebas\n",
    "corpus = [\"el gato come pescado\", \"el perro come hueso\"]\n",
    "ngram_model = NgramModelMLE(2, corpus)\n",
    "generated_sentence = generate_sentence(ngram_model, 10)\n",
    "assert len(generated_sentence.split()) <= 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd85b4d0",
   "metadata": {},
   "source": [
    "**Ejercicio 7:  Suavizado Kneser-Ney**\n",
    "    \n",
    "Implementa la clase KneserNeyModel que utilice suavizado Kneser-Ney para el cálculo de probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdca76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KneserNeyModel:\n",
    "    def __init__(self, n: int, corpus: list, discount: float):\n",
    "        self.n = n\n",
    "        self.corpus = corpus\n",
    "        self.discount = discount\n",
    "        self.ngram_counts = {}\n",
    "        self.continuation_counts = {}\n",
    "        self._train()\n",
    "\n",
    "    def _train(self):\n",
    "        # Completa el entrenamiento del modelo con suavizado Kneser-Ney\n",
    "        pass\n",
    "\n",
    "    def calculate_probability(self, sequence: list):\n",
    "        # Implementa el cálculo de probabilidad con Kneser-Ney\n",
    "        pass\n",
    "\n",
    "# Pruebas\n",
    "corpus = [\"el gato come pescado\", \"el perro come hueso\"]\n",
    "kneser_ney_model = KneserNeyModel(2, corpus, 0.75)\n",
    "assert abs(kneser_ney_model.calculate_probability([\"el\", \"gato\"])) - 0.5 < 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ac303",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84504055",
   "metadata": {},
   "source": [
    "**Ejercicio 8: Tokenización basada en reglas**\n",
    "\n",
    "Desarrolla una función rule_based_tokenization que divida un texto en oraciones, y luego en palabras, basándose en signos de puntuación y espacios. Asegúrate de mantener los signos de puntuación como tokens separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b13ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_tokenization(text: str):\n",
    "    # Completa la función de tokenización basada en reglas\n",
    "    pass\n",
    "\n",
    "# Pruebas\n",
    "assert rule_based_tokenization(\"Hola. ¿Cómo estás? Bien.\") == [[\"Hola\", \".\"], [\"¿\", \"Cómo\", \"estás\", \"?\"], [\"Bien\", \".\"]]\n",
    "assert rule_based_tokenization(\"¡Hola, mundo!\") == [[\"¡\", \"Hola\", \",\", \"mundo\", \"!\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0777fb",
   "metadata": {},
   "source": [
    "**Ejercicio 9: Algoritmo WordPiece de tokenización**\n",
    "\n",
    "Implementa una función wordpiece_tokenize que tome un vocabulario y una oración, y realice la tokenización usando el algoritmo WordPiece. El vocabulario está predefinido y cada palabra debe dividirse en subpalabras cuando no esté en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c157c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordpiece_tokenize(vocab: set, sentence: str):\n",
    "    # Completa la función de tokenización WordPiece\n",
    "    pass\n",
    "\n",
    "# Pruebas\n",
    "vocab = {\"el\", \"gato\", \"per\", \"##ro\", \"come\", \"##mos\", \"##do\"}\n",
    "assert wordpiece_tokenize(vocab, \"el perro come comida\") == [\"el\", \"per\", \"##ro\", \"come\", \"com\", \"##ida\"]\n",
    "assert wordpiece_tokenize(vocab, \"el gato come\") == [\"el\", \"gato\", \"come\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567c7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112bd85",
   "metadata": {},
   "source": [
    "**Ejercicio 10: Modelo de Lenguaje N-Grama con Backoff**\n",
    "\n",
    "Implementa una clase NgramModelWithBackoff que implemente backoff para calcular la probabilidad de secuencias que no se encuentran en el modelo de n-gramas, usando modelos más pequeños (por ejemplo, de bigramas a unigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2293788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModelWithBackoff:\n",
    "    def __init__(self, n: int, corpus: list):\n",
    "        self.n = n\n",
    "        self.corpus = corpus\n",
    "        self.ngram_counts = {}\n",
    "        self.unigram_counts = {}\n",
    "        self.total_unigrams = 0\n",
    "        self._train()\n",
    "\n",
    "    def _train(self):\n",
    "        # Completa el entrenamiento del modelo n-gram con backoff\n",
    "        pass\n",
    "\n",
    "    def calculate_probability(self, sequence: list):\n",
    "        # Implementa el cálculo de probabilidad con backoff\n",
    "        pass\n",
    "\n",
    "# Pruebas\n",
    "corpus = [\"el gato come pescado\", \"el perro come hueso\"]\n",
    "ngram_model_backoff = NgramModelWithBackoff(2, corpus)\n",
    "assert abs(ngram_model_backoff.calculate_probability([\"el\", \"gato\"])) - 0.5 < 0.1\n",
    "assert abs(ngram_model_backoff.calculate_probability([\"come\", \"hueso\"])) - 0.5 < 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eebc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3cd9c",
   "metadata": {},
   "source": [
    "**Ejercicio 11: Perplexity como promedio ponderado del factor de ramificación**\n",
    "\n",
    "Extiende la función calculate_perplexity_weighted para incluir el cálculo del factor de ramificación ponderado y así evaluar el desempeño del modelo de lenguaje n-grama}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5304ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity_weighted(ngram_model, test_data: list):\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "    branching_factors = []\n",
    "\n",
    "    for sentence in test_data:\n",
    "        words = sentence.split()\n",
    "        log_prob = ngram_model.calculate_probability(words)\n",
    "        branching_factor = len(words) ** (1 / len(words))\n",
    "        total_log_prob += log_prob\n",
    "        branching_factors.append(branching_factor)\n",
    "        total_words += len(words)\n",
    "\n",
    "    weighted_branching = sum(branching_factors) / len(branching_factors)\n",
    "    return 2 ** (-total_log_prob / (total_words * weighted_branching))\n",
    "\n",
    "# Pruebas\n",
    "test_data = [\"el gato come pescado\", \"el perro ladra fuerte\"]\n",
    "ngram_model = NgramModelWithBackoff(2, test_data)\n",
    "assert calculate_perplexity_weighted(ngram_model, test_data) < 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170c3f2",
   "metadata": {},
   "source": [
    "**Ejercicio 12: Interpolación y Backoff en modelos de lenguaje**\n",
    "\n",
    "Implementa una función interpolated_ngram_model que combine modelos unigramas, bigramas y trigramas usando interpolación y backoff para calcular las probabilidades de secuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedNgramModel:\n",
    "    def __init__(self, corpus: list):\n",
    "        self.unigram_counts = {}\n",
    "        self.bigram_counts = {}\n",
    "        self.trigram_counts = {}\n",
    "        self.total_unigrams = 0\n",
    "        self._train(corpus)\n",
    "\n",
    "    def _train(self, corpus):\n",
    "        # Completa el entrenamiento con interpolación de unigramas, bigramas y trigramas\n",
    "        pass\n",
    "\n",
    "    def calculate_probability(self, sequence: list):\n",
    "        # Implementa el cálculo de probabilidad usando interpolación y backoff\n",
    "        pass\n",
    "\n",
    "# Pruebas\n",
    "corpus = [\"el gato come pescado\", \"el perro come hueso\"]\n",
    "interpolated_model = InterpolatedNgramModel(corpus)\n",
    "assert abs(interpolated_model.calculate_probability([\"el\", \"gato\", \"come\"])) - 0.5 < 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc69585",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2310d4",
   "metadata": {},
   "source": [
    "**Ejercicio 13: Evaluación de modelos de lenguaje - Suavizado Kneser-Ney vs. MLE**\n",
    "\n",
    "Implementa una función evaluate_language_models que compare los modelos de suavizado Kneser-Ney y MLE utilizando perplexity para un conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c117acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_language_models(mle_model, kn_model, test_data: list):\n",
    "    mle_perplexity = calculate_perplexity(mle_model, test_data)\n",
    "    kn_perplexity = calculate_perplexity(kn_model, test_data)\n",
    "    # completa el codigo\n",
    "    \n",
    "    return {\"MLE\": mle_perplexity, \"Kneser-Ney\": kn_perplexity}\n",
    "\n",
    "# Pruebas\n",
    "corpus = [\"el gato come pescado\", \"el perro come hueso\"]\n",
    "test_data = [\"el gato come\", \"el perro ladra\"]\n",
    "mle_model = NgramModelMLE(2, corpus)\n",
    "kn_model = KneserNeyModel(2, corpus, 0.75)\n",
    "results = evaluate_language_models(mle_model, kn_model, test_data)\n",
    "assert results[\"MLE\"] > results[\"Kneser-Ney\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb84368",
   "metadata": {},
   "source": [
    "**Ejercicio 14: Expresiones regulares con grupos y sustitución**\n",
    "\n",
    "Implementa una función normalize_text que realice las siguientes operaciones en un texto dado:\n",
    "\n",
    "- Remplaza todos los dígitos por el token \"<NUM>\".\n",
    "- Convierte todas las secuencias de espacios múltiples en un solo espacio.\n",
    "- Reemplaza los pronombres personales \"yo\", \"tú\", \"él\" por \"PRON\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf43f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text: str):\n",
    "    # Completa la función usando expresiones regulares\n",
    "    pass\n",
    "\n",
    "# Pruebas\n",
    "assert normalize_text(\"Yo tengo 2 perros y  3 gatos\") == \"PRON tengo <NUM> perros y <NUM> gatos\"\n",
    "assert normalize_text(\"Él y  tú  están aquí\") == \"PRON y PRON están aquí\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93334161",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee360d",
   "metadata": {},
   "source": [
    "**Ejercicio 15: Algoritmo BPE con capturas y tokenización por subpalabras**\n",
    "\n",
    "Implementa una función bpe_tokenize_v2 que use el algoritmo BPE para dividir palabras en subpalabras y capture las fusiones más frecuentes. La función debe devolver las fusiones en orden descendente de frecuencia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_tokenize_v2(vocab: dict, num_merges: int):\n",
    "    # Implementa el algoritmo BPE con fusiones capturadas\n",
    "    pass\n",
    "\n",
    "# Pruebas\n",
    "vocab = {\"l o w e s t\": 5, \"n e w e s t\": 6, \"w i d e s t\": 3}\n",
    "assert bpe_tokenize_v2(vocab, 3) == [(\"e s\", 11), (\"w e\", 9), (\"s t\", 8)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7281b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a79ce0",
   "metadata": {},
   "source": [
    "**Ejercicio 16: Levenshtein y la distancia mínima de edición con operaciones**\n",
    "\n",
    "Implementa una función levenshtein_operations que, además de calcular la distancia de Levenshtein, devuelva las operaciones necesarias (inserciones, borrados, sustituciones) para transformar una palabra en otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e124526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_operations(word1: str, word2: str):\n",
    "    # Implementa el algoritmo que calcule las operaciones de edición\n",
    "    pass\n",
    "\n",
    "# Pruebas\n",
    "assert levenshtein_operations(\"kitten\", \"sitting\") == (3, [(\"replace\", \"k\", \"s\"), (\"insert\", \"t\"), (\"insert\", \"g\")])\n",
    "assert levenshtein_operations(\"flaw\", \"lawn\") == (2, [(\"delete\", \"f\"), (\"replace\", \"w\", \"n\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa163929",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc48b21",
   "metadata": {},
   "source": [
    "**Ejercicio 17:Tokenización top-down basada en reglas**\n",
    "\n",
    "Crea una función rule_based_tokenization que divida un texto en oraciones y palabras siguiendo las reglas gramaticales del español, por ejemplo:\n",
    "\n",
    "- Las oraciones terminan en puntos.\n",
    "- Las comas y los signos de exclamación o interrogación deben mantenerse como tokens separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_tokenization(text: str):\n",
    "    # Implementa la tokenización basada en reglas\n",
    "    pass\n",
    "\n",
    "# Pruebas\n",
    "assert rule_based_tokenization(\"¿Cómo estás? Bien, gracias.\") == [[\"¿\", \"Cómo\", \"estás\", \"?\"], [\"Bien\", \",\", \"gracias\", \".\"]]\n",
    "assert rule_based_tokenization(\"Hola. ¿Qué tal?\") == [[\"Hola\", \".\"], [\"¿\", \"Qué\", \"tal\", \"?\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d46bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d07b9",
   "metadata": {},
   "source": [
    "**Ejercicio 18: Modelado de lenguaje con interpolación y backoff**\n",
    "\n",
    "Implementa una función interpolated_ngram_model que combine modelos de unigramas, bigramas y trigramas usando interpolación y backoff. Debe asignar probabilidades más altas a los n-gramas más largos y, si no se encuentra un n-grama, hacer \"backoff\" al siguiente modelo más pequeño.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedNgramModel:\n",
    "    def __init__(self, corpus: list):\n",
    "        self.unigram_model = {}\n",
    "        self.bigram_model = {}\n",
    "        self.trigram_model = {}\n",
    "        # Inicializa los modelos\n",
    "        \n",
    "    def calculate_interpolated_probability(self, sequence: list):\n",
    "        # Completa el cálculo de probabilidades con interpolación y backoff\n",
    "        pass\n",
    "\n",
    "# Pruebas\n",
    "corpus = [\"el gato come pescado\", \"el perro come hueso\"]\n",
    "interpolated_model = InterpolatedNgramModel(corpus)\n",
    "assert abs(interpolated_model.calculate_interpolated_probability([\"el\", \"gato\", \"come\"])) - 0.6 < 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b49498",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40c1ea",
   "metadata": {},
   "source": [
    "**Ejercicio 19: Estimación de probabilidades con MLE y suavizado de Laplace**\n",
    "\n",
    "Desarrolla una función ngram_probability_with_smoothing que calcule las probabilidades de un modelo n-grama usando MLE y suavizado de Laplace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_probability_with_smoothing(n: int, corpus: list, sequence: list):\n",
    "    # Implementa el cálculo con MLE y suavizado de Laplace\n",
    "    pass\n",
    "\n",
    "# Pruebas\n",
    "corpus = [\"el gato come\", \"el perro ladra\"]\n",
    "assert abs(ngram_probability_with_smoothing(2, corpus, [\"el\", \"gato\"])) - 0.5 < 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35257a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e79ed8",
   "metadata": {},
   "source": [
    "**Referencias de corpus:**\n",
    "    \n",
    "- Brown Corpus: Un corpus general de inglés.\n",
    "- Gutenberg Corpus: Libros de dominio público para entrenamiento de modelos de lenguaje.\n",
    "- Europarl Corpus: Transcripciones de sesiones parlamentarias multilingües."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d65224",
   "metadata": {},
   "source": [
    "**Ejercicio 20: Modelo de lenguaje n-Grama regularizado con Dropout**\n",
    "\n",
    "Implementa una clase NgramModelWithDropout que entrene un modelo de lenguaje n-grama con regularización mediante Dropout. La probabilidad de \"eliminar\" un n-grama debe ser un hiperparámetro ajustable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NgramModelWithDropout:\n",
    "    def __init__(self, n: int, corpus: list, dropout_rate: float):\n",
    "        self.n = n\n",
    "        self.corpus = corpus\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.ngram_counts = {}\n",
    "        self._train()\n",
    "\n",
    "    def _train(self):\n",
    "        # Completa la función para entrenar el modelo con Dropout en los n-gramas\n",
    "        pass\n",
    "\n",
    "    def calculate_probability(self, sequence: list):\n",
    "        # Implementa el cálculo de probabilidad usando el modelo con Dropout\n",
    "        pass\n",
    "\n",
    "# Pruebas\n",
    "corpus = [\"el gato come pescado\", \"el perro come hueso\", \"el gato salta\"]\n",
    "ngram_dropout = NgramModelWithDropout(2, corpus, 0.2)\n",
    "assert abs(ngram_dropout.calculate_probability([\"el\", \"gato\"])) - 0.5 < 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236489e",
   "metadata": {},
   "source": [
    "**Ejercicio 21: Entrenamiento de un modelo de n-gramas usando smoothing interpolado y backoff con ajuste de hiperparámetros**\n",
    "\n",
    "Crea una clase AdvancedNgramModel que implemente interpolación y backoff, y que permita ajustar los hiperparámetros de interpolación usando búsqueda en cuadrícula (Grid Search). El objetivo es minimizar el perplexity del modelo en un conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class AdvancedNgramModel:\n",
    "    def __init__(self, n: int, corpus: list):\n",
    "        self.n = n\n",
    "        self.corpus = corpus\n",
    "        self.ngram_probs = {}\n",
    "        self._train()\n",
    "\n",
    "    def _train(self):\n",
    "        # Completa el entrenamiento con interpolación y backoff\n",
    "        pass\n",
    "\n",
    "    def calculate_perplexity(self, sequence: list):\n",
    "        # Calcula la perplexity de una secuencia\n",
    "        pass\n",
    "\n",
    "    def tune_hyperparameters(self, param_grid: dict):\n",
    "        # Implementa la búsqueda en cuadrícula para ajuste de hiperparámetros\n",
    "        pass\n",
    "\n",
    "# Pruebas\n",
    "corpus = [\"el gato come pescado\", \"el perro come hueso\", \"el gato salta alto\"]\n",
    "advanced_ngram = AdvancedNgramModel(3, corpus)\n",
    "assert abs(advanced_ngram.calculate_perplexity([\"el\", \"gato\", \"come\"])) - 10 < 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b5113",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c1ef64",
   "metadata": {},
   "source": [
    "**Ejercicio 22: Tokenización basada en WordPiece con ajuste dinámico de vocabulario**\n",
    "\n",
    "Implementa una función wordpiece_tokenize_dynamic que ajuste dinámicamente el tamaño del vocabulario según las palabras que se encuentren en el texto. Debe usar WordPiece y asignar más tokens a las palabras raras, y menos tokens a las comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordpiece_tokenize_dynamic(text: str, vocab_size: int):\n",
    "    # Completa la tokenización adaptativa usando WordPiece\n",
    "    pass\n",
    "\n",
    "# Pruebas\n",
    "text = \"el perro corre rápido y el gato también\"\n",
    "tokens = wordpiece_tokenize_dynamic(text, 10)\n",
    "assert len(tokens) <= 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
