{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0565757-ce7b-4581-b00a-b7ff49a42c46",
   "metadata": {},
   "source": [
    "### Suavizado para modelos de Lenguaje\n",
    "\n",
    "Cuando entrenamos un modelo de lenguaje, puede suceder que, durante la fase de prueba, nos encontremos con palabras que están en nuestro vocabulario (es decir, no son palabras desconocidas), pero que aparecen en un contexto no visto. Por ejemplo, estas palabras podrían aparecer después de otra palabra con la que nunca se asociaron durante el entrenamiento. Para evitar que el modelo asigne una probabilidad de cero a estos eventos no vistos, es necesario redistribuir parte de la masa de probabilidad de eventos más frecuentes hacia estos eventos raros o no observados.\n",
    "\n",
    "Esta técnica de redistribución de probabilidades se denomina suavizado o descuento (smoothing o discounting). En el estudio del suavizado para modelos de lenguaje, exploraremos varias técnicas: el suavizado de Laplace (también conocido como suavizado de adición-uno), el suavizado add-k, stupid backoff, y métodos más complejos como el suavizado de Kneser-Ney."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49df45e-392c-4b5d-b6e4-1296c44efa6d",
   "metadata": {},
   "source": [
    "### Suavizado de Laplace \n",
    "\n",
    "El suavizado de Laplace es la forma más simple de suavizado que añade uno a todos los conteos de n-gramas antes de normalizarlos en probabilidades. Esto implica que incluso los n-gramas que no se observaron durante el entrenamiento recibirán una pequeña probabilidad en el modelo suavizado. \n",
    "\n",
    "El propósito es evitar que cualquier secuencia posible tenga una probabilidad de cero, lo que puede ser crítico en tareas como generación de texto y traducción automática.\n",
    "\n",
    "Para un modelo de unigramas, la estimación de máxima verosimilitud (MLE) de la probabilidad de una palabra $w_i$ es su conteo $c_i$ normalizado por el número total de tokens de palabras $N$:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{c_i}{N}\n",
    "$$\n",
    "\n",
    "El suavizado de Laplace modifica esta estimación al añadir uno a cada conteo:\n",
    "\n",
    "$$\n",
    "P_{\\text{Laplace}}(w_i) = \\frac{c_i + 1}{N + V}\n",
    "$$\n",
    "\n",
    "Aquí, $V$ es el tamaño del vocabulario. Añadiendo uno a cada conteo, aumentamos el numerador para cada palabra en el vocabulario. Para mantener la suma de probabilidades en $1$, también necesitamos ajustar el denominador, aumentando $N$ en $V$ para tener en cuenta estas $V$ observaciones adicionales.\n",
    "\n",
    "**¿Por qué ajustamos el denominador?** Sin el ajuste del denominador, las probabilidades asignadas a los eventos existentes (vistos durante el entrenamiento) disminuirían en comparación con los eventos no vistos, distorsionando la distribución de probabilidad.\n",
    "\n",
    "#### Ajuste de conteos con suavizado de Laplace\n",
    "\n",
    "El suavizado de Laplace puede describirse en términos de un conteo ajustado $c^*$, que facilita la comparación directa con los conteos de máxima verosimilitud (MLE). El conteo ajustado para una palabra $w_i$ se define como:\n",
    "\n",
    "$$\n",
    "c^*_i = (c_i + 1) \\frac{N}{N + V}\n",
    "$$\n",
    "\n",
    "Esta expresión ajusta el conteo original $c_i$ añadiendo uno y normalizando por el factor $\\frac{N}{N + V}$. Este ajuste asegura que las probabilidades se mantengan correctamente distribuidas, de modo que la suma de todas las probabilidades es $1$.\n",
    "\n",
    "Para convertir este conteo ajustado en una probabilidad, simplemente normalizamos por $N$:\n",
    "\n",
    "$$\n",
    "P_i^* = \\frac{c^*_i}{N}\n",
    "$$\n",
    "\n",
    "#### Descuento relativo\n",
    "\n",
    "El suavizado también puede verse como una forma de descuento (reducción) de los conteos observados. Al disminuir la probabilidad de eventos frecuentes, podemos redistribuir esa masa de probabilidad a eventos raros o no vistos. El descuento relativo $d_c$ es la proporción de los conteos ajustados con respecto a los conteos originales:\n",
    "\n",
    "$$\n",
    "d_c = \\frac{c^*}{c}\n",
    "$$\n",
    "\n",
    "Esta relación permite ver el suavizado como un proceso que \"descuenta\" ciertos conteos para redistribuir la masa de probabilidad a otros eventos.\n",
    "\n",
    "#### Suavizado de Laplace para bigramas\n",
    "\n",
    "En el caso de bigramas, el suavizado de Laplace implica incrementar el conteo de cada bigrama observado en uno. La probabilidad de un bigrama suavizado se calcula como:\n",
    "\n",
    "$$\n",
    "P_{\\text{Laplace}}(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V}\n",
    "$$\n",
    "\n",
    "Aquí, $C(w_{n-1})$ es el conteo de la palabra anterior $w_{n-1}$, y $V$ es el tamaño del vocabulario. Esto asegura que incluso bigramas no observados reciban una probabilidad diferente de cero.\n",
    "\n",
    "Los conteos ajustados para bigramas pueden reconstruirse mediante:\n",
    "\n",
    "$$\n",
    "c^*(w_{n-1}w_n) = \\frac{[C(w_{n-1}w_n) + 1] \\times C(w_{n-1})}{C(w_{n-1}) + V}\n",
    "$$\n",
    "\n",
    "Este ajuste permite analizar cuánto ha cambiado el algoritmo de suavizado los conteos originales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5c996-dd99-4f69-9a87-dd83a3566e0e",
   "metadata": {},
   "source": [
    "### Ejemplos de Suavizado de Laplace\n",
    "\n",
    "El suavizado de Laplace es una técnica sencilla pero poderosa para evitar que nuestro modelo de lenguaje asigne una probabilidad de cero a eventos no observados durante el entrenamiento. A continuación, se presentan algunos ejemplos que ilustran cómo aplicar las ecuaciones mencionadas.\n",
    "\n",
    "#### Ejemplo 1: Suavizado de Laplace para unigramas\n",
    "\n",
    "Supongamos un pequeño corpus con el siguiente conjunto de palabras: \"gato\", \"perro\", \"pájaro\", \"pez\", con los siguientes conteos:\n",
    "\n",
    "- \"gato\": 3\n",
    "- \"perro\": 2\n",
    "- \"pájaro\": 1\n",
    "- \"pez\": 0\n",
    "\n",
    "El tamaño del corpus $N$ es la suma de los conteos: $N = 3 + 2 + 1 + 0 = 6$.\n",
    "\n",
    "Supongamos que nuestro vocabulario $V$ contiene todas estas palabras, es decir, $V = 4$. Sin suavizado, las probabilidades de unigramas usando máxima verosimilitud (MLE) serían:\n",
    "\n",
    "$$\n",
    "P(\\text{\"gato\"}) = \\frac{3}{6} = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{\"perro\"}) = \\frac{2}{6} \\approx 0.333\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{\"pájaro\"}) = \\frac{1}{6} \\approx 0.167\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{\"pez\"}) = \\frac{0}{6} = 0\n",
    "$$\n",
    "\n",
    "Observamos que \"pez\" tiene una probabilidad de cero, lo que podría ser problemático si \"pez\" aparece en los datos de prueba. Ahora, apliquemos el suavizado de Laplace a estas probabilidades.\n",
    "\n",
    "Con suavizado de Laplace, añadimos $1$ a cada conteo y ajustamos el denominador sumando $V$:\n",
    "\n",
    "$$\n",
    "P_{\\text{Laplace}}(w_i) = \\frac{c_i + 1}{N + V}\n",
    "$$\n",
    "\n",
    "Para cada palabra:\n",
    "\n",
    "$$\n",
    "P_{\\text{Laplace}}(\\text{\"gato\"}) = \\frac{3 + 1}{6 + 4} = \\frac{4}{10} = 0.4\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{Laplace}}(\\text{\"perro\"}) = \\frac{2 + 1}{6 + 4} = \\frac{3}{10} = 0.3\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{Laplace}}(\\text{\"pájaro\"}) = \\frac{1 + 1}{6 + 4} = \\frac{2}{10} = 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{Laplace}}(\\text{\"pez\"}) = \\frac{0 + 1}{6 + 4} = \\frac{1}{10} = 0.1\n",
    "$$\n",
    "\n",
    "Ahora, la palabra \"pez\" tiene una probabilidad distinta de cero, y la distribución de probabilidades está suavizada para que incluso las palabras no observadas tengan cierta probabilidad.\n",
    "\n",
    "#### Ejemplo 2: Ajuste de conteos con suavizado de Laplace\n",
    "\n",
    "Siguiendo con el mismo ejemplo, calculemos los conteos ajustados $c^*$ para la palabra \"gato\":\n",
    "\n",
    "$$\n",
    "c^*_{\\text{\"gato\"}} = (3 + 1) \\frac{N}{N + V} = 4 \\times \\frac{6}{10} = 2.4\n",
    "$$\n",
    "\n",
    "La probabilidad ajustada es entonces:\n",
    "\n",
    "$$\n",
    "P_{\\text{ajustado}}(\\text{\"gato\"}) = \\frac{c^*_{\\text{\"gato\"}}}{N} = \\frac{2.4}{6} = 0.4\n",
    "$$\n",
    "\n",
    "Esto coincide con la probabilidad suavizada calculada anteriormente.\n",
    "\n",
    "#### Ejemplo 3: Suavizado de Laplace para bigramas\n",
    "\n",
    "Ahora, consideremos un modelo de bigramas con un corpus simple: \"gato duerme\", \"perro ladra\", \"gato come\". Los conteos de bigramas son:\n",
    "\n",
    "- $ C(\\text{\"gato duerme\"}) = 1$\n",
    "- $C(\\text{\"perro ladra\"}) = 1$\n",
    "- $C(\\text{\"gato come\"}) = 1$\n",
    "\n",
    "El vocabulario $V$ contiene las palabras \"gato\", \"duerme\", \"perro\", \"ladra\", \"come\", entonces $V = 5$.\n",
    "\n",
    "Queremos calcular la probabilidad suavizada del bigrama \"gato come\":\n",
    "\n",
    "$$\n",
    "P_{\\text{Laplace}}(\\text{\"come\"} | \\text{\"gato\"}) = \\frac{C(\\text{\"gato come\"}) + 1}{C(\\text{\"gato\"}) + V}\n",
    "$$\n",
    "\n",
    "Primero, necesitamos los conteos unigramas:\n",
    "\n",
    "- $C(\\text{\"gato\"}) = 2 $ (porque \"gato\" aparece dos veces como primera palabra en los bigramas)\n",
    "\n",
    "Entonces,\n",
    "\n",
    "$$\n",
    "P_{\\text{Laplace}}(\\text{\"come\"} | \\text{\"gato\"}) = \\frac{1 + 1}{2 + 5} = \\frac{2}{7} \\approx 0.286\n",
    "$$\n",
    "\n",
    "Ahora calculemos los conteos ajustados para este bigrama:\n",
    "\n",
    "$$\n",
    "c^*(\\text{\"gato come\"}) = \\frac{[C(\\text{\"gato come\"}) + 1] \\times C(\\text{\"gato\"})}{C(\\text{\"gato\"}) + V} = \\frac{[1 + 1] \\times 2}{2 + 5} = \\frac{4}{7} \\approx 0.571\n",
    "$$\n",
    "\n",
    "Este conteo ajustado refleja cuánto ha cambiado el algoritmo de suavizado los conteos originales. Gracias al suavizado de Laplace, incluso los bigramas que no se observaron durante el entrenamiento ahora tienen una probabilidad diferente de cero, mejorando la capacidad del modelo para manejar eventos no vistos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab17ad1-6c20-4170-a149-99723967ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suavizado de Laplace para Unigramas\n",
    "def laplace_smoothing_unigram(corpus):\n",
    "    # Conteo de palabras en el corpus\n",
    "    word_counts = {}\n",
    "    for word in corpus:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "    # Número total de tokens en el corpus\n",
    "    N = sum(word_counts.values())\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    V = len(word_counts)\n",
    "\n",
    "    # Cálculo de las probabilidades suavizadas\n",
    "    laplace_probabilities = {}\n",
    "    for word, count in word_counts.items():\n",
    "        # Aplicando la ecuación P_Laplace(w_i) = (c_i + 1) / (N + V)\n",
    "        laplace_probabilities[word] = (count + 1) / (N + V)\n",
    "    \n",
    "    # Probabilidad para una palabra no vista\n",
    "    laplace_probabilities['<UNK>'] = 1 / (N + V)\n",
    "    \n",
    "    return laplace_probabilities\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = [\"gato\", \"perro\", \"gato\", \"pájaro\", \"perro\", \"gato\"]\n",
    "laplace_prob_unigrams = laplace_smoothing_unigram(corpus)\n",
    "print(\"Probabilidades de unigramas suavizadas con Laplace:\")\n",
    "for word, prob in laplace_prob_unigrams.items():\n",
    "    print(f\"P({word}) = {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3f73e-f1fc-4074-97a8-450f438ca31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suavizado de Laplace para bigramas\n",
    "def laplace_smoothing_bigram(corpus):\n",
    "    # Conteo de bigramas y unigrams\n",
    "    bigram_counts = {}\n",
    "    unigram_counts = {}\n",
    "    \n",
    "    # Construir bigramas\n",
    "    for i in range(len(corpus) - 1):\n",
    "        bigram = (corpus[i], corpus[i+1])\n",
    "        unigram = corpus[i]\n",
    "        \n",
    "        if bigram in bigram_counts:\n",
    "            bigram_counts[bigram] += 1\n",
    "        else:\n",
    "            bigram_counts[bigram] = 1\n",
    "        \n",
    "        if unigram in unigram_counts:\n",
    "            unigram_counts[unigram] += 1\n",
    "        else:\n",
    "            unigram_counts[unigram] = 1\n",
    "    \n",
    "    # Contar el último unigramo\n",
    "    last_word = corpus[-1]\n",
    "    if last_word in unigram_counts:\n",
    "        unigram_counts[last_word] += 1\n",
    "    else:\n",
    "        unigram_counts[last_word] = 1\n",
    "    \n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigram_counts)\n",
    "\n",
    "    # Cálculo de las probabilidades suavizadas para bigramas\n",
    "    laplace_probabilities = {}\n",
    "    for bigram, bigram_count in bigram_counts.items():\n",
    "        w_n_1 = bigram[0]\n",
    "        # Aplicando la ecuación P_Laplace(w_n | w_n-1) = (C(w_n-1 w_n) + 1) / (C(w_n-1) + V)\n",
    "        laplace_probabilities[bigram] = (bigram_count + 1) / (unigram_counts[w_n_1] + V)\n",
    "\n",
    "    # Probabilidad para un bigrama no visto\n",
    "    laplace_probabilities[('<UNK>', '<UNK>')] = 1 / (V * (V + 1))\n",
    "    \n",
    "    return laplace_probabilities\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = [\"gato\", \"duerme\", \"perro\", \"ladra\", \"gato\", \"come\"]\n",
    "laplace_prob_bigrams = laplace_smoothing_bigram(corpus)\n",
    "print(\"\\nProbabilidades de bigramas suavizadas con Laplace:\")\n",
    "for bigram, prob in laplace_prob_bigrams.items():\n",
    "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4200d-a742-4016-8102-cbe98ea9b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Suavizado de Laplace para trigramas\n",
    "\n",
    "def laplace_smoothing_trigram(corpus):\n",
    "    # Conteo de trigramas, bigramas y unigrams\n",
    "    trigram_counts = {}\n",
    "    bigram_counts = {}\n",
    "    unigram_counts = {}\n",
    "    \n",
    "    # Construir trigramas\n",
    "    for i in range(len(corpus) - 2):\n",
    "        trigram = (corpus[i], corpus[i+1], corpus[i+2])\n",
    "        bigram = (corpus[i], corpus[i+1])\n",
    "        unigram = corpus[i]\n",
    "        \n",
    "        # Contar trigramas\n",
    "        if trigram in trigram_counts:\n",
    "            trigram_counts[trigram] += 1\n",
    "        else:\n",
    "            trigram_counts[trigram] = 1\n",
    "        \n",
    "        # Contar bigramas\n",
    "        if bigram in bigram_counts:\n",
    "            bigram_counts[bigram] += 1\n",
    "        else:\n",
    "            bigram_counts[bigram] = 1\n",
    "        \n",
    "        # Contar unigrams\n",
    "        if unigram in unigram_counts:\n",
    "            unigram_counts[unigram] += 1\n",
    "        else:\n",
    "            unigram_counts[unigram] = 1\n",
    "    \n",
    "    # Contar el último bigrama y unigramo restantes\n",
    "    for i in range(len(corpus) - 1, len(corpus)):\n",
    "        unigram = corpus[i]\n",
    "        if unigram in unigram_counts:\n",
    "            unigram_counts[unigram] += 1\n",
    "        else:\n",
    "            unigram_counts[unigram] = 1\n",
    "        \n",
    "    last_bigram = (corpus[-2], corpus[-1])\n",
    "    if last_bigram in bigram_counts:\n",
    "        bigram_counts[last_bigram] += 1\n",
    "    else:\n",
    "        bigram_counts[last_bigram] = 1\n",
    "    \n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigram_counts)\n",
    "\n",
    "    # Cálculo de las probabilidades suavizadas para trigramas\n",
    "    laplace_probabilities = {}\n",
    "    for trigram, trigram_count in trigram_counts.items():\n",
    "        w_n_2_w_n_1 = (trigram[0], trigram[1])  # w_{n-2}w_{n-1}\n",
    "        # Aplicando la ecuación P_Laplace(w_n | w_{n-2} w_{n-1}) = (C(w_{n-2}w_{n-1}w_n) + 1) / (C(w_{n-2}w_{n-1}) + V)\n",
    "        laplace_probabilities[trigram] = (trigram_count + 1) / (bigram_counts[w_n_2_w_n_1] + V)\n",
    "\n",
    "    # Probabilidad para un trigrama no visto\n",
    "    laplace_probabilities[('<UNK>', '<UNK>', '<UNK>')] = 1 / (V * (V + 1) * (V + 2))\n",
    "    \n",
    "    return laplace_probabilities\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = [\"gato\", \"duerme\", \"perro\", \"ladra\", \"gato\", \"come\", \"globo\", \"vuela\"]\n",
    "laplace_prob_trigrams = laplace_smoothing_trigram(corpus)\n",
    "print(\"\\nProbabilidades de trigramas suavizadas con Laplace:\")\n",
    "for trigram, prob in laplace_prob_trigrams.items():\n",
    "    print(f\"P({trigram[2]} | {trigram[0]} {trigram[1]}) = {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30205ec1-67f7-4d30-bb45-08a098c0e9dc",
   "metadata": {},
   "source": [
    "La salida del código muestra las probabilidades suavizadas para cada trigrama en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12993b90-f2a4-4088-94d9-2621b84205c2",
   "metadata": {},
   "source": [
    "#### Ejercicios teóricos\n",
    "\n",
    "1. **Comprensión de probabilidades con suavizado de Laplace:**\n",
    "   - Supongamos un corpus de entrenamiento con las palabras: `[\"sol\", \"nube\", \"lluvia\", \"sol\", \"nube\", \"sol\"]`.\n",
    "   - Calcula manualmente las probabilidades de unigramas usando suavizado de Laplace. Utiliza las ecuaciones:\n",
    "     $$\n",
    "     P_{\\text{Laplace}}(w_i) = \\frac{c_i + 1}{N + V}\n",
    "     $$\n",
    "   - ¿Cuál es la probabilidad asignada a una palabra no vista como \"viento\"?\n",
    "\n",
    "2. **Ajuste de conteos con suavizado de Laplace:**\n",
    "   - Dado un conjunto de palabras con sus conteos $c_i$:\n",
    "     - \"gato\": 3\n",
    "     - \"perro\": 2\n",
    "     - \"ratón\": 0\n",
    "   - Utilizando la ecuación para ajustar los conteos:\n",
    "     $$\n",
    "     c^*_i = (c_i + 1) \\frac{N}{N + V}\n",
    "     $$\n",
    "     Calcula los conteos ajustados $c^*_i$ para cada palabra. ¿Cómo cambian estos conteos en comparación con los conteos originales?\n",
    "\n",
    "3. **Descuento relativo:**\n",
    "   - Utilizando los conteos ajustados $c^*_i$ obtenidos en el ejercicio anterior, calcula el descuento relativo $d_c$ para cada palabra usando la ecuación:\n",
    "     $$\n",
    "     d_c = \\frac{c^*}{c}\n",
    "     $$\n",
    "   - ¿Qué significa este descuento relativo en términos de la distribución de probabilidades?\n",
    "\n",
    "4. **Probabilidad de bigrama con suavizado de Laplace:**\n",
    "   - Dado el siguiente conjunto de bigramas y sus conteos:\n",
    "     - $C(\\text{\"el gato\"}) = 2$\n",
    "     - $C(\\text{\"gato come\"}) = 1$\n",
    "     - $C(\\text{\"el perro\"}) = 1$\n",
    "   - Calcula la probabilidad del bigrama $P_{\\text{Laplace}}(\\text{\"come\"} | \\text{\"gato\"})$ usando:\n",
    "     $$\n",
    "     P_{\\text{Laplace}}(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V}\n",
    "     $$\n",
    "   - ¿Cómo afecta el tamaño del vocabulario $V$ a la probabilidad suavizada?\n",
    "\n",
    "5. **Aplicación de suavizado de Laplace a trigramas:**\n",
    "   - Dado un conjunto de trigramas y sus conteos:\n",
    "     - $C(\\text{\"el gato come\"}) = 1$\n",
    "     - $C(\\text{\"el perro duerme\"}) = 1$\n",
    "   - Utiliza la fórmula del suavizado de Laplace para trigramas:\n",
    "     $$\n",
    "     P_{\\text{Laplace}}(w_n | w_{n-2}, w_{n-1}) = \\frac{C(w_{n-2}w_{n-1}w_n) + 1}{C(w_{n-2}w_{n-1}) + V}\n",
    "     $$\n",
    "     Calcula la probabilidad del trigrama $P(\\text{\"come\"} | \\text{\"el gato\"})$.\n",
    "\n",
    "6. Explica por qué el suavizado de Laplace evita que las palabras no vistas durante el entrenamiento tengan una probabilidad de cero. ¿Qué sucede con la distribución de probabilidades cuando se aplica el suavizado de Laplace a un conjunto de datos con un vocabulario muy grande?\n",
    "7. Dado un corpus con las siguientes palabras y sus conteos: `{\"gato\": 4, \"perro\": 2, \"pez\": 0}`:\n",
    "     - Calcula las probabilidades usando máxima verosimilitud (MLE).\n",
    "     - Luego, aplica el suavizado de Laplace para calcular las probabilidades suavizadas.\n",
    "     - Compara y analiza las diferencias entre ambos métodos. ¿Qué ventajas e inconvenientes tiene el suavizado de Laplace en este caso?\n",
    "8. Usando la fórmula de ajuste de conteos $c^*$:\n",
    "     $$\n",
    "     c^*_i = (c_i + 1) \\frac{N}{N + V}\n",
    "     $$\n",
    "     - Explica cómo esta fórmula afecta a los conteos originales. ¿Cómo cambia la distribución de probabilidades al aplicar este ajuste a un corpus con muchas palabras raras?\n",
    "9. Explica cómo el suavizado de Laplace afecta a la probabilidad de bigramas en comparación con trigramas. ¿Es más efectivo el suavizado de Laplace en modelos de bigramas o trigramas? Justifica tu respuesta.\n",
    "10. Discute cómo el suavizado de Laplace puede impactar tareas de NLP como la generación de texto y la traducción automática. ¿Hay situaciones en las que el suavizado de Laplace pueda ser contraproducente?\n",
    "\n",
    "#### Ejercicios de codificación\n",
    "\n",
    "1. **Suavizado de Laplace para trigramas con contenido real:**\n",
    "\n",
    "    - Escribe una función que implemente el suavizado de Laplace para trigramas. Utiliza un texto real (por ejemplo, un fragmento de un libro) como corpus de entrada:\n",
    "    ```python\n",
    "      def laplace_smoothing_trigram(corpus):\n",
    "    # Tu implementación aquí\n",
    "    pass\n",
    "    ```\n",
    "    - Calcula las probabilidades suavizadas de trigramas en el texto. Identifica trigramas comunes y raros, y analiza cómo el suavizado de Laplace afecta sus probabilidades.\n",
    "2. **Evaluación de probabilidades suavizadas:**\n",
    "   - Escribe un script que compare las probabilidades suavizadas de un corpus usando diferentes tamaños de vocabulario $V$:\n",
    "   ```python\n",
    "   def evaluate_laplace_smoothing(corpus, vocab_sizes):\n",
    "       # Tu implementación aquí\n",
    "       pass\n",
    "   ```\n",
    "   - Experimenta con diferentes valores de $V$ y analiza cómo el tamaño del vocabulario afecta las probabilidades suavizadas en unigramas y bigramas.\n",
    "\n",
    "3. **Suavizado de Laplace en un modelo de lenguaje simplificado:**\n",
    "   - Crea un modelo de lenguaje simplificado que use el suavizado de Laplace para predecir la siguiente palabra en una secuencia basada en bigramas. La función debe predecir la palabra más probable dada una palabra anterior:\n",
    "   ```python\n",
    "   def predict_next_word(corpus, previous_word):\n",
    "       # Tu implementación aquí\n",
    "       pass\n",
    "   ```\n",
    "   - Prueba tu modelo con diferentes secuencias y analiza la calidad de las predicciones con y sin suavizado de Laplace.\n",
    "\n",
    "4. **Comparación entre suavizado de Laplace y MLE:**\n",
    "   - Implementa un script en Python que compare las probabilidades de unigramas, bigramas y trigramas calculadas con suavizado de Laplace y máxima verosimilitud (MLE).\n",
    "   - Grafica las probabilidades obtenidas para visualizar las diferencias entre los dos métodos.\n",
    "\n",
    "5. Después de implementar y probar el suavizado de Laplace para unigramas, bigramas y trigramas, reflexiona sobre las diferencias entre las probabilidades suavizadas y las probabilidades MLE. ¿Cómo afecta el suavizado al comportamiento del modelo en la práctica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ed38a-27f5-4498-8f41-579e89ff20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a2c2c-bf28-4ac8-9768-ae7c16cb9a82",
   "metadata": {},
   "source": [
    "### Suavizado add-k\n",
    "\n",
    "Una alternativa al suavizado de add-k es transferir un poco menos de la masa de probabilidad de los eventos vistos a los no vistos. En lugar de añadir 1 a cada conteo, añadimos un conteo fraccional $k$ (¿0.5? ¿0.05? ¿0.01?). Este algoritmo se llama por lo tanto suavizado de add-k.\n",
    "\n",
    "$$P^*_{\\text{Add-k}}(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + k}{C(w_{n-1}) + kV}$$\n",
    "\n",
    "El suavizado de add-k requiere que tengamos un método para elegir $k$. Esto puede hacerse, por ejemplo, optimizando en un conjunto de desarrollo o validación. Aunque el suavizado de `add-k` es útil para algunas tareas (incluida la clasificación de textos), resulta que todavía no funciona bien para el modelado de lenguajes, generando conteos con varianzas pobres y, a menudo, descuentos inapropiados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9e6b9-6468-4432-8208-744ccb472d8c",
   "metadata": {},
   "source": [
    "#### Ejemplos Teóricos\n",
    "\n",
    "#### Ejemplo 1: Suavizado Add-k para unigramas\n",
    "\n",
    "Supongamos un corpus con los siguientes conteos unigramas:\n",
    "- \"gato\": 3\n",
    "- \"perro\": 2\n",
    "- \"ratón\": 0\n",
    "\n",
    "El tamaño del vocabulario $V = 3$ y el número total de tokens $N = 5$.\n",
    "\n",
    "Para el suavizado add-k con $k = 0.5$:\n",
    "1. **Probabilidad suavizada para \"gato\":**\n",
    "   $$\n",
    "   P^*_{\\text{Add-0.5}}(\\text{\"gato\"}) = \\frac{3 + 0.5}{5 + 0.5 \\times 3} = \\frac{3.5}{6.5} \\approx 0.538\n",
    "   $$\n",
    "\n",
    "2. **Probabilidad suavizada para \"perro\":**\n",
    "   $$\n",
    "   P^*_{\\text{Add-0.5}}(\\text{\"perro\"}) = \\frac{2 + 0.5}{5 + 0.5 \\times 3} = \\frac{2.5}{6.5} \\approx 0.385\n",
    "   $$\n",
    "\n",
    "3. **Probabilidad suavizada para \"ratón\":**\n",
    "   $$\n",
    "   P^*_{\\text{Add-0.5}}(\\text{\"ratón\"}) = \\frac{0 + 0.5}{5 + 0.5 \\times 3} = \\frac{0.5}{6.5} \\approx 0.077\n",
    "   $$\n",
    "\n",
    "#### Ejemplo 2: Suavizado Add-k para bigrama\n",
    "\n",
    "Supongamos un corpus con los siguientes bigramas y sus conteos:\n",
    "- $C(\\text{\"gato come\"}) = 2$\n",
    "- $C(\\text{\"perro come\"}) = 1$\n",
    "\n",
    "El tamaño del vocabulario $V = 3$. Queremos calcular la probabilidad suavizada para el bigrama \"come\" dado \"gato\" usando $k = 0.1$:\n",
    "\n",
    "$$\n",
    "P^*_{\\text{Add-0.1}}(\\text{\"come\"} | \\text{\"gato\"}) = \\frac{C(\\text{\"gato come\"}) + k}{C(\\text{\"gato\"}) + kV} = \\frac{2 + 0.1}{2 + 0.1 \\times 3} = \\frac{2.1}{2.3} \\approx 0.913\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab644bf7-8c82-4224-afe7-ba0c9ec4bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suavizado Add-k para unigramas\n",
    "def add_k_smoothing_unigram(corpus, k):\n",
    "    # Conteo de palabras en el corpus\n",
    "    word_counts = {}\n",
    "    for word in corpus:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "    # Número total de tokens en el corpus\n",
    "    N = sum(word_counts.values())\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    V = len(word_counts)\n",
    "\n",
    "    # Cálculo de las probabilidades suavizadas\n",
    "    add_k_probabilities = {}\n",
    "    for word, count in word_counts.items():\n",
    "        # Aplicando la ecuación P_Add-k(w_i) = (c_i + k) / (N + kV)\n",
    "        add_k_probabilities[word] = (count + k) / (N + k * V)\n",
    "    \n",
    "    # Probabilidad para una palabra no vista\n",
    "    add_k_probabilities['<UNK>'] = k / (N + k * V)\n",
    "    \n",
    "    return add_k_probabilities\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = [\"gato\", \"perro\", \"gato\", \"pájaro\", \"gato\"]\n",
    "k = 0.5\n",
    "add_k_prob_unigrams = add_k_smoothing_unigram(corpus, k)\n",
    "print(\"Probabilidades de unigramas suavizadas con Add-k (k=0.5):\")\n",
    "for word, prob in add_k_prob_unigrams.items():\n",
    "    print(f\"P({word}) = {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc858c-514c-4be0-b9b0-2d9c8f1bbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suavizado add-k para bigrama\n",
    "def add_k_smoothing_bigram(corpus, k):\n",
    "    # Conteo de bigramas y unigrams\n",
    "    bigram_counts = {}\n",
    "    unigram_counts = {}\n",
    "\n",
    "    # Construir bigramas\n",
    "    for i in range(len(corpus) - 1):\n",
    "        bigram = (corpus[i], corpus[i + 1])\n",
    "        unigram = corpus[i]\n",
    "\n",
    "        # Conteo de bigramas\n",
    "        if bigram in bigram_counts:\n",
    "            bigram_counts[bigram] += 1\n",
    "        else:\n",
    "            bigram_counts[bigram] = 1\n",
    "\n",
    "        # Conteo de unigrams\n",
    "        if unigram in unigram_counts:\n",
    "            unigram_counts[unigram] += 1\n",
    "        else:\n",
    "            unigram_counts[unigram] = 1\n",
    "\n",
    "    # Contar el último unigramo\n",
    "    last_word = corpus[-1]\n",
    "    if last_word in unigram_counts:\n",
    "        unigram_counts[last_word] += 1\n",
    "    else:\n",
    "        unigram_counts[last_word] = 1\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigram_counts)\n",
    "\n",
    "    # Cálculo de las probabilidades suavizadas para bigramas\n",
    "    add_k_probabilities = {}\n",
    "    for bigram, bigram_count in bigram_counts.items():\n",
    "        w_n_1 = bigram[0]  # w_{n-1}\n",
    "        # Aplicando la ecuación P_Add-k(w_n | w_{n-1}) = (C(w_{n-1}w_n) + k) / (C(w_{n-1}) + kV)\n",
    "        add_k_probabilities[bigram] = (bigram_count + k) / (unigram_counts[w_n_1] + k * V)\n",
    "\n",
    "    # Probabilidad para un bigrama no visto\n",
    "    add_k_probabilities[('<UNK>', '<UNK>')] = k / (V * (V + k))\n",
    "    \n",
    "    return add_k_probabilities\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = [\"gato\", \"come\", \"pájaro\", \"vuela\", \"gato\", \"duerme\"]\n",
    "k = 0.1\n",
    "add_k_prob_bigrams = add_k_smoothing_bigram(corpus, k)\n",
    "print(\"\\nProbabilidades de bigramas suavizadas con Add-k (k=0.1):\")\n",
    "for bigram, prob in add_k_prob_bigrams.items():\n",
    "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d4307-3c36-4876-bd7a-27281ecdc3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suavizado add-k para trigramas\n",
    "\n",
    "def add_k_smoothing_trigram(corpus, k):\n",
    "    # Conteo de trigramas, bigramas y unigrams\n",
    "    trigram_counts = {}\n",
    "    bigram_counts = {}\n",
    "    unigram_counts = {}\n",
    "    \n",
    "    # Construir trigramas\n",
    "    for i in range(len(corpus) - 2):\n",
    "        trigram = (corpus[i], corpus[i + 1], corpus[i + 2])\n",
    "        bigram = (corpus[i], corpus[i + 1])\n",
    "        unigram = corpus[i]\n",
    "        \n",
    "        # Contar trigramas\n",
    "        if trigram in trigram_counts:\n",
    "            trigram_counts[trigram] += 1\n",
    "        else:\n",
    "            trigram_counts[trigram] = 1\n",
    "        \n",
    "        # Contar bigramas\n",
    "        if bigram in bigram_counts:\n",
    "            bigram_counts[bigram] += 1\n",
    "        else:\n",
    "            bigram_counts[bigram] = 1\n",
    "        \n",
    "        # Contar unigrams\n",
    "        if unigram in unigram_counts:\n",
    "            unigram_counts[unigram] += 1\n",
    "        else:\n",
    "            unigram_counts[unigram] = 1\n",
    "\n",
    "    # Contar el último unigramo\n",
    "    last_word = corpus[-1]\n",
    "    if last_word in unigram_counts:\n",
    "        unigram_counts[last_word] += 1\n",
    "    else:\n",
    "        unigram_counts[last_word] = 1\n",
    "    \n",
    "    # Contar los últimos bigramas restantes\n",
    "    for i in range(len(corpus) - 1, len(corpus)):\n",
    "        last_bigram = (corpus[i - 1], corpus[i])\n",
    "        if last_bigram in bigram_counts:\n",
    "            bigram_counts[last_bigram] += 1\n",
    "        else:\n",
    "            bigram_counts[last_bigram] = 1\n",
    "    \n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigram_counts)\n",
    "\n",
    "    # Cálculo de las probabilidades suavizadas para trigramas\n",
    "    add_k_probabilities = {}\n",
    "    for trigram, trigram_count in trigram_counts.items():\n",
    "        w_n_2_w_n_1 = (trigram[0], trigram[1])  # w_{n-2}w_{n-1}\n",
    "        # Aplicando la ecuación P_Add-k(w_n | w_{n-2}w_{n-1}) = (C(w_{n-2}w_{n-1}w_n) + k) / (C(w_{n-2}w_{n-1}) + kV)\n",
    "        add_k_probabilities[trigram] = (trigram_count + k) / (bigram_counts[w_n_2_w_n_1] + k * V)\n",
    "\n",
    "    # Probabilidad para un trigrama no visto\n",
    "    add_k_probabilities[('<UNK>', '<UNK>', '<UNK>')] = k / (V * (V + k))\n",
    "    \n",
    "    return add_k_probabilities\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = [\"el\", \"gato\", \"come\", \"pájaro\", \"vuela\", \"gato\", \"duerme\", \"el\", \"gato\"]\n",
    "k = 0.1\n",
    "add_k_prob_trigrams = add_k_smoothing_trigram(corpus, k)\n",
    "print(\"\\nProbabilidades de trigramas suavizadas con Add-k (k=0.1):\")\n",
    "for trigram, prob in add_k_prob_trigrams.items():\n",
    "    print(f\"P({trigram[2]} | {trigram[0]} {trigram[1]}) = {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72f14eb-bfe8-4cfe-9870-12a3768fbe92",
   "metadata": {},
   "source": [
    "La salida de este código muestra las probabilidades suavizadas para los trigramas presentes en el corpus, asignando una probabilidad distinta de cero a los trigramas, incluso si no han sido observados durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c554eb-0e11-4dac-8703-269201513512",
   "metadata": {},
   "source": [
    "#### Ejercicios teóricos\n",
    "\n",
    "1. **Comparación entre suavizado de Laplace y Add-k:**\n",
    "   - Dado un conjunto de palabras con los siguientes conteos: \n",
    "     - \"gato\": 5\n",
    "     - \"perro\": 3\n",
    "     - \"ratón\": 0\n",
    "   - Supón que el tamaño del vocabulario $V = 3$ y $N = 8$. Calcula las probabilidades suavizadas usando:\n",
    "     - **Suavizado de Laplace**:\n",
    "       $$\n",
    "       P_{\\text{Laplace}}(w_i) = \\frac{c_i + 1}{N + V}\n",
    "       $$\n",
    "     - **Suavizado add-k** con \\( k = 0.5 \\):\n",
    "       $$\n",
    "       P^*_{\\text{Add-k}}(w_i) = \\frac{c_i + k}{N + kV}\n",
    "       $$\n",
    "   - Compara las probabilidades obtenidas con ambos métodos y discute las diferencias. ¿Qué sucede con la probabilidad asignada a la palabra \"ratón\"?\n",
    "\n",
    "2. **Elección del valor $k$ en suavizado Add-k:**\n",
    "   - Explica por qué es importante elegir el valor adecuado de $k$ en el suavizado **add-k**. ¿Qué sucede si $k$ es demasiado grande o demasiado pequeño? ¿Cómo afecta esto a la distribución de probabilidades?\n",
    "   - Sugiere un método para elegir el valor óptimo de $k$ utilizando un conjunto de validación. ¿Qué métricas utilizarías para evaluar la calidad del suavizado?\n",
    "\n",
    "3. **Aplicación en modelos de lenguaje:**\n",
    "   - Discute por qué el suavizado **add-k** puede no ser efectivo para el modelado de lenguajes en comparación con otras técnicas de suavizado más avanzadas. ¿Qué problemas podrían surgir al usar suavizado **add-k** en modelos de lenguaje n-grama de mayor orden (bigramas, trigramas, etc.)?\n",
    "\n",
    "4. **Impacto del suavizado en la clasificación de textos:**\n",
    "   - El suavizado **add-k** se ha utilizado en tareas de clasificación de textos. Explica cómo el suavizado puede afectar la clasificación de documentos cuando se utilizan modelos de n-gramas. ¿En qué casos podría el suavizado **add-k** ser beneficioso para la clasificación de textos?\n",
    "\n",
    "5. **Evaluación de suavizado en conjuntos de datos desbalanceados:**\n",
    "   - Dado un conjunto de datos desbalanceado en el que algunas palabras ocurren con mucha más frecuencia que otras, ¿cómo afectaría el suavizado **add-k** a la probabilidad de palabras raras? ¿El suavizado de Laplace sería más o menos efectivo en este escenario?\n",
    "\n",
    "#### Ejercicios prácticos\n",
    "\n",
    "1. **Implementación del suavizado Add-k para unigramas y comparación con Laplace:**\n",
    "   - Implementa una función en Python para calcular las probabilidades suavizadas usando el suavizado **add-k** para unigramas y compárala con el suavizado de Laplace:\n",
    "   ```python\n",
    "   def add_k_smoothing_unigram(corpus, k):\n",
    "       # Tu implementación aquí\n",
    "       pass\n",
    "\n",
    "   def laplace_smoothing_unigram(corpus):\n",
    "       # Implementación del suavizado de Laplace\n",
    "       pass\n",
    "   ```\n",
    "   - Prueba ambas funciones con el siguiente corpus: `[\"gato\", \"perro\", \"gato\", \"ratón\", \"gato\"]`.\n",
    "   - Compara las probabilidades generadas por ambos métodos y analiza cómo el valor de $k$ afecta a los resultados.\n",
    "\n",
    "2. **Optimización del valor $k$ usando validación cruzada:**\n",
    "   - Implementa un script en Python que utilice validación cruzada para encontrar el valor óptimo de $k$ para un conjunto de datos dado. La función debe maximizar la precisión de las probabilidades suavizadas:\n",
    "   ```python\n",
    "   def find_optimal_k(corpus, k_values):\n",
    "       # Tu implementación aquí\n",
    "       pass\n",
    "   ```\n",
    "   - Utiliza esta función con un rango de valores de $k$ ( $k = [0.01, 0.05, 0.1, 0.5]$) para encontrar el valor óptimo que minimiza el error en un conjunto de validación.\n",
    "\n",
    "3. **Implementación del suavizado Add-k para bigrama:**\n",
    "   - Escribe una función en Python para calcular las probabilidades suavizadas usando el suavizado **add-k** para bigramas:\n",
    "   ```python\n",
    "   def add_k_smoothing_bigram(corpus, k):\n",
    "       # Tu implementación aquí\n",
    "       pass\n",
    "   ```\n",
    "   - Usa la función con el siguiente corpus: `[\"el\", \"gato\", \"come\", \"ratón\", \"el\", \"perro\", \"duerme\"]` y compara los resultados con el suavizado de Laplace. ¿Qué cambios observas en las probabilidades de bigramas?\n",
    "\n",
    "4. **Análisis de sensibilidad del valor $k$:**\n",
    "   - Implementa un experimento que grafique la variación de las probabilidades suavizadas con diferentes valores de $k$ en un conjunto de bigramas. Utiliza las funciones implementadas para suavizado **add-k** y suavizado de Laplace:\n",
    "   ```python\n",
    "   def plot_smoothing_variation(corpus, k_values):\n",
    "       # Tu implementación aquí\n",
    "       pass\n",
    "   ```\n",
    "   - Analiza los resultados y discute cómo la variación de $k$ afecta las probabilidades suavizadas.\n",
    "\n",
    "5. **Aplicación en clasificación de textos con N-Gramas:**\n",
    "   - Implementa un clasificador de textos simple basado en n-gramas con suavizado **add-k**. Utiliza los trigramas de un corpus de textos para clasificar las oraciones:\n",
    "   ```python\n",
    "   def classify_texts_with_add_k(corpus, k):\n",
    "       # Tu implementación aquí\n",
    "       pass\n",
    "   ```\n",
    "   - Evalúa el rendimiento del clasificador con diferentes valores de $k$ y compara los resultados con un clasificador que utiliza suavizado de Laplace.\n",
    "6. Después de completar los ejercicios prácticos, reflexiona sobre los resultados obtenidos. ¿Cuándo resulta más efectivo el suavizado **add-k** que el suavizado de Laplace? ¿Qué situaciones requieren ajustes más finos en la transferencia de masa de probabilidad a eventos no vistos?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34eedd-af84-41b1-9e0c-58d92d22e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625a653-5393-49aa-bf56-f9cbd40cccf3",
   "metadata": {},
   "source": [
    "### Backoff y interpolación\n",
    "\n",
    "El descuento que hemos estado discutiendo hasta ahora puede ayudar a resolver el problema de n-gramas de frecuencia cero. Pero hay una fuente adicional de conocimiento de la que podemos aprovechar. Si estamos tratando de calcular $P(w_n |w_{n-2} w_{n-1})$ pero no tenemos ejemplos de un trigram específico $w_{n-2} w_{n-1} w_n$, podemos estimar su probabilidad usando la probabilidad del bigrama $P(w_n|w_{n-1})$. De manera similar, si no tenemos conteos para calcular $P(w_n|w_{n-1})$, podemos recurrir al unigram $P(w_n)$.\n",
    "\n",
    "En otras palabras, a veces usar menos contexto es algo bueno, ayudando a generalizar más para los contextos sobre los que el modelo no ha aprendido mucho. Hay dos formas de utilizar esta \"jerarquía\" de n-gramas. En el backoff, usamos el trigrama si la evidencia es suficiente, de lo contrario usamos el bigrama, y si no, el unigrama. En otras palabras, solo \"retrocedemos\" a un n-grama de menor orden si no tenemos evidencia de un n-grama de mayor orden. Por el contrario, en la interpolación, siempre combinamos las estimaciones de probabilidad de todos los estimadores de n-gramas, ponderando y combinando los conteos de trigramas, bigramas y unigrams.\n",
    "\n",
    "En la interpolación lineal simple, combinamos n-gramas de diferentes órdenes interpolándolos linealmente. Así, estimamos la probabilidad del trigrama $P(w_n |w_{n-2} w_{n-1})$ mezclando las probabilidades de unigramas, bigramas y trigramas, cada una ponderada por un $\\lambda$:\n",
    "\n",
    "$$\\hat{P}(w_n | w_{n-2}w_{n-1}) = \\lambda_1 P(w_n) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_3 P(w_n | w_{n-2}w_{n-1})$$\n",
    "\n",
    "\n",
    "Los $\\lambda$ deben sumar `1`, haciendo que la ecuación sea equivalente a un promedio ponderado. En una versión ligeramente más sofisticada de la interpolación lineal, cada peso $\\lambda$ se calcula condicionando en el contexto. De esta manera, si tenemos conteos particularmente precisos para un bigrama específico, asumimos que los conteos de los trigramas basados en este bigrama serán más confiables, por lo que podemos aumentar los $\\lambda$ para esos trigramas y, por lo tanto, darle más peso al trigrama en la interpolación. La ecuación  muestra la ecuación para la interpolación con pesos condicionados por el contexto:\n",
    "\n",
    "$$\\hat{P}(w_n | w_{n-2}w_{n-1}) = \\lambda_1 (w_{n-2}w_{n-1}) P(w_n) + \\lambda_2 (w_{n-2}w_{n-1}) P(w_n | w_{n-1}) + \\lambda_3 (w_{n-2}w_{n-1}) P(w_n | w_{n-2}w_{n-1})$$\n",
    "\n",
    "\n",
    "¿Cómo se establecen estos valores de $\\lambda$? \n",
    "\n",
    "Tanto la interpolación simple como la interpolación condicionada de $\\lambda$ se aprenden a partir de un corpus reservado. Un corpus reservado es un corpus de entrenamiento adicional, llamado así porque lo reservamos del conjunto de entrenamiento, y lo usamos para establecer hiperparámetros como estos valores $\\lambda$. Lo hacemos eligiendo los valores de $\\lambda$ que maximizan la probabilidad del corpus reservado. Es decir, fijamos las probabilidades de n-gramas y luego buscamos los valores de $\\lambda$ que, cuando se introducen la ecuaciones anteriores, nos dan la mayor probabilidad para el conjunto reservado. Hay varias formas de encontrar este conjunto óptimo de $\\lambda$. Una forma es usar el algoritmo EM, un algoritmo de aprendizaje iterativo que converge en valores $\\lambda$ localmente óptimos.\n",
    "\n",
    "En un modelo de n-gramas backoff, si el n-grama que necesitamos tiene conteos cero, lo aproximamos retrocediendo al (n-1)-grama. Continuamos retrocediendo hasta llegar a un historial que tenga algunos conteos. Para que un modelo backoff proporcione una distribución de probabilidad correcta, tenemos que descontar los n-gramas de mayor orden para reservar algo de masa de probabilidad para los n-gramas de menor orden. Al igual que con el suavizado add-k, si los n-gramas de mayor orden no se descuentan y usamos simplemente la probabilidad MLE sin descuentos, entonces, tan pronto como reemplazáramos un n-grama con probabilidad cero por un n-grama de menor orden, estaríamos añadiendo masa de probabilidad, y la probabilidad total asignada a todas las cadenas posibles por el modelo de lenguaje sería mayor a 1. Además de este factor de descuento explícito, necesitaremos una función $\\alpha$ para distribuir esta masa de probabilidad a los n-gramas de menor orden.\n",
    "\n",
    "Este tipo de backoff con descuento también se llama backoff Katz. En el backoff de Katz confiamos en una probabilidad descontada $P^∗$ si hemos visto este n-grama antes (es decir, si tenemos conteos no nulos). De lo contrario, retrocedemos recursivamente a la probabilidad de Katz para el n-grama de historia más corta $(n-1)$. La probabilidad para un n-grama backoff $P_{BO}$ se calcula de la siguiente manera:\n",
    "\n",
    "$$\n",
    "P_{\\text{BO}}(w_n | w_{n-N+1:n-1}) = \n",
    "\\begin{cases} \n",
    "P^*(w_n | w_{n-N+1:n-1}), & \\text{if } C(w_{n-N+1:n-1}) > 0 \\\\\n",
    "\\alpha(w_{n-N+1:n-1}) P_{\\text{BO}}(w_n | w_{n-N+2:n-1}), & \\text{en otros casos.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "El backoff de Katz a menudo se combina con un método de suavizado llamado Good-Turing. El algoritmo combinado de backoff Good-Turing implica cálculos bastante detallados para estimar el suavizado Good-Turing y los valores $P^*$ y $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da0be4-8c8d-4f99-bd96-09b5e3c491cd",
   "metadata": {},
   "source": [
    "### Ejemplos\n",
    "\n",
    "#### 1. Backoff\n",
    "\n",
    "**Descripción**: El enfoque de **backoff** retrocede a un n-grama de menor orden cuando no se encuentran suficientes evidencias para un n-grama de mayor orden. Por ejemplo, si no tenemos conteos para un trigram $P(w_n | w_{n-2} w_{n-1})$, retrocedemos al bigrama $P(w_n | w_{n-1})$. Si tampoco hay evidencia suficiente para el bigrama, retrocedemos al unigrama $P(w_n)$.\n",
    "\n",
    "**Ejemplo**: \n",
    "Supongamos el siguiente corpus:\n",
    "- \"el gato duerme\"\n",
    "- \"el perro ladra\"\n",
    "\n",
    "1. **Conteos de trigramas**:\n",
    "   - $C(\\text{\"el gato duerme\"}) = 1$\n",
    "   - $C(\\text{\"gato duerme rápido\"}) = 0$  (no existe)\n",
    "\n",
    "2. **Probabilidad de trigramas**:\n",
    "   - $P(\\text{\"duerme\"} | \\text{\"el gato\"})$ tiene un conteo, así que se utiliza:  \n",
    "   $$\n",
    "   P(\\text{\"duerme\"} | \\text{\"el gato\"}) = \\frac{C(\\text{\"el gato duerme\"})}{C(\\text{\"el gato\"})} = \\frac{1}{1} = 1\n",
    "   $$\n",
    "\n",
    "3. **Backoff**:\n",
    "   - Si buscamos $P(\\text{\"rápido\"} | \\text{\"gato duerme\"})$, no tenemos el trigrama. Así que retrocedemos al bigrama:\n",
    "   $$\n",
    "   P(\\text{\"rápido\"} | \\text{\"duerme\"}) \\approx P(\\text{\"rápido\"})  \\quad \\text{(si tampoco hay bigrama)}\n",
    "   $$\n",
    "\n",
    "#### 2. Interpolación\n",
    "\n",
    "**Descripción**: En la **interpolación**, en lugar de retroceder, combinamos las probabilidades de diferentes órdenes de n-gramas. Por ejemplo, en la interpolación lineal, ponderamos las probabilidades de unigramas, bigramas y trigramas para obtener una estimación más general.\n",
    "\n",
    "**Ejemplo**:\n",
    "Dado el corpus anterior y las siguientes probabilidades (supongamos suavizadas):\n",
    "- $P(\\text{\"duerme\"} | \\text{\"el gato\"}) = 0.5$\n",
    "- $P(\\text{\"duerme\"} | \\text{\"gato\"}) = 0.3$\n",
    "- $P(\\text{\"duerme\"}) = 0.2$\n",
    "\n",
    "Supongamos los pesos $\\lambda_1 = 0.1$, $\\lambda_2 = 0.3$ y $\\lambda_3 = 0.6$. La probabilidad interpolada es:\n",
    "\n",
    "$$\n",
    "\\hat{P}(\\text{\"duerme\"} | \\text{\"el gato\"}) = \\lambda_1 P(\\text{\"duerme\"}) + \\lambda_2 P(\\text{\"duerme\"} | \\text{\"gato\"}) + \\lambda_3 P(\\text{\"duerme\"} | \\text{\"el gato\"})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{P}(\\text{\"duerme\"} | \\text{\"el gato\"}) = 0.1 \\times 0.2 + 0.3 \\times 0.3 + 0.6 \\times 0.5 = 0.02 + 0.09 + 0.3 = 0.41\n",
    "$$\n",
    "\n",
    "#### 3. Modelo de N-Gramas Backoff\n",
    "\n",
    "**Descripción**: En un modelo de n-gramas backoff, retrocedemos de trigramas a bigramas y luego a unigramas si no tenemos suficiente evidencia para los n-gramas de mayor orden. Necesitamos un factor de descuento explícito para mantener la correcta distribución de probabilidad.\n",
    "\n",
    "**Ejemplo**:\n",
    "Supongamos que utilizamos el siguiente corpus:\n",
    "- \"el gato come\"\n",
    "- \"el perro duerme\"\n",
    "\n",
    "Queremos calcular $P_{\\text{BO}}(\\text{\"come\"} | \\text{\"el gato\"})$.\n",
    "\n",
    "1. **Trigramas**:\n",
    "   - $C(\\text{\"el gato come\"}) = 1$\n",
    "\n",
    "2. **Bigramas**:\n",
    "   - $C(\\text{\"gato come\"}) = 1$\n",
    "\n",
    "3. **Probabilidad con Backoff**:\n",
    "   - Si el trigram no existiera $( C(\\text{\"el gato come\"}) = 0 $), usaríamos el bigrama:\n",
    "   $$\n",
    "   P_{\\text{BO}}(\\text{\"come\"} | \\text{\"gato\"}) = \\alpha P(\\text{\"come\"} | \\text{\"gato\"})\n",
    "   $$\n",
    "\n",
    "#### 4. Backoff de Katz\n",
    "\n",
    "**Descripción**: El **backoff de Katz** utiliza una probabilidad descontada si el n-grama ha sido visto antes; si no, retrocede al n-grama de menor orden. Incluye un factor de descuento $\\alpha$ para distribuir la masa de probabilidad correctamente.\n",
    "\n",
    "**Ejemplo**:\n",
    "Supongamos los siguientes conteos:\n",
    "- $C(\\text{\"el gato come\"}) = 1$\n",
    "- $C(\\text{\"gato come\"}) = 1$\n",
    "\n",
    "La probabilidad para un n-grama backoff se calcula como:\n",
    "\n",
    "$$\n",
    "P_{\\text{BO}}(\\text{\"come\"} | \\text{\"el gato\"}) = \n",
    "\\begin{cases} \n",
    "P^*(\\text{\"come\"} | \\text{\"el gato\"}), & \\text{si } C(\\text{\"el gato\"}) > 0 \\\\\n",
    "\\alpha(\\text{\"el gato\"}) P_{\\text{BO}}(\\text{\"come\"} | \\text{\"gato\"}), & \\text{en otros casos}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Supongamos que $\\alpha(\\text{\"el gato\"}) = 0.4$. Si no hay suficiente conteo para el trigrama:\n",
    "\n",
    "$$\n",
    "P_{\\text{BO}}(\\text{\"come\"} | \\text{\"el gato\"}) = 0.4 \\times P(\\text{\"come\"} | \\text{\"gato\"})\n",
    "$$\n",
    "\n",
    "#### 5: Good-Turing\n",
    "\n",
    "**Descripción**: La estimación de **Good-Turing** ajusta los conteos de n-gramas observados para reservar masa de probabilidad a los n-gramas no vistos. Esta técnica redistribuye la probabilidad de manera que incluso las secuencias no observadas tengan una probabilidad distinta de cero.\n",
    "\n",
    "#### Ejemplo Continuado:\n",
    "Supongamos que tenemos los siguientes conteos de bigramas:\n",
    "- $C(\\text{\"gato duerme\"}) = 5$\n",
    "- $C(\\text{\"perro duerme\"}) = 1$\n",
    "- $C(\\text{\"ratón duerme\"}) = 0$\n",
    "\n",
    "Queremos calcular las probabilidades ajustadas usando la estimación de **Good-Turing**.\n",
    "\n",
    "1. **Paso 1: Calcular los números de conteo**:\n",
    "   - Número de bigramas con conteo 1 ($ N_1$): Número de bigramas que ocurren exactamente una vez en el corpus. En este caso, $N_1 = 1$ (\"perro duerme\").\n",
    "   - Número de bigramas con conteo 5 ($N_5$): Número de bigramas que ocurren exactamente cinco veces en el corpus. Aquí, $N_5 = 1$ (\"gato duerme\").\n",
    "\n",
    "2. **Paso 2: Aplicar la estimación de Good-Turing**:\n",
    "   - Para bigramas con conteo mayor a cero, ajustamos los conteos:\n",
    "     - Para \"gato duerme\" ($C = 5$):\n",
    "       $$\n",
    "       P^*(\\text{\"gato duerme\"}) = \\frac{(C + 1) \\times \\frac{N_{C+1}}{N_C}}{N}\n",
    "       $$\n",
    "       Como no hay bigramas con \\( C+1 = 6 \\), podemos aproximar esta fracción con \\( \\frac{N_5}{N_4} \\approx \\frac{N_1}{N_5} \\):\n",
    "       $$\n",
    "       P^*(\\text{\"gato duerme\"}) \\approx \\frac{(5 + 1) \\times \\frac{N_1}{N_5}}{N} = \\frac{6 \\times \\frac{1}{1}}{N} = \\frac{6}{N}\n",
    "       $$\n",
    "     - Normalizamos para obtener la probabilidad:\n",
    "       $$\n",
    "       P^*(\\text{\"gato duerme\"}) = \\frac{6}{N}\n",
    "       $$\n",
    "\n",
    "   - Para \"ratón duerme\" ($C = 0$):\n",
    "     - Redistribuimos la masa de probabilidad:\n",
    "       $$\n",
    "       P^*(\\text{\"ratón duerme\"}) = \\frac{N_1}{N}\n",
    "       $$\n",
    "\n",
    "3. **Paso 3: Probabilidad ajustada para \"gato duerme\" y \"ratón duerme\"**:\n",
    "   - $N$: Total de bigramas en el corpus. Supongamos $N = 10$ (suma de todos los bigramas).\n",
    "   - Probabilidad ajustada para \"gato duerme\":\n",
    "     $$\n",
    "     P^*(\\text{\"gato duerme\"}) = \\frac{6}{10} = 0.6\n",
    "     $$\n",
    "   - Probabilidad ajustada para \"ratón duerme\" (evento no visto):\n",
    "     $$\n",
    "     P^*(\\text{\"ratón duerme\"}) = \\frac{1}{10} = 0.1\n",
    "     $$\n",
    "\n",
    "### 6. Algoritmo combinado de Backoff Good-Turing\n",
    "\n",
    "**Descripción**: Este algoritmo combina el backoff de Katz con la estimación de Good-Turing. Primero, se aplican los descuentos de Good-Turing a los n-gramas observados, y luego se utiliza el backoff para distribuir la masa de probabilidad restante a los n-gramas de menor orden.\n",
    "\n",
    "#### Ejemplo:\n",
    "Queremos calcular $P_{\\text{BO}}(\\text{\"juega\"} | \\text{\"el gato\"})$ utilizando el backoff de Katz y el suavizado de Good-Turing.\n",
    "\n",
    "1. **Paso 1: Estimar conteos con Good-Turing**:\n",
    "   - Supongamos que los conteos ajustados de Good-Turing para algunos bigramas son:\n",
    "     - $C^*(\\text{\"el gato\"}) = 3.5$\n",
    "     - $C^*(\\text{\"gato duerme\"}) = 2.0$\n",
    "     - $C^*(\\text{\"gato juega\"}) = 0.0$ (no observado)\n",
    "\n",
    "2. **Paso 2: Calcular la probabilidad ajustada**:\n",
    "   - Para $P^*(\\text{\"duerme\"} | \\text{\"el gato\"})$:\n",
    "     $$\n",
    "     P^*(\\text{\"duerme\"} | \\text{\"el gato\"}) = \\frac{C^*(\\text{\"el gato duerme\"})}{C^*(\\text{\"el gato\"})} = \\frac{2.0}{3.5} \\approx 0.571\n",
    "     $$\n",
    "\n",
    "   - Para \"gato juega\", como $C^*(\\text{\"gato juega\"}) = 0$:\n",
    "     - Utilizamos backoff y asignamos una probabilidad:\n",
    "       $$\n",
    "       P_{\\text{BO}}(\\text{\"juega\"} | \\text{\"gato\"}) = \\alpha(\\text{\"gato\"}) P^*(\\text{\"juega\"} | \\text{\"gato\"})\n",
    "       $$\n",
    "\n",
    "3. **Paso 3: Backoff**:\n",
    "   - Asumamos que el factor de backoff $\\alpha(\\text{\"gato\"}) = 0.4$:\n",
    "     $$\n",
    "     P_{\\text{BO}}(\\text{\"juega\"} | \\text{\"gato\"}) = 0.4 \\times P(\\text{\"juega\"})\n",
    "     $$\n",
    "\n",
    "4. **Paso 4: Probabilidad final**:\n",
    "   - Combinamos las probabilidades ajustadas y backoff:\n",
    "     $$\n",
    "     P_{\\text{BO}}(\\text{\"juega\"} | \\text{\"el gato\"}) = 0.4 \\times P(\\text{\"juega\"})\n",
    "     $$\n",
    "\n",
    "Este método asegura que las probabilidades se redistribuyan correctamente, incluso cuando no hay suficientes datos para los n-gramas de mayor orden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e699f-ab60-40b1-8403-82d35463b745",
   "metadata": {},
   "source": [
    "#### Ejemplo con un corpus más grande\n",
    "\n",
    "#### Corpus:\n",
    "Supongamos el siguiente corpus, con 8 oraciones para mayor complejidad:\n",
    "\n",
    "1. \"el gato duerme\"\n",
    "2. \"el gato come pescado\"\n",
    "3. \"el perro ladra\"\n",
    "4. \"el perro duerme\"\n",
    "5. \"el gato corre\"\n",
    "6. \"el perro come carne\"\n",
    "7. \"el ratón come queso\"\n",
    "8. \"el gato y el perro juegan\"\n",
    "\n",
    "Queremos calcular la probabilidad de la secuencia \"el gato come\" utilizando interpolación y backoff de Katz.\n",
    "\n",
    "#### Conteos de N-gramas\n",
    "\n",
    "1. **Unigramas** (frecuencia de palabras individuales):\n",
    "   - $C(\\text{\"el\"}) = 8$\n",
    "   - $C(\\text{\"gato\"}) = 4$\n",
    "   - $C(\\text{\"duerme\"}) = 2$\n",
    "   - $C(\\text{\"come\"}) = 3$\n",
    "   - $C(\\text{\"perro\"}) = 4$\n",
    "   - $C(\\text{\"pescado\"}) = 1$\n",
    "   - $C(\\text{\"carne\"}) = 1$\n",
    "   - $C(\\text{\"ratón\"}) = 1$\n",
    "   - $C(\\text{\"queso\"}) = 1$\n",
    "   - $C(\\text{\"corre\"}) = 1$\n",
    "   - $C(\\text{\"ladra\"}) = 1$\n",
    "   - $C(\\text{\"y\"}) = 1$\n",
    "   - $C(\\text{\"juegan\"}) = 1$\n",
    "\n",
    "2. **Bigramas** (frecuencia de pares de palabras):\n",
    "   - $C(\\text{\"el gato\"}) = 4$\n",
    "   - $C(\\text{\"gato duerme\"}) = 1$\n",
    "   - $C(\\text{\"gato come\"}) = 1$\n",
    "   - $C(\\text{\"el perro\"}) = 4$\n",
    "   - $C(\\text{\"perro ladra\"}) = 1$\n",
    "   - $C(\\text{\"perro duerme\"}) = 1$\n",
    "   - $C(\\text{\"perro come\"}) = 1$\n",
    "   - $C(\\text{\"ratón come\"}) = 1$\n",
    "   - $C(\\text{\"come pescado\"}) = 1$\n",
    "   - $C(\\text{\"come carne\"}) = 1$\n",
    "   - $C(\\text{\"come queso\"}) = 1$\n",
    "   - $C(\\text{\"gato corre\"}) = 1$\n",
    "   - $C(\\text{\"gato y\"}) = 1$\n",
    "   - $C(\\text{\"y el\"}) = 1$\n",
    "   - $C(\\text{\"el perro\"}) = 2$\n",
    "   - $C(\\text{\"perro juegan\"}) = 1$\n",
    "\n",
    "3. **Trigramas** (frecuencia de secuencias de tres palabras):\n",
    "   - $C(\\text{\"el gato duerme\"}) = 1$\n",
    "   - $C(\\text{\"el gato come\"}) = 1$\n",
    "   - $C(\\text{\"gato come pescado\"}) = 1$\n",
    "   - $C(\\text{\"el perro ladra\"}) = 1$\n",
    "   - $C(\\text{\"el perro duerme\"}) = 1$\n",
    "   - $C(\\text{\"perro come carne\"}) = 1$\n",
    "   - $C(\\text{\"ratón come queso\"}) = 1$\n",
    "   - $C(\\text{\"el gato corre\"}) = 1$\n",
    "   - $C(\\text{\"el gato y\"}) = 1$\n",
    "   - $C(\\text{\"gato y el\"}) = 1$\n",
    "   - $C(\\text{\"y el perro\"}) = 1$\n",
    "   - $C(\\text{\"el perro juegan\"}) = 1$\n",
    "\n",
    "### 1. Interpolación lineal para el trigrama \"el gato come\"\n",
    "\n",
    "Queremos calcular la probabilidad interpolada de $P(\\text{\"come\"} | \\text{\"el gato\"})$ utilizando interpolación lineal con $\\lambda_1 = 0.2$, $\\lambda_2 = 0.3$, $\\lambda_3 = 0.5$:\n",
    "\n",
    "1. **Probabilidades MLE**:\n",
    "   - Unigrama: \n",
    "     $$\n",
    "     P(\\text{\"come\"}) = \\frac{C(\\text{\"come\"})}{\\text{total de unigramas}} = \\frac{3}{24} = 0.125\n",
    "     $$\n",
    "   - Bigrama:\n",
    "     $$\n",
    "     P(\\text{\"come\"} | \\text{\"gato\"}) = \\frac{C(\\text{\"gato come\"})}{C(\\text{\"gato\"})} = \\frac{1}{4} = 0.25\n",
    "     $$\n",
    "   - Trigrama:\n",
    "     $$\n",
    "     P(\\text{\"come\"} | \\text{\"el gato\"}) = \\frac{C(\\text{\"el gato come\"})}{C(\\text{\"el gato\"})} = \\frac{1}{4} = 0.25\n",
    "     $$\n",
    "\n",
    "2. **Probabilidad interpolada**:\n",
    "   $$\n",
    "   \\hat{P}(\\text{\"come\"} | \\text{\"el gato\"}) = \\lambda_1 P(\\text{\"come\"}) + \\lambda_2 P(\\text{\"come\"} | \\text{\"gato\"}) + \\lambda_3 P(\\text{\"come\"} | \\text{\"el gato\"})\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\hat{P}(\\text{\"come\"} | \\text{\"el gato\"}) = 0.2 \\times 0.125 + 0.3 \\times 0.25 + 0.5 \\times 0.25\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\hat{P}(\\text{\"come\"} | \\text{\"el gato\"}) = 0.025 + 0.075 + 0.125 = 0.225\n",
    "   $$\n",
    "\n",
    "#### 2. Backoff de Katz para el trigrama \"el gato corre\"\n",
    "\n",
    "Supongamos que queremos calcular $P_{\\text{BO}}(\\text{\"corre\"} | \\text{\"el gato\"})$ usando backoff de Katz:\n",
    "\n",
    "1. **Conteos**:\n",
    "   - Trigrama: $C(\\text{\"el gato corre\"}) = 1$\n",
    "   - Bigrama: $C(\\text{\"gato corre\"}) = 1$\n",
    "\n",
    "2. **Descuento**:\n",
    "   - Para trigramas con conteos, usamos $P^*(w_n | w_{n-2} w_{n-1})$. Supongamos que aplicamos un descuento $D = 0.75$:\n",
    "   $$\n",
    "   P^*(\\text{\"corre\"} | \\text{\"el gato\"}) = \\frac{C(\\text{\"el gato corre\"}) - D}{C(\\text{\"el gato\"})}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   P^*(\\text{\"corre\"} | \\text{\"el gato\"}) = \\frac{1 - 0.75}{4} = \\frac{0.25}{4} = 0.0625\n",
    "   $$\n",
    "\n",
    "3. **Backoff**:\n",
    "   - Si el trigrama no tuviera suficiente conteo, usaríamos el bigrama y aplicamos el factor $\\alpha$:\n",
    "   $$\n",
    "   P_{\\text{BO}}(\\text{\"corre\"} | \\text{\"el gato\"}) = \\alpha(\\text{\"el gato\"}) P(\\text{\"corre\"} | \\text{\"gato\"})\n",
    "   $$\n",
    "\n",
    "4. **Probabilidad final**:\n",
    "   - Dado que $C(\\text{\"el gato corre\"}) > 0$:\n",
    "   $$\n",
    "   P_{\\text{BO}}(\\text{\"corre\"} | \\text{\"el gato\"}) = P^*(\\text{\"corre\"} | \\text{\"el gato\"}) = 0.0625\n",
    "   $$\n",
    "\n",
    "\n",
    "- **Interpolación**: Nos permite combinar información de n-gramas de diferentes órdenes para una estimación más robusta. En este caso, la probabilidad interpolada de \"el gato come\" fue $0.225$, al incorporar información de unigramas, bigramas y trigramas.\n",
    "- **Backoff de Katz**: Proporciona una forma de retroceder a n-gramas de menor orden cuando los conteos de n-gramas de mayor orden son insuficientes. En este caso, se utilizó un factor de descuento para ajustar la probabilidad del trigrama \"el gato corre\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d32ab-87d3-4174-8f72-1b98ed054585",
   "metadata": {},
   "source": [
    "### Implementaciones\n",
    "\n",
    "#### 1. Backoff\n",
    "\n",
    "**Descripción**: El método **backoff** retrocede a un n-grama de menor orden cuando no se encuentran suficientes evidencias para un n-grama de mayor orden. Si no tenemos un trigram $P(w_n | w_{n-2} w_{n-1})$, retrocedemos al bigrama $P(w_n | w_{n-1})$. Si tampoco existe, retrocedemos al unigrama $P(w_n)$.\n",
    "\n",
    "**Ecuación**:\n",
    "\n",
    "$$\n",
    "P_{\\text{BO}}(w_n | w_{n-2} w_{n-1}) = \n",
    "\\begin{cases} \n",
    "P^*(w_n | w_{n-2} w_{n-1}), & \\text{si } C(w_{n-2} w_{n-1} w_n) > 0 \\\\\n",
    "\\alpha(w_{n-2} w_{n-1}) P_{\\text{BO}}(w_n | w_{n-1}), & \\text{si no}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4323e6-f8df-4941-9eab-4d8c72e37d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_unigram(unigram_counts, total_tokens):\n",
    "    # Calcula las probabilidades de unigramas\n",
    "    unigram_probs = {}\n",
    "    for word, count in unigram_counts.items():\n",
    "        unigram_probs[word] = count / total_tokens\n",
    "    return unigram_probs\n",
    "\n",
    "def backoff_bigram(bigram_counts, unigram_counts, total_tokens):\n",
    "    # Calcula las probabilidades de bigramas con retroceso a unigramas\n",
    "    bigram_probs = {}\n",
    "    for (w1, w2), count in bigram_counts.items():\n",
    "        if unigram_counts[w1] > 0:\n",
    "            bigram_probs[(w1, w2)] = count / unigram_counts[w1]\n",
    "        else:\n",
    "            bigram_probs[(w1, w2)] = backoff_unigram(unigram_counts, total_tokens).get(w2, 1 / total_tokens)\n",
    "    return bigram_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ff43d-a969-442e-8a6d-f4ce118d73f8",
   "metadata": {},
   "source": [
    "#### 2. Interpolación\n",
    "\n",
    "**Descripción**: La **interpolación** combina las probabilidades de diferentes órdenes de n-gramas (unigramas, bigramas, trigramas) para obtener una estimación más general. La interpolación lineal pondera estas probabilidades mediante los parámetros $\\lambda$ que deben sumar 1.\n",
    "\n",
    "**Ecuación**:\n",
    "\n",
    "$$\n",
    "\\hat{P}(w_n | w_{n-2}w_{n-1}) = \\lambda_1 P(w_n) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_3 P(w_n | w_{n-2}w_{n-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba1ea3-c2f5-4b1f-a20d-b808362b3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(unigram_probs, bigram_probs, trigram_probs, lambda1, lambda2, lambda3):\n",
    "    def interpolated_prob(w_n, w_n1, w_n2):\n",
    "        # Calcula la probabilidad interpolada\n",
    "        p_unigram = unigram_probs.get(w_n, 0)\n",
    "        p_bigram = bigram_probs.get((w_n1, w_n), 0)\n",
    "        p_trigram = trigram_probs.get((w_n2, w_n1, w_n), 0)\n",
    "        return lambda1 * p_unigram + lambda2 * p_bigram + lambda3 * p_trigram\n",
    "    return interpolated_prob\n",
    "\n",
    "# Ejemplo de uso\n",
    "unigram_probs = {'gato': 0.2, 'duerme': 0.1}\n",
    "bigram_probs = {('el', 'gato'): 0.5, ('gato', 'duerme'): 0.3}\n",
    "trigram_probs = {('el', 'gato', 'duerme'): 0.6}\n",
    "lambda1, lambda2, lambda3 = 0.1, 0.3, 0.6\n",
    "interpolated_prob = interpolation(unigram_probs, bigram_probs, trigram_probs, lambda1, lambda2, lambda3)\n",
    "print(interpolated_prob('duerme', 'gato', 'el'))  # Resultado interpolado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d067c-2ed5-44b9-9c14-15f033e33de3",
   "metadata": {},
   "source": [
    "#### 3. Modelo de N-Gramas Backoff\n",
    "\n",
    "**Descripción**: En un modelo de n-gramas backoff, retrocedemos de trigramas a bigramas y luego a unigramas si no tenemos suficiente evidencia para los n-gramas de mayor orden. Debemos descontar los n-gramas de mayor orden para reservar masa de probabilidad para los n-gramas de menor orden.\n",
    "\n",
    "**Ecuación**:\n",
    "$$\n",
    "P_{\\text{BO}}(w_n | w_{n-2} w_{n-1}) = \n",
    "\\begin{cases} \n",
    "P^*(w_n | w_{n-2} w_{n-1}), & \\text{si } C(w_{n-2} w_{n-1} w_n) > 0 \\\\\n",
    "\\alpha(w_{n-2} w_{n-1}) P_{\\text{BO}}(w_n | w_{n-1}), & \\text{si no}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e433a2b-c603-43b3-ad29-4eb04e93bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_backoff(unigram_probs, bigram_probs, trigram_probs, alpha):\n",
    "    def backoff_prob(w_n, w_n1, w_n2):\n",
    "        # Trigrama\n",
    "        if (w_n2, w_n1, w_n) in trigram_probs:\n",
    "            return trigram_probs[(w_n2, w_n1, w_n)]\n",
    "        # Bigram\n",
    "        elif (w_n1, w_n) in bigram_probs:\n",
    "            return alpha * bigram_probs[(w_n1, w_n)]\n",
    "        # Unigram\n",
    "        else:\n",
    "            return alpha * unigram_probs.get(w_n, 1 / len(unigram_probs))\n",
    "    return backoff_prob\n",
    "\n",
    "# Ejemplo de uso\n",
    "alpha = 0.4\n",
    "backoff_prob = ngram_backoff(unigram_probs, bigram_probs, trigram_probs, alpha)\n",
    "print(backoff_prob('duerme', 'gato', 'el'))  # Resultado con retroceso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227177e7-effc-4554-bce7-5689e5e1a2ac",
   "metadata": {},
   "source": [
    "#### 4. Backoff de Katz\n",
    "\n",
    "**Descripción**: El **backoff de Katz** usa un factor de descuento explícito y una función de backoff $\\alpha$ para reasignar masa de probabilidad a los n-gramas de menor orden cuando los n-gramas de mayor orden no tienen suficiente conteo.\n",
    "\n",
    "**Ecuación**:\n",
    "$$\n",
    "P_{\\text{BO}}(w_n | w_{n-2} w_{n-1}) = \n",
    "\\begin{cases} \n",
    "P^*(w_n | w_{n-2} w_{n-1}), & \\text{si } C(w_{n-2} w_{n-1} w_n) > 0 \\\\\n",
    "\\alpha(w_{n-2} w_{n-1}) P_{\\text{BO}}(w_n | w_{n-1}), & \\text{si no}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789656c-aa12-4558-9dcf-aa4fd1728365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def katz_backoff(unigram_counts, bigram_counts, trigram_counts, D=0.75):\n",
    "    def discounted_prob(count, context_count):\n",
    "        return max(count - D, 0) / context_count\n",
    "\n",
    "    def backoff_prob(w_n, w_n1, w_n2):\n",
    "        # Trigrama\n",
    "        trigram = (w_n2, w_n1, w_n)\n",
    "        if trigram in trigram_counts:\n",
    "            count = trigram_counts[trigram]\n",
    "            context_count = bigram_counts.get((w_n2, w_n1), 1)\n",
    "            return discounted_prob(count, context_count)\n",
    "        # Bigram\n",
    "        bigram = (w_n1, w_n)\n",
    "        if bigram in bigram_counts:\n",
    "            count = bigram_counts[bigram]\n",
    "            context_count = unigram_counts.get(w_n1, 1)\n",
    "            return discounted_prob(count, context_count)\n",
    "        # Unigram\n",
    "        return unigram_counts.get(w_n, 1) / sum(unigram_counts.values())\n",
    "\n",
    "    return backoff_prob\n",
    "\n",
    "# Ejemplo de uso\n",
    "unigram_counts = {'gato': 5, 'duerme': 3}\n",
    "bigram_counts = {('el', 'gato'): 4, ('gato', 'duerme'): 2}\n",
    "trigram_counts = {('el', 'gato', 'duerme'): 1}\n",
    "katz_prob = katz_backoff(unigram_counts, bigram_counts, trigram_counts)\n",
    "print(katz_prob('duerme', 'gato', 'el'))  # Resultado con backoff de Katz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d699f-777f-45b5-8793-198815d65897",
   "metadata": {},
   "source": [
    "#### 5. Good-Turing\n",
    "\n",
    "**Descripción**: La estimación **Good-Turing** ajusta los conteos observados de n-gramas para reasignar masa de probabilidad a los eventos no observados.\n",
    "\n",
    "**Ecuación**:\n",
    "$$\n",
    "P^*(w_n | w_{n-2} w_{n-1}) = \\frac{(C + 1) \\times \\frac{N_{C+1}}{N_C}}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24efb4e-f423-41f7-83cd-bee8d36bca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def good_turing_counts(ngram_counts):\n",
    "    count_of_counts = Counter(ngram_counts.values())\n",
    "    adjusted_counts = {}\n",
    "    \n",
    "    for ngram, count in ngram_counts.items():\n",
    "        if count + 1 in count_of_counts:\n",
    "            adjusted_counts[ngram] = (count + 1) * (count_of_counts[count + 1] / count_of_counts[count])\n",
    "        else:\n",
    "            adjusted_counts[ngram] = count\n",
    "\n",
    "    return adjusted_counts\n",
    "\n",
    "# Ejemplo de uso\n",
    "ngram_counts = {'el gato duerme': 5, 'perro duerme': 1, 'ratón duerme': 0}\n",
    "adjusted_counts = good_turing_counts(ngram_counts)\n",
    "print(adjusted_counts)  # Conteos ajustados con Good-Turing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e49b90-5a5d-4ac7-acf2-11e0b438dc06",
   "metadata": {},
   "source": [
    "#### 6. Algoritmo combinado de backoff Good-Turing\n",
    "\n",
    "**Descripción**: Este algoritmo combina el backoff de Katz con la estimación de Good-Turing. Descuenta los n-gramas y distribuye la masa de probabilidad usando Good-Turing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce48fc-0bd9-4e12-8b19-ff819483c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_good_turing(unigram_counts, bigram_counts, trigram_counts, D=0.75):\n",
    "    adjusted_counts = good_turing_counts(trigram_counts)\n",
    "\n",
    "    def backoff_prob(w_n, w_n1, w_n2):\n",
    "        trigram = (w_n2, w_n1, w_n)\n",
    "        if trigram in adjusted_counts:\n",
    "            count = adjusted_counts[trigram]\n",
    "            context_count = bigram_counts.get((w_n2, w_n1), 1)\n",
    "            return count / context_count\n",
    "        return katz_backoff(unigram_counts, bigram_counts, adjusted_counts)(w_n, w_n1, w_n2)\n",
    "\n",
    "    return backoff_prob\n",
    "\n",
    "# Ejemplo de uso\n",
    "backoff_good_turing_prob = backoff_good_turing(unigram_counts, bigram_counts, trigram_counts)\n",
    "print(backoff_good_turing_prob('duerme', 'gato', 'el'))  # Resultado con backoff Good-Turing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e047e1-ba15-4814-aa5e-203a58c8fb6a",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "#### **Ejercicio 1: Análisis de backoff en un corpus grande**\n",
    "\n",
    "Dado un corpus extenso con los siguientes conteos:\n",
    "\n",
    "- **Unigramas**:\n",
    "  - $C(\\text{\"el\"}) = 1000$\n",
    "  - $C(\\text{\"gato\"}) = 500$\n",
    "  - $C(\\text{\"duerme\"}) = 200$\n",
    "\n",
    "- **Bigramas**:\n",
    "  - $C(\\text{\"el gato\"}) = 300$\n",
    "  - $C(\\text{\"gato duerme\"}) = 100$\n",
    "\n",
    "- **Trigramas**:\n",
    "  - $C(\\text{\"el gato duerme\"}) = 50$\n",
    "\n",
    "1. Calcula la probabilidad del trigram \"el gato duerme\" utilizando **backoff** si el trigrama \"el gato duerme\" no existiera en el corpus. \n",
    "2. Explica los pasos para calcular esta probabilidad, retrocediendo a bigramas y luego a unigramas si es necesario.\n",
    "3. Analiza las ventajas y desventajas de utilizar backoff en lugar de depender exclusivamente de n-gramas de mayor orden.\n",
    "\n",
    "#### **Ejercicio 2: Optimización de pesos en interpolación**\n",
    "\n",
    "Suponga que tiene los siguientes valores de probabilidad suavizada para un trigrama:\n",
    "\n",
    "- $P(\\text{\"gato\"}) = 0.2$ (Unigrama)\n",
    "- $P(\\text{\"duerme\"} | \\text{\"gato\"}) = 0.4$ (Bigrama)\n",
    "- $P(\\text{\"duerme\"} | \\text{\"el gato\"}) = 0.6$ (Trigrama)\n",
    "\n",
    "Los pesos iniciales para la interpolación lineal son $\\lambda_1 = 0.2$, $\\lambda_2 = 0.3$, $\\lambda_3 = 0.5$.\n",
    "\n",
    "1. Determina la probabilidad interpolada $\\hat{P}(\\text{\"duerme\"} | \\text{\"el gato\"})$ con los pesos dados.\n",
    "2. Proporciona una estrategia para optimizar estos pesos $\\lambda$ utilizando un conjunto de validación con el objetivo de maximizar la probabilidad total del corpus.\n",
    "3. Explica cómo el algoritmo EM podría ser utilizado para ajustar iterativamente estos pesos de interpolación.\n",
    "\n",
    "#### **Ejercicio 3: Implementación de descuentos en backoff de Katz**\n",
    "\n",
    "Dado el siguiente conjunto de datos:\n",
    "\n",
    "- **Trigramas**:\n",
    "  - $C(\\text{\"el gato duerme\"}) = 2$\n",
    "- **Bigramas**:\n",
    "  - $C(\\text{\"gato duerme\"}) = 5$\n",
    "- **Unigramas**:\n",
    "  - $C(\\text{\"duerme\"}) = 10$\n",
    "\n",
    "1. Define cómo calcular el descuento para el trigram \"el gato duerme\" usando el backoff de Katz.\n",
    "2. Determina la probabilidad descontada \\( P^*(\\text{\"duerme\"} | \\text{\"el gato\"}) \\) para este trigram.\n",
    "3. Analiza cómo el factor de descuento afecta la distribución de probabilidad y qué sucede con los n-gramas de menor orden cuando se aplica backoff de Katz.\n",
    "\n",
    "\n",
    "#### **Ejercicio 4: Ajuste de conteos con Good-Turing**\n",
    "\n",
    "Considera el siguiente conjunto de conteos para un corpus grande:\n",
    "\n",
    "- $C(\\text{\"gato duerme\"}) = 4$\n",
    "- $C(\\text{\"perro duerme\"}) = 2$\n",
    "- $C(\\text{\"ratón duerme\"}) = 0$\n",
    "- Número de bigramas que ocurren exactamente una vez $( N_1) = 3$\n",
    "- Número de bigramas que ocurren exactamente dos veces $( N_2 ) = 2$.\n",
    "\n",
    "1. Calcula la probabilidad ajustada usando **Good-Turing** para el bigrama \"gato duerme\".\n",
    "2. Explica cómo Good-Turing reasigna la masa de probabilidad a los eventos no observados y por qué esto es útil en el modelado de lenguajes.\n",
    "3. Determina la probabilidad para un bigrama no observado como \"ratón duerme\" y justifique cómo Good-Turing evita que se asigne una probabilidad de cero a estos eventos.\n",
    "\n",
    "#### **Ejercicio 5: Análisis comparativo de backoff Good-Turing**\n",
    "\n",
    "Suponga que está trabajando con un corpus de entrenamiento y un conjunto de desarrollo separado. El modelo utiliza el **algoritmo combinado de Backoff Good-Turing** para calcular las probabilidades de n-gramas.\n",
    "\n",
    "1. Describe el proceso completo para calcular la probabilidad de un n-grama utilizando el algoritmo combinado de Backoff Good-Turing.\n",
    "2. Compara el rendimiento de este modelo con un modelo de n-grama simple que solo utilice interpolación. ¿En qué situaciones el algoritmo combinado proporciona una ventaja significativa?\n",
    "3. Propone una estrategia para evaluar la precisión del modelo en un conjunto de prueba y cómo ajustar los parámetros (como el factor de descuento $D$ para mejorar el rendimiento general.\n",
    "\n",
    "#### **Ejercicio 6: Aplicación práctica en clasificación de textos**\n",
    "\n",
    "Suponga que está diseñando un sistema de clasificación de textos que utiliza n-gramas para representar documentos. Decide implementar un modelo que combine **backoff** e **interpolación**.\n",
    "\n",
    "1. Explica cómo utilizaría las técnicas de backoff y interpolación para calcular las probabilidades de n-gramas en este contexto.\n",
    "2. Describe un procedimiento para ajustar los pesos de interpolación y los factores de descuento de backoff en función de un conjunto de datos de entrenamiento.\n",
    "3. Analiza los posibles desafíos y soluciones cuando se trabaja con un vocabulario grande y datos desbalanceados en la clasificación de textos.\n",
    "\n",
    "\n",
    "#### **Ejercicio 7: Evaluación del impacto de backoff en la generalización del modelo**\n",
    "\n",
    "Se te proporciona un corpus con las siguientes características:\n",
    "\n",
    "- El corpus contiene 1 millón de palabras.\n",
    "- Hay un conjunto significativo de trigramas y bigramas raros (con conteos menores o iguales a 2).\n",
    "\n",
    "1. Explica cómo el uso del **backoff** puede ayudar a mejorar la generalización del modelo en presencia de estos n-gramas raros.\n",
    "2. Propone una métrica para evaluar el impacto del backoff en la generalización del modelo, particularmente en un conjunto de prueba que contiene contextos no vistos.\n",
    "3. Realiza un análisis teórico sobre cómo la reducción del contexto (retrocediendo a bigramas y unigramas) afecta la precisión del modelo en situaciones con datos escasos.\n",
    "\n",
    "#### **Ejercicio 8: Ajuste dinámico de pesos en interpolación condicionada**\n",
    "\n",
    "Imagina que estás desarrollando un modelo de lenguaje que necesita ajustar dinámicamente los pesos $\\lambda$ en la interpolación en función del contexto.\n",
    "\n",
    "1. Diseña un esquema para ajustar dinámicamente los pesos de interpolación $\\lambda$ basados en la precisión de los conteos en diferentes contextos (unigrama, bigrama, trigrama).\n",
    "2. Describe cómo utilizarías un conjunto de validación para aprender estos pesos condicionales y qué técnica utilizarías para evitar el sobreajuste.\n",
    "3. Analiza los pros y los contras de utilizar interpolación simple frente a interpolación condicionada en escenarios donde los contextos varían significativamente en términos de frecuencia.\n",
    "\n",
    "#### **Ejercicio 9: Análisis de la masa de probabilidad en backoff de Katz**\n",
    "\n",
    "Dado un modelo que utiliza el **backoff de Katz** con un factor de descuento $D = 0.75$:\n",
    "\n",
    "1. Explica cómo se redistribuye la masa de probabilidad cuando se encuentra un n-grama con conteo cero.\n",
    "2. Dado un contexto específico donde se retrocede a un bigrama y luego a un unigrama, analice cómo se distribuye la probabilidad entre los diferentes órdenes de n-gramas.\n",
    "3. Proporciona una estrategia para ajustar el factor de descuento $D$ en función del tamaño del corpus y la frecuencia de los n-gramas de mayor orden.\n",
    "\n",
    "#### **Ejercicio 10: Evaluación del rendimiento de Good-Turing en corpus multilingüe**\n",
    "\n",
    "Tienes un corpus multilingüe con textos en inglés, español y francés. Cada idioma tiene un conjunto distinto de unigramas y bigramas con diferentes frecuencias.\n",
    "\n",
    "1. Explica cómo el método de **Good-Turing** puede utilizarse para ajustar las probabilidades de n-gramas en este corpus multilingüe.\n",
    "2. Analiza cómo la variación en la distribución de frecuencias entre los diferentes idiomas puede afectar la estimación de Good-Turing y cómo manejarías esta variabilidad.\n",
    "3. Diseña un experimento para evaluar si Good-Turing mejora la precisión de un modelo de lenguaje en cada idioma individualmente y en un corpus combinado. ¿Qué métricas utilizarías para esta evaluación?\n",
    "\n",
    "#### **Ejercicio 11: Construcción de un modelo de lenguaje robustecido con Backoff Good-Turing**\n",
    "\n",
    "Estás desarrollando un modelo de lenguaje que necesita ser robusto frente a la presencia de múltiples formas de un mismo n-grama (variaciones sintácticas).\n",
    "\n",
    "1. Describe cómo implementarías un modelo de lenguaje que utilice el **algoritmo combinado de Backoff Good-Turing** para manejar estas variaciones de n-gramas.\n",
    "2. Explica cómo equilibrarías el uso de backoff y Good-Turing para garantizar que se asigne masa de probabilidad suficiente a las variaciones no observadas de los n-gramas.\n",
    "3. Analiza el rendimiento esperado del modelo en comparación con un modelo estándar de trigramas sin suavizado, particularmente en un corpus donde las variaciones son frecuentes.\n",
    "\n",
    "#### **Ejercicio 12: Interpolación y suavizado en modelos de lenguaje contextuales**\n",
    "\n",
    "Supón que estás trabajando con un modelo de lenguaje contextual que necesita combinar la información de n-gramas con características contextuales adicionales (como la posición en la oración).\n",
    "\n",
    "1. Diseña un enfoque que combine la **interpolación** de n-gramas con características contextuales adicionales, explicando cómo integrarías estas características en la función de interpolación.\n",
    "2. Proporciona un análisis sobre cómo las características contextuales pueden influir en los pesos de interpolación $\\lambda$ y cómo ajustarías estos pesos utilizando un conjunto de desarrollo.\n",
    "3. Discute los desafíos y beneficios de combinar interpolación de n-gramas con otras características contextuales en modelos de lenguaje modernos.\n",
    "\n",
    "\n",
    "#### **Ejercicio 13: Exploración de modelos Backoff en sistemas de reconocimiento de voz**\n",
    "\n",
    "Estás implementando un sistema de reconocimiento de voz que utiliza un modelo de lenguaje basado en trigramas para mejorar la precisión del reconocimiento.\n",
    "\n",
    "1. Explica cómo implementarías el **backoff de Katz** para gestionar la alta variabilidad en las secuencias de palabras reconocidas.\n",
    "2. Propone un método para ajustar el factor de backoff $\\alpha$ en función de la calidad del reconocimiento de voz en tiempo real.\n",
    "3. Analiza cómo la precisión del modelo de lenguaje backoff puede afectar la tasa de error de reconocimiento y cómo podrías mejorarla mediante un ajuste fino de los factores de descuento y backoff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3450f-4aee-4c35-9509-8782821af11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
