{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b12548d-f135-41a9-bce0-01cfbceb6288",
   "metadata": {},
   "source": [
    "### Kneser-Ney Smoothing\n",
    "\n",
    "El **suavizado Kneser-Ney (Kneser-Ney Smoothing)** es uno de los métodos de suavizado más avanzados y efectivos utilizados en los **modelos de lenguaje n-grama**. Fue desarrollado para mejorar la capacidad de los modelos de lenguaje para manejar n-gramas raros o no observados y asignar probabilidades más precisas a las palabras en contextos donde aparecen con baja frecuencia.\n",
    "\n",
    "El Kneser-Ney se basa en el principio de **descuento absoluto**, pero introduce un concepto fundamental que lo diferencia de otros métodos de suavizado: **la probabilidad de continuación**. Este enfoque hace que Kneser-Ney sea especialmente efectivo para asignar probabilidades a n-gramas raros en función de cómo se distribuyen en contextos diferentes.\n",
    "\n",
    "#### Motivación detrás de Kneser-Ney\n",
    "\n",
    "Los modelos de lenguaje basados en n-gramas clásicos, como el suavizado de Laplace o Good-Turing, tienden a calcular la probabilidad de una secuencia de palabras basándose en la frecuencia de ocurrencia de esa secuencia en el conjunto de entrenamiento. Sin embargo, este enfoque puede ser problemático en el caso de n-gramas que aparecen en contextos muy específicos, ya que estos modelos no tienen en cuenta **cuántos contextos únicos preceden a una palabra**.\n",
    "\n",
    "#### Problema con los métodos tradicionales\n",
    "\n",
    "Supongamos que queremos predecir la probabilidad de la palabra **\"York\"** en los contextos \"New York\" y \"York University\". Un modelo basado solo en la frecuencia de aparición puede sobrevalorar la probabilidad de **\"York\"** en cualquier contexto si aparece muchas veces en \"New York\", sin tener en cuenta que \"York\" aparece en muy pocos contextos diferentes (predominantemente en \"New York\").\n",
    "\n",
    "Esto es exactamente lo que Kneser-Ney corrige, asignando una mayor probabilidad a las palabras que aparecen en una variedad de contextos diferentes, y una menor probabilidad a las palabras que aparecen en un solo contexto repetidamente.\n",
    "\n",
    "\n",
    "#### Concepto clave de Kneser-Ney: probabilidad de continuación\n",
    "\n",
    "El **suavizado Kneser-Ney** se basa en la idea de que no solo importa la frecuencia con la que una palabra aparece en un n-grama, sino también **cuántos contextos diferentes preceden a esa palabra**.\n",
    "\n",
    "#### Ejemplo de probabilidad de continuación\n",
    "\n",
    "Imagina que estás modelando el siguiente n-grama:\n",
    "\n",
    "- \"New York\" aparece muy frecuentemente en el corpus.\n",
    "- \"York University\" aparece raramente.\n",
    "\n",
    "Un modelo tradicional podría asignar una probabilidad muy alta a \"York\" simplemente porque ocurre muchas veces. Sin embargo, lo que Kneser-Ney destaca es que **\"York\" solo aparece en muy pocos contextos (principalmente \"New York\")**, lo que sugiere que debería recibir una probabilidad menor en otros contextos desconocidos.\n",
    "\n",
    "La probabilidad de una palabra en Kneser-Ney, especialmente en el nivel de unigramas, se basa en **cuántos contextos únicos** preceden a esa palabra, no solo en su frecuencia total.\n",
    "\n",
    "#### Fórmula de la probabilidad de continuación\n",
    "\n",
    "La **probabilidad de continuación** en Kneser-Ney se calcula para un unigram de la siguiente manera:\n",
    "\n",
    "$$\n",
    "P_{\\text{cont}}(w_n) = \\frac{|\\{ w_{n-1} : C(w_{n-1}, w_n) > 0 \\}|}{|\\{w_{n-1} : C(w_{n-1}) > 0\\}|}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- **$C(w_{n-1}, w_n)$** es el número de veces que el bigrama $w_{n-1} w_n$ aparece en el corpus.\n",
    "- **$|\\{ w_{n-1} : C(w_{n-1}, w_n) > 0 \\}|$** es el número de contextos únicos precedentes para la palabra $w_n$.\n",
    "\n",
    "El enfoque clave es que una palabra como \"York\", que aparece en pocos contextos únicos, tendrá una probabilidad de continuación más baja, mientras que una palabra que aparece en muchos contextos diferentes (como \"the\") tendrá una probabilidad de continuación más alta.\n",
    "\n",
    "\n",
    "#### Descuento absoluto en Kneser-Ney\n",
    "\n",
    "El suavizado Kneser-Ney también se basa en el **descuento absoluto**. Esto significa que, en lugar de utilizar directamente la frecuencia de aparición de un n-grama, se aplica un **descuento** a las probabilidades de los n-gramas observados y se redistribuye esa masa de probabilidad a los n-gramas no observados.\n",
    "\n",
    "#### Fórmula del descuento absoluto\n",
    "\n",
    "La probabilidad de un n-grama en Kneser-Ney se calcula aplicando un descuento a los n-gramas observados:\n",
    "\n",
    "$$\n",
    "P^*(w_n | w_{n-1}) = \\frac{C(w_{n-1}, w_n) - D}{C(w_{n-1})}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- **$C(w_{n-1}, w_n)$** es el conteo del bigrama.\n",
    "- **$D$** es el descuento, una constante que ajusta la masa de probabilidad asignada a los n-gramas observados.\n",
    "- **$C(w_{n-1})$** es el conteo total del unigram $w_{n-1}$.\n",
    "\n",
    "Este descuento asegura que no toda la probabilidad se asigne a los n-gramas observados, dejando algo de masa de probabilidad para los n-gramas no observados, que se manejan mediante la probabilidad de continuación.\n",
    "\n",
    "\n",
    "#### Kneser-Ney para trigramas\n",
    "\n",
    "El suavizado Kneser-Ney se extiende fácilmente a **n-gramas de mayor orden**, como trigramas, bigramas, etc. El objetivo sigue siendo el mismo: aplicar un descuento a los n-gramas observados y redistribuir la probabilidad a los n-gramas de menor orden, donde la probabilidad de continuación juega un papel clave.\n",
    "\n",
    "#### Fórmula de Kneser-Ney para trigramas\n",
    "\n",
    "Para un **trigrama**, la fórmula de Kneser-Ney se define de la siguiente manera:\n",
    "\n",
    "$$\n",
    "P_{\\text{KN}}(w_n | w_{n-2}, w_{n-1}) = \n",
    "\\begin{cases}\n",
    "\\frac{C(w_{n-2}, w_{n-1}, w_n) - D}{C(w_{n-2}, w_{n-1})} + \\lambda(w_{n-2}, w_{n-1}) P_{\\text{KN}}(w_n | w_{n-1}), & \\text{si } C(w_{n-2}, w_{n-1}) > 0 \\\\\n",
    "\\lambda(w_{n-2}, w_{n-1}) P_{\\text{KN}}(w_n | w_{n-1}), & \\text{si } C(w_{n-2}, w_{n-1}) = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- **$C(w_{n-2}, w_{n-1}, w_n)$** es el conteo del trigram.\n",
    "- **$D$** es el descuento.\n",
    "- **$P_{\\text{KN}}(w_n | w_{n-1})$** es la probabilidad del bigrama suavizada con Kneser-Ney.\n",
    "- **$\\lambda(w_{n-2}, w_{n-1})$** es el factor de normalización que asegura que la suma de probabilidades sea 1.\n",
    "\n",
    "El proceso de suavizado continúa de esta manera hasta que se llega a la probabilidad de continuación en el nivel del unigram.\n",
    "\n",
    "---\n",
    "\n",
    "#### Factores de normalización ($\\lambda$)\n",
    "\n",
    "En Kneser-Ney, el factor **$\\lambda(w_{n-1})$** es crucial para garantizar que la suma de todas las probabilidades en el modelo sea igual a 1. Este factor ajusta la cantidad de probabilidad que se redistribuye a los n-gramas de menor orden.\n",
    "\n",
    "Se calcula de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\lambda(w_{n-1}) = \\frac{D \\cdot |\\{ w_n : C(w_{n-1}, w_n) > 0 \\}|}{C(w_{n-1})}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- **$|\\{ w_n : C(w_{n-1}, w_n) > 0 \\}|$** es el número de n-gramas de mayor orden observados para el contexto $w_{n-1}$.\n",
    "- **$D$** es el descuento aplicado a los n-gramas observados.\n",
    "\n",
    "Este factor de normalización asegura que la probabilidad asignada a los n-gramas de menor orden sea proporcional a la cantidad de n-gramas observados que comparten ese contexto.\n",
    "\n",
    "\n",
    "#### Ventajas del Kneser-Ney smoothing\n",
    "\n",
    "1. **Mejor manejo de n-gramas raros**: Kneser-Ney distribuye la probabilidad de manera más uniforme entre los n-gramas raros, lo que ayuda a evitar asignar una probabilidad demasiado alta a palabras que solo aparecen en contextos muy específicos.\n",
    "\n",
    "2. **Generalización efectiva**: Al considerar el número de contextos únicos en los que aparece una palabra, Kneser-Ney permite que el modelo generalice mejor en textos no observados, lo que lo hace más efectivo en situaciones donde hay datos limitados.\n",
    "\n",
    "3. **Corrección de sesgos de frecuencia**: Los métodos tradicionales pueden asignar una probabilidad demasiado alta a palabras comunes en un solo contexto. Kneser-Ney corrige esto asignando probabilidades más bajas a palabras que solo aparecen en unos pocos contextos repetidamente.\n",
    "\n",
    "4. **Extensión natural a trigramas y n-gramas de mayor orden**: Kneser-Ney se adapta fácilmente a modelos de lenguaje de mayor orden (trigramas, 4-gramas, etc.), aplicando el mismo enfoque de probabilidad de continuación y descuento en todos los niveles.\n",
    "\n",
    "---\n",
    "\n",
    "#### Desventajas del Kneser-Ney smoothing\n",
    "\n",
    "1. **Complejidad computacional**: Aunque es muy efectivo, Kneser-Ney es más complejo de implementar y computacionalmente más costoso que métodos más simples como el suavizado de Laplace o Good-Turing. El cálculo de la probabilidad de continuación y los factores de normalización añaden una sobrecarga considerable.\n",
    "\n",
    "2. **Requiere un buen ajuste del descuento ($D$)**: La elección del descuento adecuado es crucial para el rendimiento de Kneser-Ney. Si el descuento es demasiado grande o demasiado pequeño, el modelo puede asignar probabilidades inexactas a los n-gramas no observados.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparación con otros métodos de suavizado\n",
    "\n",
    "| Método                  | Descripción                                               | Ventajas                        | Desventajas                       |\n",
    "|-------------------------|-----------------------------------------------------------|---------------------------------|-----------------------------------|\n",
    "| **Kneser-Ney Smoothing** | Ajusta las probabilidades en función de los contextos únicos precedentes | Excelente manejo de n-gramas raros | Complejidad computacional         |\n",
    "| **Good-Turing Smoothing**| Ajusta los conteos observados para redistribuir probabilidad | Simple y efectivo para eventos raros | Menos preciso que Kneser-Ney      |\n",
    "| **Suavizado de Laplace** | Añade una constante a todos los conteos para evitar probabilidades de cero | Simple de implementar            | No maneja bien la frecuencia de contextos |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "El **Kneser-Ney Smoothing** es una de las técnicas más avanzadas y efectivas para suavizar las probabilidades en modelos de lenguaje n-grama. Su concepto clave de **probabilidad de continuación** permite que el modelo asigne probabilidades más precisas basadas no solo en la frecuencia de aparición de los n-gramas, sino también en la cantidad de contextos únicos en los que aparecen. Esta capacidad para manejar n-gramas raros y no observados lo convierte en una opción ideal para modelos de lenguaje donde se requiere un alto nivel de precisión y generalización.\n",
    "\n",
    "Aunque es más complejo y computacionalmente costoso que otros métodos, el suavizado Kneser-Ney sigue siendo uno de los métodos más utilizados en aplicaciones que requieren modelos de lenguaje n-grama robustos y efectivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d809ef0-2735-4553-a68a-8de53d222838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Preprocesamiento del Corpus\n",
    "corpus = [\n",
    "    \"el gato está en la casa\",\n",
    "    \"el perro está en el jardín\",\n",
    "    \"la casa es grande\",\n",
    "    \"el gato y el perro son amigos\",\n",
    "    \"el jardín tiene flores\",\n",
    "    \"la casa tiene un jardín\",\n",
    "    \"el perro juega en el jardín\",\n",
    "    \"el gato duerme en la casa\",\n",
    "    \"la casa y el jardín son hermosos\"\n",
    "]\n",
    "\n",
    "def tokenize_corpus(corpus: List[str]) -> List[List[str]]:\n",
    "    tokenized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        tokens = sentence.lower().split()\n",
    "        tokens = ['<s>'] + tokens + ['</s>']  # Añadimos tokens de inicio y fin de oración\n",
    "        tokenized_corpus.append(tokens)\n",
    "    return tokenized_corpus\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "\n",
    "# Conteo de N-gramas\n",
    "def count_ngrams(tokenized_corpus: List[List[str]], n: int) -> Dict[Tuple[str, ...], int]:\n",
    "    ngram_counts = collections.Counter()\n",
    "    for tokens in tokenized_corpus:\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i + n])\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "unigram_counts = count_ngrams(tokenized_corpus, 1)\n",
    "bigram_counts = count_ngrams(tokenized_corpus, 2)\n",
    "trigram_counts = count_ngrams(tokenized_corpus, 3)\n",
    "\n",
    "#Cálculo de conteos de contextos unicos (para probabilidad de continuación)\n",
    "def compute_unique_context_counts(bigram_counts: Dict[Tuple[str, str], int]) -> Dict[str, int]:\n",
    "    continuation_counts = collections.Counter()\n",
    "    for (w_prev, w_next) in bigram_counts:\n",
    "        continuation_counts[w_next] += 1\n",
    "    return continuation_counts\n",
    "\n",
    "continuation_counts = compute_unique_context_counts(bigram_counts)\n",
    "total_continuations = len(bigram_counts)\n",
    "\n",
    "D = 0.75  # Descuento absoluto comúnmente utilizado\n",
    "\n",
    "#Cálculo de la probabilidad de continuación\n",
    "def continuation_probability(word: str, continuation_counts: Dict[str, int], total_continuations: int) -> float:\n",
    "    return continuation_counts[word] / total_continuations\n",
    "\n",
    "#Cálculo del factor de normalización\n",
    "def compute_lambda(context: Tuple[str, ...], ngram_counts: Dict[Tuple[str, ...], int], lower_order_counts: Dict[Tuple[str, ...], int]) -> float:\n",
    "    context_count = sum([count for ngram, count in ngram_counts.items() if ngram[:-1] == context])\n",
    "    unique_continuations = len([1 for ngram in ngram_counts if ngram[:-1] == context])\n",
    "    return (D * unique_continuations) / context_count if context_count > 0 else 0\n",
    "\n",
    "#Cálculo de las probabilidades suavizadas con Kneser-Ney\n",
    "def kneser_ney_probability(trigram: Tuple[str, str, str],\n",
    "                           trigram_counts: Dict[Tuple[str, str, str], int],\n",
    "                           bigram_counts: Dict[Tuple[str, str], int],\n",
    "                           unigram_counts: Dict[Tuple[str], int],\n",
    "                           continuation_counts: Dict[str, int],\n",
    "                           total_continuations: int) -> float:\n",
    "    w1, w2, w3 = trigram\n",
    "    trigram_count = trigram_counts.get((w1, w2, w3), 0)\n",
    "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
    "\n",
    "    if bigram_count > 0:\n",
    "        lambda_w1_w2 = compute_lambda((w1, w2), trigram_counts, bigram_counts)\n",
    "        p_continuation = kneser_ney_probability_bigram((w2, w3),\n",
    "                                                       bigram_counts,\n",
    "                                                       unigram_counts,\n",
    "                                                       continuation_counts,\n",
    "                                                       total_continuations)\n",
    "        probability = max(trigram_count - D, 0) / bigram_count + lambda_w1_w2 * p_continuation\n",
    "    else:\n",
    "        probability = kneser_ney_probability_bigram((w2, w3),\n",
    "                                                   bigram_counts,\n",
    "                                                   unigram_counts,\n",
    "                                                   continuation_counts,\n",
    "                                                   total_continuations)\n",
    "    return probability\n",
    "\n",
    "def kneser_ney_probability_bigram(bigram: Tuple[str, str],\n",
    "                                  bigram_counts: Dict[Tuple[str, str], int],\n",
    "                                  unigram_counts: Dict[Tuple[str], int],\n",
    "                                  continuation_counts: Dict[str, int],\n",
    "                                  total_continuations: int) -> float:\n",
    "    w1, w2 = bigram\n",
    "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
    "    unigram_count = unigram_counts.get((w1,), 0)\n",
    "\n",
    "    if unigram_count > 0:\n",
    "        lambda_w1 = compute_lambda((w1,), bigram_counts, unigram_counts)\n",
    "        p_continuation = continuation_probability(w2, continuation_counts, total_continuations)\n",
    "        probability = max(bigram_count - D, 0) / unigram_count + lambda_w1 * p_continuation\n",
    "    else:\n",
    "        probability = continuation_probability(w2, continuation_counts, total_continuations)\n",
    "    return probability\n",
    "# Uso del modelo para calcular la probabilidad de una oración\n",
    "def sentence_probability(sentence: str,\n",
    "                         trigram_counts: Dict[Tuple[str, str, str], int],\n",
    "                         bigram_counts: Dict[Tuple[str, str], int],\n",
    "                         unigram_counts: Dict[Tuple[str], int],\n",
    "                         continuation_counts: Dict[str, int],\n",
    "                         total_continuations: int) -> float:\n",
    "    tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    probability_log_sum = 0.0\n",
    "    for i in range(len(tokens) - 2):\n",
    "        trigram = (tokens[i], tokens[i+1], tokens[i+2])\n",
    "        prob = kneser_ney_probability(trigram, trigram_counts, bigram_counts, unigram_counts, continuation_counts, total_continuations)\n",
    "        probability_log_sum += math.log(prob) if prob > 0 else float('-inf')\n",
    "    return math.exp(probability_log_sum)\n",
    "\n",
    "# Ejemplo de uso\n",
    "\n",
    "test_sentence = \"el gato juega en el jardín\"\n",
    "prob = sentence_probability(test_sentence, trigram_counts, bigram_counts, unigram_counts, continuation_counts, total_continuations)\n",
    "print(f\"La probabilidad de la oración '{test_sentence}' es: {prob}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a0d00-2277-4c48-8707-47294fdaa3d1",
   "metadata": {},
   "source": [
    "¿Puedes explicar estos resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff92f2d-4f07-4082-b55e-6122a9ddaf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e0287-2983-4346-b1df-d2d9f31365c9",
   "metadata": {},
   "source": [
    "### Suavizado Good-Turing (Good-Turing Smoothing)\n",
    "\n",
    "El **Suavizado Good-Turing** es una técnica de ajuste de probabilidades en modelos de lenguaje n-grama, diseñada para mejorar la estimación de probabilidades para eventos raros o no observados en un corpus de entrenamiento. Fue desarrollado por **Alan Turing** y popularizado por su colega **I.J. Good**, de donde proviene su nombre. El suavizado Good-Turing es utilizado principalmente cuando se enfrenta a eventos raros (es decir, n-gramas que ocurren muy pocas veces) o a eventos no observados (n-gramas con un conteo de cero), asegurando que todos los posibles n-gramas tengan una probabilidad no nula.\n",
    "\n",
    "El objetivo principal de **Good-Turing** es corregir las probabilidades estimadas asignadas a los eventos observados en el corpus, particularmente a los que ocurren pocas veces, redistribuyendo parte de la probabilidad de los eventos observados a los eventos no observados.\n",
    "\n",
    "#### Problema: N-gramas raros y no observados\n",
    "\n",
    "En los modelos de lenguaje basados en **n-gramas**, un problema común es la **escasez de datos**, lo que significa que algunas secuencias de palabras (n-gramas) ocurren muy pocas veces o no ocurren en absoluto en el corpus de entrenamiento. Sin embargo, estas secuencias podrían ocurrir en nuevas muestras de texto. Sin un ajuste adecuado, los **eventos no observados** obtendrían una probabilidad de **cero**, lo que puede ser problemático para la generalización del modelo.\n",
    "\n",
    "El suavizado **Good-Turing** aborda este problema ajustando las probabilidades de los **eventos observados** y asignando una pequeña parte de la masa de probabilidad a los **eventos no observados**.\n",
    "\n",
    "#### Intuición del suavizado Good-Turing\n",
    "\n",
    "La clave del suavizado Good-Turing es la idea de que:\n",
    "\n",
    "- **Eventos raros** (que se han observado muy pocas veces) probablemente han sido subestimados en términos de probabilidad. Por lo tanto, la probabilidad asignada a estos eventos debe ajustarse.\n",
    "- **Eventos no observados** podrían haber ocurrido si tuviéramos más datos, por lo que se debe reservar algo de probabilidad para ellos.\n",
    "\n",
    "En lugar de asignar una probabilidad basada solo en el conteo directo de un n-grama, Good-Turing ajusta los conteos observados para reflejar mejor la frecuencia esperada de eventos raros o no observados.\n",
    "\n",
    "#### Definición formal de Good-Turing\n",
    "\n",
    "En el suavizado Good-Turing, el objetivo es ajustar los **conteos observados** ($C(w_{n-N+1:n})$) de los n-gramas para asignar una probabilidad adecuada tanto a los **n-gramas raros** como a los **n-gramas no observados**. Para hacer esto, se define un nuevo conteo ajustado llamado **conteo ajustado Good-Turing** ($C^*$), el cual reemplaza al conteo observado.\n",
    "\n",
    "El ajuste de los conteos sigue la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "C^*(w_{n-N+1:n}) = (C(w_{n-N+1:n}) + 1) \\cdot \\frac{N(C+1)}{N(C)}\n",
    "$$\n",
    "\n",
    "#### Componentes de la fórmula:\n",
    "\n",
    "1. **$C(w_{n-N+1:n})$**: Es el **conteo observado** del n-grama. Es la cantidad de veces que el n-grama específico ha aparecido en el corpus de entrenamiento.\n",
    "\n",
    "2. **$N(C)$**: Es el número de n-gramas que tienen exactamente **$C$** conteos. Es decir, cuántos n-gramas se han observado exactamente **C veces** en el corpus.\n",
    "\n",
    "   - **$N(C)$** es crucial para determinar cuántos n-gramas tienen un conteo específico. Esto ayuda a entender cuán frecuente es una secuencia de palabras con ese número de ocurrencias en el corpus.\n",
    "\n",
    "3. **$C^*(w_{n-N+1:n})$**: Es el **conteo ajustado Good-Turing** para el n-grama $w_{n-N+1:n}$, el cual se utiliza en lugar del conteo observado para calcular la probabilidad ajustada.\n",
    "\n",
    "#### Interpretación de la fórmula\n",
    "\n",
    "- La fórmula **$C^*(w_{n-N+1:n}) = (C + 1) \\cdot \\frac{N(C+1)}{N(C)}$** significa que el nuevo conteo ajustado para un n-grama que ocurre **C** veces se calcula usando la relación entre los n-gramas que ocurren **C+1** veces y los que ocurren **C** veces.\n",
    "- La idea es que si muchos n-gramas tienen un conteo de **C**, y pocos n-gramas tienen un conteo de **C+1**, entonces el conteo observado de los n-gramas con frecuencia **C** probablemente está subestimado, y por lo tanto debe ser ajustado hacia arriba.\n",
    "\n",
    "#### Ejemplo de aplicación\n",
    "\n",
    "Imagina que estamos calculando la probabilidad de n-gramas en un corpus de entrenamiento y observamos los siguientes conteos:\n",
    "\n",
    "- Hay **10 n-gramas** que han aparecido exactamente **2 veces** en el corpus.\n",
    "- Hay **5 n-gramas** que han aparecido exactamente **3 veces** en el corpus.\n",
    "\n",
    "Queremos calcular el conteo ajustado para un n-grama que ha aparecido **2 veces**.\n",
    "\n",
    "El conteo ajustado se calcula como:\n",
    "\n",
    "$$\n",
    "C^*(w_{n-N+1:n}) = (2 + 1) \\cdot \\frac{5}{10} = 3 \\cdot 0.5 = 1.5\n",
    "$$\n",
    "\n",
    "En lugar de utilizar el conteo observado de **2**, el suavizado Good-Turing ajusta este valor a **1.5**, ya que, basándose en el patrón observado en el corpus, es probable que los n-gramas con conteo de **2** ocurran con una frecuencia ligeramente menor de lo que indica el conteo observado.\n",
    "\n",
    "#### Cálculo de la probabilidad con Good-Turing\n",
    "\n",
    "Una vez que se han ajustado los conteos utilizando **Good-Turing**, podemos calcular las probabilidades ajustadas. La probabilidad ajustada de un n-grama con suavizado Good-Turing es:\n",
    "\n",
    "$$\n",
    "P(w_{n-N+1:n}) = \\frac{C^*(w_{n-N+1:n})}{N}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- **$C^*(w_{n-N+1:n})$** es el conteo ajustado Good-Turing.\n",
    "- **$N$** es el número total de n-gramas en el corpus.\n",
    "\n",
    "Esta fórmula asegura que las probabilidades de los n-gramas observados se ajusten correctamente y que los **n-gramas no observados** reciban una probabilidad distinta de cero.\n",
    "\n",
    "#### Manejo de los N-gramas no observados\n",
    "\n",
    "El suavizado Good-Turing asigna una pequeña probabilidad a los n-gramas que no han sido observados en el corpus de entrenamiento. Para hacer esto, calcula la probabilidad de los eventos no observados de la siguiente manera:\n",
    "\n",
    "1. **Conteo de eventos no observados**: El número total de eventos no observados se define como **$N(0)$**, que es el número de n-gramas que no aparecen en el corpus.\n",
    "   \n",
    "2. **Probabilidad de eventos no observados**: La probabilidad de un evento no observado se calcula asignando una pequeña fracción de la masa de probabilidad a todos los eventos no observados. Para los n-gramas no observados, la probabilidad se define como:\n",
    "\n",
    "   $$ P(\\text{no observado}) = \\frac{N(1)}{N} $$\n",
    "\n",
    "   Donde **$N(1)$** es el número de n-gramas que han sido observados exactamente **1 vez** en el corpus.\n",
    "\n",
    "   La idea es que los n-gramas no observados se comporten de manera similar a los n-gramas que se han observado exactamente **1 vez**. Por lo tanto, se les asigna una probabilidad proporcional a los n-gramas observados una sola vez.\n",
    "\n",
    "#### Resumen del proceso de Good-Turing\n",
    "\n",
    "1. **Recolectar conteos de n-gramas**: Calcula los conteos observados de todos los n-gramas en el corpus.\n",
    "2. **Calcular $N(C)$**: Determina cuántos n-gramas tienen exactamente **C** conteos.\n",
    "3. **Ajustar conteos con Good-Turing**: Utiliza la fórmula $C^* = (C + 1) \\cdot \\frac{N(C+1)}{N(C)}$ para ajustar los conteos observados.\n",
    "4. **Calcular probabilidades**: Calcula la probabilidad ajustada para cada n-grama utilizando los conteos ajustados.\n",
    "5. **Asignar probabilidad a eventos no observados**: Distribuye una pequeña parte de la masa de probabilidad a los n-gramas no observados utilizando la fórmula $P(\\text{no observado}) = \\frac{N(1)}{N}$.\n",
    "\n",
    "#### Beneficios del suavizado Good-Turing\n",
    "\n",
    "- **Manejo efectivo de eventos raros**: Good-Turing ajusta las probabilidades de los n-gramas raros, asegurando que no se sobreestimen o subestimen.\n",
    "-\n",
    "\n",
    " **Asignación de probabilidad a eventos no observados**: Good-Turing asigna una pequeña probabilidad a eventos que no han sido observados en el corpus, evitando que se les asigne una probabilidad de cero.\n",
    "- **Generalización**: Al redistribuir la masa de probabilidad, Good-Turing mejora la capacidad del modelo para generalizar a nuevos textos que contienen secuencias de palabras no observadas en el corpus de entrenamiento.\n",
    "\n",
    "#### Limitaciones del suavizado Good-Turing\n",
    "\n",
    "- **Complejidad computacional**: Aunque Good-Turing es efectivo, calcular los valores de $N(C)$ para diferentes valores de $C$ puede ser computacionalmente costoso, especialmente en grandes corpus.\n",
    "- **Falta de adaptación a contextos específicos**: Good-Turing ajusta los conteos basándose en la frecuencia general de los n-gramas en el corpus, pero no tiene en cuenta la **distribución contextual** de las palabras, lo que significa que podría no ser tan efectivo en casos donde el contexto juega un papel crucial.\n",
    "\n",
    "#### Aplicaciones del suavizado Good-Turing\n",
    "\n",
    "El suavizado Good-Turing se utiliza en varias aplicaciones de procesamiento del lenguaje natural y en modelos de lenguaje n-grama, como:\n",
    "\n",
    "- **Modelado de lenguaje**: Good-Turing se utiliza para ajustar las probabilidades en modelos de lenguaje n-grama, especialmente cuando se entrenan con datos limitados.\n",
    "- **Sistemas de reconocimiento de voz**: Good-Turing ayuda a los modelos de lenguaje utilizados en sistemas de reconocimiento de voz a manejar palabras y frases raras o no observadas.\n",
    "- **Sistemas de búsqueda y motores de búsqueda**: Good-Turing se usa para mejorar las estimaciones de probabilidad en motores de búsqueda, donde algunas consultas de búsqueda pueden no haber sido observadas previamente.\n",
    "\n",
    "\n",
    "El **Suavizado Good-Turing** es una técnica eficaz para ajustar las probabilidades en modelos de lenguaje n-grama, particularmente cuando se enfrentan a **n-gramas raros** o **no observados**. Ajusta los conteos observados para reflejar mejor la frecuencia esperada de eventos raros y asigna una pequeña parte de la probabilidad a los eventos no observados. Aunque es más simple que algunos métodos modernos, sigue siendo una técnica poderosa en situaciones donde la escasez de datos es un problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17650a1c-ce1c-4a0f-b0a9-45738efae8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "# Preprocesamiento del corpus\n",
    "corpus = [\n",
    "    \"el gato está en la casa\",\n",
    "    \"el perro está en el jardín\",\n",
    "    \"la casa es grande\",\n",
    "    \"el gato y el perro son amigos\",\n",
    "    \"el jardín tiene flores\",\n",
    "    \"la casa tiene un jardín\",\n",
    "    \"el perro juega en el jardín\",\n",
    "    \"el gato duerme en la casa\",\n",
    "    \"la casa y el jardín son hermosos\"\n",
    "]\n",
    "\n",
    "def tokenize_corpus(corpus: List[str]) -> List[List[str]]:\n",
    "    tokenized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        tokens = sentence.lower().split()\n",
    "        tokens = ['<s>'] + tokens + ['</s>']  # Añadimos tokens de inicio y fin de oración\n",
    "        tokenized_corpus.append(tokens)\n",
    "    return tokenized_corpus\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "\n",
    "# Conteo de N-gramas\n",
    "def count_ngrams(tokenized_corpus: List[List[str]], n: int) -> Dict[Tuple[str, ...], int]:\n",
    "    ngram_counts = collections.Counter()\n",
    "    for tokens in tokenized_corpus:\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i + n])\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "unigram_counts = count_ngrams(tokenized_corpus, 1)\n",
    "bigram_counts = count_ngrams(tokenized_corpus, 2)\n",
    "trigram_counts = count_ngrams(tokenized_corpus, 3)\n",
    "\n",
    "#  Cálculo de N(C)\n",
    "def calculate_NC(ngram_counts: Dict[Tuple[str, ...], int]) -> Dict[int, int]:\n",
    "    count_of_counts = collections.Counter()\n",
    "    for count in ngram_counts.values():\n",
    "        count_of_counts[count] += 1\n",
    "    return count_of_counts\n",
    "\n",
    "def sort_NC(NC: Dict[int, int]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    counts = np.array(list(NC.keys()))\n",
    "    frequencies = np.array([NC[count] for count in counts])\n",
    "    sorted_indices = np.argsort(counts)\n",
    "    return counts[sorted_indices], frequencies[sorted_indices]\n",
    "\n",
    "# Calculamos N(C) para bigramas (puede aplicarse a unigramas y trigramas de manera similar)\n",
    "NC_bigram = calculate_NC(bigram_counts)\n",
    "counts_bigram, frequencies_bigram = sort_NC(NC_bigram)\n",
    "\n",
    "# Suavizado Good-Turing\n",
    "def good_turing_discounting(ngram_counts: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], float]:\n",
    "    # Calculamos N(C)\n",
    "    NC = calculate_NC(ngram_counts)\n",
    "    counts, frequencies = sort_NC(NC)\n",
    "    \n",
    "    # Ajuste de conteos\n",
    "    total_ngrams = sum(ngram_counts.values())\n",
    "    max_count = max(counts)\n",
    "    adjusted_counts = {}\n",
    "    \n",
    "    for ngram, count in ngram_counts.items():\n",
    "        if count < max_count:\n",
    "            Nc = NC[count]\n",
    "            Nc1 = NC.get(count + 1, 0)\n",
    "            if Nc > 0:\n",
    "                C_star = (count + 1) * (Nc1 / Nc)\n",
    "                adjusted_counts[ngram] = C_star\n",
    "            else:\n",
    "                adjusted_counts[ngram] = count\n",
    "        else:\n",
    "            adjusted_counts[ngram] = count  # Para conteos máximos, no ajustamos\n",
    "    return adjusted_counts\n",
    "\n",
    "adjusted_bigram_counts = good_turing_discounting(bigram_counts)\n",
    "\n",
    "#Cálculo de probabilidades ajustadas\n",
    "def calculate_probabilities(adjusted_counts: Dict[Tuple[str, ...], float], n_minus1_counts: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], float]:\n",
    "    probabilities = {}\n",
    "    for ngram, adjusted_count in adjusted_counts.items():\n",
    "        context = ngram[:-1]\n",
    "        context_count = n_minus1_counts.get(context, sum(n_minus1_counts.values()))\n",
    "        probability = adjusted_count / context_count if context_count > 0 else 0.0\n",
    "        probabilities[ngram] = probability\n",
    "    return probabilities\n",
    "\n",
    "# Calculamos las probabilidades ajustadas para bigramas\n",
    "unigram_total_count = sum(unigram_counts.values())\n",
    "bigram_probabilities = calculate_probabilities(adjusted_bigram_counts, unigram_counts)\n",
    "\n",
    "# Asignación de probabilidad a N-gramas no observados\n",
    "def probability_of_unseen(NC: Dict[int, int], total_ngrams: int) -> float:\n",
    "    N1 = NC.get(1, 0)\n",
    "    return N1 / total_ngrams if total_ngrams > 0 else 0.0\n",
    "\n",
    "# Probabilidad de n-gramas no observados para bigramas\n",
    "total_bigrams = sum(bigram_counts.values())\n",
    "P_unseen_bigram = probability_of_unseen(NC_bigram, total_bigrams)\n",
    "\n",
    "#Uso del modelo para calcular la probabilidad de una oración\n",
    "def sentence_probability(sentence: str, bigram_probabilities: Dict[Tuple[str, str], float], P_unseen: float) -> float:\n",
    "    tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    probability_log_sum = 0.0\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = (tokens[i], tokens[i+1])\n",
    "        prob = bigram_probabilities.get(bigram, P_unseen)\n",
    "        probability_log_sum += math.log(prob) if prob > 0 else float('-inf')\n",
    "    return math.exp(probability_log_sum)\n",
    "\n",
    "# Ejemplo de uso\n",
    "test_sentence = \"el gato juega en el jardín\"\n",
    "prob = sentence_probability(test_sentence, bigram_probabilities, P_unseen_bigram)\n",
    "print(f\"La probabilidad de la oración '{test_sentence}' es: {prob}\")\n",
    "\n",
    "#Ajuste suavizado para conteos cero\n",
    "def adjusted_zero_count_probability(NC: Dict[int, int], total_ngrams: int) -> float:\n",
    "    N1 = NC.get(1, 0)\n",
    "    P_zero = N1 / total_ngrams if total_ngrams > 0 else 0.0\n",
    "    return P_zero\n",
    "\n",
    "# Ajustamos la probabilidad para n-gramas con conteo cero\n",
    "P_zero_bigram = adjusted_zero_count_probability(NC_bigram, total_bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa84e4-9a24-4471-8e0c-5a2c6f01531c",
   "metadata": {},
   "source": [
    "¿Puedes explicar estos resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc623c8-f9b4-4f58-9e93-8e5161282adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1231b-919b-4cc1-ac6e-635c1267fb79",
   "metadata": {},
   "source": [
    "### Label Smoothing\n",
    "\n",
    "**Label Smoothing** es una técnica de regularización aplicada principalmente en la fase de entrenamiento de **redes neuronales**, incluidos los **Modelos de Lenguaje Neuronal (NLMs)** y los **Modelos de Lenguaje Grandes (LLMs)**. Su objetivo principal es mejorar la generalización del modelo y prevenir el **overfitting** al suavizar las distribuciones de probabilidad de las etiquetas de salida. A continuación, entraremos en detalle sobre qué es **Label Smoothing**, por qué es importante en el entrenamiento de modelos de lenguaje, cómo se implementa y cómo se relaciona con los **LLMs**.\n",
    "\n",
    "#### ¿Qué es Label Smoothing?\n",
    "\n",
    "En el entrenamiento de un modelo de clasificación o un modelo de lenguaje basado en redes neuronales, las etiquetas de las clases objetivo a menudo se representan mediante una **distribución one-hot**. En una distribución **one-hot**, la clase correcta tiene una probabilidad de 1, mientras que todas las demás clases tienen una probabilidad de 0. Esta representación es simple y directa, pero tiene ciertas limitaciones que pueden llevar al modelo a **sobreajustarse** (overfitting) y convertirse en excesivamente confiado en sus predicciones.\n",
    "\n",
    "**Label Smoothing** introduce una pequeña cantidad de incertidumbre en esta representación, reemplazando la distribución one-hot por una distribución suavizada, donde la clase correcta no tiene una probabilidad de 1, sino una probabilidad ligeramente menor, y las otras clases tienen pequeñas probabilidades no nulas. Esto reduce la confianza extrema del modelo en sus predicciones y hace que el entrenamiento sea más estable y generalice mejor a los datos de prueba.\n",
    "\n",
    "\n",
    "#### ¿Por qué es necesario el Label Smoothing?\n",
    "\n",
    "En los modelos de lenguaje y modelos de clasificación tradicionales, usar una distribución one-hot puede llevar a varios problemas:\n",
    "\n",
    "1. **Confianza excesiva del Modelo**: Al entrenar un modelo con una distribución one-hot, el modelo puede volverse extremadamente confiado en sus predicciones, asignando una probabilidad cercana a 1 para la clase predicha. Esto puede ser perjudicial cuando se presentan datos fuera del conjunto de entrenamiento, ya que el modelo no tiene en cuenta la incertidumbre inherente en los datos.\n",
    "\n",
    "2. **Overfitting**: La confianza excesiva en las predicciones de entrenamiento puede llevar a que el modelo **se sobreajuste** a los datos de entrenamiento, es decir, el modelo aprende las particularidades de los datos de entrenamiento en lugar de generalizar bien a nuevos datos.\n",
    "\n",
    "3. **Errores en las etiquetas**: En algunos conjuntos de datos, las etiquetas pueden no ser completamente correctas debido a errores de etiquetado. Un modelo entrenado con una distribución one-hot no es capaz de manejar esta incertidumbre y puede sobreajustarse a las etiquetas incorrectas.\n",
    "\n",
    "4. **Entropía cruzada y gradientes explosivos**: Las distribuciones one-hot tienen una entropía muy baja, lo que puede llevar a gradientes extremos y dificultades en el entrenamiento del modelo.\n",
    "\n",
    "El **Label Smoothing** se utiliza para mitigar estos problemas al suavizar la distribución de las etiquetas, introduciendo una pequeña probabilidad para las clases incorrectas y reduciendo la probabilidad de la clase correcta.\n",
    "\n",
    "\n",
    "\n",
    "#### ¿Cómo funciona el Label Smoothing?\n",
    "\n",
    "El **Label Smoothing** modifica la representación one-hot de las etiquetas objetivo para que la clase correcta no tenga una probabilidad de 1, sino una probabilidad ligeramente menor. El resto de la probabilidad se distribuye entre las otras clases, lo que reduce la confianza extrema del modelo.\n",
    "\n",
    "#### Fórmula de Label Smoothing\n",
    "\n",
    "Supongamos que tenemos un vocabulario de tamaño $V$ en un problema de clasificación o en un modelo de lenguaje. La representación original **one-hot** de la etiqueta correcta $w_n$ es:\n",
    "\n",
    "$$\n",
    "P_{\\text{one-hot}}(w_n) =\n",
    "\\begin{cases}\n",
    "1, & \\text{si } w_n \\text{ es la palabra correcta} \\\\\n",
    "0, & \\text{si } w_n \\text{ no es la palabra correcta}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Con **Label Smoothing**, suavizamos esta distribución mediante un pequeño valor $\\epsilon$ (por ejemplo, 0.1), que controla el grado de suavizado. La fórmula ajustada para la distribución suavizada es:\n",
    "\n",
    "$$\n",
    "P_{\\text{smoothed}}(w_n) = (1 - \\epsilon) \\cdot P_{\\text{one-hot}}(w_n) + \\frac{\\epsilon}{V}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- **$P_{\\text{one-hot}}(w_n)$** es la distribución one-hot original.\n",
    "- **$\\epsilon$** es el valor de suavizado que determina cuánto de la probabilidad se redistribuye entre las clases incorrectas.\n",
    "- **$V$** es el tamaño del vocabulario o el número total de clases.\n",
    "\n",
    "Esto implica que la probabilidad de la palabra correcta será **$(1 - \\epsilon)$**, y la probabilidad restante **$\\frac{\\epsilon}{V}$** se distribuye uniformemente entre todas las palabras del vocabulario.\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Supongamos que tenemos un modelo de lenguaje con un vocabulario de tamaño $V = 10000$ y un valor de suavizado $\\epsilon = 0.1$. En una distribución one-hot, la palabra correcta tiene una probabilidad de 1, y todas las demás tienen una probabilidad de 0. Con Label Smoothing, la probabilidad de la palabra correcta se reduce a:\n",
    "\n",
    "$$\n",
    "P_{\\text{correcto}} = 1 - \\epsilon = 0.9\n",
    "$$\n",
    "\n",
    "La probabilidad para cada palabra incorrecta se ajusta a:\n",
    "\n",
    "$$\n",
    "P_{\\text{incorrecto}} = \\frac{\\epsilon}{V} = \\frac{0.1}{10000} = 0.00001\n",
    "$$\n",
    "\n",
    "De este modo, el modelo no está completamente seguro de que la palabra correcta sea la única opción, lo que introduce un grado de incertidumbre y mejora la generalización.\n",
    "\n",
    "#### Beneficios del Label Smoothing\n",
    "\n",
    "**Label Smoothing** ofrece varias ventajas que mejoran tanto el entrenamiento como la generalización de los modelos:\n",
    "\n",
    "#### 1. **Mejora de la generalización**\n",
    "Al suavizar las etiquetas, el modelo evita el overfitting al no volverse excesivamente confiado en las predicciones de entrenamiento. Esta reducción de la confianza extrema permite que el modelo generalice mejor a los datos de prueba.\n",
    "\n",
    "#### 2. **Manejo de errores en las etiquetas**\n",
    "En conjuntos de datos grandes y ruidosos, es posible que haya errores de etiquetado. Por ejemplo, algunas palabras pueden estar mal etiquetadas en problemas de modelado de lenguaje. **Label Smoothing** ayuda a mitigar el impacto de estos errores, ya que introduce incertidumbre en la predicción, reduciendo la penalización del modelo por predecir una palabra incorrecta debido a un error de etiquetado.\n",
    "\n",
    "#### 3. **Prevención del overfitting**\n",
    "Al suavizar la distribución objetivo, el modelo no se ajusta tanto a las peculiaridades del conjunto de entrenamiento, lo que ayuda a reducir el riesgo de **sobreajuste**. El modelo no asigna una probabilidad del 100% a ninguna clase, lo que introduce robustez contra datos que podrían estar ligeramente fuera de la distribución del conjunto de entrenamiento.\n",
    "\n",
    "#### 4. **Entrenamiento estable**\n",
    "El **Label Smoothing** mejora la estabilidad del entrenamiento al reducir las fluctuaciones en las actualizaciones de los gradientes. Las distribuciones one-hot puras pueden dar lugar a gradientes explosivos, lo que dificulta el entrenamiento. Al suavizar las etiquetas, los gradientes se vuelven más suaves y el entrenamiento es más estable.\n",
    "\n",
    "#### 5. **Evita la confianza extrema en los LLMs**\n",
    "Los **Modelos de Lenguaje Grandes (LLMs)** como **GPT** y **BERT** suelen trabajar con vocabularios masivos y tareas complejas, donde la precisión de las predicciones y la capacidad de generalización son cruciales. **Label Smoothing** evita que los modelos se vuelvan extremadamente confiados en predicciones incorrectas, lo que es vital en escenarios de predicción secuencial como la generación de texto, donde una predicción incorrecta puede tener un efecto dominó sobre las siguientes predicciones.\n",
    "\n",
    "\n",
    "#### Relación entre Label Smoothing y los modelos de lenguaje grandes (LLMs)\n",
    "\n",
    "El **Label Smoothing** ha sido adoptado ampliamente en los **Modelos de Lenguaje Grandes (LLMs)** debido a su capacidad para mejorar el rendimiento de los modelos en una variedad de tareas. A continuación, explicamos cómo se integra esta técnica en los LLMs:\n",
    "\n",
    "#### 1. **Mejora de la generalización en LLMs**\n",
    "\n",
    "Los **LLMs** como **GPT** o **BERT** se entrenan en grandes corpus de texto y tienen que aprender representaciones de lenguaje que generalicen bien a contextos nuevos. El **Label Smoothing** ayuda a evitar que el modelo se sobreajuste a las particularidades del conjunto de entrenamiento, introduciendo una pequeña cantidad de incertidumbre en la predicción, lo que le permite adaptarse mejor a ejemplos no vistos.\n",
    "\n",
    "Por ejemplo, en el contexto de **predicción de la siguiente palabra** (tarea común en los LLMs), el modelo no debe volverse extremadamente confiado en predecir una palabra específica cuando puede haber múltiples palabras válidas en ese contexto. El **Label Smoothing** permite que el modelo mantenga cierta flexibilidad en sus predicciones.\n",
    "\n",
    "#### 2. **Manejo de vocabularios extensos**\n",
    "\n",
    "Los **LLMs** suelen manejar vocabularios masivos que pueden incluir decenas de miles o millones de palabras o\n",
    "\n",
    " tokens. En tales casos, la representación one-hot estándar de las etiquetas puede no ser adecuada debido a la sobreconfianza del modelo en las etiquetas correctas. El **Label Smoothing** distribuye una pequeña cantidad de probabilidad entre las palabras no etiquetadas, lo que es crucial para la **robustez** del modelo en entornos de predicción con vocabularios grandes.\n",
    "\n",
    "#### 3. **Mejor generalización para tareas de transferencia**\n",
    "\n",
    "Los **LLMs** a menudo se entrenan en tareas de pre-entrenamiento general (como enmascarar palabras en **BERT** o predecir la siguiente palabra en **GPT**), y luego se **ajustan finamente (fine-tuned)** en tareas específicas. El **Label Smoothing** permite que los LLMs generalicen mejor durante este ajuste fino, lo que mejora el rendimiento en tareas específicas como la clasificación de texto, la traducción automática o el análisis de sentimientos.\n",
    "\n",
    "#### 4. **Mejor rendimiento en tareas multimodales**\n",
    "\n",
    "Los LLMs que integran información de múltiples dominios (como texto e imágenes, en el caso de modelos multimodales) también pueden beneficiarse de **Label Smoothing**. Cuando los modelos tienen que integrar representaciones de diferentes modalidades, es probable que haya incertidumbre en las predicciones. El **Label Smoothing** permite que los modelos mantengan flexibilidad en este contexto, mejorando la calidad de las predicciones en tareas complejas.\n",
    "\n",
    "\n",
    "#### Implementación de Label Smoothing en LLMs\n",
    "\n",
    "En la práctica, implementar **Label Smoothing** en modelos como **Transformers** o **GPT** es sencillo y generalmente se realiza durante el cálculo de la **pérdida de entropía cruzada**. En lugar de comparar la predicción del modelo con una distribución one-hot estricta, se compara con una **distribución suavizada**. \n",
    "\n",
    "El **Label Smoothing** se integra directamente en la función de pérdida, modificando cómo se mide la discrepancia entre la predicción del modelo y la etiqueta real.\n",
    "\n",
    "\n",
    "El **Label Smoothing** es una técnica simple pero poderosa para mejorar la generalización y estabilidad de los **Modelos de Lenguaje Neuronal (NLMs)** y los **Modelos de Lenguaje Grandes (LLMs)**. Al suavizar la representación one-hot de las etiquetas, se introducen pequeñas probabilidades en las clases incorrectas, lo que reduce la confianza extrema del modelo y mejora su capacidad para manejar datos ruidosos y errores en las etiquetas.\n",
    "\n",
    "En los **LLMs**, el **Label Smoothing** es particularmente útil para manejar vocabularios extensos y tareas de predicción secuencial, donde la incertidumbre en las predicciones es inherente. Al reducir el overfitting y mejorar la estabilidad del entrenamiento, el **Label Smoothing** se ha convertido en una técnica estándar para entrenar modelos de lenguaje de última generación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ea1e7-6c56-41c5-b15f-e3c6e3310b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "#  Configuración de semillas aleatorias\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "#Preparación del Corpus\n",
    "corpus = [\n",
    "    \"el gato está en la casa\",\n",
    "    \"el perro está en el jardín\",\n",
    "    \"la casa es grande\",\n",
    "    \"el gato y el perro son amigos\",\n",
    "    \"el jardín tiene flores\",\n",
    "    \"la casa tiene un jardín\",\n",
    "    \"el perro juega en el jardín\",\n",
    "    \"el gato duerme en la casa\",\n",
    "    \"la casa y el jardín son hermosos\"\n",
    "]\n",
    "\n",
    "# Tokenización y construcción del vocabulario\n",
    "def tokenize_corpus(corpus: List[str]) -> List[List[str]]:\n",
    "    tokenized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        tokens = sentence.lower().split()\n",
    "        tokenized_corpus.append(tokens)\n",
    "    return tokenized_corpus\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "\n",
    "# Construcción del vocabulario\n",
    "def build_vocab(tokenized_corpus: List[List[str]]) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for tokens in tokenized_corpus:\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    inv_vocab = {idx: token for token, idx in vocab.items()}\n",
    "    return vocab, inv_vocab\n",
    "\n",
    "vocab, inv_vocab = build_vocab(tokenized_corpus)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Tamaño del vocabulario: {vocab_size}\")\n",
    "\n",
    "#  Creación del Dataset y DataLoader\n",
    "class LanguageModelDataset(Dataset):\n",
    "    def __init__(self, tokenized_corpus: List[List[str]], vocab: Dict[str, int], context_size: int = 2):\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "        self.context_size = context_size\n",
    "        for tokens in tokenized_corpus:\n",
    "            indexed_tokens = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "            for i in range(context_size, len(indexed_tokens)):\n",
    "                context = indexed_tokens[i - context_size:i]\n",
    "                target = indexed_tokens[i]\n",
    "                self.data.append((context, target))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "context_size = 2  # Usaremos bigramas como contexto\n",
    "dataset = LanguageModelDataset(tokenized_corpus, vocab, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Definición del modelo de lenguaje neuronal\n",
    "class NeuralLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, context_size: int):\n",
    "        super(NeuralLanguageModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view(inputs.size(0), -1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "embedding_dim = 50\n",
    "model = NeuralLanguageModel(vocab_size, embedding_dim, context_size)\n",
    "\n",
    "# Definición de la función de pérdida con Label Smoothing\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, label_smoothing: float, vocab_size: int, ignore_index: int = -100):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        assert 0.0 <= label_smoothing < 1.0\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # pred: [batch_size, vocab_size]\n",
    "        # target: [batch_size]\n",
    "        \n",
    "        confidence = 1.0 - self.label_smoothing\n",
    "        smoothing = self.label_smoothing / (self.vocab_size - 1)\n",
    "        \n",
    "        true_dist = torch.zeros_like(pred)\n",
    "        true_dist.fill_(smoothing)\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), confidence)\n",
    "        \n",
    "        # Aplicamos log_softmax a las predicciones\n",
    "        pred = nn.functional.log_softmax(pred, dim=1)\n",
    "        \n",
    "        # Calculamos la pérdida\n",
    "        loss = -torch.sum(pred * true_dist, dim=1)\n",
    "        \n",
    "        # Ignoramos los índices especificados\n",
    "        if self.ignore_index >= 0:\n",
    "            mask = (target != self.ignore_index).float()\n",
    "            loss = loss * mask\n",
    "            loss = loss.sum() / mask.sum()\n",
    "        else:\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        return loss\n",
    "# Entrenamiento del modelo con y sin Label Smoothing\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs: int = 5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for context, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(context)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Pérdida promedio: {avg_loss:.4f}\")\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for context, target in dataloader:\n",
    "            output = model(context)\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total_loss += correct\n",
    "    accuracy = total_loss / len(dataloader.dataset)\n",
    "    print(f\"Precisión del modelo: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Entrenamiento sin Label Smoothing\n",
    "# Creamos una instancia del modelo\n",
    "model_no_smoothing = NeuralLanguageModel(vocab_size, embedding_dim, context_size)\n",
    "\n",
    "# Definimos el criterio y el optimizador\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_no_smoothing.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Entrenamiento sin Label Smoothing\")\n",
    "train_model(model_no_smoothing, dataloader, criterion_ce, optimizer, num_epochs=10)\n",
    "evaluate_model(model_no_smoothing, dataloader)\n",
    "\n",
    "#Entrenamiento con Label Smoothing\n",
    "# Creamos una instancia del modelo\n",
    "model_with_smoothing = NeuralLanguageModel(vocab_size, embedding_dim, context_size)\n",
    "\n",
    "# Definimos el criterio con Label Smoothing y el optimizador\n",
    "label_smoothing = 0.1\n",
    "criterion_ls = LabelSmoothingLoss(label_smoothing=label_smoothing, vocab_size=vocab_size)\n",
    "optimizer = optim.Adam(model_with_smoothing.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nEntrenamiento con Label Smoothing\")\n",
    "train_model(model_with_smoothing, dataloader, criterion_ls, optimizer, num_epochs=10)\n",
    "evaluate_model(model_with_smoothing, dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60533e08-aa78-442a-98c4-57f7cd56be51",
   "metadata": {},
   "source": [
    "### Soft Attention\n",
    "\n",
    "La **Soft Attention** (atención suavizada) es uno de los componentes clave en los modelos de lenguaje modernos, como los **Transformers** y los **Modelos de Lenguaje Grandes (LLMs)**, incluidos **GPT**, **BERT** y otros. Introducido originalmente en el contexto de la traducción automática con redes neuronales recurrentes (RNN) por **Bahdanau et al.** en 2015, el mecanismo de atención ha evolucionado y se ha convertido en una herramienta crucial para capturar dependencias contextuales en secuencias de datos.\n",
    "\n",
    "#### Contexto general del mecanismo de atención\n",
    "\n",
    "El concepto de **atención** en redes neuronales tiene su origen en la idea de que, al procesar secuencias largas (por ejemplo, una oración), no todas las palabras son igualmente importantes para generar la siguiente palabra o el siguiente paso en la secuencia. El mecanismo de atención permite que un modelo preste **atención diferencial** a diferentes partes de la entrada, seleccionando de manera más eficiente la información relevante para cada paso del procesamiento.\n",
    "\n",
    "El **Soft Attention** es una versión suavizada del mecanismo de atención en la que el modelo asigna una **distribución de probabilidades** (a través de una función **softmax**) sobre todas las entradas. Así, en lugar de enfocarse en una única palabra o token, como ocurre con la atención dura (**hard attention**), el modelo distribuye su atención sobre varias palabras, asignando pesos diferentes a cada una en función de su relevancia para la tarea en curso.\n",
    "\n",
    "### ¿Cómo funciona el Soft Attention?\n",
    "\n",
    "El **Soft Attention** asigna **pesos** a diferentes palabras en una secuencia de entrada basándose en su relevancia en un contexto particular. Estos pesos se utilizan para calcular una combinación ponderada de los vectores de representación de las palabras, lo que permite al modelo tomar decisiones considerando una parte más amplia del contexto.\n",
    "\n",
    "#### Componentes clave de Soft Attention\n",
    "\n",
    "El mecanismo de **Soft Attention** tiene tres componentes principales:\n",
    "\n",
    "1. **Query (Consulta)**: Es una representación del token que está siendo procesado y que determina qué partes de la secuencia anterior son relevantes. En otras palabras, la query busca identificar la información clave en los otros tokens.\n",
    "   \n",
    "2. **Key (Clave)**: Es una representación del contexto global o de los tokens que están siendo considerados como relevantes en el proceso de atención. Las **keys** permiten determinar la relación de similitud entre el token actual (query) y los tokens anteriores en la secuencia.\n",
    "\n",
    "3. **Value (Valor)**: Son los vectores de las palabras o tokens en la secuencia que se consideran después de calcular los pesos de atención. Los **values** se combinan de acuerdo con la atención asignada para producir una representación ponderada del contexto.\n",
    "\n",
    "#### Cálculo de la atención suavizada\n",
    "\n",
    "La atención suavizada (soft attention) se calcula como una ponderación de los valores asociados a las claves, donde la ponderación está determinada por una función **softmax** aplicada sobre la similitud entre las consultas y las claves. Esta operación asegura que el modelo pueda prestar atención a diferentes partes de la secuencia con diferentes grados de importancia.\n",
    "\n",
    "La ecuación de **Soft Attention** en los Transformers se expresa como:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- **$Q$** es la matriz de **queries** (consultas).\n",
    "- **$K$** es la matriz de **keys** (claves).\n",
    "- **$V$** es la matriz de **values** (valores).\n",
    "- **$d_k$** es la dimensionalidad de las claves y consultas.\n",
    "- **$\\text{softmax}$** asegura que las ponderaciones de atención sumen 1, proporcionando una distribución de probabilidad sobre los tokens.\n",
    "\n",
    "Este proceso calcula una combinación ponderada de los valores $V$ basándose en las similitudes entre los queries y las keys, lo que permite que el modelo determine qué partes del contexto son más relevantes para la tarea de predicción.\n",
    "\n",
    "#### Funcionamiento por pasos del Soft Attention\n",
    "\n",
    "1. **Cálculo de la similitud (puntaje de atención)**: Primero, se calcula la similitud entre las **queries** y las **keys** para determinar la relevancia de cada palabra o token en la secuencia. Esta similitud se calcula como el producto escalar entre las matrices de consultas ($Q$) y las claves ($K$):\n",
    "\n",
    "   $$\n",
    "   \\text{Similitud} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "   Esta operación da lugar a una matriz de puntuaciones que mide la relación de cada consulta con las claves.\n",
    "\n",
    "2. **Normalización con softmax**: Las puntuaciones de similitud se normalizan utilizando la función **softmax**. Esto convierte los valores en una distribución de probabilidades, asegurando que la suma de los pesos de atención sea 1:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{ij} = \\frac{\\exp(\\text{Similitud}_{ij})}{\\sum_{k} \\exp(\\text{Similitud}_{ik})}\n",
    "   $$\n",
    "\n",
    "   Aquí, $\\alpha_{ij}$ representa la cantidad de atención que se asigna a la palabra $j$ en relación con la palabra $i$.\n",
    "\n",
    "3. **Combinación ponderada**: Una vez que se han calculado los pesos de atención, se utiliza esta distribución de atención para ponderar los **values** ($V$), lo que da como resultado una representación contextualizada para cada palabra en la secuencia:\n",
    "\n",
    "   $$\n",
    "   \\text{Atención Suavizada}(Q, K, V) = \\sum_{i} \\alpha_{ij} V_j\n",
    "   $$\n",
    "\n",
    "   Esta representación final es una combinación ponderada de los vectores de las palabras, donde los pesos se han asignado según la relevancia de las palabras anteriores para el token actual.\n",
    "\n",
    "#### Escalado por $\\sqrt{d_k}$\n",
    "\n",
    "El factor de **escalado** $\\sqrt{d_k}$ es crucial en los Transformers para evitar que los productos escalares (similaridades) sean demasiado grandes en dimensiones altas, lo que podría llevar a valores de softmax extremadamente pequeños (o grandes) que dificultarían la capacidad del modelo de asignar atención de manera eficiente. Este escalado estabiliza los valores de atención y asegura que los gradientes fluyan de manera adecuada durante el entrenamiento.\n",
    "\n",
    "### Soft Attention en modelos de lenguaje y LLMs\n",
    "\n",
    "El **Soft Attention** es fundamental en la forma en que los **Modelos de Lenguaje Grandes (LLMs)**, como **GPT**, **BERT**, y **T5**, procesan el texto. En particular, estos modelos dependen de la atención suavizada para modelar eficientemente **relaciones a largo plazo** entre palabras, lo que es crucial para tareas como la traducción, la generación de texto y el resumen automático.\n",
    "\n",
    "#### 1. **Transformers y Soft Attention**\n",
    "\n",
    "Los **Transformers** son la arquitectura que más ha aprovechado el mecanismo de **Soft Attention**. Los Transformers están diseñados para procesar secuencias en paralelo, en lugar de hacerlo secuencialmente como en los **RNNs**. Esto permite que los modelos basados en Transformers capturen dependencias entre palabras sin importar cuán lejos estén en la secuencia.\n",
    "\n",
    "El modelo Transformer está compuesto por múltiples capas de atención, cada una de las cuales calcula la **atención multi-cabezal** (multi-head attention), una extensión del soft attention. Cada cabeza de atención puede enfocarse en diferentes aspectos de la secuencia, lo que permite al modelo aprender relaciones complejas.\n",
    "\n",
    "##### Atención multi-cabezal\n",
    "\n",
    "La **atención multi-cabezal** divide las consultas, claves y valores en diferentes \"cabezas\" de atención, lo que permite que el modelo se enfoque en diferentes partes del contexto simultáneamente:\n",
    "\n",
    "$$\n",
    "\\text{Multi-Head Attention}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W_O\n",
    "$$\n",
    "\n",
    "Cada cabeza de atención es simplemente una aplicación independiente del mecanismo de atención, que luego se concatena para formar una representación completa del contexto.\n",
    "\n",
    "#### 2. **Soft Attention en GPT y BERT**\n",
    "\n",
    "Los modelos de lenguaje como **GPT** y **BERT** utilizan **Soft Attention** para aprender relaciones entre tokens en una secuencia de manera eficiente. Ambos modelos se basan en múltiples capas de **atención multi-cabezal**, pero tienen diferencias en la forma en que aplican la atención:\n",
    "\n",
    "- **GPT**: Utiliza una variante causal de la atención, donde el modelo solo puede atender a los tokens anteriores en la secuencia, lo que lo hace adecuado para tareas de generación de texto autoregresivo.\n",
    "- **BERT**: Utiliza atención bidireccional, donde el modelo puede atender tanto a los tokens anteriores como a los posteriores en la secuencia, lo que lo hace ideal para tareas de comprensión del lenguaje como el análisis de texto o la clasificación.\n",
    "\n",
    "#### 3. **Captura de dependencias a largo plazo**\n",
    "\n",
    "El **Soft Attention** permite que los **LLMs** capturen dependencias **a largo plazo** en el texto. Por ejemplo, en una oración larga, la palabra relevante para determinar el significado de una palabra actual puede estar muy lejos. La atención suavizada permite que el modelo asigne diferentes grados de relevancia a palabras cercanas y distantes, capturando con precisión el contexto necesario para una predicción o generación precisa.\n",
    "\n",
    "#### 4. **Preentrenamiento y ajuste Fino (Fine-tuning)**\n",
    "\n",
    "En los **LLMs**, como GPT y BERT, el mecanismo de **Soft Attention** juega un papel clave durante el **preentrenamiento** y el **ajuste fino**. Durante el preentrenamiento, el modelo aprende a asignar atención a diferentes partes del texto en una variedad de tareas sin supervisión. Luego, en el ajuste fino, el modelo puede ajustar los pesos de atención para tareas específicas como la clasificación de texto, la respuesta a preguntas o la traducción automática.\n",
    "\n",
    "#### Ventajas del Soft Attention\n",
    "\n",
    "1. **Mejora la captura del contexto**: Soft Attention permite que el modelo considere tanto las palabras cercanas como las más lejanas con diferentes niveles de importancia, lo que mejora la capacidad de capturar dependencias contextuales complejas.\n",
    "   \n",
    "2. **Procesamiento en paralelo**: A diferencia de los modelos secuenciales como los RNNs, Soft Attention en los Transformers permite el procesamiento en paralelo, lo que hace que el entrenamiento y la inferencia sean mucho más rápidos y eficientes.\n",
    "\n",
    "3. **Flexibilidad**: El Soft Attention permite ajustar el foco de atención a diferentes partes de la secuencia, lo que lo hace adecuado para tareas de lenguaje natural que requieren comprensión tanto local como global.\n",
    "\n",
    "4. **Generalización a tareas diferentes**: Al preentrenar los LLMs utilizando Soft Attention, el modelo puede ser ajustado para diversas tareas de procesamiento de lenguaje natural (NLP) sin necesidad de reentrenar desde cero, lo que mejora la eficiencia del ajuste fino.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6f4c8-e650-4651-80b1-a3348f0a99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Configuración de semillas aleatorias\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# . Preparación de los datos\n",
    "# Pares de oraciones (español, inglés)\n",
    "data = [\n",
    "    (\"hola\", \"hello\"),\n",
    "    (\"¿cómo estás?\", \"how are you?\"),\n",
    "    (\"buenos días\", \"good morning\"),\n",
    "    (\"buenas noches\", \"good night\"),\n",
    "    (\"gracias\", \"thank you\"),\n",
    "    (\"lo siento\", \"i am sorry\"),\n",
    "    (\"sí\", \"yes\"),\n",
    "    (\"no\", \"no\"),\n",
    "    (\"por favor\", \"please\"),\n",
    "    (\"hasta luego\", \"see you later\")\n",
    "]\n",
    "# Preprocesamiento y tokenización\n",
    "def tokenize(sentence: str) -> List[str]:\n",
    "    return sentence.lower().split()\n",
    "\n",
    "# Construcción de vocabularios\n",
    "def build_vocab(sentences: List[str]) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "    for sentence in sentences:\n",
    "        for token in tokenize(sentence):\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    inv_vocab = {idx: token for token, idx in vocab.items()}\n",
    "    return vocab, inv_vocab\n",
    "\n",
    "# Crear listas de oraciones en español e inglés\n",
    "sentences_es = [pair[0] for pair in data]\n",
    "sentences_en = [pair[1] for pair in data]\n",
    "\n",
    "# Construir vocabularios\n",
    "vocab_es, inv_vocab_es = build_vocab(sentences_es)\n",
    "vocab_en, inv_vocab_en = build_vocab(sentences_en)\n",
    "\n",
    "print(f\"Vocabulario Español: {vocab_es}\")\n",
    "print(f\"Vocabulario Inglés: {vocab_en}\")\n",
    "\n",
    "# Creación del Dataset y DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[str, str]], vocab_src: Dict[str, int], vocab_tgt: Dict[str, int]):\n",
    "        self.pairs = data\n",
    "        self.vocab_src = vocab_src\n",
    "        self.vocab_tgt = vocab_tgt\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence, tgt_sentence = self.pairs[idx]\n",
    "        src_tokens = [self.vocab_src.get(token, self.vocab_src['<UNK>']) for token in tokenize(src_sentence)]\n",
    "        tgt_tokens = [self.vocab_tgt['<SOS>']] + [self.vocab_tgt.get(token, self.vocab_tgt['<UNK>']) for token in tokenize(tgt_sentence)] + [self.vocab_tgt['<EOS>']]\n",
    "        return torch.tensor(src_tokens, dtype=torch.long), torch.tensor(tgt_tokens, dtype=torch.long)\n",
    "\n",
    "dataset = TranslationDataset(data, vocab_es, vocab_en)\n",
    "\n",
    "# Definición de la función de Collate para el DataLoader\n",
    "# La función collate_fn aplica padding a las secuencias para que todas tengan la misma longitud en cada batch.\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_lengths = [len(s) for s in src_batch]\n",
    "    tgt_lengths = [len(t) for t in tgt_batch]\n",
    "    \n",
    "    src_padded = nn.utils.rnn.pad_sequence(src_batch, padding_value=vocab_es['<PAD>'], batch_first=True)\n",
    "    tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=vocab_en['<PAD>'], batch_first=True)\n",
    "    \n",
    "    return src_padded, torch.tensor(src_lengths), tgt_padded, torch.tensor(tgt_lengths)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Definición del modelo con Soft Attention\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, src, src_lengths):\n",
    "        embedded = self.embedding(src)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        return outputs, hidden  # outputs: [batch_size, src_len, hid_dim], hidden: [1, batch_size, hid_dim]\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: [batch_size, hid_dim]\n",
    "        # encoder_outputs: [batch_size, src_len, hid_dim]\n",
    "        src_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Repetimos el hidden state para cada paso de la secuencia de entrada\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        # Calculamos la energía de atención\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        # Calculamos los pesos de atención\n",
    "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
    "        \n",
    "        # Aplicamos la máscara\n",
    "        attention = attention.masked_fill(mask.squeeze(1) == 0, -1e10)\n",
    "        \n",
    "        # Obtenemos la distribución de atención\n",
    "        return nn.functional.softmax(attention, dim=1)  # [batch_size, src_len]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.attention = attention\n",
    "        self.rnn = nn.GRU(hid_dim + emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        # input: [batch_size]\n",
    "        # hidden: [1, batch_size, hid_dim]\n",
    "        # encoder_outputs: [batch_size, src_len, hid_dim]\n",
    "        input = input.unsqueeze(1)  # [batch_size, 1]\n",
    "        embedded = self.embedding(input)  # [batch_size, 1, emb_dim]\n",
    "        \n",
    "        attn_weights = self.attention(hidden[-1], encoder_outputs, mask)  # [batch_size, src_len]\n",
    "        attn_weights = attn_weights.unsqueeze(1)  # [batch_size, 1, src_len]\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # [batch_size, 1, hid_dim]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, emb_dim + hid_dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        output = output.squeeze(1)  # [batch_size, hid_dim]\n",
    "        context = context.squeeze(1)  # [batch_size, hid_dim]\n",
    "        embedded = embedded.squeeze(1)  # [batch_size, emb_dim]\n",
    "        output = self.fc_out(torch.cat((output, context, embedded), dim=1))  # [batch_size, output_dim]\n",
    "        \n",
    "        return output, hidden, attn_weights.squeeze(1)  # output: [batch_size, output_dim]\n",
    "\n",
    "# Definición de la máscara de atención\n",
    "def create_mask(src, src_lengths):\n",
    "    mask = (src != vocab_es['<PAD>']).unsqueeze(1)  # [batch_size, 1, src_len]\n",
    "    return mask  # [batch_size, 1, src_len]\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, src_lengths, tgt, tgt_lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        src_lengths, tgt_lengths = src_lengths.to(device), tgt_lengths.to(device)\n",
    "        \n",
    "        encoder_outputs, hidden = model.encoder(src, src_lengths)\n",
    "        mask = create_mask(src, src_lengths).to(device)\n",
    "        \n",
    "        input = tgt[:, 0]  # Primer token (<SOS>)\n",
    "        outputs = torch.zeros(tgt.size(0), tgt.size(1) - 1, model.decoder.output_dim).to(device)\n",
    "        \n",
    "        for t in range(1, tgt.size(1)):\n",
    "            output, hidden, _ = model.decoder(input, hidden, encoder_outputs, mask)\n",
    "            outputs[:, t - 1, :] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = tgt[:, t]  # Teacher forcing\n",
    "            \n",
    "        # Ignoramos el primer token (<SOS>) en la pérdida\n",
    "        output_dim = outputs.size(-1)\n",
    "        outputs = outputs.view(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(outputs, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, src_lengths, tgt, tgt_lengths in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            src_lengths, tgt_lengths = src_lengths.to(device), tgt_lengths.to(device)\n",
    "            \n",
    "            encoder_outputs, hidden = model.encoder(src, src_lengths)\n",
    "            mask = create_mask(src, src_lengths).to(device)\n",
    "            \n",
    "            input = tgt[:, 0]  # Primer token (<SOS>)\n",
    "            outputs = torch.zeros(tgt.size(0), tgt.size(1) - 1, model.decoder.output_dim).to(device)\n",
    "            \n",
    "            for t in range(1, tgt.size(1)):\n",
    "                output, hidden, _ = model.decoder(input, hidden, encoder_outputs, mask)\n",
    "                outputs[:, t - 1, :] = output\n",
    "                top1 = output.argmax(1)\n",
    "                input = top1  # Sin teacher forcing\n",
    "                \n",
    "            output_dim = outputs.size(-1)\n",
    "            outputs = outputs.view(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(outputs, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# Configuración del modelo y entrenamiento\n",
    "# Configuración del dispositivo (CPU o GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Tamaños y parámetros\n",
    "INPUT_DIM = len(vocab_es)\n",
    "OUTPUT_DIM = len(vocab_en)\n",
    "ENC_EMB_DIM = 64\n",
    "DEC_EMB_DIM = 64\n",
    "HID_DIM = 128\n",
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "\n",
    "# Inicialización del modelo, criterio y optimizador\n",
    "attn = Attention(HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, attn)\n",
    "\n",
    "model = nn.Module()\n",
    "model.encoder = enc.to(device)\n",
    "model.decoder = dec.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_en['<PAD>'])\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, CLIP)\n",
    "    eval_loss = evaluate(model, dataloader, criterion)\n",
    "    print(f\"Epoch: {epoch+1}/{N_EPOCHS}, Pérdida de entrenamiento: {train_loss:.4f}, Pérdida de evaluación: {eval_loss:.4f}\")\n",
    "\n",
    "#Función para traducir nuevas oraciones\n",
    "def translate_sentence(sentence, model, vocab_src, vocab_tgt, inv_vocab_tgt, max_len=10):\n",
    "    model.eval()\n",
    "    tokens = tokenize(sentence)\n",
    "    src_indexes = [vocab_src.get(token, vocab_src['<UNK>']) for token in tokens]\n",
    "    src_tensor = torch.tensor([src_indexes], dtype=torch.long).to(device)\n",
    "    src_lengths = torch.tensor([len(src_indexes)]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_lengths)\n",
    "        \n",
    "    mask = create_mask(src_tensor, src_lengths).to(device)\n",
    "    input = torch.tensor([vocab_tgt['<SOS>']], dtype=torch.long).to(device)\n",
    "    outputs = []\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, _ = model.decoder(input, hidden, encoder_outputs, mask)\n",
    "        top1 = output.argmax(1)\n",
    "        if top1.item() == vocab_tgt['<EOS>']:\n",
    "            break\n",
    "        else:\n",
    "            outputs.append(inv_vocab_tgt[top1.item()])\n",
    "        input = top1\n",
    "    \n",
    "    return ' '.join(outputs)\n",
    "\n",
    "# Prueba del modelo\n",
    "\n",
    "test_sentences = [\n",
    "    \"hola\",\n",
    "    \"buenos días\",\n",
    "    \"gracias\",\n",
    "    \"hasta pronto\",\n",
    "    \"¿cómo estás?\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translation = translate_sentence(sentence, model, vocab_es, vocab_en, inv_vocab_en)\n",
    "    print(f\"Oración en español: {sentence}\")\n",
    "    print(f\"Traducción al inglés: {translation}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bfc35-5311-4073-a65c-14e5816b7297",
   "metadata": {},
   "source": [
    "### Más sobre la interpolación\n",
    "\n",
    "La **interpolación** es una técnica matemática que consiste en combinar o estimar valores intermedios basados en un conjunto de datos. En el contexto de los **modelos de lenguaje**, la interpolación nos permite combinar probabilidades estimadas a partir de n-gramas de diferentes órdenes (unigramas, bigramas, trigramas, etc.), para obtener una probabilidad general más precisa. En lugar de depender exclusivamente de un solo n-grama, la interpolación distribuye la confianza entre diferentes niveles de contexto, lo que mejora la robustez del modelo.\n",
    "\n",
    "#### El descuento en modelos de n-gramas\n",
    "\n",
    "El **descuento** es una técnica empleada para ajustar las probabilidades de n-gramas en un modelo, especialmente cuando algunas secuencias de palabras nunca han sido observadas en los datos de entrenamiento. En lugar de asignarles una probabilidad de cero, que podría ser problemático en ciertas aplicaciones, se reduce (\"descuenta\") ligeramente la probabilidad de las secuencias observadas para dejar algo de probabilidad para los n-gramas no observados.\n",
    "\n",
    "Este concepto es fundamental para modelos de n-gramas clásicos porque es frecuente encontrar combinaciones de palabras en los textos que no han sido vistas antes, especialmente cuando se trabaja con vocabularios grandes. El descuento ayuda a evitar que el modelo sea demasiado confiado en los datos vistos y permite generalizar mejor.\n",
    "\n",
    "#### Tipos de Interpolación\n",
    "\n",
    "#### 1. **Interpolación lineal simple**\n",
    "En la **interpolación lineal simple**, combinamos las probabilidades de n-gramas de diferentes órdenes (unigramas, bigramas, trigramas, etc.) mediante un promedio ponderado. Los pesos de la interpolación, representados por $\\lambda$, determinan cuánto contribuye cada n-grama a la probabilidad final.\n",
    "\n",
    "La fórmula para estimar la probabilidad del trigrama $P(w_n | w_{n-2}w_{n-1})$ en función de los n-gramas de orden inferior es la siguiente:\n",
    "\n",
    "$$\n",
    "\\hat{P}(w_n | w_{n-2}w_{n-1}) = \\lambda_1 P(w_n) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_3 P(w_n | w_{n-2}w_{n-1})\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $P(w_n)$ es la probabilidad del unigram.\n",
    "- $P(w_n | w_{n-1})$ es la probabilidad del bigrama.\n",
    "- $P(w_n | w_{n-2}w_{n-1})$ es la probabilidad del trigrama.\n",
    "- $\\lambda_1$, $\\lambda_2$, y $\\lambda_3$ son los pesos que se asignan a cada una de estas probabilidades. Estos pesos deben sumar 1, es decir, $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$.\n",
    "\n",
    "#### 2. **Interpolación condicionada**\n",
    "La interpolación condicionada es una versión más avanzada de la interpolación lineal simple. En este caso, los pesos $\\lambda$ no son constantes, sino que dependen del contexto específico. Por ejemplo, si el modelo tiene una gran confianza en un determinado bigrama basado en el contexto, puede asignar más peso a la probabilidad del bigrama en lugar de al trigrama.\n",
    "\n",
    "La fórmula para la interpolación condicionada es:\n",
    "\n",
    "$$\n",
    "\\hat{P}(w_n | w_{n-2}w_{n-1}) = \\lambda_1 (w_{n-2}w_{n-1}) P(w_n) + \\lambda_2 (w_{n-2}w_{n-1}) P(w_n | w_{n-1}) + \\lambda_3 (w_{n-2}w_{n-1}) P(w_n | w_{n-2}w_{n-1})\n",
    "$$\n",
    "\n",
    "Donde los $\\lambda_1 (w_{n-2}w_{n-1})$, $\\lambda_2 (w_{n-2}w_{n-1})$, y $\\lambda_3 (w_{n-2}w_{n-1})$ son funciones que dependen del contexto $w_{n-2} w_{n-1}$.\n",
    "\n",
    "#### ¿Cómo se establecen los pesos $\\lambda$?\n",
    "\n",
    "Los valores de $\\lambda$ no se seleccionan arbitrariamente; se aprenden a partir de un **corpus reservado**, es decir, un subconjunto de datos separado del conjunto de entrenamiento que se utiliza exclusivamente para ajustar hiperparámetros. El objetivo es encontrar los valores de $\\lambda$ que maximicen la probabilidad del corpus reservado.\n",
    "\n",
    "Un enfoque común para aprender los valores óptimos de $\\lambda$ es usar el **algoritmo de Expectation-Maximization (EM)**, un algoritmo iterativo que ajusta los pesos para maximizar la probabilidad observada en el conjunto reservado.\n",
    "\n",
    "#### Interpolación en modelos de lenguaje\n",
    "\n",
    "En los **modelos de lenguaje clásicos** basados en n-gramas, la interpolación es una técnica estándar para suavizar la probabilidad de secuencias de palabras no observadas o raramente observadas en los datos de entrenamiento. La idea es que, en lugar de depender exclusivamente de un n-grama de orden superior como un trigrama (que puede no estar presente en los datos), la interpolación permite combinar las probabilidades de unigrama, bigrama y trigrama para obtener una mejor estimación de la probabilidad de una secuencia.\n",
    "\n",
    "Este método permite mejorar la generalización del modelo, ya que incluso si no se ha observado un trigrama específico, el modelo puede recurrir a la probabilidad del bigrama o incluso del unigrama, lo que mitiga el problema de datos escasos.\n",
    "\n",
    "#### Interpolación en modelos de lenguaje neuronal (NLM)\n",
    "\n",
    "Los **modelos de lenguaje neuronal** (NLMs) son más sofisticados que los modelos basados en n-gramas porque aprenden representaciones distribuidas de palabras (embeddings) y pueden capturar dependencias de largo alcance a través de capas profundas. Sin embargo, el concepto de interpolación sigue siendo relevante. Algunas aplicaciones en modelos neuronales incluyen:\n",
    "\n",
    "1. **Interpolación de contextos**: Se puede combinar la información de palabras cercanas y lejanas dentro de una secuencia. Por ejemplo, una capa de un modelo neuronal puede enfocarse en el contexto inmediato, mientras que otra capa puede aprender representaciones basadas en palabras más alejadas en la secuencia.\n",
    "\n",
    "2. **Interpolación entre capas**: En redes neuronales profundas, las representaciones intermedias de diferentes capas pueden interpolarse para mejorar la capacidad del modelo de capturar diferentes niveles de abstracción. Esto es similar a combinar n-gramas de diferentes órdenes, pero aplicado a los niveles de procesamiento de una red neuronal.\n",
    "\n",
    "3. **Interpolación de modelos**: En algunos enfoques, se puede interpolar las salidas de varios modelos de lenguaje. Por ejemplo, se puede combinar la salida de un modelo más grande, que utiliza más contexto (como un Transformer), con la salida de un modelo más pequeño y eficiente, para lograr un equilibrio entre rendimiento y precisión.\n",
    "\n",
    "#### Interpolación en modelos de lenguaje grandes (LLM)\n",
    "\n",
    "En los **modelos de lenguaje grandes** (LLM), como GPT o BERT, la interpolación adquiere un significado más amplio:\n",
    "\n",
    "1. **Mezcla de representaciones**: Los LLMs generalmente utilizan múltiples capas de atención para generar representaciones del texto. Se puede considerar que estas capas están interpolando la información proveniente de diferentes partes del texto, combinando la atención a palabras cercanas con la atención a palabras más distantes.\n",
    "\n",
    "2. **Interpolación en entrenamientos mixtos**: En algunos casos, los LLMs son entrenados mediante la interpolación de diferentes tipos de datos o tareas. Por ejemplo, se puede interpolar entre tareas de clasificación de texto y generación de texto, lo que permite que el modelo aprenda una representación más robusta que generaliza mejor a múltiples tareas.\n",
    "\n",
    "3. **Interpolación entre arquitecturas**: Algunos LLMs modernos combinan las ventajas de diferentes arquitecturas. Por ejemplo, se puede interpolar entre un modelo Transformer estándar y una variante que emplea mecanismos como atenciones locales o espaciales para optimizar la eficiencia y la precisión.\n",
    "\n",
    "\n",
    "La **interpolación** es una técnica versátil y poderosa tanto en los modelos tradicionales de n-gramas como en los modelos más avanzados de lenguaje neuronal y los LLMs. En los primeros, la interpolación permite combinar probabilidades de diferentes órdenes para mejorar la estimación de secuencias de palabras raras o no observadas. En los modelos de lenguaje neuronal, se puede aplicar la interpolación a representaciones aprendidas en diferentes capas o contextos, mientras que en los LLMs, la interpolación permite mezclar información entre capas de atención o incluso entre diferentes arquitecturas para mejorar el rendimiento y la generalización.\n",
    "\n",
    "La interpolación, por lo tanto, no solo suaviza la salida de los modelos de lenguaje, sino que también juega un rol crítico en la capacidad de los sistemas para generalizar en un amplio rango de tareas lingüísticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e439f8-26e4-42f1-a4b0-2c379e68e89a",
   "metadata": {},
   "source": [
    "#### Aplicación del algoritmo EM en la interpolación de modelos de lenguaje\n",
    "\n",
    "El **algoritmo de Expectation-Maximization (EM)** es un método iterativo utilizado para encontrar los valores óptimos de los parámetros en modelos probabilísticos, especialmente cuando hay variables latentes o no observadas. En el caso de la interpolación en modelos de lenguaje basados en n-gramas, el EM se utiliza para ajustar los pesos de interpolación $\\lambda$ de manera que maximicen la probabilidad de un **corpus reservado**. \n",
    "\n",
    "Vamos a desglosar cómo se aplica el algoritmo EM para calcular estos pesos de interpolación en modelos de n-gramas.\n",
    "\n",
    "#### Paso 1: Definición del problema\n",
    "\n",
    "En la interpolación, estamos buscando los valores óptimos para los pesos $\\lambda_1$, $\\lambda_2$, y $\\lambda_3$ en la siguiente ecuación para un trigrama:\n",
    "\n",
    "$$\n",
    "\\hat{P}(w_n | w_{n-2}w_{n-1}) = \\lambda_1 P(w_n) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_3 P(w_n | w_{n-2}w_{n-1})\n",
    "$$\n",
    "\n",
    "El objetivo del algoritmo EM es encontrar los valores de $\\lambda_1$, $\\lambda_2$, y $\\lambda_3$ que maximicen la probabilidad del conjunto reservado de datos, es decir, el subconjunto de datos que hemos separado para evaluar el rendimiento del modelo y ajustar los parámetros.\n",
    "\n",
    "#### Paso 2: Componentes del algoritmo EM\n",
    "\n",
    "El algoritmo EM se divide en dos fases principales: **Expectation (E)** y **Maximization (M)**.\n",
    "\n",
    "1. **Fase E (Expectation)**: \n",
    "   En esta fase, calculamos la **probabilidad posterior** de que un evento (una secuencia de palabras) haya sido generado por cada uno de los n-gramas. Es decir, si tenemos un trigrama $w_{n-2}w_{n-1}w_n$, queremos calcular cuánto de la probabilidad se debe al unigrama, al bigrama y al trigrama.\n",
    "\n",
    "2. **Fase M (Maximization)**: \n",
    "   En esta fase, ajustamos los pesos $\\lambda$ para maximizar la probabilidad del corpus reservado, dados los valores actuales de las distribuciones de probabilidades obtenidos en la fase E. Esto implica encontrar los valores de $\\lambda$ que maximicen la probabilidad total.\n",
    "\n",
    "#### Paso 3: Detalle del algoritmo EM para interpolación\n",
    "\n",
    "1. **Inicialización**: \n",
    "   Se eligen valores iniciales para los pesos de interpolación $\\lambda_1$, $\\lambda_2$, y $\\lambda_3$. Una elección simple es asignar los mismos pesos a cada n-grama, es decir, $\\lambda_1 = \\lambda_2 = \\lambda_3 = \\frac{1}{3}$.\n",
    "\n",
    "2. **Fase E (Expectation)**: \n",
    "   Para cada trigrama en el corpus reservado, calculamos la probabilidad de que cada modelo (unigrama, bigrama y trigrama) haya generado la siguiente palabra. \n",
    "\n",
    "   La probabilidad que contribuye cada n-grama a la secuencia $w_{n-2}w_{n-1}w_n$ está dada por:\n",
    "\n",
    "   - Para el unigrama: $P(w_n)$\n",
    "   - Para el bigrama: $P(w_n | w_{n-1})$\n",
    "   - Para el trigrama: $P(w_n | w_{n-2}w_{n-1})$\n",
    "\n",
    "   Luego calculamos la probabilidad combinada de acuerdo con los pesos $\\lambda$ actuales:\n",
    "\n",
    "   $$\n",
    "   \\hat{P}(w_n | w_{n-2}w_{n-1}) = \\lambda_1 P(w_n) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_3 P(w_n | w_{n-2}w_{n-1})\n",
    "   $$\n",
    "\n",
    "   Usando esta probabilidad combinada, calculamos la **contribución posterior** de cada modelo de n-grama (unigrama, bigrama y trigrama) a la predicción de $w_n$. Esto se hace con la siguiente fórmula:\n",
    "\n",
    "   $$\n",
    "   \\text{Posterior del Unigrama} = \\frac{\\lambda_1 P(w_n)}{\\hat{P}(w_n | w_{n-2}w_{n-1})}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\text{Posterior del Bigrama} = \\frac{\\lambda_2 P(w_n | w_{n-1})}{\\hat{P}(w_n | w_{n-2}w_{n-1})}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\text{Posterior del Trigrama} = \\frac{\\lambda_3 P(w_n | w_{n-2}w_{n-1})}{\\hat{P}(w_n | w_{n-2}w_{n-1})}\n",
    "   $$\n",
    "\n",
    "   Este cálculo se realiza para cada n-grama en el corpus reservado.\n",
    "\n",
    "3. **Fase M (Maximization)**: \n",
    "   Una vez que tenemos las contribuciones posteriores de cada n-grama (unigrama, bigrama y trigrama) para todos los datos del corpus reservado, actualizamos los pesos $\\lambda_1$, $\\lambda_2$, y $\\lambda_3$. Los nuevos valores de $\\lambda$ son la suma de las probabilidades posteriores normalizadas:\n",
    "\n",
    "   $$\n",
    "   \\lambda_1 = \\frac{\\sum \\text{Posterior del Unigrama}}{\\text{Total de n-gramas}}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\lambda_2 = \\frac{\\sum \\text{Posterior del Bigrama}}{\\text{Total de n-gramas}}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\lambda_3 = \\frac{\\sum \\text{Posterior del Trigrama}}{\\text{Total de n-gramas}}\n",
    "   $$\n",
    "\n",
    "   De esta manera, los pesos $\\lambda$ se ajustan iterativamente para maximizar la probabilidad del corpus reservado.\n",
    "\n",
    "#### Paso 4: Convergencia\n",
    "\n",
    "El algoritmo EM repite las fases de Expectation y Maximization hasta que los pesos $\\lambda$ convergen, es decir, hasta que las actualizaciones de los pesos sean lo suficientemente pequeñas y no cambien significativamente. En ese punto, se considera que se ha alcanzado un conjunto localmente óptimo de $\\lambda$.\n",
    "\n",
    "#### Ejemplo numérico\n",
    "\n",
    "Supongamos que estamos modelando una secuencia de palabras con los siguientes n-gramas:\n",
    "\n",
    "- Unigrama: $P(w_n) = 0.05$\n",
    "- Bigrama: $P(w_n | w_{n-1}) = 0.1$\n",
    "- Trigrama: $P(w_n | w_{n-2}w_{n-1}) = 0.2$\n",
    "\n",
    "Iniciamos con $\\lambda_1 = \\lambda_2 = \\lambda_3 = \\frac{1}{3}$, por lo que la probabilidad combinada será:\n",
    "\n",
    "$$\n",
    "\\hat{P}(w_n | w_{n-2}w_{n-1}) = \\frac{1}{3} \\times 0.05 + \\frac{1}{3} \\times 0.1 + \\frac{1}{3} \\times 0.2 = 0.1167\n",
    "$$\n",
    "\n",
    "Luego, calculamos los posteriors:\n",
    "\n",
    "$$\n",
    "\\text{Posterior del Unigrama} = \\frac{\\frac{1}{3} \\times 0.05}{0.1167} \\approx 0.1429\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Posterior del Bigrama} = \\frac{\\frac{1}{3} \\times 0.1}{0.1167} \\approx 0.2857\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Posterior del Trigrama} = \\frac{\\frac{1}{3} \\times 0.2}{0.1167} \\approx 0.5714\n",
    "$$\n",
    "\n",
    "Después de sumar estas contribuciones para todos los n-gramas en el corpus reservado, actualizamos los pesos $\\lambda$ en la fase de Maximization. El proceso se repite hasta la convergencia.\n",
    "\n",
    "\n",
    "El algoritmo EM ajusta iterativamente los pesos de interpolación $\\lambda$ en modelos de n-gramas. La **Fase E** calcula las probabilidades posteriores, es decir, la proporción en la que cada modelo de n-grama contribuye a la predicción de una secuencia de palabras. Luego, en la **Fase M**, se actualizan los pesos $\\lambda$ para maximizar la probabilidad del corpus reservado. Este proceso se repite hasta que los pesos $\\lambda$ convergen, asegurando que se obtenga una interpolación óptima de las probabilidades de n-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a36cda-559c-4c5d-9c53-5327a3a03394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Preparación del corpus\n",
    "corpus = [\n",
    "    \"el gato come pescado\",\n",
    "    \"el perro come carne\",\n",
    "    \"el gato y el perro son amigos\",\n",
    "    \"el gato come y duerme\",\n",
    "    \"el perro juega y corre\",\n",
    "    \"el gato y el perro comen juntos\",\n",
    "    \"el perro duerme en la casa\",\n",
    "    \"el gato juega con el ratón\",\n",
    "    \"el perro y el gato comen carne y pescado\"\n",
    "]\n",
    "\n",
    "# Preprocesamiento y tokenización\n",
    "def preprocess_corpus(corpus: List[str]) -> List[List[str]]:\n",
    "    tokenized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        tokens = sentence.lower().split()\n",
    "        tokens = ['<s>'] + tokens + ['</s>']  # Añadimos tokens de inicio y fin de oración\n",
    "        tokenized_corpus.append(tokens)\n",
    "    return tokenized_corpus\n",
    "\n",
    "tokenized_corpus = preprocess_corpus(corpus)\n",
    "\n",
    "# Construcción del vocabulario\n",
    "def build_vocabulary(tokenized_corpus: List[List[str]]) -> Dict[str, int]:\n",
    "    vocab = {}\n",
    "    for tokens in tokenized_corpus:\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocabulary(tokenized_corpus)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Tamaño del vocabulario: {vocab_size}\")\n",
    "\n",
    "#División del corpus en conjuntos de entrenamiento y validación\n",
    "def split_corpus(tokenized_corpus: List[List[str]], validation_split: float = 0.3):\n",
    "    random.shuffle(tokenized_corpus)\n",
    "    split_index = int(len(tokenized_corpus) * (1 - validation_split))\n",
    "    train_corpus = tokenized_corpus[:split_index]\n",
    "    validation_corpus = tokenized_corpus[split_index:]\n",
    "    return train_corpus, validation_corpus\n",
    "\n",
    "train_corpus, validation_corpus = split_corpus(tokenized_corpus)\n",
    "print(f\"Oraciones de entrenamiento: {len(train_corpus)}, Oraciones de validación: {len(validation_corpus)}\")\n",
    "\n",
    "#Conteo de N-gramas en el corpus de entrenamiento\n",
    "def count_ngrams(corpus: List[List[str]], n: int) -> Dict[Tuple[str, ...], int]:\n",
    "    ngram_counts = collections.Counter()\n",
    "    for tokens in corpus:\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i + n])\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "unigram_counts = count_ngrams(train_corpus, 1)\n",
    "bigram_counts = count_ngrams(train_corpus, 2)\n",
    "trigram_counts = count_ngrams(train_corpus, 3)\n",
    "\n",
    "#  Cálculo de probabilidades máxima verosimilitud (MLE) para N-gramas\n",
    "def calculate_unigram_probabilities(unigram_counts: Dict[Tuple[str], int]) -> Dict[Tuple[str], float]:\n",
    "    total_count = sum(unigram_counts.values())\n",
    "    unigram_probs = {unigram: count / total_count for unigram, count in unigram_counts.items()}\n",
    "    return unigram_probs\n",
    "\n",
    "def calculate_bigram_probabilities(bigram_counts: Dict[Tuple[str, str], int],\n",
    "                                   unigram_counts: Dict[Tuple[str], int]) -> Dict[Tuple[str, str], float]:\n",
    "    bigram_probs = {}\n",
    "    for bigram, count in bigram_counts.items():\n",
    "        unigram = (bigram[0],)\n",
    "        bigram_probs[bigram] = count / unigram_counts[unigram]\n",
    "    return bigram_probs\n",
    "\n",
    "def calculate_trigram_probabilities(trigram_counts: Dict[Tuple[str, str, str], int],\n",
    "                                    bigram_counts: Dict[Tuple[str, str], int]) -> Dict[Tuple[str, str, str], float]:\n",
    "    trigram_probs = {}\n",
    "    for trigram, count in trigram_counts.items():\n",
    "        bigram = (trigram[0], trigram[1])\n",
    "        trigram_probs[trigram] = count / bigram_counts.get(bigram, 1)  # Evitamos división por cero\n",
    "    return trigram_probs\n",
    "\n",
    "unigram_probs = calculate_unigram_probabilities(unigram_counts)\n",
    "bigram_probs = calculate_bigram_probabilities(bigram_counts, unigram_counts)\n",
    "trigram_probs = calculate_trigram_probabilities(trigram_counts, bigram_counts)\n",
    "\n",
    "# Inicialización de los pesos de interpolación \n",
    "lambda_values = [1/3, 1/3, 1/3]  # [lambda_unigram, lambda_bigram, lambda_trigram]\n",
    "\n",
    "# Implementación del algoritmos EM\n",
    "#  Fase E: Cálculo de las probabilidades posteriores\n",
    "def expectation_step(validation_corpus: List[List[str]],\n",
    "                     unigram_probs: Dict[Tuple[str], float],\n",
    "                     bigram_probs: Dict[Tuple[str, str], float],\n",
    "                     trigram_probs: Dict[Tuple[str, str, str], float],\n",
    "                     lambda_values: List[float]) -> List[Dict[str, float]]:\n",
    "    posteriors = []\n",
    "    for tokens in validation_corpus:\n",
    "        for i in range(2, len(tokens)):\n",
    "            w1, w2, w3 = tokens[i - 2], tokens[i - 1], tokens[i]\n",
    "            # Probabilidades individuales\n",
    "            P_unigram = unigram_probs.get((w3,), 1e-6)  # Evitamos probabilidad cero\n",
    "            P_bigram = bigram_probs.get((w2, w3), 1e-6)\n",
    "            P_trigram = trigram_probs.get((w1, w2, w3), 1e-6)\n",
    "            # Probabilidad combinada\n",
    "            P_combined = (lambda_values[0] * P_unigram +\n",
    "                          lambda_values[1] * P_bigram +\n",
    "                          lambda_values[2] * P_trigram)\n",
    "            # Cálculo de los posteriors\n",
    "            posterior_unigram = (lambda_values[0] * P_unigram) / P_combined\n",
    "            posterior_bigram = (lambda_values[1] * P_bigram) / P_combined\n",
    "            posterior_trigram = (lambda_values[2] * P_trigram) / P_combined\n",
    "            posteriors.append({\n",
    "                'unigram': posterior_unigram,\n",
    "                'bigram': posterior_bigram,\n",
    "                'trigram': posterior_trigram\n",
    "            })\n",
    "    return posteriors\n",
    "\n",
    "#  Fase M: actualización de los pesos\n",
    "def maximization_step(posteriors: List[Dict[str, float]]) -> List[float]:\n",
    "    total_counts = len(posteriors)\n",
    "    sum_unigram = sum(p['unigram'] for p in posteriors)\n",
    "    sum_bigram = sum(p['bigram'] for p in posteriors)\n",
    "    sum_trigram = sum(p['trigram'] for p in posteriors)\n",
    "    lambda_unigram = sum_unigram / total_counts\n",
    "    lambda_bigram = sum_bigram / total_counts\n",
    "    lambda_trigram = sum_trigram / total_counts\n",
    "    # Normalizamos los pesos para que sumen 1\n",
    "    total_lambda = lambda_unigram + lambda_bigram + lambda_trigram\n",
    "    lambda_unigram /= total_lambda\n",
    "    lambda_bigram /= total_lambda\n",
    "    lambda_trigram /= total_lambda\n",
    "    return [lambda_unigram, lambda_bigram, lambda_trigram]\n",
    "\n",
    "# Iteración del algoritmo EM hasta la convergencia\n",
    "def em_algorithm(validation_corpus: List[List[str]],\n",
    "                 unigram_probs: Dict[Tuple[str], float],\n",
    "                 bigram_probs: Dict[Tuple[str, str], float],\n",
    "                 trigram_probs: Dict[Tuple[str, str, str], float],\n",
    "                 lambda_values: List[float],\n",
    "                 max_iterations: int = 100,\n",
    "                 epsilon: float = 1e-6) -> List[float]:\n",
    "    for iteration in range(max_iterations):\n",
    "        old_lambda_values = lambda_values.copy()\n",
    "        # Fase E\n",
    "        posteriors = expectation_step(validation_corpus, unigram_probs, bigram_probs, trigram_probs, lambda_values)\n",
    "        # Fase M\n",
    "        lambda_values = maximization_step(posteriors)\n",
    "        # Verificación de convergencia\n",
    "        deltas = [abs(old - new) for old, new in zip(old_lambda_values, lambda_values)]\n",
    "        if all(delta < epsilon for delta in deltas):\n",
    "            print(f\"Convergencia alcanzada en la iteración {iteration + 1}\")\n",
    "            break\n",
    "    return lambda_values\n",
    "\n",
    "# Ejecutamos el algoritmo EM\n",
    "optimized_lambda_values = em_algorithm(validation_corpus, unigram_probs, bigram_probs, trigram_probs, lambda_values)\n",
    "print(f\"Pesos de interpolación optimizados: {optimized_lambda_values}\")\n",
    "\n",
    "# Evaluacion:  Función para calcular la probabilidad logarítmica promedio\n",
    "def calculate_log_probability(corpus: List[List[str]],\n",
    "                              unigram_probs: Dict[Tuple[str], float],\n",
    "                              bigram_probs: Dict[Tuple[str, str], float],\n",
    "                              trigram_probs: Dict[Tuple[str, str, str], float],\n",
    "                              lambda_values: List[float]) -> float:\n",
    "    total_log_prob = 0.0\n",
    "    total_ngrams = 0\n",
    "    for tokens in corpus:\n",
    "        for i in range(2, len(tokens)):\n",
    "            total_ngrams += 1\n",
    "            w1, w2, w3 = tokens[i - 2], tokens[i - 1], tokens[i]\n",
    "            # Probabilidades individuales\n",
    "            P_unigram = unigram_probs.get((w3,), 1e-6)\n",
    "            P_bigram = bigram_probs.get((w2, w3), 1e-6)\n",
    "            P_trigram = trigram_probs.get((w1, w2, w3), 1e-6)\n",
    "            # Probabilidad combinada\n",
    "            P_combined = (lambda_values[0] * P_unigram +\n",
    "                          lambda_values[1] * P_bigram +\n",
    "                          lambda_values[2] * P_trigram)\n",
    "            total_log_prob += math.log(P_combined)\n",
    "    average_log_prob = total_log_prob / total_ngrams\n",
    "    return average_log_prob\n",
    "\n",
    "# Cálculo de la probabilidad logarítmica promedio antes y después de EM\n",
    "# Antes de EM\n",
    "initial_log_prob = calculate_log_probability(validation_corpus, unigram_probs, bigram_probs, trigram_probs, [1/3, 1/3, 1/3])\n",
    "print(f\"Probabilidad logarítmica promedio antes de EM: {initial_log_prob}\")\n",
    "\n",
    "# Después de EM\n",
    "optimized_log_prob = calculate_log_probability(validation_corpus, unigram_probs, bigram_probs, trigram_probs, optimized_lambda_values)\n",
    "print(f\"Probabilidad logarítmica promedio después de EM: {optimized_log_prob}\")\n",
    "\n",
    "# Uso del modelo para predecir la próxima palabra\n",
    "def predict_next_word(w1: str, w2: str,\n",
    "                      unigram_probs: Dict[Tuple[str], float],\n",
    "                      bigram_probs: Dict[Tuple[str, str], float],\n",
    "                      trigram_probs: Dict[Tuple[str, str, str], float],\n",
    "                      lambda_values: List[float],\n",
    "                      vocab: Dict[str, int]) -> str:\n",
    "    max_prob = 0.0\n",
    "    next_word = None\n",
    "    for word in vocab.keys():\n",
    "        P_unigram = unigram_probs.get((word,), 1e-6)\n",
    "        P_bigram = bigram_probs.get((w2, word), 1e-6)\n",
    "        P_trigram = trigram_probs.get((w1, w2, word), 1e-6)\n",
    "        P_combined = (lambda_values[0] * P_unigram +\n",
    "                      lambda_values[1] * P_bigram +\n",
    "                      lambda_values[2] * P_trigram)\n",
    "        if P_combined > max_prob:\n",
    "            max_prob = P_combined\n",
    "            next_word = word\n",
    "    return next_word\n",
    "\n",
    "# Ejemplo de predicción\n",
    "w1 = 'el'\n",
    "w2 = 'gato'\n",
    "predicted_word = predict_next_word(w1, w2, unigram_probs, bigram_probs, trigram_probs, optimized_lambda_values, vocab)\n",
    "print(f\"Dada la secuencia '{w1} {w2}', la próxima palabra predicha es: '{predicted_word}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a90aa-e64c-456e-972d-489a33542394",
   "metadata": {},
   "source": [
    "### Relación entre interpolación y Backoff\n",
    "\n",
    "Tanto la **interpolación** como el **backoff** son técnicas utilizadas en modelos de lenguaje basados en **n-gramas** para abordar el problema de la escasez de datos, es decir, para manejar situaciones en las que no se han observado suficientes ejemplos de ciertos n-gramas en el conjunto de entrenamiento. Aunque ambas técnicas tratan de resolver el mismo problema, lo hacen de maneras diferentes y tienen aplicaciones específicas en diversas implementaciones de modelos.\n",
    "\n",
    "A continuación, se explica cómo se relacionan los diferentes tipos de interpolación con el **backoff** y sus variantes, como el **stupid backoff** y el **Katz backoff**.\n",
    "\n",
    "#### Backoff\n",
    "\n",
    "En el **backoff**, si un modelo no tiene suficiente información para un n-grama de orden superior (como un trigrama), entonces retrocede o \"hace backoff\" a un n-grama de orden inferior (como un bigrama o un unigram). Este proceso continúa hasta que se encuentra suficiente evidencia para estimar una probabilidad. En esencia, el modelo confía en n-gramas de mayor orden siempre que haya suficientes datos, y retrocede a un modelo más simple si los datos de mayor orden son escasos.\n",
    "\n",
    "Por ejemplo, para calcular $P(w_n | w_{n-2}w_{n-1})$, si no hay suficientes datos para el trigrama $w_{n-2}w_{n-1}w_n$, entonces el modelo usará el bigrama $P(w_n | w_{n-1})$, y si tampoco hay suficientes datos para el bigrama, recurrirá al unigrama $P(w_n)$.\n",
    "\n",
    "El backoff es jerárquico y solo recurre a modelos de menor orden cuando los modelos de mayor orden no tienen suficientes datos. **No hay combinación de probabilidades** de diferentes niveles de n-gramas, a diferencia de la interpolación.\n",
    "\n",
    "#### Interpolación vs. Backoff\n",
    "\n",
    "- **Interpolación**: Siempre combina probabilidades de diferentes órdenes de n-gramas (unigrama, bigrama, trigrama, etc.) mediante pesos $\\lambda$. Así, incluso si hay suficientes datos para un trigrama, el modelo también tiene en cuenta los bigramas y unigramas para calcular la probabilidad final. En el ejemplo de interpolación lineal:\n",
    "\n",
    "  $$\n",
    "  \\hat{P}(w_n | w_{n-2}w_{n-1}) = \\lambda_1 P(w_n) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_3 P(w_n | w_{n-2}w_{n-1})\n",
    "  $$\n",
    "\n",
    "  Este enfoque ponderado permite que el modelo tenga en cuenta tanto el contexto local (n-gramas de orden superior) como el contexto más general (n-gramas de orden inferior).\n",
    "\n",
    "- **Backoff**: Solo retrocede a n-gramas de menor orden cuando no tiene suficientes datos para el n-grama de mayor orden. La probabilidad final no es una combinación, sino que proviene de un solo modelo (el más confiable según los datos disponibles).\n",
    "\n",
    "#### Katz Backoff\n",
    "\n",
    "El **Katz backoff** es una versión mejorada del backoff estándar que incorpora la idea de **descuento** para asignar probabilidades a n-gramas no observados. El Katz backoff sigue un enfoque jerárquico similar al backoff clásico, pero con una diferencia clave: cuando el modelo retrocede a un n-grama de menor orden, lo hace aplicando un descuento a las probabilidades de los n-gramas de mayor orden observados.\n",
    "\n",
    "La fórmula básica del Katz backoff se puede describir de la siguiente manera:\n",
    "\n",
    "- Si el n-grama $w_{n-2}w_{n-1}w_n$ tiene suficiente evidencia, se usa la probabilidad máxima-likelihood (MLE) para calcular su probabilidad.\n",
    "- Si el n-grama no tiene suficiente evidencia, el modelo retrocede al bigrama o unigrama, pero el backoff aplica un **factor de descuento** que redistribuye algo de la probabilidad de los n-gramas observados a aquellos que no han sido observados.\n",
    "\n",
    "Este enfoque equilibra el uso de n-gramas de diferentes órdenes, pero, a diferencia de la interpolación, no combina las probabilidades de varios órdenes al mismo tiempo, sino que selecciona un modelo en función de la disponibilidad de datos.\n",
    "\n",
    "#### Stupid Backoff\n",
    "\n",
    "El **stupid backoff** es una variante del backoff que se usa en grandes corpus de texto (como el utilizado por Google N-grams), y es una aproximación más simple y computacionalmente eficiente. No utiliza el descuento ni ajusta probabilidades para n-gramas no observados, y no garantiza que las probabilidades resultantes sumen exactamente 1 (por lo tanto, no es un modelo probabilístico en sentido estricto).\n",
    "\n",
    "La fórmula del **stupid backoff** es la siguiente:\n",
    "\n",
    "- Si existe el trigrama $P(w_n | w_{n-2}w_{n-1})$, se usa directamente.\n",
    "- Si no, se retrocede al bigrama $P(w_n | w_{n-1})$, y se escala por un factor fijo $\\alpha$ (por lo general, $\\alpha = 0.4$).\n",
    "- Si tampoco existe el bigrama, se retrocede al unigrama $P(w_n)$.\n",
    "\n",
    "Es decir:\n",
    "\n",
    "$$\n",
    "P(w_n | w_{n-2}w_{n-1}) = \n",
    "\\begin{cases} \n",
    "P(w_n | w_{n-2}w_{n-1}), & \\text{si el trigrama está presente} \\\\\n",
    "\\alpha \\cdot P(w_n | w_{n-1}), & \\text{si solo el bigrama está presente} \\\\\n",
    "\\alpha^2 \\cdot P(w_n), & \\text{si solo el unigrama está presente}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "El **stupid backoff** es extremadamente eficiente porque no intenta aprender pesos ni aplicar descuentos complicados como Katz backoff o interpolación. Sin embargo, es menos preciso porque no es un verdadero modelo probabilístico.\n",
    "\n",
    "#### Relación entre interpolación y Katz Backoff\n",
    "\n",
    "La interpolación y Katz backoff tienen una relación interesante:\n",
    "\n",
    "- **Interpolación**: Siempre combina las probabilidades de n-gramas de diferentes órdenes, incluso si hay suficiente información para los n-gramas de mayor orden. Los pesos $\\lambda$ se ajustan para equilibrar la contribución de cada n-grama según la cantidad de evidencia disponible.\n",
    "\n",
    "- **Katz Backoff**: Similar a la interpolación en cuanto a que maneja n-gramas de diferentes órdenes, pero solo usa el n-grama de menor orden cuando no hay suficiente evidencia para el de mayor orden. Además, aplica un descuento para redistribuir probabilidades a n-gramas no observados.\n",
    "\n",
    "Ambos métodos tratan de suavizar las probabilidades de n-gramas no observados, pero la interpolación hace una combinación ponderada, mientras que el Katz backoff sigue un enfoque jerárquico con un factor de descuento.\n",
    "\n",
    "\n",
    "\n",
    "| Método                  | Enfoque                                           | Combinación de N-gramas                          | Descuento |\n",
    "|-------------------------|---------------------------------------------------|-------------------------------------------------|-----------|\n",
    "| **Interpolación**        | Combina probabilidades de todos los n-gramas      | Siempre combina unigramas, bigramas y trigramas  | No        |\n",
    "| **Backoff clásico**      | Retrocede a un n-grama de menor orden si es necesario | Solo usa un n-grama a la vez                     | No        |\n",
    "| **Katz Backoff**         | Retrocede con descuento                           | Solo usa un n-grama a la vez, con descuento      | Sí        |\n",
    "| **Stupid Backoff**       | Retrocede con un factor de escala fijo            | Solo usa un n-grama a la vez, con factor fijo $\\alpha$ | No        |\n",
    "\n",
    "#### Interpolación en modelos neuronales y LLMs\n",
    "\n",
    "Aunque el **backoff** y sus variantes son técnicas útiles en modelos de n-gramas, la **interpolación** tiene aplicaciones más amplias en los modelos de lenguaje neurales (NLMs) y los **Modelos de Lenguaje Grandes (]LLMs)**. En estos modelos avanzados, la interpolación de representaciones se utiliza para combinar capas y representaciones de contexto a diferentes niveles de abstracción. Las probabilidades o activaciones de diferentes capas se pueden interpolar para mejorar la generalización del modelo.\n",
    "\n",
    "En resumen, mientras que el backoff es una técnica jerárquica para manejar datos escasos en modelos de n-gramas, la interpolación permite una combinación más flexible de información de diferentes órdenes, lo que es crucial tanto en modelos clásicos como en modelos más avanzados como los LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa585ab-7165-49ed-af95-1c1a17f3bfe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import collections\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Descarga de recursos necesarios de NLTK\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Preparación y preprocesamiento del corpus\n",
    "# Seleccionamos categorías específicas o utilizamos una muestra aleatoria de documentos\n",
    "categories = ['news']  # Puedes cambiar o añadir categorías\n",
    "documents = brown.sents(categories=categories)\n",
    "\n",
    "# Limitamos el número de documentos para acelerar la ejecución\n",
    "max_documents = 500  # Ajusta este número según tus necesidades\n",
    "documents = documents[:max_documents]\n",
    "\n",
    "# Preprocesamiento y tokenización\n",
    "def preprocess_documents(documents: List[List[str]]) -> List[List[str]]:\n",
    "    processed_docs = []\n",
    "    for doc in documents:\n",
    "        tokens = [word.lower() for word in doc if word.isalpha()]\n",
    "        processed_docs.append(tokens)\n",
    "    return processed_docs\n",
    "\n",
    "corpus = preprocess_documents(documents)\n",
    "\n",
    "# Dividimos el corpus en entrenamiento y prueba\n",
    "def split_corpus(corpus: List[List[str]], test_size: float = 0.2):\n",
    "    random.shuffle(corpus)\n",
    "    split_point = int(len(corpus) * (1 - test_size))\n",
    "    train_corpus = corpus[:split_point]\n",
    "    test_corpus = corpus[split_point:]\n",
    "    return train_corpus, test_corpus\n",
    "\n",
    "train_corpus, test_corpus = split_corpus(corpus)\n",
    "\n",
    "# Construcción del vocabulario\n",
    "def build_vocabulary(corpus: List[List[str]]) -> Dict[str, int]:\n",
    "    vocab = {}\n",
    "    for document in corpus:\n",
    "        for token in document:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocabulary(train_corpus)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Tamaño del vocabulario: {vocab_size}\")\n",
    "\n",
    "# Implementación de modelos N-grama\n",
    "class NGramModel:\n",
    "    def __init__(self, n: int):\n",
    "        self.n = n\n",
    "        self.ngram_counts = collections.Counter()\n",
    "        self.context_counts = collections.Counter()\n",
    "        self.vocab = set()\n",
    "        self.total_ngrams = 0\n",
    "\n",
    "    def train(self, corpus: List[List[str]]):\n",
    "        for document in corpus:\n",
    "            tokens = ['<s>'] * (self.n - 1) + document + ['</s>']\n",
    "            self.vocab.update(tokens)\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i + self.n])\n",
    "                context = tuple(tokens[i:i + self.n - 1])\n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "                self.total_ngrams += 1\n",
    "\n",
    "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
    "        count = self.ngram_counts.get(ngram, 0)\n",
    "        context = ngram[:-1]\n",
    "        context_count = self.context_counts.get(context, 0)\n",
    "        if context_count == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return count / context_count\n",
    "\n",
    "    def get_sentence_probability(self, sentence: List[str]) -> float:\n",
    "        tokens = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "        probability = 1.0\n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            ngram = tuple(tokens[i:i + self.n])\n",
    "            prob = self.get_ngram_prob(ngram)\n",
    "            if prob > 0:\n",
    "                probability *= prob\n",
    "            else:\n",
    "                # Asignamos una pequeña probabilidad para evitar cero\n",
    "                probability *= 1e-6\n",
    "        return probability\n",
    "\n",
    "# Implementación de Interpolación\n",
    "class InterpolatedNGramModel(NGramModel):\n",
    "    def __init__(self, n: int, lambdas: List[float], models: List[NGramModel]):\n",
    "        super().__init__(n)\n",
    "        self.lambdas = lambdas  # Pesos de interpolación\n",
    "        self.models = models    # Lista de modelos de diferentes órdenes\n",
    "        # Actualizamos self.vocab con la unión de los vocabularios de los modelos\n",
    "        self.vocab = set()\n",
    "        for model in self.models:\n",
    "            self.vocab.update(model.vocab)\n",
    "\n",
    "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
    "        probs = []\n",
    "        for lambda_, model in zip(self.lambdas, self.models):\n",
    "            # Ajustamos el n-grama al orden del modelo\n",
    "            ngram_adjusted = ngram[-model.n:]\n",
    "            prob = model.get_ngram_prob(ngram_adjusted)\n",
    "            probs.append(lambda_ * prob)\n",
    "        return sum(probs)\n",
    "\n",
    "# Implementación de Backoff Estándar\n",
    "class BackoffNGramModel(NGramModel):\n",
    "    def __init__(self, n: int, models: List[NGramModel]):\n",
    "        super().__init__(n)\n",
    "        self.models = models  # Lista de modelos de diferentes órdenes, ordenados de mayor a menor\n",
    "        # Actualizamos self.vocab con la unión de los vocabularios de los modelos\n",
    "        self.vocab = set()\n",
    "        for model in self.models:\n",
    "            self.vocab.update(model.vocab)\n",
    "\n",
    "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
    "        for model in self.models:\n",
    "            ngram_adjusted = ngram[-model.n:]\n",
    "            prob = model.get_ngram_prob(ngram_adjusted)\n",
    "            if prob > 0:\n",
    "                return prob\n",
    "        # Si ningún modelo tiene el n-grama, asignamos una pequeña probabilidad\n",
    "        return 1e-6\n",
    "\n",
    "# Implementación de Katz Backoff con descuento\n",
    "class KatzBackoffNGramModel(NGramModel):\n",
    "    def __init__(self, n: int, discount: float = 0.5):\n",
    "        super().__init__(n)\n",
    "        self.discount = discount  # Factor de descuento\n",
    "        self.alpha = {}  # Coeficientes de normalización\n",
    "\n",
    "    def train(self, corpus: List[List[str]]):\n",
    "        super().train(corpus)\n",
    "        # Calculamos los coeficientes de normalización alpha\n",
    "        self.calculate_alpha()\n",
    "\n",
    "    def calculate_alpha(self):\n",
    "        self.alpha = {}\n",
    "        for context in self.context_counts:\n",
    "            total_count = self.context_counts[context]\n",
    "            observed = [ngram[-1] for ngram in self.ngram_counts if ngram[:-1] == context and self.ngram_counts[ngram] > 0]\n",
    "            total_observed_prob = sum((self.ngram_counts[(context + (w,))] - self.discount) / total_count for w in observed)\n",
    "            self.alpha[context] = (1.0 - total_observed_prob) / (len(self.vocab) - len(observed))\n",
    "\n",
    "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
    "        context = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        count = self.ngram_counts.get(ngram, 0)\n",
    "        context_count = self.context_counts.get(context, 0)\n",
    "\n",
    "        if count > 0:\n",
    "            # Aplicamos el descuento\n",
    "            return (count - self.discount) / context_count\n",
    "        else:\n",
    "            # Retrocedemos al modelo de menor orden\n",
    "            if self.n > 1:\n",
    "                lower_order_model = KatzBackoffNGramModel(self.n - 1, discount=self.discount)\n",
    "                lower_order_model.ngram_counts = self.ngram_counts\n",
    "                lower_order_model.context_counts = self.context_counts\n",
    "                lower_order_model.vocab = self.vocab\n",
    "                alpha = self.alpha.get(context, 1.0)\n",
    "                lower_order_prob = lower_order_model.get_ngram_prob(ngram[1:])\n",
    "                return alpha * lower_order_prob\n",
    "            else:\n",
    "                # Caso del unigrama\n",
    "                return 1.0 / len(self.vocab)\n",
    "\n",
    "# Implementación del Stupid Backoff\n",
    "class StupidBackoffNGramModel(NGramModel):\n",
    "    def __init__(self, n: int, models: List[NGramModel], alpha: float = 0.4):\n",
    "        super().__init__(n)\n",
    "        self.models = models  # Lista de modelos de diferentes órdenes, ordenados de mayor a menor\n",
    "        self.alpha = alpha    # Factor de escala fijo\n",
    "        # Actualizamos self.vocab con la unión de los vocabularios de los modelos\n",
    "        self.vocab = set()\n",
    "        for model in self.models:\n",
    "            self.vocab.update(model.vocab)\n",
    "\n",
    "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
    "        for i, model in enumerate(self.models):\n",
    "            ngram_adjusted = ngram[-model.n:]\n",
    "            prob = model.get_ngram_prob(ngram_adjusted)\n",
    "            if prob > 0:\n",
    "                return (self.alpha ** i) * prob\n",
    "        # Si ningún modelo tiene el n-grama, asignamos una pequeña probabilidad\n",
    "        return (self.alpha ** len(self.models)) * (1.0 / len(self.vocab))\n",
    "\n",
    "# Entrenamiento de los modelos\n",
    "# Entrenamos modelos base\n",
    "unigram_model = NGramModel(n=1)\n",
    "bigram_model = NGramModel(n=2)\n",
    "trigram_model = NGramModel(n=3)\n",
    "\n",
    "unigram_model.train(train_corpus)\n",
    "bigram_model.train(train_corpus)\n",
    "trigram_model.train(train_corpus)\n",
    "\n",
    "# Modelos para interpolación\n",
    "lambdas = [0.1, 0.3, 0.6]  # Pesos de interpolación (pueden ajustarse)\n",
    "interpolated_model = InterpolatedNGramModel(n=3, lambdas=lambdas, models=[unigram_model, bigram_model, trigram_model])\n",
    "\n",
    "# Modelos para backoff estándar\n",
    "backoff_model = BackoffNGramModel(n=3, models=[trigram_model, bigram_model, unigram_model])\n",
    "\n",
    "# Modelo Katz Backoff\n",
    "katz_model = KatzBackoffNGramModel(n=3)\n",
    "katz_model.ngram_counts = trigram_model.ngram_counts\n",
    "katz_model.context_counts = trigram_model.context_counts\n",
    "katz_model.vocab = trigram_model.vocab\n",
    "katz_model.calculate_alpha()\n",
    "\n",
    "# Modelo Stupid Backoff\n",
    "stupid_backoff_model = StupidBackoffNGramModel(n=3, models=[trigram_model, bigram_model, unigram_model], alpha=0.4)\n",
    "\n",
    "# Cálculo de la perplejidad\n",
    "def calculate_perplexity(model: NGramModel, corpus: List[List[str]]) -> float:\n",
    "    total_log_prob = 0.0\n",
    "    total_tokens = 0\n",
    "    for document in corpus:\n",
    "        tokens = ['<s>'] * (model.n - 1) + document + ['</s>']\n",
    "        for i in range(len(tokens) - model.n + 1):\n",
    "            ngram = tuple(tokens[i:i + model.n])\n",
    "            prob = model.get_ngram_prob(ngram)\n",
    "            if prob > 0:\n",
    "                log_prob = -math.log2(prob)\n",
    "            else:\n",
    "                log_prob = -math.log2(1e-6)  # Asignamos una pequeña probabilidad\n",
    "            total_log_prob += log_prob\n",
    "            total_tokens += 1\n",
    "    avg_log_prob = total_log_prob / total_tokens\n",
    "    perplexity = 2 ** avg_log_prob\n",
    "    return perplexity\n",
    "\n",
    "# Evaluación de los modelos\n",
    "# Perplejidad de los modelos\n",
    "perplexity_interpolated = calculate_perplexity(interpolated_model, test_corpus)\n",
    "perplexity_backoff = calculate_perplexity(backoff_model, test_corpus)\n",
    "perplexity_katz = calculate_perplexity(katz_model, test_corpus)\n",
    "perplexity_stupid_backoff = calculate_perplexity(stupid_backoff_model, test_corpus)\n",
    "\n",
    "print(f\"Perplejidad Modelo Interpolado: {perplexity_interpolated:.4f}\")\n",
    "print(f\"Perplejidad Modelo Backoff: {perplexity_backoff:.4f}\")\n",
    "print(f\"Perplejidad Modelo Katz Backoff: {perplexity_katz:.4f}\")\n",
    "print(f\"Perplejidad Modelo Stupid Backoff: {perplexity_stupid_backoff:.4f}\")\n",
    "\n",
    "# Visualización de resultados\n",
    "models = ['Interpolación', 'Backoff', 'Katz Backoff', 'Stupid Backoff']\n",
    "perplexities = [perplexity_interpolated, perplexity_backoff, perplexity_katz, perplexity_stupid_backoff]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, perplexities, color='skyblue')\n",
    "plt.ylabel('Perplejidad')\n",
    "plt.title('Comparación de Perplejidad entre Modelos')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5123e2cd-927b-4a23-b9f3-84c0074626a6",
   "metadata": {},
   "source": [
    "¿Puedes explicar estos resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d5e56-3995-430d-90eb-085631e0b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12789c7c-663c-4a9b-808a-6ab3199eeed8",
   "metadata": {},
   "source": [
    "### Perplejidad y Entropía\n",
    "\n",
    "Introdujimos la perplejidad  como una forma de evaluar los modelos n-grama en un conjunto de prueba. Un mejor modelo n-grama es aquel que asigna una mayor probabilidad a los datos de prueba, y la perplejidad es una versión normalizada de la probabilidad del conjunto de prueba. La medida de perplejidad en realidad surge del concepto teórico de la información de la **entropía cruzada**, lo que explica propiedades misteriosas de la perplejidad (¿por qué la probabilidad inversa, por ejemplo?) y su relación con la entropía. \n",
    "\n",
    "La **entropía** es una medida de información. Dada una variable aleatoria **X** que abarca lo que estamos prediciendo (palabras, letras,categoria gramaticales), cuyo conjunto llamaremos $\\chi$, y con una función de probabilidad particular, llamémosla $p(x)$, la entropía de la variable aleatoria **X** es:\n",
    "\n",
    "$$ H(X) = - \\sum_{x \\in \\chi} p(x) \\log_2 p(x) $$\n",
    "\n",
    "El logaritmo puede, en principio, calcularse en cualquier base. Si usamos el logaritmo en base 2, el valor resultante de la entropía se medirá en bits.\n",
    "\n",
    "Una forma intuitiva de pensar en la entropía es como un límite inferior en el número de bits que se necesitarían para codificar una cierta decisión o pieza de información en el esquema de codificación óptimo. Considera un ejemplo del libro de texto estándar de teoría de la información de Cover y Thomas (1991). \n",
    "\n",
    "Imagina que queremos hacer una apuesta en una carrera de caballos, pero está demasiado lejos para ir hasta el hipódromo de Yonkers, por lo que nos gustaría enviar un mensaje corto al corredor de apuestas para decirle en qué caballo apostar. Una forma de codificar este mensaje es simplemente usar la representación binaria del número del caballo como el código; por ejemplo, el caballo 1 sería 001, el caballo 2 sería 010, el caballo 3 sería 011, y así sucesivamente, con el caballo 8 codificado como 000. Si pasamos todo el día apostando y cada caballo se codifica con 3 bits, en promedio estaríamos enviando 3 bits por carrera.\n",
    "\n",
    "¿Podemos hacerlo mejor? Supongamos que la distribución real de las apuestas está representada como la probabilidad previa de cada caballo de la siguiente manera:\n",
    "\n",
    "| Caballo | Probabilidad |\n",
    "|---------|--------------|\n",
    "| 1       | 1/2          |\n",
    "| 2       | 1/4          |\n",
    "| 3       | 1/8          |\n",
    "| 4       | 1/16         |\n",
    "| 5-8     | 1/64         |\n",
    "\n",
    "La entropía de la variable aleatoria **X** que varía sobre los caballos nos da un límite inferior en el número de bits y es:\n",
    "\n",
    "$$ H(X) = - \\sum_{i=1}^{ i=8} p(i) \\log_2 p(i) = $$\n",
    "$$= -\\frac{1}{2}\\log_2\\frac{1}{2} -\\frac{1}{4}\\log_2\\frac{1}{4}-\\frac{1}{8}\\log_2\\frac{1}{8}- \\frac{1}{16}\\log_2\\frac{1}{16} -4(\\frac{1}{64}\\log_2\\frac{1}{64})$$ \n",
    "$$ = 2 \\text{bits}$$.\n",
    "\n",
    "Un código que promedie 2 bits por carrera se puede construir con codificaciones cortas para los caballos más probables y codificaciones más largas para los caballos menos probables. Por ejemplo, podríamos codificar al caballo más probable con el código 0, y a los caballos restantes como 10, luego 110, 1110, 111100, 111101, 111110, y 111111.\n",
    "\n",
    "¿Qué pasa si los caballos son igualmente probables? Vimos anteriormente que si usáramos un código binario de longitud igual para los números de los caballos, cada caballo tomaría 3 bits para codificar, por lo que el promedio sería 3. ¿Es la entropía la misma? En este caso, cada caballo tendría una probabilidad de $1/8$. La entropía de la elección de los caballos sería entonces:\n",
    "\n",
    "$$ H(X) = - \\sum_{i=1}^{i=8} \\frac{1}{8} \\log_2 \\frac{1}{8} = -\\log_2\\frac{1}{8} = 3 \\text{bits}$$ \n",
    "\n",
    "Hasta ahora, hemos estado calculando la entropía de una sola variable. Pero la mayoría de las veces utilizamos la entropía para secuencias. Por ejemplo, para una gramática, calcularemos la entropía de alguna secuencia de palabras $W = \\{w_1, w_2, ..., w_n\\}$. Una forma de hacer esto es tener una variable que abarque secuencias de palabras. Por ejemplo, podemos calcular la entropía de una variable aleatoria que abarque todas las secuencias de palabras de longitud $n$ en algún lenguaje $L$ de la siguiente manera:\n",
    "\n",
    "$$ H(w_1, w_2, ..., w_n) = - \\sum_{w_1:n \\in L} p(w_1:n) \\log_2 p(w_1:n) $$\n",
    "\n",
    "Podríamos definir la **tasa de entropía** (también podríamos pensar en esto como la entropía por palabra) como la entropía de esta secuencia dividida por el número de palabras:\n",
    "\n",
    "$$ \\frac{1}{n} H(w_1:n) = - \\frac{1}{n} \\sum_{w_1:n \\in L} p(w_1:n) \\log p(w_1:n) $$\n",
    "\n",
    "Pero para medir la verdadera entropía de un lenguaje, necesitamos considerar secuencias de longitud infinita. Si pensamos en un lenguaje como un proceso estocástico $L$ que produce una secuencia de palabras, y permitimos que $W$ represente la secuencia de palabras $w_1, w_2, ..., w_n$, entonces la tasa de entropía de $L$ está definida como:\n",
    "\n",
    "$$ H(L) = \\lim_{n \\to \\infty} \\frac{1}{n} H(w_1:n) $$\n",
    "$$H(L) =  -\\lim_{n \\to \\infty}\\frac{1}{n} \\sum_{W \\in L} p(w_1:n) \\log p(w_1:n) $$\n",
    "\n",
    "El **teorema de Shannon-McMillan-Breiman** (Algoet y Cover 1988, Cover y Thomas 1991) establece que si el lenguaje es regular de ciertas maneras (para ser exactos, si es estacionario y ergódico),\n",
    "\n",
    "$$ H(L) = \\lim_{n \\to \\infty} - \\frac{1}{n} \\log p(w_1:n) $$\n",
    "\n",
    "Es decir, podemos tomar una única secuencia lo suficientemente larga en lugar de sumar sobre todas las posibles secuencias. La intuición del teorema de Shannon-McMillan-Breiman es que una secuencia lo suficientemente larga de palabras contendrá en sí misma muchas otras secuencias más cortas, y que cada una de estas secuencias más cortas se repetirá en la secuencia más larga de acuerdo con sus probabilidades.\n",
    "\n",
    "Un proceso estocástico se dice que es **estacionario** si las probabilidades que asigna a una secuencia son invariantes con respecto a desplazamientos en el índice de tiempo. En otras palabras, la distribución de probabilidad para las palabras en el tiempo $t$ es la misma que la distribución de probabilidad en el tiempo $t+1$. Los modelos de Markov, y por lo tanto los n-grams, son estacionarios. Por ejemplo, en un bigrama, $P_i$ depende solo de $P_{i-1}$. Entonces, si desplazamos nuestro índice de tiempo por $x$, $P_{i+x}$ sigue dependiendo de $P_{i+x-1}$. Sin embargo, el lenguaje natural no es estacionario, ya que, como mostramos en el Apéndice D, la probabilidad de palabras futuras puede depender de eventos que fueron arbitrariamente distantes y dependientes del tiempo. Por lo tanto, nuestros modelos estadísticos solo proporcionan una aproximación a las distribuciones correctas y entropías del lenguaje natural.\n",
    "\n",
    "En resumen, al hacer algunas suposiciones incorrectas pero convenientes, podemos calcular la entropía de algún proceso estocástico tomando una muestra muy larga de la salida y calculando su probabilidad logarítmica promedio.\n",
    "\n",
    "Ahora estamos listos para introducir la **entropía cruzada**. La entropía cruzada es útil cuando no conocemos la distribución de probabilidad real $p$ que generó algunos datos. Nos permite usar algún modelo $m$, que es una aproximación de $$p$$. La entropía cruzada de $m$ sobre $p$ se define por:\n",
    "\n",
    "$$ H(p,m) = \\lim_{n \\to \\infty} - \\frac{1}{n} \\sum_{W \\in L} p(w_1, ..., w_n) \\log m(w_1, ..., w_n) $$\n",
    "\n",
    "Es decir, extraemos secuencias de acuerdo con la distribución de probabilidad $p$, pero sumamos el logaritmo de sus probabilidades según $m$.\n",
    "\n",
    "Nuevamente, siguiendo el teorema de Shannon-McMillan-Breiman, para un proceso estacionario y ergódico:\n",
    "\n",
    "$$ H(p,m) = \\lim_{n \\to \\infty} - \\frac{1}{n} \\log m(w_1 w_2 ... w_n) $$\n",
    "\n",
    "Esto significa que, al igual que con la entropía, podemos estimar la entropía cruzada de un modelo $m$ sobre alguna distribución $p$ tomando una única secuencia lo suficientemente larga en lugar de sumar sobre todas las posibles secuencias.\n",
    "\n",
    "Lo que hace útil la entropía cruzada  es que **H(p,m)** es un límite superior a la entropía **H(p)**. Para cualquier modelo **m**:\n",
    "\n",
    "$$ H(p) \\leq H(p,m) $$\n",
    "\n",
    "Esto significa que podemos usar algún modelo simplificado **m** para ayudar a estimar la verdadera entropía de una secuencia de símbolos extraída de acuerdo con la probabilidad **p**. Cuanto más preciso sea **m**, más cercana estará la entropía cruzada **H(p,m)** de la verdadera entropía **H(p)**. Por lo tanto, la diferencia entre **H(p,m)** y **H(p)** es una medida de cuán preciso es un modelo. Entre dos modelos **m1** y **m2**, el modelo más preciso será aquel con la entropía cruzada más baja. (La entropía cruzada nunca puede ser menor que la verdadera entropía, por lo que un modelo no puede errar subestimando la verdadera entropía).\n",
    "\n",
    "Finalmente, estamos listos para ver la relación entre la perplejidad y la entropía cruzada. La entropía cruzada se define en el límite cuando la longitud de la secuencia de palabras observada tiende a infinito. Aproximamos esta entropía cruzada confiando en una secuencia (suficientemente larga) de longitud fija. Esta aproximación de la entropía cruzada de un modelo $$M = P(w_i | w_{i-N+1:i-1})$$ en una secuencia de palabras $W$ es:\n",
    "\n",
    "$$ H(W) = - \\frac{1}{N} \\log P(w_1 w_2 ... w_N) $$\n",
    "\n",
    "La **perplejidad** de un modelo $P$ en una secuencia de palabras $W$ ahora se define formalmente como:\n",
    "\n",
    "$$ \\text{Perplejidad}(W) = 2^{H(W)} = P(w_1 w_2 ... w_N)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P(w_1 w_2 ... w_N)}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2f3de-d722-4d62-9807-f8b03072bea3",
   "metadata": {},
   "source": [
    "\n",
    "### Contexto\n",
    "\n",
    "Imaginemos una carrera de caballos con 8 caballos, y queremos enviar un mensaje al corredor de apuestas diciéndole en qué caballo apostar. La idea es codificar este mensaje en el menor número de bits posible. Aquí es donde entra en juego el concepto de **entropía**, que nos da una medida de la cantidad de información necesaria para representar un evento aleatorio (en este caso, la elección de un caballo).\n",
    "\n",
    "La **entropía** nos dice cuál es el número mínimo promedio de bits que necesitamos para representar cada posible elección de caballo, teniendo en cuenta sus probabilidades. En este ejemplo, tenemos dos casos: \n",
    "1. **Probabilidades desiguales** (caballos con diferentes probabilidades de ganar).\n",
    "2. **Probabilidades iguales** (todos los caballos son igualmente probables).\n",
    "\n",
    "### Caso 1: Probabilidades desiguales\n",
    "\n",
    "Se nos da la probabilidad previa de que cada caballo gane la carrera:\n",
    "\n",
    "| Caballo | Probabilidad |\n",
    "|---------|--------------|\n",
    "| 1       | 1/2          |\n",
    "| 2       | 1/4          |\n",
    "| 3       | 1/8          |\n",
    "| 4       | 1/16         |\n",
    "| 5-8     | 1/64         |\n",
    "\n",
    "Esto significa que el caballo 1 es el más probable de ganar, con una probabilidad de 1/2, y los caballos 5 al 8 tienen una probabilidad mucho menor, de 1/64 cada uno.\n",
    "\n",
    "La **entropía** de esta distribución de probabilidades se calcula con la fórmula:\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{i=1}^{8} p(i) \\log_2 p(i)\n",
    "$$\n",
    "\n",
    "Para cada caballo, tomamos su probabilidad $p(i)$, calculamos el logaritmo en base 2 de esa probabilidad, y luego lo multiplicamos por la probabilidad $p(i)$. Sumamos estos términos para todos los caballos. En este caso, los cálculos se ven así:\n",
    "\n",
    "$$\n",
    "H(X) = - \\left( \\frac{1}{2} \\log_2 \\frac{1}{2} + \\frac{1}{4} \\log_2 \\frac{1}{4} + \\frac{1}{8} \\log_2 \\frac{1}{8} + \\frac{1}{16} \\log_2 \\frac{1}{16} + 4 \\cdot \\frac{1}{64} \\log_2 \\frac{1}{64} \\right)\n",
    "$$\n",
    "\n",
    "Haciendo los cálculos:\n",
    "\n",
    "- Para el caballo 1: $$\\frac{1}{2} \\log_2 \\frac{1}{2} = \\frac{1}{2} \\cdot (-1) = -\\frac{1}{2}$$\n",
    "- Para el caballo 2: $$\\frac{1}{4} \\log_2 \\frac{1}{4} = \\frac{1}{4} \\cdot (-2) = -\\frac{2}{4} = -\\frac{1}{2}$$\n",
    "- Para el caballo 3: $$\\frac{1}{8} \\log_2 \\frac{1}{8} = \\frac{1}{8} \\cdot (-3) = -\\frac{3}{8}$$\n",
    "- Para el caballo 4: $$\\frac{1}{16} \\log_2 \\frac{1}{16} = \\frac{1}{16} \\cdot (-4) = -\\frac{4}{16} = -\\frac{1}{4}$$\n",
    "- Para los caballos 5 al 8 (probabilidad $$\\frac{1}{64}$$): cada uno contribuye con:\n",
    "  $$\\frac{1}{64} \\log_2 \\frac{1}{64} = \\frac{1}{64} \\cdot (-6) = -\\frac{6}{64} = -\\frac{3}{32}$$\n",
    "  Como son 4 caballos, sumamos esto 4 veces:\n",
    "  $$4 \\cdot -\\frac{3}{32} = -\\frac{12}{32} = -\\frac{3}{8}$$\n",
    "\n",
    "Sumando todas estas contribuciones, tenemos:\n",
    "\n",
    "$$\n",
    "H(X) = -\\left( \\frac{1}{2} + \\frac{1}{2} + \\frac{3}{8} + \\frac{1}{4} + \\frac{3}{8} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(X) = -\\left( 2 \\text{ bits} \\right)\n",
    "$$\n",
    "\n",
    "Por lo tanto, la entropía total es **2 bits**.\n",
    "\n",
    "### Interpretación\n",
    "\n",
    "¿Qué significa esto? La entropía de 2 bits nos dice que, en promedio, necesitamos **2 bits** para codificar la información sobre cuál de los 8 caballos ha ganado, si usamos un esquema de codificación **óptimo** (donde los caballos más probables tienen codificaciones más cortas).\n",
    "\n",
    "Por ejemplo, podríamos asignar códigos como:\n",
    "\n",
    "- Caballo 1 (más probable): 0\n",
    "- Caballo 2: 10\n",
    "- Caballo 3: 110\n",
    "- Caballo 4: 1110\n",
    "- Caballos 5-8: 111100, 111101, 111110, 111111\n",
    "\n",
    "Esto muestra que el caballo más probable tiene el código más corto (1 bit) y los caballos menos probables tienen códigos más largos (hasta 6 bits).\n",
    "\n",
    "### Caso 2: Probabilidades iguales\n",
    "\n",
    "Si los 8 caballos fueran igualmente probables, cada caballo tendría una probabilidad de $\\frac{1}{8}$. En este caso, la entropía sería:\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{i=1}^{8} \\frac{1}{8} \\log_2 \\frac{1}{8} = - \\log_2 \\frac{1}{8}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(X) = - \\log_2 \\frac{1}{8} = 3 \\text{ bits}\n",
    "$$\n",
    "\n",
    "Cuando las probabilidades son iguales, la entropía es mayor, **3 bits**. Esto se debe a que no podemos aprovechar las diferencias en probabilidad para usar códigos más cortos. En este caso, la codificación binaria directa es óptima, y cada caballo tendría un código de **3 bits** (por ejemplo, 000, 001, 010, ..., 111).\n",
    "\n",
    "\n",
    "\n",
    "La entropía nos da una medida de la cantidad mínima promedio de información (en bits) que necesitamos para describir un evento aleatorio. En el caso de probabilidades desiguales, la entropía es más baja (2 bits), lo que refleja que podemos aprovechar las diferencias en probabilidad para codificar la información de manera más eficiente. Cuando las probabilidades son iguales, necesitamos más bits (3 bits) para codificar la información, ya que no hay caballos más probables que otros a los que podamos asignar códigos más cortos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75663d7-fa88-499c-bf77-121b722fc34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import collections\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#  Preparación y preprocesamiento del corpus\n",
    "# Obtenemos los archivos del corpus Reuters\n",
    "file_ids = reuters.fileids()\n",
    "\n",
    "# Dividimos el corpus en entrenamiento y prueba\n",
    "train_ids = [fid for fid in file_ids if fid.startswith('training/')]\n",
    "test_ids = [fid for fid in file_ids if fid.startswith('test/')]\n",
    "\n",
    "# Cargamos los documentos y los tokenizamos\n",
    "def load_documents(file_ids: List[str]) -> List[List[str]]:\n",
    "    documents = []\n",
    "    for fid in file_ids:\n",
    "        text = reuters.raw(fid)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        # Filtramos tokens no alfabéticos\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "train_corpus = load_documents(train_ids)\n",
    "test_corpus = load_documents(test_ids)\n",
    "\n",
    "\n",
    "#  Construcción del vocabulario\n",
    "def build_vocabulary(corpus: List[List[str]]) -> Dict[str, int]:\n",
    "    vocab = {}\n",
    "    for document in corpus:\n",
    "        for token in document:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocabulary(train_corpus)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Tamaño del vocabulario: {vocab_size}\")\n",
    "\n",
    "# Implementación de modelos N-grama\n",
    "class NGramModel:\n",
    "    def __init__(self, n: int):\n",
    "        self.n = n\n",
    "        self.ngram_counts = collections.Counter()\n",
    "        self.context_counts = collections.Counter()\n",
    "        self.vocab = set()\n",
    "        self.total_ngrams = 0\n",
    "\n",
    "    def train(self, corpus: List[List[str]]):\n",
    "        for document in corpus:\n",
    "            tokens = ['<s>'] * (self.n - 1) + document + ['</s>']\n",
    "            self.vocab.update(tokens)\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i + self.n])\n",
    "                context = tuple(tokens[i:i + self.n - 1])\n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "                self.total_ngrams += 1\n",
    "\n",
    "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
    "        count = self.ngram_counts.get(ngram, 0)\n",
    "        context = ngram[:-1]\n",
    "        context_count = self.context_counts.get(context, 0)\n",
    "        if context_count == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return count / context_count\n",
    "\n",
    "    def get_sentence_probability(self, sentence: List[str]) -> float:\n",
    "        tokens = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "        probability = 1.0\n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            ngram = tuple(tokens[i:i + self.n])\n",
    "            prob = self.get_ngram_prob(ngram)\n",
    "            if prob > 0:\n",
    "                probability *= prob\n",
    "            else:\n",
    "                # Asignamos una pequeña probabilidad para evitar cero\n",
    "                probability *= 1e-6\n",
    "        return probability\n",
    "\n",
    "#  Entrenamiento de modelos unigrama, bigrama y trigrama\n",
    "unigram_model = NGramModel(n=1)\n",
    "bigram_model = NGramModel(n=2)\n",
    "trigram_model = NGramModel(n=3)\n",
    "\n",
    "unigram_model.train(train_corpus)\n",
    "bigram_model.train(train_corpus)\n",
    "trigram_model.train(train_corpus)\n",
    "\n",
    "#Cálculo de entropía\n",
    "def calculate_entropy(model: NGramModel, corpus: List[List[str]]) -> float:\n",
    "    total_entropy = 0.0\n",
    "    total_tokens = 0\n",
    "    for document in corpus:\n",
    "        tokens = ['<s>'] * (model.n - 1) + document + ['</s>']\n",
    "        for i in range(len(tokens) - model.n + 1):\n",
    "            ngram = tuple(tokens[i:i + model.n])\n",
    "            prob = model.get_ngram_prob(ngram)\n",
    "            if prob > 0:\n",
    "                entropy = -math.log2(prob)\n",
    "            else:\n",
    "                entropy = -math.log2(1e-6)  # Asignamos una pequeña probabilidad\n",
    "            total_entropy += entropy\n",
    "            total_tokens += 1\n",
    "    average_entropy = total_entropy / total_tokens\n",
    "    return average_entropy\n",
    "\n",
    "# Calculamos la entropía para cada modelo\n",
    "entropy_unigram = calculate_entropy(unigram_model, test_corpus)\n",
    "entropy_bigram = calculate_entropy(bigram_model, test_corpus)\n",
    "entropy_trigram = calculate_entropy(trigram_model, test_corpus)\n",
    "\n",
    "print(f\"Entropía Unigrama: {entropy_unigram:.4f} bits\")\n",
    "print(f\"Entropía Bigrama: {entropy_bigram:.4f} bits\")\n",
    "print(f\"Entropía Trigrama: {entropy_trigram:.4f} bits\")\n",
    "\n",
    "# Cálculo de Entropía Cruzada\n",
    "def calculate_cross_entropy(model_p: NGramModel, model_q: NGramModel, corpus: List[List[str]]) -> float:\n",
    "    total_cross_entropy = 0.0\n",
    "    total_tokens = 0\n",
    "    for document in corpus:\n",
    "        tokens = ['<s>'] * (model_p.n - 1) + document + ['</s>']\n",
    "        for i in range(len(tokens) - model_p.n + 1):\n",
    "            ngram = tuple(tokens[i:i + model_p.n])\n",
    "            prob_p = model_p.get_ngram_prob(ngram)\n",
    "            prob_q = model_q.get_ngram_prob(ngram)\n",
    "            if prob_q > 0:\n",
    "                cross_entropy = -prob_p * math.log2(prob_q)\n",
    "            else:\n",
    "                cross_entropy = -prob_p * math.log2(1e-6)\n",
    "            total_cross_entropy += cross_entropy\n",
    "            total_tokens += 1\n",
    "    average_cross_entropy = total_cross_entropy / total_tokens\n",
    "    return average_cross_entropy\n",
    "\n",
    "# Calculamos la entropía cruzada entre modelos\n",
    "cross_entropy_pq = calculate_cross_entropy(bigram_model, trigram_model, test_corpus)\n",
    "print(f\"Entropía Cruzada entre Bigrama y Trigrama: {cross_entropy_pq:.4f} bits\")\n",
    "\n",
    "# Cálculo de perplejidad\n",
    "def calculate_perplexity(model: NGramModel, corpus: List[List[str]]) -> float:\n",
    "    total_log_prob = 0.0\n",
    "    total_tokens = 0\n",
    "    for document in corpus:\n",
    "        tokens = ['<s>'] * (model.n - 1) + document + ['</s>']\n",
    "        for i in range(len(tokens) - model.n + 1):\n",
    "            ngram = tuple(tokens[i:i + model.n])\n",
    "            prob = model.get_ngram_prob(ngram)\n",
    "            if prob > 0:\n",
    "                log_prob = -math.log2(prob)\n",
    "            else:\n",
    "                log_prob = -math.log2(1e-6)\n",
    "            total_log_prob += log_prob\n",
    "            total_tokens += 1\n",
    "    avg_log_prob = total_log_prob / total_tokens\n",
    "    perplexity = 2 ** avg_log_prob\n",
    "    return perplexity\n",
    "\n",
    "# Calculamos la perplejidad para cada modelo\n",
    "perplexity_unigram = calculate_perplexity(unigram_model, test_corpus)\n",
    "perplexity_bigram = calculate_perplexity(bigram_model, test_corpus)\n",
    "perplexity_trigram = calculate_perplexity(trigram_model, test_corpus)\n",
    "\n",
    "print(f\"Perplejidad Unigrama: {perplexity_unigram:.4f}\")\n",
    "print(f\"Perplejidad Bigrama: {perplexity_bigram:.4f}\")\n",
    "print(f\"Perplejidad Trigrama: {perplexity_trigram:.4f}\")\n",
    "\n",
    "# Implementación del Add-One Laplace Smoothing\n",
    "class SmoothedNGramModel(NGramModel):\n",
    "    def __init__(self, n: int, vocab_size: int):\n",
    "        super().__init__(n)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
    "        count = self.ngram_counts.get(ngram, 0) + 1  # Añadimos uno al conteo\n",
    "        context = ngram[:-1]\n",
    "        context_count = self.context_counts.get(context, 0) + self.vocab_size  # Añadimos vocab_size al contexto\n",
    "        return count / context_count\n",
    "\n",
    "# Entrenamos modelos suavizados\n",
    "smoothed_unigram_model = SmoothedNGramModel(n=1, vocab_size=vocab_size)\n",
    "smoothed_bigram_model = SmoothedNGramModel(n=2, vocab_size=vocab_size)\n",
    "smoothed_trigram_model = SmoothedNGramModel(n=3, vocab_size=vocab_size)\n",
    "\n",
    "smoothed_unigram_model.train(train_corpus)\n",
    "smoothed_bigram_model.train(train_corpus)\n",
    "smoothed_trigram_model.train(train_corpus)\n",
    "\n",
    "# Calculamos la perplejidad para los modelos suavizados\n",
    "perplexity_smoothed_unigram = calculate_perplexity(smoothed_unigram_model, test_corpus)\n",
    "perplexity_smoothed_bigram = calculate_perplexity(smoothed_bigram_model, test_corpus)\n",
    "perplexity_smoothed_trigram = calculate_perplexity(smoothed_trigram_model, test_corpus)\n",
    "\n",
    "print(f\"Perplejidad Unigrama Suavizado: {perplexity_smoothed_unigram:.4f}\")\n",
    "print(f\"Perplejidad Bigrama Suavizado: {perplexity_smoothed_bigram:.4f}\")\n",
    "print(f\"Perplejidad Trigrama Suavizado: {perplexity_smoothed_trigram:.4f}\")\n",
    "\n",
    "# Visualizacion de resultados\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['Unigrama', 'Bigrama', 'Trigrama']\n",
    "perplexities = [perplexity_unigram, perplexity_bigram, perplexity_trigram]\n",
    "smoothed_perplexities = [perplexity_smoothed_unigram, perplexity_smoothed_bigram, perplexity_smoothed_trigram]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, perplexities, width, label='Sin Suavizado')\n",
    "rects2 = ax.bar(x + width/2, smoothed_perplexities, width, label='Suavizado Laplace')\n",
    "\n",
    "ax.set_ylabel('Perplejidad')\n",
    "ax.set_title('Perplejidad por Modelo y Suavizado')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d62f3-df3d-4d19-a599-b4281311e780",
   "metadata": {},
   "source": [
    "¿Puedes explicar la salida de los resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a0d9d-e0bd-469b-af2d-4418bdb62c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
